{"searchDocs":[{"title":"Linux下 pipe介紹","type":0,"sectionRef":"#","url":"/2013/03/29/c-pipe","content":"#[User] 不論是bash,tcsh,又或者是windows的cmd，都有一種叫做PIPE的功能 能夠將兩個獨立的程式給串接起來，把前面程式的輸出當作下一個程式的輸入 擁有這個指令，就能將本來當一功能的程式給組合起來變成複雜的工具了 舉例來說，我想要知道我當前目錄下有多少個檔案 就可以使用ls跟wc兩個指令合作完成， 使用 ls | wc 就會將ls的結果(檔案列表)當作輸入傳給wc這隻程式，然後就可以輕鬆地算出當前目錄的檔案數量 或者是有時候想要搜尋某些特定的字串，都會使用grep這個指令，譬如想要搜尋某個特定使用者正在執行的所有程序 ps auxww | grep username 所以pipe對於系統管理來說，是個非常重要的概念，能夠將每個獨立細小的程式給串接起來完成複雜的工作。 #[程式設計] 在FreeBSD(linux)上，shell能夠辦得到這樣的功能，實際上是利用了kernel中pipe的功能，這邊就已linux kernel 3.5.4為架構。 在程式中，pipe的概念就是一個水管，這個水管有兩個端口，一端負責寫資料到pipe，一端負責將資料從pipe中讀出來，所以我們可以做個簡單的測試。 int main(){ int rand1,rand2; int fd[2];// declare a two-d array, store file_descriptor of the pipe (two side) // fd[0] mease read side, fd[1] means write side pid_t pid;//child process的pid pipe(fd); //call system call (pipe) to create a pipe //use fork to create a child process //child process will wrtie data to pipe, and parent will read data from pipe //child process if((pid=fork())==0){ srand(getpid()); close(fd[READ_END]);//child won't use read size, so close it rand1=rand()%RANGE; //create random number write(fd[WRITE_END],&amp;rand1,sizeof(rand1)); //write to pipe close(fd[WRITE_END]);//close write side when write over printf(&quot;%d has been created In Child Process \\n&quot;,rand1); exit(1); } else if(pid&gt;0){ srand(getpid()); close(fd[WRITE_END]);//parent won't use write size, so close it。 rand2=rand()%RANGE;//create random number read(fd[READ_END],&amp;rand1,sizeof(rand1));//read the data from pipe printf(&quot;%d has been created In Parent Process \\n&quot;,rand2); wait(); printf(&quot;Parent Process calulate sum is :%d \\n&quot;,rand1+rand2); close(fd[READ_END]);//close read side exit(1); } return 0; } 執行結果: 8 has been created In Child Process 5 has been created In Parent Process Parent Process calulate sum is :13 3 has been created In Child Process 3 has been created In Parent Process Parent Process calulate sum is :6 實際上，如果想要對同個端口去進行寫跟讀的動作，是行不通的，乍看之下會覺得PIPE只是一個 buffer，放置資料而已，實際上在kernel中，pipe被視為是一個file，當我們呼叫pipe時，真正最後會 呼叫到do_pipe這個function，在這個function中，會針對pipe的兩個端口分別去設定 O_RDONLY;O_WRONLY的標籤，這樣的設定使得pipe的端口就真的一邊只能讀，一邊只能寫。 有空在來講述一下file_descriptor file file_operation三者的關係，以及到底 file,socket,pipe...等這些device到底在kernel中如何運作。","keywords":"","version":null},{"title":"c++ explicit修飾","type":0,"sectionRef":"#","url":"/2013/03/29/cpp","content":"C++中，有所謂的顯性轉換跟隱性轉換 在宣告時，加入explicit 這個關鍵字，可以禁止使用顯性轉換，以下為例 using namespace std; class Stack { public: Stack(int a){}; }; void Test(Stack b){ } int main() { Stack s1 = 1; //ok Stack s2 = Stack(12); //ok Stack s3(s1); //ok Test(123); //ok return 0; } using namespace std; class Stack { public: explicit Stack(int){}; }; void Test(Stack b){ } int main() { Stack s1 = 123; //error Stack s2 = Stack(123); //ok Stack s3(s1); //ok Test(123); // error return 0; } ","keywords":"","version":null},{"title":"Matlab 簡單練習","type":0,"sectionRef":"#","url":"/2013/03/29/matlab-1","content":"應朋友的要求，用matlab幫忙寫了一個簡單的腳本 需要能夠ˋ彈出對話框選擇一個資料夾，讀取資料夾底下的影像檔，然後與某個特定的影像檔做相減，並命名輸出 這部分用到了一些指令，在這邊紀錄下來 %choose directory target_path = uigetdir(); file_path = [target_path '/C00*.tif']; background = [target_path '/REF.tif']; file_struct = dir(file_path); back_struct = dir(background); %load background image back = imread([target_path '/' back_struct.name]); for i=1:length(file_struct) temp_image = imread([target_path '/' file_struct(i).name]); result_image = imsubtract(temp_image,back); imwrite(result_image,[target_path '/new' file_struct(i).name]); end ====END======= 首先使用到了uigetdir,與其類似的還有uigetfile 呼叫此函數後，會彈出directoryOpenDialog的介面，選擇完畢後，會把選擇的路徑回傳 接下來我想要移動到該路徑，於是希望透過 cd 這個指令，無奈 cd這個指令沒有辦法吃參數，只能吃完整路徑，所以就必須要改換成其他的方法 由於我已經知道圖檔的命名規則，於是先用 [] 的方式，把字串給連接起來，這邊使用regular的方式，之後再搜尋檔案的時候會更方便 接者使用dir這個指令，就可以得到我想要的所有檔案，dir回傳的是一個struct，內容包含了檔案的 name -- 檔案名稱 date -- 修改日期 bytes -- 檔案大小 isdir -- 是否為資料夾 datenum -- Matlab特定的修改日期 這邊我只需要它的名稱，於是透過一個迴圈，把所有的路徑檔案都以圖片的方式(imread)給讀取近來 在與事先讀取好的背景圖片(back)使用imsubtract做相減，得到新的圖片，再透過imwrite給寫出檔案","keywords":"","version":null},{"title":"檢查port使用情況","type":0,"sectionRef":"#","url":"/2013/03/29/port-check","content":"有時候根據應用需求，會需要針對去檢查目前系統上有哪些port正在被使用 #[FreeBSD] 可以使用 sockstat 這個command 來檢查系統上port的使用。 USER COMMAND PID FD PROTO LOCAL ADDRESS FOREIGN ADDRESS root cron 93468 4 udp4 :638 :* 在預設的情況下，會輸出 使用者名稱，執行的程序，該程序的pid，在該程序中使用該port的file descriptor是多少 使用何種協定，以及address 如果使用 sockstat -4lP tcp 就可以找出 使用tcp &amp; ipv4 ，並且正在listen的port 這對於要尋找是否有人在寫Socket programming來說是很方便的。 詳細的可以man sockstat #[Linux]可以使用 netstat 這個工具來檢視，搭配一些參數還可以看到該 port 被那些 process 使用 netstat -anptn tcp 1 0 127.0.0.1:40147 127.0.0.1:36524 CLOSE_WAIT 7147/vim tcp 1 0 127.0.0.1:58289 127.0.0.1:52849 CLOSE_WAIT 19421/vi ... #[Windows] 可以使用netstat來檢視，netstat能夠顯示的資訊非常的多，為了精簡我們的需求，必須去過濾這些資訊 在windows上使用find這個指令，類似於UNIX中grep的功能 舉例來說，netstat -an |find /i “listening&quot; 這個指令 netstat -an 會顯示所有連線以及正在監聽的port，並且以數字的形式來顯示IP以及PORT find /i “listening&quot; 則會以不區分的方式去搜尋每一行，若包含listening則將該行印出 EX: TCP 192.168.1.116:139 0.0.0.0:0 LISTENING TCP 192.168.1.116:49156 216.52.233.65:12975 ESTABLISHED ref:www.microsoft.com/resources/","keywords":"","version":null},{"title":"Windows VPN","type":0,"sectionRef":"#","url":"/2013/03/29/windows-vpn","content":"最近因為某個教授的要求，希望windows開機就可以自動vpn連線，所以這部份花了一些時間去研究，雖然我認為每次開機自己動手點兩下好像也沒有多困難阿~冏 這個概念其實不難，寫一個可以連線的batch file,每次開機的時候，自動去執行該batch file，就可以達到連線的功能了。 在網際網路那邊手動增加一個VPN連線，假設該VPN連線名稱為 vpn_connection。 寫一個batch file,內容增加一行 rasdial &quot;my_vpn_connection&quot; &quot;myname&quot; &quot;mypasswd&quot; 這時候可以手動執行看看，看會不會連線成功，如果連線不會成功，就根據錯誤代碼去解決。 執行taskschd.msc 這個排班程式，把該batch file加入至開機執行，並且在網路連線成功後才執行。 重開機測試!","keywords":"","version":null},{"title":"MSN LOG解析以C#","type":0,"sectionRef":"#","url":"/2013/04/21/msnlogparse","content":"Msn的log採用的格式是XML，隨便打開一個LOG後仔細檢視，可以發現msn log的訊息格式大概是採這樣 &lt;Message Date=&quot;2012/3/23&quot; Time=&quot;下午 11:33:12&quot; DateTime=&quot;2012-03-23T15:33:12.790Z&quot; SessionID=&quot;1&quot;&gt; &lt;From&gt;&lt;User FriendlyName=&quot;邱 渣&quot;/&gt;&lt;/From&gt; &lt;To&gt;&lt;User FriendlyName=&quot;XXX&quot;/&gt;&lt;/To&gt; &lt;Text&gt;明天會到否&lt;/Text&gt; &lt;/Message&gt; 每一則訊息，本身的屬性會包含該訊息的發送時間 ，有兩種格式，後面的790Z就不清楚是什麼意思了，SessionID這個屬性 也不是很清楚，但是這些都不重要 利用Date跟Time就可以取得基本時間了。 接者可以看到底下有三個屬性，代表訊息發送者，訊息接收者，以及發送的訊息為何 如果有啟動顏色跟字型的話，TEXT欄位就會變成下列樣子，會有屬性標示其顏色與字型 &lt;Text Style=&quot;font-family:Microsoft JhengHei; color:#000000; &quot;&gt; test &lt;/Text&gt; 在C#中，我這次使用XmlElement來做為解析XML的工具，載入檔案後，因為我們只關心訊息的傳送， 所以先利用GetElementsByTagName(&quot;message&quot;)來取得所有Message有關的nodes 接者針對這個結點內的所有資料去進行資料抓取，我們的目標有 時間、發送者、傳送文字 先將XmlNode轉型為XmlElement的類型，這樣方便處理，然後利用GetAttribute來取得Message的屬性 我們就可以知道每個對話的Date跟Time。接者要存取其child(From,To,Text)這些的值 這邊比較要注意的是這兩種的差別 &lt;From&gt;&quot;邱渣&quot;&lt;/From&gt; &lt;From&gt;&lt;User FriendlyName=&quot;邱渣&quot;/&gt;&lt;/From&gt; 以Type1來說，邱渣是From這個結點的值，可以利用childList[0].value 取得發送者的名稱 但是對Type2來說，邱渣是From這個結點底下的一個結點中的屬性，所以就要利用childList[0].FirstChild 的方式來取得&lt;User&gt;這個結點，再搭配Attributes[0].Value來取得第一個屬性的值，如此才可以取得&quot;邱渣&quot;的值 所以利用childList[2].FirstChild.Attributeds[0].Value就可以取得文字訊息了! 另外，如果要取得文字的顏色跟字型的話，利用 childList[2].GetAttribute(&quot;Style&quot;) 接者在去自己處理字串來取得字型跟顏色。 範例code如下 xml = new XmlDocument(); xml.Load(filename); XmlNodeList nodeList = xml.GetElementsByTagName(&quot;Message&quot;); foreach (XmlNode parentNode in nodeList) { XmlElement element = (XmlElement)parentNode; string Date = element.GetAttribute(&quot;Date&quot;); string Time = element.GetAttribute(&quot;Time&quot;); XmlNodeList childList = element.ChildNodes; data += childList[0].FirstChild.Attributes[0].Value + &quot; 說 (&quot; + Time + &quot;)\\r\\n&quot;; } ","keywords":"","version":null},{"title":"Python-translate","type":0,"sectionRef":"#","url":"/2013/06/13/python-translate","content":"Python中有個很強大的字串轉換工具 maketrans 跟 translate str.translate(table[, deletechars]); Parameters table -- You can use the maketrans() helper function in the string module to create a translation table. deletechars -- The list of characters to be removed from the source string. 字串中只要有符合deletechars中的字元都會被刪除，然後剩下的字元就會依照table裡面的mapping來做轉換。 這個mapping的就要利用string.maketrans()來幫忙產生囉, str.maketrans(intab, outtab]); Parameters intab -- This is the string having actual characters. outtab -- This is the string having corresponding mapping character. intab跟outtab兩者的長度必須要一樣，會把intab中每一個字元與outtab中相同位置的字元做mapping。 舉例來說 intab = &quot;aeiou&quot; outtab = &quot;12345&quot; trantab = maketrans(intab, outtab) 就會產生一個mapping,把aeiou分別轉換成12345。 input=&quot;abcdefgh&quot; input = input.translate(trantab) input就會變成 &quot;1bcd2fgh&quot; 那如果改成 input=&quot;abcdefgh&quot; input = input.translate(trantab,&quot;fgh&quot;) input就會變成 &quot;1bcd2&quot; 再來個簡單範例，希望能夠把所有的小寫轉成大寫，並把非英文字母外的所有字元都給刪除掉。 import string #取得所有英文大小寫的集合 lower = ''.join(map(chr,range(97,123))) upper = lower.upper() #創立一個對照表，可以把所有小寫轉成大寫 ltu = string.maketrans(lower,upper) #接下來要利用捕集的方式取得非英文字母以外的所有字元，因此就用所有字元-英文字母 #創立一個代表所有字元的字元表 allchars = string.maketrans('','') #利用translate的方式，取得所有非英文字母的集合 delete = allchars.translate(allchars,lower+upper) #定義一個對應的function,傳入的字串利用ltu跟delete，就能夠把所有非英文字母都刪除，並且小寫轉大寫了。 def makefilter(input): print input.translate(ltu,delete) ","keywords":"","version":null},{"title":"nmap","type":0,"sectionRef":"#","url":"/2013/06/15/nmap","content":"nmap是一個linux下的工具 nmap - Network exploration tool and security / port scanner 這邊記錄一下nmap的用法 nmap -sP 140.113.214.79/27 -sP: Ping Scan - go no further than determining if host is online 用ping去掃目標內的所有IP，並顯示有回應的IP，所以若對方是windows7且沒有打開ping的回應，則也會被當作host down nmap -sL 140.113.214.79/27 -sL: List Scan - simply list targets to scan 只是單純的列出對方的hostname以及IP，不送出任何封包去檢測 nmap -O 140.113.214.94 nmap -A 140.113.214.94 -O: Enable OS detection -A: Enables OS detection and Version detection, Script scanning and Traceroute 掃描對方主機的OS系統 nmap -PS/PA/PU/PY[portlist] 140.113.214.94 -PS/PA/PU/PY[portlist]: TCP SYN/ACK, UDP or SCTP discovery to given ports 用不同的方式去掃描特定的PORT。 PS 用TCP 搭配 SYN FLAG去偵測。PA 用TCP 搭配 ACK FLAG去偵測。PU 用UDP去偵測。PY 用SCTP去偵測。 nmap -sS/sT/sU 140.113.214.94 採用不同的方式去掃描所有port。 sS (TCP SYN scan) .sT (TCP connect scan)sU (UDP) nmap -v 140.113.214.94 顯示出詳細一點的資訊","keywords":"","version":null},{"title":"Python-pack_unpack","type":0,"sectionRef":"#","url":"/2013/07/01/python-pack-unpack","content":"","keywords":"","version":null},{"title":"Example​","type":1,"pageTitle":"Python-pack_unpack","url":"/2013/07/01/python-pack-unpack#example","content":"假設 version = 1playerID = 56x = 123y = 2341momey = 5566217profession = &quot;warrior&quot;level = 128experience = 2147383611 data = pack('2B2HI10sBI',version,playerID,x,y,momey,profession,level,experience) //'\\x018{\\x00%\\t\\x00\\x00\\t\\xefT\\x00warrior\\x00\\x00\\x00\\x80\\x00\\xdb\\xff\\xff\\x7f' unpack('2B2HI10sBI',data) (1, 56, 123, 2341, 5566217, 'warrior\\x00\\x00\\x00', 128, 2147483611)  ","version":null,"tagName":"h3"},{"title":"Execution Floodlight","type":0,"sectionRef":"#","url":"/2013/08/21/floodlight-env","content":"","keywords":"","version":null},{"title":"Floodlight configuraion:​","type":1,"pageTitle":"Execution Floodlight","url":"/2013/08/21/floodlight-env#floodlight-configuraion","content":"--configFile ${configuration path} ","version":null,"tagName":"h3"},{"title":"Log configuraion:​","type":1,"pageTitle":"Execution Floodlight","url":"/2013/08/21/floodlight-env#log-configuraion","content":"-Dlogback.configurationFile=${FL_LOGBACK} ","version":null,"tagName":"h3"},{"title":"範例​","type":1,"pageTitle":"Execution Floodlight","url":"/2013/08/21/floodlight-env#範例","content":"java -Dlogback.configurationFile=logback.xml floodlight.jar --configFile floodlightdefault.properties ","version":null,"tagName":"h2"},{"title":"cscope 使用筆記","type":0,"sectionRef":"#","url":"/2013/10/05/cscope-note","content":"","keywords":"","version":null},{"title":"Introducion​","type":1,"pageTitle":"cscope 使用筆記","url":"/2013/10/05/cscope-note#introducion","content":"Cscope 是一個用來trace code還滿方便的工具 我通常都用他來trace linuxe kernel code,雖然說有網頁版的reference可以使用，但是用起來不順手，網頁會卡卡的 因此還是習慣使用這種互動式的trace tools ","version":null,"tagName":"h2"},{"title":"Install​","type":1,"pageTitle":"cscope 使用筆記","url":"/2013/10/05/cscope-note#install","content":"sudo apt-get install cscope on Ubuntu portmaster devel/cscope on FreeBSd ","version":null,"tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"cscope 使用筆記","url":"/2013/10/05/cscope-note#usage","content":"詳細的可以參考man page. 通常我只有使用 -R 來觀看而已 第一次執行的時候，會花比較久的時間去建立一個cscope.out的檔案，會把一些相關資訊放進去 下次執行的時候就會利用該out檔案來作查詢。 ","version":null,"tagName":"h3"},{"title":"其他​","type":1,"pageTitle":"cscope 使用筆記","url":"/2013/10/05/cscope-note#其他","content":"預設的情況下，cscope只能讀取 .c.h.l.y 想要讓他讀取java或是cpp的專案，就必須要先自己建置該資料庫 find ./ -name *.cpp &gt; cscope.filesfine ./ -name *.java &gt;&gt; cscope.filescscope -bkq 前面兩行會把所有的檔案路徑都寫入倒cscope.files裡面 b:建立索引文件k:建立索引文件時不會去搜尋/usr/local/目錄q:生成cscope.out，加速索引,該檔案包含 locate functionsfunction callsmacrosvariablespreprocessor symbols 接下來只要使用cscope就可以了 ","version":null,"tagName":"h3"},{"title":"Install News server on FreeBSD 9.1R","type":0,"sectionRef":"#","url":"/2013/10/05/news-server","content":"","keywords":"","version":null},{"title":"文章轉移​","type":1,"pageTitle":"Install News server on FreeBSD 9.1R","url":"/2013/10/05/news-server#文章轉移","content":"rsync cycbuffrsync db/history重新建立overview ctlinnd pause 'make overview'makehistory -x -O -b x: won't write out history file entries. O: Create the overview database b: Delete any messages found in the spool that do not have valid Message-ID: headers in them.makedbz -i i:To ignore the old databasectlinnd go 'over' ","version":null,"tagName":"h2"},{"title":"設定檔檢查​","type":1,"pageTitle":"Install News server on FreeBSD 9.1R","url":"/2013/10/05/news-server#設定檔檢查","content":"inncheck (inn.conf)scanspool -v (active, spool) ","version":null,"tagName":"h2"},{"title":"更新相關設定​","type":1,"pageTitle":"Install News server on FreeBSD 9.1R","url":"/2013/10/05/news-server#更新相關設定","content":"重新編譯innd,進入innd src底下./configure --opetionsmake &amp;&amp; make update ","version":null,"tagName":"h2"},{"title":"創新的newsgroup​","type":1,"pageTitle":"Install News server on FreeBSD 9.1R","url":"/2013/10/05/news-server#創新的newsgroup","content":"ctlinnd newgroup namemodity db/newsgroup ","version":null,"tagName":"h2"},{"title":"其他​","type":1,"pageTitle":"Install News server on FreeBSD 9.1R","url":"/2013/10/05/news-server#其他","content":"創新newsgorup 執行innd &amp; nnrpd 會噴權限不足 檢查/news/bin/innbind 有無SUID ","version":null,"tagName":"h2"},{"title":"Install Sphinx on Ubuntu 12.04 LTS","type":0,"sectionRef":"#","url":"/2013/10/05/sphinx","content":"","keywords":"","version":null},{"title":"Install​","type":1,"pageTitle":"Install Sphinx on Ubuntu 12.04 LTS","url":"/2013/10/05/sphinx#install","content":"直接透過atp-get 安裝即可 sudo apt-get install sphinx ","version":null,"tagName":"h2"},{"title":"Config​","type":1,"pageTitle":"Install Sphinx on Ubuntu 12.04 LTS","url":"/2013/10/05/sphinx#config","content":"安裝完畢後，執行 sphinx-quickstart就可以基本設定了 每個選項都有說明，基本上都採用預設值即可 設定檔: conf.py 外掛管理資料夾結構管理一些通用參數，如作者名稱，版本...等 主要的檔案: index.rst -. 檔案的結構 -. toctree ","version":null,"tagName":"h2"},{"title":"index.rsta​","type":1,"pageTitle":"Install Sphinx on Ubuntu 12.04 LTS","url":"/2013/10/05/sphinx#indexrsta","content":"Lab Meetgins ============= .. toctree:: :maxdepth: 4 :titlesonly: 20130924.rst 20131001.rst 國科會 meetings =============== .. toctree:: :maxdepth: 4 :titlesonly: 20130925.rst  這邊我定義兩個toctree，每個toctree底下又會有其他的rst，結構大概是這樣 Lab Meetings 20130924.rst20131001.rst 國科會 meetings 20130925.rst 總共兩個分類，每個分類底下的文章都是一個額外的rst檔案 在toctree底下的都是一些設定參數 maxdepth : 最大深度titlesonly : 在首頁面只顯示子類的標題 ","version":null,"tagName":"h2"},{"title":"Write​","type":1,"pageTitle":"Install Sphinx on Ubuntu 12.04 LTS","url":"/2013/10/05/sphinx#write","content":"Sphinx採用的reStructuredText格式跟markdown很類似，但是複雜了一些 官方網站有滿詳細的介紹，有需要時再去參考即可 ","version":null,"tagName":"h2"},{"title":"Build​","type":1,"pageTitle":"Install Sphinx on Ubuntu 12.04 LTS","url":"/2013/10/05/sphinx#build","content":"如果想要轉成html網頁，有兩種方法可以執行 sphinx-build -b html . NSLMeeting 意思是建置html的網頁， 然後以當前目錄為source 來源，然後把檔案build到NSLMetting去。 make html 在Makefile中定義了相關得動作，當執行make html的時候，其實就是執行sphinx-build -b html . _build/html 這邊因為我想要直接弄到別的資料夾，所以我直接設定aliase去執行方法1 目前對於這套軟體還在學習階段，有任何學習會繼續紀錄。 ","version":null,"tagName":"h2"},{"title":"Vim & Nerdtree","type":0,"sectionRef":"#","url":"/2013/10/11/vim-plugin","content":"","keywords":"","version":null},{"title":"vimrc 設定​","type":1,"pageTitle":"Vim & Nerdtree","url":"/2013/10/11/vim-plugin#vimrc-設定","content":"set encoding=utf-8 set fileencodings=ucs-bom,utf-8,big5,latin1 set fileencoding=utf-8 set termencoding=utf-8 set number &quot; 行號 set statusline=%&lt;\\ %n:%f\\ %m%r%y%=%-35.(line:\\ %l\\ of\\ %L,\\ col:\\ %c%V\\ (%P)%) set ai &quot; 自動縮排 syntax on &quot; 色彩標示 set tabstop=4 &quot; tab使用四個空白取代 set shiftwidth=4 &quot; 縮排空白數，要搭配set cin使用 set cin set cursorline &quot; 該行的線 set t_Co=256 &quot; 支援 256 色 set textwidth=0 set backspace=2 &quot;按下backspace會後退，道行首後會刪除到前一行 set showmatch &quot;顯示括號配對情況 set nocompatible &quot;用vim的特性去運行，捨棄vi的特性 &quot; Pathogen call pathogen#infect() call pathogen#helptags() filetype plugin indent on &quot; Nerdtree autocmd VimEnter * NERDTree autocmd VimEnter * wincmd p let NERDTreeShowBookmarks=1 let NERDTreeChDirMode=0 let NERDTreeQuitOnOpen=0 let NERDTreeMouseMode=2 let NERDTreeShowHidden=1 let NERDTreeIgnore=['\\.pyc','\\~$','\\.swo$','\\.swp$','\\.git','\\.hg','\\.svn','\\.bzr'] let NERDTreeKeepTreeInNewTab=1 let g:nerdtree_tabs_open_on_gui_startup=0 set background=dark &quot;背景顏色 colorscheme wombat nnoremap &lt;silent&gt; &lt;F5&gt; :NERDTree&lt;CR&gt; &quot;normal mode的時候+數字 可以切換tab nnoremap &lt;Esc&gt;1 gt1 nnoremap &lt;Esc&gt;2 gt2 nnoremap &lt;Esc&gt;3 gt3 nnoremap &lt;Esc&gt;4 gt4 nnoremap &lt;Esc&gt;5 gt5 nnoremap &lt;Esc&gt;6 gt6 nnoremap &lt;Esc&gt;7 gt7 nnoremap &lt;Esc&gt;8 gt8  ","version":null,"tagName":"h3"},{"title":"NERDTree​","type":1,"pageTitle":"Vim & Nerdtree","url":"/2013/10/11/vim-plugin#nerdtree","content":"","version":null,"tagName":"h3"},{"title":"更改呼叫方式，使用F5​","type":1,"pageTitle":"Vim & Nerdtree","url":"/2013/10/11/vim-plugin#更改呼叫方式使用f5","content":"nnoremap &lt;silent&gt; &lt;F5&gt; :NERDTree&lt;CR&gt;  ","version":null,"tagName":"h3"},{"title":"在各界面中移動​","type":1,"pageTitle":"Vim & Nerdtree","url":"/2013/10/11/vim-plugin#在各界面中移動","content":"按照順序往下移動 (crtl+w+w)上一個view (ctrl+w+h)下一個view (ctrl+w+l) ","version":null,"tagName":"h3"},{"title":"切割視窗​","type":1,"pageTitle":"Vim & Nerdtree","url":"/2013/10/11/vim-plugin#切割視窗","content":"水平切割 (在該檔案前按i)垂直切割 (在該檔案前按s) i :水平 s :垂直 ","version":null,"tagName":"h3"},{"title":"tab使用​","type":1,"pageTitle":"Vim & Nerdtree","url":"/2013/10/11/vim-plugin#tab使用","content":"開新tab並且切換過去 (t)開新tab但不切換過去 (T)下一個tab (gt)上一個tab (gT) ","version":null,"tagName":"h3"},{"title":"ZFS 筆記","type":0,"sectionRef":"#","url":"/2013/10/12/zfs","content":"","keywords":"","version":null},{"title":"Files​","type":1,"pageTitle":"ZFS 筆記","url":"/2013/10/12/zfs#files","content":"sudo dd if=/dev/zero of=/zfs1 bs=1M count=256sudo dd if=/dev/zero of=/zfs2 bs=1M count=256 ","version":null,"tagName":"h2"},{"title":"Zpool​","type":1,"pageTitle":"ZFS 筆記","url":"/2013/10/12/zfs#zpool","content":"create a mirror pool zpool create ftphome mirror /zfs1 /zfs2 destroy a pool zpool destroy ftphome check zpool status zpool status &lt;pool&gt; export pool ( 把某些pool export出去，暫時不使用) zpool export ftphome import pool ( 把被export 的pool 重新import回來) zpool import -d / ftphome (用-d指定你檔案的位置，預設會去吃/dev/)以我的範例來說，當import回來後，名稱會變成 //zfs1, //zfs2，多了一個/，原因不明中。 attach ( 只能對mirror使用) zpool attach ftphome /xxx detach ( 只能對mirror使用) zpool detach ftphome /zfs1 還有offline,online,remove...，剩下的就要用的時候去man zpool,還滿詳細說明的。 ","version":null,"tagName":"h2"},{"title":"ZFS database​","type":1,"pageTitle":"ZFS 筆記","url":"/2013/10/12/zfs#zfs-database","content":"set attributes zfs set key=value &lt;filesystem|volume|snapshot&gt; zfs get compression ftphomezfs set mountpoint=/home/ftp ftphome get attributes zfs get key &lt;filesystem|volume|snapshot&gt; zfs get compression ftphome snapshot zfs snapshot ftphome@today zfs list -t snapshot ","version":null,"tagName":"h2"},{"title":"其他​","type":1,"pageTitle":"ZFS 筆記","url":"/2013/10/12/zfs#其他","content":"假如你的ZFS有使用snapshot同時空間又滿的話，這時候會發現所有檔案都會刪除失敗，都會得到空間不足的訊息,這邊稍微模擬一下該情況，並且想辦法解決此問題。 ","version":null,"tagName":"h2"},{"title":"模擬情況​","type":1,"pageTitle":"ZFS 筆記","url":"/2013/10/12/zfs#模擬情況","content":"snatshot 該zfs zfs snapshot ftphome@todayzfs list -t snapshot 看一下是否有成功 塞爆該空間 zfs list 看一下還剩下多少空間dd if=/dev/random of=/home/ftp/file bs=1M count=256cd /home/ftprm file =&gt; 應該會得到 No space left on device空間不足的訊息。 ","version":null,"tagName":"h3"},{"title":"解決問題​","type":1,"pageTitle":"ZFS 筆記","url":"/2013/10/12/zfs#解決問題","content":"ZFS 變大容易(多塞個硬碟即可)，變小困難(幾乎無法)，因此當ZFS的硬碟滿的時候，有兩種做法 再加入兩個新的硬碟，然後合併到目前的zpool,可是這樣就會變成有兩份mirror。準備兩個更大的硬碟，把原本的zpool內的data全都複製過去。 這邊使用第二種做法 先幫本來的pool加入一個檔案，增加本來的空間，如此一來才可以做更多操作 dd if=/dev/zero of=/zfs5 bs=1M count=128dd if=/dev/zero of=/zfs6 bs=1M count=128zpool add ftphome mirror /zfs5 /zfs6zfs list(此時可以看到本來的空間變大了) 創造一個更大的zpool來取代 dd if=/dev/zero of=/zfs3 bs=1M count=512dd if=/dev/zero of=/zfs4 bs=1M count=512zpool create ftphome3 mirror /zfs3 /zfs4zfs set compression=gzip-9 ftphome2 把資料複製過去 zfs snapshot ftphome@sendzfs send ftphome@send | zfs receive -F ftphome2zfs list 看一下大小是否相同 mount新的，舊的砍掉 zfs umount ftphomezfs set mountpoint=/home/ftp/ ftphome2zpool destroy ftphome 做到這邊，就算完成了，成功的把本來的資料複製過去。 如果想要改變zpool的名稱，可以用export跟import來改名稱。 ","version":null,"tagName":"h3"},{"title":"Shell Script 筆記","type":0,"sectionRef":"#","url":"/2013/11/24/shell-note","content":"本篇文章是用來記錄以前修課關於 Shell Script 的作業 Introduction 用Unix的指令，透過pipe的方式完成下列要求 計算當前目錄底下資料夾的總數計算當前目錄底下檔案總數 只有計算regular file.不考慮FIFO、LINK 計算所有檔案大小和 (Byte)顯示前五大的檔案名稱只能使用PIPE，不能使用 $$ ; || &amp; &gt; &gt;&gt; &lt; Implement 使用ls來取得所有資料夾跟檔案的資訊 -l 可以顯示詳細資訊，這邊我們要取得的是 檔案大小-R 遞迴的往每個資料夾繼續往下找-A 不要把.跟..給顯示出來，因為這種當前目錄的東西我們不需要 使用sort來幫忙排序，找出檔案大小前五個 -r 排序結果反過來，由大到小排序-n 排序的時候，採用數字的方式去排序，不使用字母大小去排序-k 指定第幾個欄位要排序 使用awk作最後的處理，找出前五大，印出所有檔案大小和 因為再ls -l的結果中，會有很多資訊，包含 ./cs/.svn/pristine/74: 或者 total 28，所以awk再處理的時候，先用NF判斷該行的欄位數，至少要有9個欄位才處理 if(NF&gt;=9)接下來針對檔案是資料夾還是檔案，做全部的計數，可以由 -rw-r--r-- 的第一個欄位來決定，如果是d就代表資料夾，否則就是檔案。 這邊我使用 regular expression來判斷 ($1 ~/^d/)? (dir=dir+1) : (file=file+1)(size=size+$5)，此外如果是檔案得話，就順便把大小也計算一下執行過程中，因為剛剛已經排序過了，所以 前六行都把大小印出來， if(NR&lt;6) print NR&quot;: &quot;$5&quot; &quot;$9}最後就把所有資訊都列印出來 ls -RlA | sort -rnk 5 | awk '{ if(NF&gt;=9) ($1 ~/^d/)? (dir=dir+1) : (file=file+1)(size=size+$5); if(NR&lt;6) print NR&quot;: &quot;$5&quot; &quot;$9} END{ print &quot;Dir = &quot;dir&quot;\\n&quot; &quot; File = &quot; file&quot;\\n&quot; &quot;total = &quot;size}' ","keywords":"","version":null},{"title":"Git 筆記","type":0,"sectionRef":"#","url":"/2014/07/28/git","content":"","keywords":"","version":null},{"title":"Basic​","type":1,"pageTitle":"Git 筆記","url":"/2014/07/28/git#basic","content":"commit所使用的編輯器會依照下列優先度去選擇， GIT_EDITOR 環境變數core.editor 的設定VISUAL 環境變數EDITOR 環境變數vi 指令 變動檔案請用 git mv，使用git rm要注意檔案系統內的檔案會被真的刪除。 git log可以列出簡略的coommit資訊 git show [commit id] 可以看詳細的commit資訊，可以加上commit ＩＤ來指定特定的commit git show-branch --more=10 可以看當前bracnh的詳細commit資訊，由--more控制數量 ","version":null,"tagName":"h2"},{"title":"Configuraion​","type":1,"pageTitle":"Git 筆記","url":"/2014/07/28/git#configuraion","content":"總共有三種設定方式，優先度如順序 .git/config， 可以用 --file或是預設的方式操作~/.gitconfig， 可以用 --global操作/etc/gitconfig，可以用 --system操作 git config --global user.name &quot;hwchiu&quot; (2) git config user.email &quot;hwchiu@cs.nctu.edu.tw&quot; (1)  可以透過 git config -l列出當前所有的設定可以透過 --unset來移除設定 git config --unset --global user.name  ","version":null,"tagName":"h2"},{"title":"Costco 去骨雞腿排紀錄","type":0,"sectionRef":"#","url":"/2018/08/08/costco-chicken","content":"本身算是 Costco 愛好者，每個禮拜的蛋白質基本上都是從 Costco 取得 常買的有 雞胸肉雞腿排牛奶蛋白丁蝦仁鮭魚 這篇文章主要用來記錄雞腿排的資訊，包含來源，熱量等資訊 來源 基本上 Costco 雞腿肉的來源有兩家，分別大成 以及 卜蜂 這兩家供應商。 購買時包裝上面都會顯示這兩家的名稱與Logo，除了供應商名稱不同外，其價格與大小大致上並沒有差異。 大小 根據 Costco 線上官網 的圖示說明，每包 Costco 去骨雞腿都是一次六包為一個單位 如下圖 六包的重量大概都是落在 2.5kg 上下左右，平均下來每包平均 400多克左右。 但是實際上在食用時，每一包內的去骨雞腿牌數量則是 2-3 片，這部份就是隨機的。 為了能夠簡單的估算每一包雞腿排的營養素，我們必須要先知道每一塊的重量 這邊實際拿電子秤來實測看看這兩片雞腿排的重量 根據上述的結果，每片雞腿排的重量大概落在 170 公克左右 營養 根據食品藥物消費者知識服務網 提供的食品營養素估算表 我們使用肉類-&gt;雞類 -&gt;清腿 這個分類來估算營養成分 將常見的營養素基於 100 以及 170 公克列舉出來 100公克\t170公克熱量\t157\t267 粗蛋白\t18.5\t31.4 粗脂肪\t8.7\t14.8 飽和脂肪\t2.5\t4.3 總碳水化合物\t0\t0 這樣來看如果今天一餐吃包，攝取的蛋白質則介於 60 ~ 90 公克之間，取決於你那一包裡面有多少片雞腿排 烹調 雞腿肉相對於雞胸肉來說料理非常簡單，基本上只要現煮(烤/煎/)等各式料理方法都不會太難吃，所以就邊就不多述相關的料理方式了 #參考來源 食品藥物消費者知識服務網Costco 線上官網","keywords":"k8s cni","version":null},{"title":"Anki 使用感想","type":0,"sectionRef":"#","url":"/2017/03/01/anki-thoughts","content":"還記得以前在工三讀書時，常常看到 Chun Norris 坐在我前面，然後畫面上是一張一張的卡片在不停地翻動，每張卡片上面都標記者一個日文單字， 看他快速地翻閱這些卡片，感覺就是在背頌單字，那時候也就沒有去想太多了。 沒想到過了幾年後，Chun Norris 竟然出書了!!!英、日語同步Anki自學法：我是靠此神器，最短時間通過日檢N1、多益975分 看到這個消息後，就馬上預購了一本這個書，不但捧朋友的場，同時也順便瞭解看看到底 Anki 是什麼樣的東西。 對於想瞭解 Anki 基本操作的，可以參考這邊經過了一陣摸索後，也開始使用了 Anki 來幫助我背單字，不過由於內建的一些卡片集(Deck)大都偏向特定主題，如托福、多益等 ，所以我後來也自己創建了一個卡片集給我自己使用。 在卡片編輯的部分，原本都是透過Anki application上面的 GUI 去操作編輯，填寫正反兩面的卡片資訊。 所以本來的流程是這樣 我看小說/文章使用手機的 APP 去查詢單字定期開啟 Anki 將 APP 內的歷史單字一個一個透過線上字典服務去查詢將查詢的結果複製貼上到 Anki內，並轉成Anki的卡片 上面第三步驟最花費時間，當卡片數量一多的時候，其實要非常可怕的 那時候APP內大概有五六百個查詢過的單字，每個單字花20秒去填寫，也要整整三個小時不間斷才有辦法處理完畢。 有鑑於未來單字量只會愈來愈多，這樣手動下去實在不是辦法，因此腦筋就轉了彎一下，看有沒有辦法讓上面的步驟簡單化 上述(1),(2)這兩個步驟是不可避免的，那(3),(4)這兩個步驟就是主要處理的對象了。 針對我的目標，我將其拆成兩部分 給定一個單字，想辦法自動獲得其發音及解釋從上述的輸出中，將其自動變成 Anki 的卡片，然後塞到我的卡片集(Deck)中 針對第一點，我使用 python 作為我的程式語言，然後很偷懶的就拿了 yahoo 字典當作我的搜索來源 於是用了 python + BeautifulSoup + urllib 來爬網頁， 於是就網頁爬阿爬~就爬出了發音跟解釋了，這邊一切搞定 再來第二點，想要自動加入倒 Anki 的資料庫中，於是就到github去翻一翻，還真的翻到有人寫好已經可以用的工具addToAnki，這個作者使用 python 撰寫，提供一個 tool 讓第三方可以將卡片的內容輸入進去後，自動轉為卡片並且塞到對應的卡片集中。 雖然我個人是更偏向去呼叫 library 而不是只接呼叫 tool 來處理，不過我的目的能達成就好，所以就將我前面的程式碼跟他的結合起來。目前將此結果放在這邊同時我也發送了一個 pull request 到原先作者那邊，把我的使用情境當作一個多的範例使用，不過我看作者也兩三年沒碰了，應該也不會去接收我的 PR XDD 最後我的流程就是 遇到不會的單字使用手機查詢單字定期將查詢過的單字匯出成一個文字檔將該文字檔送給我的 python 去處理，等他自動處理好即可。 上次一口氣加了四百多個單字，總共花費十五分鐘左右，大部分的時間應該都是消耗再去爬 yahoo dict這邊，目前這樣已經滿足我的需求。","keywords":"","version":null},{"title":"LeetCode - 314","type":0,"sectionRef":"#","url":"/2017/03/01/leetcode-314","content":"","keywords":"","version":null},{"title":"314 Binary Tree Vertical Order Traversal​","type":1,"pageTitle":"LeetCode - 314","url":"/2017/03/01/leetcode-314#314-binary-tree-vertical-order-traversal","content":"原題目是付費題目，有興趣看到完整的請自行付費觀賞，在此就不提供超連結了。 ","version":null,"tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"LeetCode - 314","url":"/2017/03/01/leetcode-314#introduction","content":"給定一個 binary tree，將此 tree 以 vertical 的方式走過，輸出時，從最左邊開始輸出相同 colume 的算同一個 group，若屬於同 row 且同 colume，則從左邊開始算起 ","version":null,"tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"LeetCode - 314","url":"/2017/03/01/leetcode-314#example","content":" 0 / \\ 1 4 / \\ / \\ 2 35 6 / \\ 7 8  輸出為[2][1][0,3,5][4,7][6][8] ","version":null,"tagName":"h2"},{"title":"Solution​","type":1,"pageTitle":"LeetCode - 314","url":"/2017/03/01/leetcode-314#solution","content":"這題不太困難，基本上可以採用 BFS 來搜尋整個 tree，然後加入一個 index 的欄位，root 的 index 是 0，往左遞減，往右遞增，在 BFS 的過程中，就把相同 index 都收集起來，最後再一口氣輸出即可。 pseudo code 如下 queue.push(pair(0, root)); while (!queue.empty()) { index = queue.front().first; node = queue.front().second; ans[index].push(node-&gt;val) if (node-&gt;left) queue.push(pair(index-1, node-&gt;left); if (node-&gt;right) queue.push(pair(index+1, node-&gt;right); } return ans;  ","version":null,"tagName":"h2"},{"title":"perf_for_drbd_9.0","type":0,"sectionRef":"#","url":"/2017/05/19/perf-for-drbd","content":"","keywords":"","version":null},{"title":"Testing Environment​","type":1,"pageTitle":"perf_for_drbd_9.0","url":"/2017/05/19/perf-for-drbd#testing-environment","content":"為了進行效能上的分析，首要條件就是先將 DRBD 給衝到效能瓶頸才有機會去觀察，所以本文採用下列的環境與工具來進行這項作業 Environment​ CPU: Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz Storage: Non-Volatile memory controller(NVME) Tool: fioOS: Ubuntu 16.04 with linux 4.4.0-78-generic Setup​ 為了更方便觀察 drbd 的運行，我們將 drbd 創造的 kernel thread 都分別綁在不同的 cpu 上，這樣可以讓每隻 kernel thread 盡可能去使用cpu。 透過 ps or htop 取得 kernel thread 的 pid,這邊可以關注的有 drbd_s_r0 (sender)drbd_r_r0 (receiver)drbd_as_r0 (ack sender)drbd_a_r0 (ack receiver)drbd_w_r0 (worker) 透過 taskset 這個指令將上述程式分別綁到不同的 cpu 上 taskset -p 0x100 18888 ...  Stress​ 本文使用 fio 來進行資料的讀取，下方提供一個簡易的 fio 設定檔，可依照自行的環境變換修改。 [global] iodepth=512 numjobs=3 direct=1 time_based runtime=30 group_reporting size=5G ioengine=libaio filename=/mnt/beegfs/fio1.test [rrw] bs=4k rw=randrw rwmixread=75 [rr] bs=4k rw=randread [rw] bs=4k rw=randwrite [sr] bs=64k rw=read [sw] bs=64k rw=write  我們 fio 採用 client/server 的架構，要是可支援多台 client 同時一起進行資料讀取，提供更高的壓力測試。 假設該設定檔名稱為 fio.cfg，並且放置於 /tmp/fio.cfg則首先在 node-1 上面執行下列指令以再背景跑一個 fio server fio -S &amp;  接下來要運行的時候，執行下列指令來運行 fio，其中若想要改變測試的類型，可透過 --secion進行切換。 /fio --client=node-1 /tmp/fio.cfg --section=rw  這時候可以透過 htop 以及 iostat 的資訊去觀察，如下圖 當前透過 iostat 觀察到的確對 drbd0 有大量的讀寫動作同時由 htop (記得開啟 kernel thread觀察功能)，可以看到 drbd_s_r0 以及 drbd_a_r0 都各自吃掉一個 cpu，大概都快接近 100% 的使用率。 Profile​ 有了上述的環境後，我們就可以準備來分析 drbd 程式碼運行狀況。 Environemnt​ 這邊使用 perf 這套程式來分析，基本上 kernel 新一點的版本都已經內建此功能了，比較舊的 kernel 則需要自己重新開啟該 kernel config然後重新 build kernel，所以這邊使用 Ubuntu 16.04 with linux 4.4.0-78-generic 相對起來非常簡單。 直接執行 perf 這個指令，若系統上有缺少 linux-tools-4.4.0-78 相關 tool 的話會有文字提示你，如下所示，按照提示使用 apt-get 將相關的套件安裝完畢後，就可以使用 perf 了。 WARNING: perf not found for kernel 4.4.0.78 You may need to install the following packages for this specific kernel: linux-tools-4.4.0-78-4.4.0-78 linux-cloud-tools-4.4.0-78-4.4.0-78  Run​ perf 的功能非常強大，可以參考 wiki, 這邊我們使用 perf top 的方式來觀察結果。 為了可以順便觀看 call graph 的過程，再執行perf的時候要多下-g這個參數 指令為 perf top -g -p $PID，如 perf top -g -p 18888。 在這邊我嘗試觀察 drbd_a 這隻，結果如下列 drbd_a​ 這邊可以觀察到三隻吃比較兇的 function 都吃很兇,分別是 native_queue_spin_lock_slowpath 、 tr_release 以及 idr_get_next。 這邊比較麻煩的是你看這個只能知道就是卡在 spin_locK，系統上是不是 multithread，然後有太多的資料要搬移導致 spin_lock ? 這些搬移的資料是誰放進去的?，這些資料是什麼? 以及更多的問題都必須要看程式碼去理解其整體設計架構，才有辦法講出一套完整的流程說明這個結果。 這部份等之後能夠完整理解整個 drbd 的 write-path 或是 read-path 時再重新來看一次這張圖，到時候應該會有完全不一樣的思維。  ","version":null,"tagName":"h3"},{"title":"curl with fewer malloc","type":0,"sectionRef":"#","url":"/2017/07/05/curl-malloc","content":"不久前有一篇文章https://daniel.haxx.se/blog/2017/04/22/fewer-mallocs-in-curl/指出， curl 開發者嘗試將 malloc 呼叫的次數減少，結果對整體的影響帶來的顯著的提升 使用 curl http://localhost/512M 當作第一個比較 原始版本的 curl 關於 malloc 相關數據如下 Mallocs: 33901 Reallocs: 5 Callocs: 24 Strdups: 31 Wcsdups: 0 Frees: 33956 Allocations: 33961 Maximum allocated: 160385 而修改後的版本為 Mallocs: 69 Reallocs: 5 Callocs: 24 Strdups: 31 Wcsdups: 0 Frees: 124 Allocations: 129 Maximum allocated: 153247 比較起來可以發現， malloc 呼叫的次數有急遽的下降，從 33901 降落到 69，而整體使用的記憶體也少了 7KB 左右 此外，若比較兩者傳輸的速度，抓取一個 80GB 的檔案 Original: 2200MB/sec Modified: 2900MB/sec 在傳輸方面提升了 30% 左右的速率，非常驚人 若使用 time 指令來比較新舊版本抓取 80G 檔案的差別 Old code: real 0m36.705s user 0m20.176s sys 0m16.072s New code: real 0m29.032s user 0m12.196s sys 0m12.820s 修改相關的 commit 如下 llist: no longer uses mallocmulti: make curl_multi_wait avoid malloc in the typical case 簡單來說就是將 malloc 的部分都拔除，盡量使用 stack 來提供記憶體，藉此減少呼叫 malloc 的部分。 主要是因為 curl 在傳輸的過程中，會有非常大量且小空間的 malloc 被呼叫到，這部分拖慢的整體的運行速度","keywords":"","version":null},{"title":"2020-年度回顧","type":0,"sectionRef":"#","url":"/2020/12/31/2020-review","content":"2020 年度個人回顧 美國工作順利活下來，已達一年六個月美國總共搬家四次，其中只有一次跟我有關洗衣機卡門事件讓我領悟到世事難料年度公開演講 8 場，其中六場是個人自給自足的線上演講。而且有五場是美國時間早上六點起來準時分享。必須說真的滿累的，一大早又要講底層的東西常常會恍神年度公開文章 58 篇，其中 30 篇是 2020 參加鐵人賽完賽的文章。今年度有增加更多 devops 相關的文章，也還是有保持幾篇研究 kernel 等底層的文章參加 2020 鐵人賽，今年運氣不錯得到佳作，同時團體賽也順利完賽！續約微軟 MVP開設三門線上課程，講述 kubernetes 各類資訊，課程總長度將近33小時參加兩次 podcast 閒聊一整年剪頭髮次數: 2 次美國遠端工作八個月，回台的14天隔離根本小case八個月沒有進入健身房，覺得人生少了一點什麼，實測發現硬舉最大重量少了70公斤 :(脫魯感謝前國手們的閒聊，最後開設個人粉絲頁分享各類資訊，累積人樹目前 2k 2021 展望 美國工作繼續順利繼續保持線上演講的衝勁與動力，希望至少一年六次文章希望內容可以廣度與深度兼具今年我要成為 ebpf 王繼續參加鐵人賽剪頭髮次數: 3 次繼續開設三門線上課程撰寫第一本書籍粉絲頁固定每兩天一篇文章分享長達一年街頭健身完成前水平保持脫魯 結語 2020 受到疫情影響，導致有八個月的時間都長期遠端工作，整體工作生活習慣改變，包含工作，煮飯，運動等彼此的時間調整。 也因為如此有更多的彈性時間去思考自己要的東西，同時準備課程與鐵人賽文章。整體的時間管理能力目前還算滿意","keywords":"","version":null},{"title":"costco 冷凍鮭魚排紀錄","type":0,"sectionRef":"#","url":"/2018/10/28/costco-salmon","content":"本身算是 Costco 愛好者，每個禮拜的蛋白質基本上都是從 Costco 取得 常買的有 雞胸肉雞腿排牛奶蛋白丁蝦仁鮭魚 這篇文章主要用來記錄冷凍鮭魚排的資訊，每一份的大小以及熱量等資訊 來源 Costco 基本上有滿多的鮭魚可以購買，有在冷藏處比較新鮮的挪威鮭魚，也有在冷凍庫的冷凍鮭魚。 由於我大部分都是製作便當，因此任何的食材在烹飪完畢後都會冷藏並且隔天透過微波爐來加熱飲食 因此在選擇上我都還是以冷凍鮭魚為主，只有特別想要當天吃才會選擇冷藏鮭魚 本篇主要以冷凍鮭魚為主,其包裝如下 大小 根據 包裝背後的標示，本包裝大概含有 2KG 的鮭魚，然而實際上裡面的包裝物並不是真的如其所說有 20 份這麼多，其實每片鮭魚大小都頗大的。 實際上拿出一片使用電子秤來進行實測，測出來的重量大概是 300g 左右 因此一整包的份量大概會落在7片左右，畢竟每片鮭魚的大小都會有點差距，不過大概可以記住一片300g就好 營養 本篇就直接使用該包裝背後所提供的成份表來計算每片鮭魚排的營養素 將常見的營養素基於 100 以及 300 公克列舉出來 100公克\t300公克熱量\t149 KCal\t447 KCal 蛋白質\t17.9 g\t53.7 g 粗脂肪\t8.5 g\t25.5 g 飽和脂肪\t2.2 g\t6.6 g 碳水化合物\t0.1 g\t0.3 g 鈉\t59 mg\t177 mg 這樣來看如果今天一餐吃一片冷凍鮭魚排的話，，攝取的蛋白質大概是 54 g 左右，熱量450卡，如果單純考慮價錢與蛋白質的比例的話，還是雞胸/雞腿的比例比較好，不過換換口味吃個鮭魚也不錯 烹調 基本上好好的煎就沒有什麼問題了，唯一要注意的是要確認內部比較厚的部分需要比較長的時間才會熟透，可以切半去料理會比較快。","keywords":"","version":null},{"title":"Blktrace, Blkparse and Fio example","type":0,"sectionRef":"#","url":"/2017/06/02/blktrace","content":"","keywords":"","version":null},{"title":"Step1​","type":1,"pageTitle":"Blktrace, Blkparse and Fio example","url":"/2017/06/02/blktrace#step1","content":"In order to make the output of blkparse more easily to read, we set the numjobs to 1. Following is my fio config [global] iodepth=256 numjobs=1 direct=1 time_based runtime=120 group_reporting size=5G ioengine=libaio filename=/dev/nvme1n1 [rw] bs=4k rw=randwrite [sw] bs=64k rw=write  After we setup the fio config, use the fio to generate the IO request. In this example, we ask the fio to generate the IO via sequence write pattern. fio ${path_of_config} section=sw  During the experiment, you can use the tool iostat to monitor the I/O information about the device we want to observe. ","version":null,"tagName":"h2"},{"title":"Step2​","type":1,"pageTitle":"Blktrace, Blkparse and Fio example","url":"/2017/06/02/blktrace#step2","content":"Open other terminal and use blktrace to collection the data, there are two parameter we need to use, First one is -d, which indicate what target device blktrace will monitor to. Second, is -w, we use it to limit the time (seconds) how long blktrace will run. So, our final command looks like below. blktrace -d /dev/nvme1n1 -w 60  In the end of blktrace, you can discover some new files has created by blktrace and its prefix name is nvme1n1.blktrac.xxThe number of files is depends how may CPUs in your system. -rw-r--r-- 1 root root 821152 Jun 2 10:39 nvme1n1.blktrace.0 -rw-r--r-- 1 root root 21044368 Jun 2 10:39 nvme1n1.blktrace.1 -rw-r--r-- 1 root root 462864 Jun 2 10:39 nvme1n1.blktrace.10 -rw-r--r-- 1 root root 737960 Jun 2 10:39 nvme1n1.blktrace.11 -rw-r--r-- 1 root root 865872 Jun 2 10:39 nvme1n1.blktrace.12 -rw-r--r-- 1 root root 755248 Jun 2 10:39 nvme1n1.blktrace.13 -rw-r--r-- 1 root root 4675176 Jun 2 10:39 nvme1n1.blktrace.14 -rw-r--r-- 1 root root 4471480 Jun 2 10:39 nvme1n1.blktrace.15 -rw-r--r-- 1 root root 5070264 Jun 2 10:39 nvme1n1.blktrace.16 -rw-r--r-- 1 root root 5075040 Jun 2 10:39 nvme1n1.blktrace.17 -rw-r--r-- 1 root root 5062104 Jun 2 10:39 nvme1n1.blktrace.18 -rw-r--r-- 1 root root 5586936 Jun 2 10:39 nvme1n1.blktrace.19 -rw-r--r-- 1 root root 3718848 Jun 2 10:39 nvme1n1.blktrace.2  ","version":null,"tagName":"h2"},{"title":"Step3​","type":1,"pageTitle":"Blktrace, Blkparse and Fio example","url":"/2017/06/02/blktrace#step3","content":"Now, we can use the blkparse to regenerate human-readable output form the output we get via blktrace before. We need to indicate source files, you can just use the device name without .blktrace.xx, for example,nvmen1, it will search all files which match the pattern nvmen1.blktrace.xx and put together to analyze. Then, the -f option used to foramt the output data, you can find more about it via man blkparse OUTPUT DESCRIPTION AND FORMATTING The output from blkparse can be tailored for specific use -- in particular, to ease parsing of output, and/or limit output fields to those the user wants to see. The data for fields which can be output include: a Action, a (small) string (1 or 2 characters) -- see table below for more details c CPU id C Command d RWBS field, a (small) string (1-3 characters) -- see section below for more details D 7-character string containing the major and minor numbers of the event's device (separated by a comma). e Error value m Minor number of event's device. M Major number of event's device. n Number of blocks N Number of bytes p Process ID P Display packet data -- series of hexadecimal values s Sequence numbers S Sector number t Time stamp (nanoseconds) T Time stamp (seconds) u Elapsed value in microseconds (-t command line option) U Payload unsigned integer  For our observation, we use %5T.%9t, %p, %C, %a, %S\\n to format our result containing timestamp, command, process ID, action and sequence number. Since the data I/O contains many action, such as complete, queued, inserted..ect. we can use option -a to filter actions, you can find more info via man blktrace. In this case, we use the write to filter the actions. In the end, use the -o options to indicate the output file name. barrier: barrier attribute complete: completed by driver fs: requests issue: issued to driver pc: packet command events queue: queue operations read: read traces requeue: requeue operations sync: synchronous attribute write: write traces notify: trace messages drv_data: additional driver specific trace  The command will look like below and it will output the result to file output.txt. blkparse nvme1n1 -f &quot;%5T.%9t, %p, %C, %a, %S\\n&quot; -a write -o output.txt  open the file, the result looks like  0.000000000, 22890, fio, Q, 1720960 0.000001857, 22890, fio, G, 1720960 0.000005803, 22890, fio, I, 1720960 0.000009234, 22890, fio, D, 1720960 0.000036821, 0, swapper/0, C, 1996928 0.000067519, 22890, fio, Q, 1721088 0.000068538, 22890, fio, G, 1721088 0.000071531, 22890, fio, I, 1721088 0.000073102, 22890, fio, D, 1721088 0.000093464, 0, swapper/0, C, 1994624 0.000123806, 0, swapper/0, C, 1785472 0.000147436, 22892, fio, C, 1784576 0.000159977, 22891, fio, C, 1997312 0.000166653, 22891, fio, Q, 2006912 0.000167632, 22891, fio, G, 2006912 0.000169422, 22891, fio, I, 2006912 0.000171178, 22891, fio, D, 2006912 0.000188830, 22892, fio, Q, 1817728 0.000189783, 22892, fio, G, 1817728 0.000191405, 22892, fio, I, 1817728 0.000192830, 22892, fio, D, 1817728 0.000202367, 22891, fio, Q, 2007040 0.000203160, 22891, fio, G, 2007040 0.000205969, 22891, fio, I, 2007040 0.000207524, 22891, fio, D, 2007040 0.000227655, 22892, fio, Q, 1817856 0.000228457, 22892, fio, G, 1817856 0.000231936, 22892, fio, I, 1817856 ....  Since the fio will fork to two process to handle the process, we use the grep to focus on one specific process (pid=22892). grep &quot;22892, fio&quot; output.txt | more  Now, the result seems good, we can discover the sequence number (fifth column) is increasing. One thing we need to care about is the row which action is &quot;C&quot;, which means the completed, since we don't know how NVME handle those request and reply to upper layer. we only need to focus on other action. such as &quot;Q (queued This notes intent to queue i/o at the given location. No real requests exists yet.)&quot; or &quot;I (inserted A request is being sent to the i/o scheduler for addition to the internal queue and later service by the driver. The request is fully formed at this time)&quot;.  0.000147436, 22892, fio, C, 1784576 0.000188830, 22892, fio, Q, 1817728 0.000189783, 22892, fio, G, 1817728 0.000191405, 22892, fio, I, 1817728 0.000192830, 22892, fio, D, 1817728 0.000227655, 22892, fio, Q, 1817856 0.000228457, 22892, fio, G, 1817856 0.000231936, 22892, fio, I, 1817856 0.000233530, 22892, fio, D, 1817856 0.000360361, 22892, fio, Q, 1817984 0.000361310, 22892, fio, G, 1817984 0.000364163, 22892, fio, I, 1817984 0.000366696, 22892, fio, D, 1817984 0.000536731, 22892, fio, Q, 1818112 0.000537758, 22892, fio, G, 1818112 0.000539371, 22892, fio, I, 1818112 0.000541407, 22892, fio, D, 1818112 0.000670209, 22892, fio, Q, 1818240 0.000671345, 22892, fio, G, 1818240 0.000673383, 22892, fio, I, 1818240 0.000676260, 22892, fio, D, 1818240 0.001885543, 22892, fio, Q, 1818368 0.001887444, 22892, fio, G, 1818368 0.001891353, 22892, fio, I, 1818368 0.001895917, 22892, fio, D, 1818368 0.001934546, 22892, fio, Q, 1818496 0.001935468, 22892, fio, G, 1818496 0.001936891, 22892, fio, I, 1818496 0.001938742, 22892, fio, D, 1818496 0.001965818, 22892, fio, Q, 1818624  Now, we can do all above command again and change the section to rw for fio using the randon write pattern. The blkparse result will show the random sequence number. Summary In this article, we try to use tools blktrace and blkparse to analysiz the block level I/O for fio request. We observe the filed sequence number to make sure thhat the fio can generate the sequence or random according to its config. Reference 539-blktrace-and-btt-example-to-debug-and-tune-disk-i-o-on-linux ","version":null,"tagName":"h2"},{"title":"閱讀筆記: 「How to enforce Kubernetes network security policies using OPA」","type":0,"sectionRef":"#","url":"/2021/11/02/reading-note-1","content":"連結: https://www.cncf.io/blog/2020/09/09/how-to-enforce-kubernetes-network-security-policies-using-opa 不知道大家是否都有使用 Network Policy 來設定 Kubernetes 內部的 ACL? 這邊有個叫做 OPA 的工具可以用幫你驗證你的 Network Policy 是否運作良好，甚至當有新的應用服務要部署的時候，也會確定是否有跟 Network Policy 衝突 有興趣的人可以研究看看","keywords":"","version":null},{"title":"閱讀筆記: 「淺談 Service Mesh」","type":0,"sectionRef":"#","url":"/2021/11/04/reading-note-2","content":"連結: https://buoyant.io/service-mesh-manifesto/ 一篇關於 Service Mesh 的好文，發布已經有段時間了不過還是值得一讀， 文章作者是非常早期 Service Mesh 項目: Linkerd 的核心開發成員之一也是新創公司 Buoyant 公司的 CEO 相信大家應該對於 Service Mesh 一詞已經不陌生，可能對於這個名詞比較熟悉的朋友大多是從另一個 Service Mesh 項目: Istio 去了解 Service Mesh 的面貌，從這篇文章你可以從不同觀點認識 Service Mesh ， 全文非常長內容涵蓋： Service Mesh 詳盡介紹為什麼 Service Mesh 可以被施行？為什麼 Service Mesh 是個好的 idea (比起其他方法)？Service Mesh 幫助了什麼？Service Mesh 有解決掉所有問題嗎？為什麼在現今 Service Mesh 可以被施行？為什麼人們那麼愛談論 Service Mesh？身為一個謙虛的開發者需要關注 Service Mesh 嗎?一系列F&amp;Q 這裡對 Service Mesh 的需求做個小結，Service Mesh 帶來了三大好處： Reliability: 包含提供請求重試、超時、金絲雀部署(Traffic shifting/splitting) 等功能Observability: 包含提供請求成功率、延時、粒度到個別服Security: ACL 及 Mutual TLS (客戶端及服務端互信） 值得一提的是，本篇作者 William Morgan 對於 istio 持負面的態度，並不是因為 istio 與 linkerd 處於競爭關係的兩個產品，而是對於 istio 在 service mesh 做了太多的商業性 marketing 操作（大部分來自Go ogle的操作) 有興趣的朋友也可以在 Podcast 上聽到作者在 Podcast 上的訪談: https://reurl.cc/N6GbW9","keywords":"","version":null},{"title":"閱讀筆記: 「Contaienr 底層實作與 CVE 介紹」","type":0,"sectionRef":"#","url":"/2021/11/08/reading-note-4","content":"連結: https://teamt5.org/tw/posts/container-escape-101/ 這篇分享一篇非常有趣的問題，從 Container 的實作開始介紹，最後介紹幾個與 Container 相關的 CVE 對於資安有興趣的可以研究看看","keywords":"","version":null},{"title":"2020 疫情下的矽谷 - 遠端工作的探討","type":0,"sectionRef":"#","url":"/2020/09/05/coivd-wfh","content":"","keywords":"","version":null},{"title":"交通​","type":1,"pageTitle":"2020 疫情下的矽谷 - 遠端工作的探討","url":"/2020/09/05/coivd-wfh#交通","content":"矽谷灣區最著名的一個特色就是塞車，上班塞車，下班塞車，永遠都在塞車，所以過往很多人都會採取彈性上班的方式來避免車流，或是採取大眾運輸工具的方式來通勤。 以我個人為例，過往每天上班的通勤時間大概平均兩個小時，主要分成開車與大眾運輸 大眾運輸: 腳踏車配上火車通勤，由於火車時刻表固定，所以上下班時間比較沒有彈性開車: 開車的話大概可以將時間縮短到單趟 40 分鐘左右，但是整個高速公路都是一片紅，其實開起來很悶 遠端工作後，每天可以省下大約兩個小時的時間，這部分可以拿來上班工作，也可以拿來處理私人事情，時間上更加彈性 ","version":null,"tagName":"h2"},{"title":"工作​","type":1,"pageTitle":"2020 疫情下的矽谷 - 遠端工作的探討","url":"/2020/09/05/coivd-wfh#工作","content":"遠端在家工作後，最大的幾個變化就是 工作自律 基本上一整天就是在電腦前面，所以沒有開會的時候，其實都是自己去控制自己的工作節奏，這部分好壞的意見都有聽過，大家也會彼此分享如何讓自己工作更加專心，而不會被家裡的舒適環境給影響。 會議全面遠端化且數量上升 過往一些小事情要討論時，可能就直接現場約一約，到小會議室直接討論。然而目前則是因為遠端會議，所以全部都要事先預約時間來確保對方目前在電腦前。有時候一個小事情卻要花更多的時間來定案，我個人認為效率是不如過往的。 此外之前也出現如 Zoom Fatigue 這樣的說法，過多的會議內容導致大家開會疲倦，分心 就我個人的心得來看，自己能夠維持好工作的節奏，該完成的工作事項能完成就好，此外被打擾中斷工作的機會也更少了，整體而言利大於弊， ","version":null,"tagName":"h2"},{"title":"互動​","type":1,"pageTitle":"2020 疫情下的矽谷 - 遠端工作的探討","url":"/2020/09/05/coivd-wfh#互動","content":"員工之間的會面都是透過線上會議，缺少實體見面的互動，這部分也是見仁見智。有些人不喜歡交流，覺得 「上班當同事，下班不認識」是其工作原則，那應該會滿喜歡這種模式的。但是也有人喜歡實體交流，每天吃個一起吃個午餐，聊聊一些工作以外的事情，分享彼此的事情也都是一種生活。 這部分我就沒有太大意見，有好有壞，偶而還是會透過 slack 與同事聊聊幹話，分享最近的生活，只是少了聲音的互動。 ","version":null,"tagName":"h2"},{"title":"福利​","type":1,"pageTitle":"2020 疫情下的矽谷 - 遠端工作的探討","url":"/2020/09/05/coivd-wfh#福利","content":"遠端工作以後，公司內本來的福利近乎全滅，什麼咖啡，零食，飲料，冰箱內各種飲品，甚至免費午餐等都不再擁有，一切都要依賴自己處理。 對於一個自住的邊緣人來說，變成每天都要煩惱該怎麼處理今天的飲食，「每餐叫外送，荷包先消瘦」是一個顧慮點，自己煮飯又是一個額外的挑戰，買菜/備料/烹飪/善後四個步驟往往也花費不少時間在處理，如果將這個時間與通勤時間組合起來，未必可以省下時間。 我個人平常就習慣煮飯，已經練就如何快速料理與一次準備多日便當，所以這個情況比較不會有太大問題。 倒是我有聽過朋友因為沒有公司零食的迫害，遠端工作期間被迫減肥，也算是一個附加好處 一個美好的夢想就是辦公室可以縮編，「公司省租金，員工可加薪」 美國現況 疫情以來，各大公司陸陸續續宣布相關的遠端工作政策，而規範的工作日期也隨者疫情的擴散而延後，下面就整理目前我知道部分公司的工作內容，此外政策不一定適用於所有員工，必要性的情況下，部分員工還是要定期前往辦公室。 Apple: 員工可以遠端工作直到 2021 初期 Google: 員工至少可以遠端工作到 2021/07 Facebook: 員工至少可以遠端工作到 2021/07 Twitter: 員工可以選擇永遠遠端工作 Fujitsu: 員工可以選擇永遠遠端工作 Microsoft: 可以遠端工作直到 2021/02 Amazon: 員工至少可以遠端工作到 2021/01 Netflix: 員工遠端工作直到全面接受疫苗 此外，也有人對於遠端工作提出了一些負面說詞，譬如 Netflix 董事長則認為遠端工作沒有帶來正面效果，反而使得討論工作更加困難。 這部分真的就是沒有定案，完全見仁見智，而 Google 最近的調查則顯示只有 10% 不到的員工想要回到過往天天進辦公室的日子， 以下節錄自 Google 官方推特 除了公司本身政策的變動外，居住地遷移最近也是活動頻繁，譬如 Oklahoma 洲則有 Tulsa Remote 計畫，向所有遠端工作者提供一萬美元的獎勵，只要你願意搬來這邊即可。 根據報導指出，舊金山的兩房公寓每個月可能略低於四千美金，而 Tusla 則不到一千美金，再租金花費方面可以說是節省不少開銷。 心得 就我過去五年的台灣工作經驗，我認為台灣公司要跟進遠端工作政策實屬不易，幾個原因如下 勞基法規定下，公司都要提出員工上下班出勤時間的紀錄，這部分會用來評估是否有加班超時 對於遠端工作者來說，要如何打卡來滿足這些紀錄是一個行政上的問題，而不是技術上的問題 更多時候是公司行政團隊願不願意嘗試探討可能性，並嘗試看看沒人監督，老闆放不下 這種情況下，我認為信任是最基本的基礎，老闆信任員工可以遠端工作依然保持良好效率，員工也真的能夠滿足一定的效率來證明制度可行。過往就有聽過台灣發生過老闆接受遠端工作，結果員工私下兼差來賺錢，最後兩邊信任破壞，一切回歸辦公室制度。 Reference https://www.cnbc.com/2019/01/10/vermont-will-pay-you-10000-to-move-there-and-work-remotely---.htmlhttps://www.wsj.com/articles/facebook-to-shift-permanently-toward-more-remote-work-after-coronavirus-11590081300https://www.wsj.com/articles/google-to-keep-employees-home-until-summer-2021-amid-coronavirus-pandemic-11595854201https://www.cnn.com/2020/07/27/tech/google-work-from-home-extension/index.htmlhttps://www.bbc.com/news/business-53303364https://www.cnbc.com/2020/09/23/google-ceo-sundar-pichai-considering-hybrid-work-from-home-models.html ","version":null,"tagName":"h2"},{"title":"閱讀筆記: 「Amazon EKS Upgrade Journey From 1.17 to 1.18」","type":0,"sectionRef":"#","url":"/2021/10/31/reading-note-0","content":"連結: https://medium.com/swlh/amazon-eks-upgrade-journey-from-1-17-to-1-18-e35e134ca898 這邊跟大家分享一篇 EKS 升級的心得文章，該文章記錄了 EKS 從 k8s 1.17 到 1.18 的過程，並且先分享了幾個 1.18 主要新功能，包含了 Topology Manager (Beta)Service Side Apply (Beta)Pod Topology Spread (Beta) ... 等 詳細升級過程看起來無痛輕鬆，有興趣的可以參考全文 當然升級 K8S 最重要的還是要注意 Resource 的 API 版本是否有變，譬如 1.16 就讓很多人採到 Deployment 使用 extensions/v1beta1 的錯誤，所以每次升級請先檢查有沒有哪些過舊的 API 版本被丟棄，以免升&gt; 級後現有的服務部屬不上去 題外話： ingress 如果還是使用 extensions/v1beta1 的話，建議都換成 networking.k8s.io/v1beta1 (k8s 1.14+), 到了 1.22 後本來的 extensions/v1beta1 就不能用囉","keywords":"","version":null},{"title":"閱讀筆記: 「CRD 與 Operator 的探討」","type":0,"sectionRef":"#","url":"/2021/11/16/reading-note-8","content":"連結: https://twitter.com/ibuildthecloud/status/1295810776179961856?fbclid=IwAR3zVNFSodC-PK7JBUDA63vNONwrovxJP7qBvaTtq735dWonROlD5xWN13s 想必大家應該都聽過 Operator 的概念，透過 CRD 自定義資源格式並且配上程式化的運作邏輯來控管相關資源的操作。甚至有廠商針對 Operator 的概念來設計一個 Framework 讓大家能夠更輕鬆或是有效率的撰寫屬&gt; 於自己的 Operator。 然而 Operator 真的一定好嗎? 底下這則推文則是來自於 Darren Shepherd(CTO/Co-founder Rancher Lab ) 對於一篇由 RedHat 所發表關於 Operator 好處文章的反面看法。 其推文最後表示:「Right now invest your IT teams time in GitOps, not operators.」 快來看看 Darren 與其他網友針對這些議題的討論，並且分享看看你的想法","keywords":"","version":null},{"title":"閱讀筆記: 「Kubernetes CNI 效能比較」","type":0,"sectionRef":"#","url":"/2021/11/10/reading-note-5","content":"連結: https://www.hwchiu.com/cni-performance-2020.html Kube-OVN 不但資源吃很多，效能還很不好Canal/Calico/Flannel 三者的運算資源使用量都不多，且效能都很好Kube-Router 的效能都很差，資源使用方便也不是特別出色WeaveNet 與 Cilium 效能都不差，但是 Cilium 吃的效能很高，可說跟 Kube-OVN 同等級，而 WeaveNet 用到的資源少這次的實驗評比我認為其實能看到的東西有限，主要是不同的 CNI 所搭配的解決方案不同，目標要配合的情境也不同，雖然從圖表中可以看到 Kube-OVN 的綜合評比最差，但是其要用的場景本&gt;身就不太一樣，單純用最原始的流量互打來判別優劣其實不太對如果今天要選擇網路 CNI 的時候，可以看到效能跟資源方面， Flannel/Calico/Canal 幾乎等級都差不多，而且 Calico 還支援加密與 Network Policy 等功能。此外，目前 Flannel 也從 Kubeadm 的官方教學頁面中移除，因為太多問題且維護者沒有要修復。所以我認為如果沒有特別使用情境需求的話，可以考慮使用 Calico.Cilium 對於安全性以及 load-balancing 方面也有別的功能，就如同(5)點所提到，不同的場景有不同的需求，有些功能是獨占的。","keywords":"","version":null},{"title":"閱讀筆記: 「terraform,Terraform Module 依賴性討論」","type":0,"sectionRef":"#","url":"/2021/11/14/reading-note-7","content":"連結: https://medium.com/hashicorp-engineering/creating-module-dependencies-in-terraform-0-13-4322702dac4a Terraform 這個工具想必大家都玩過也聽過，這邊非常推薦大家升級到 0.13 版本，這個版本中解決了關於 Module 之間依賴性的問題，能夠使用原先就有的 depends_on 的語法來直接描述，而不需要按照過往以前用&gt; 各種 fake resource 等機制來完成，整個 Terraform 程式碼會更佳清晰與簡單!","keywords":"","version":null},{"title":"閱讀筆記: 「七個邁向 Cloud Native 的挑戰!!」","type":0,"sectionRef":"#","url":"/2021/11/06/reading-note-3","content":"連結: https://www.facebook.com/technologynoteniu/posts/125741595926648 本篇文章列出了七個企業想要踏入 Cloud Native 之路上最常遇到的問題 以下幫大家總結並節錄一點小內文 過於緩慢的發布週期 創新需要有能力很快速地針對每次的修改去快速發布。使用過時的技術 作者認為時時關注當前這個迅速發展的世界是非常重要的，特別是的相關開源專案。綁定特定服務供應商且成長方面缺乏彈性 當服務與特定廠商的解決方案綁定太深時，很容易遇到所有功能都由該廠商綁定，想要做什麼都會綁手綁腳。缺乏專業性人才 根據 2019 一篇調查，只有 7% 的 IT 主管再招聘與慰留人才方面沒有遇到困難安全性 人們總是當問題發生的時候才會開始注意安全性的問題，但是往往這些問題的代價都很高。儘管安全防護是一個複雜且困難的領域，但是擁有一個資安的實踐守則還是非常重要。過高的運營與技術成本 滿多企業都會使用雲端服務來減少自行維護伺服器所需的成本，然而 Cloud Native 的環境常常會用到各式各樣的元件，這些元件所消耗的成本都會隨者規模放大而有所影響，如何去最佳化你的雲端資源使用量來盡可能的減少你的花費也是一大挑戰Cloud Native 的概念難以溝通 Cloud Native 的觀念難以溝通與理解，對於任何想要導入 Cloud Native 到團隊中的企業來說，領導團隊必須要先理解到底這些解決方案的重要性與複雜性。 甚至可能還會因為微服務，容器等其他概念的認知不同而花時間理解。","keywords":"","version":null},{"title":"閱讀筆記: 「CPU Limit 造成的效能低落」","type":0,"sectionRef":"#","url":"/2021/11/12/reading-note-6","content":"連結: https://erickhun.com/posts/kubernetes-faster-services-no-cpu-limits/ 想必大家一定都有使用過 CPU Limit 的經驗，透過這個機制能夠確保每個 Container 使用的 CPU 資源量，也可以保證每個節點上面會有足夠 CPU 供 Kubernetes 原生服務 (kubelet) 使用。 然而本篇文章就要來跟大家分享一個設定 CPU Limit 反而造成效能更差的故事，故事中當 CPU 設定為 800ms 的時候，卻發現實際運行的 Container 最高大概就只有 200ms 左右，這一切的一切都是因為 Liniux Kernel 的臭蟲導致! 一個直接的做法就是針對那些本來就沒有過高 CPU 使用量服務取消其 CPU Limit，作者於文章中也探討了一些機制要如何保護與應對這些被移除 CPU 限制的服務。 這個臭蟲於 Linux Kernel 4.19 後已經修復，但是要注意你使用的發行版本是否有有包含這個修復，作者列出一些已知的發行版本修復狀況 Debian: The latest version buster has the fix, it looks quite recent (august 2020). Some previous version might have get patched. Ubuntu: The latest version Ubuntu Focal Fosa 20.04 has the fix. EKS has the fix since December 2019, Upgrade your AMI if necessary. kops: Since June 2020, kops 1.18+ will start using Ubuntu 20.04 as the default host image. GKE: THe kernel fix was merged in January 2020. But it does looks like throttling are still happening.","keywords":"","version":null},{"title":"閱讀筆記: 「Java 應用程式於容器內的效能問題」","type":0,"sectionRef":"#","url":"/2021/11/18/reading-note-9","content":"連結: https://mucahit.io/2020/01/27/finding-ideal-jvm-thread-pool-size-with-kubernetes-and-docker/ 如果有在 Kubernetes 內部署 Java 應用程式的人，千萬不要錯過這篇文章，此文章中分享 Java 應用程式關於 Thread Pool Size 的問題，同時當 Java 應用程式容器化並且部署到 Kubernettes 內之後，該怎麼設定 JVM 來讓其能夠更高效率的於容器化環境下工作","keywords":"","version":null},{"title":"閱讀筆記: 「Rancher v2.5 Release」","type":0,"sectionRef":"#","url":"/2021/11/20/reading-note-10","content":"連結: https://www.suse.com/c/rancher_blog/rancher-2-5-delivers-on-computing-everywhere-strategy/ Rancher v2.5 版本與過往的差異，這邊就來重點節錄一些改變 強化與雲端環境 EKS 與 輕量級 K3s 環境的整合，此外宣稱所有 Kubernetes 服務上面都可以安裝 Ranche 用其來幫忙管理 Rancher v2.5 釋出! 這幾天 Rancher 正式釋出 v2.5 版本，這邊就來重點節錄一些改變強化與雲端環境 EKS 與 輕量級 K3s 環境的整合，此外宣稱所有 Kubernetes 服務上面都可以安裝 Ranche 用其來幫忙管理針對美國環境要求而開發更具安全性的發行版，符合 FIPS(Federal Information Processing Standars)整合 GitOps 部署，針對大規模 Edge 叢集的自動部署解決方案 fleetMonitoring 強化，減少與 Rancher 本身的連接性，反而更加使用 Prometheus operator 來提供服務。管理人員可以直接創建相關的 CRD 提供服務，而這些資訊也都會被 Rancher UI 給一併呈現 其中 (4) 裡面還提供的 cluster-level 的客製化設定，就不需要向過往一樣要開很多個 project-level 的 prometheus 來處理，這方面輕鬆不少 資料來源： https://rancher.com/.../rancher-2-5-delivers-computing...https://github.com/rancher/fleethttps://fleet.rancher.io/https://github.com/rancher/rancher/issues/23239 針對美國環境要求而開發更具安全性的發行版，符合 FIPS(Federal Information Processing Standars)整合 GitOps 部署，針對大規模 Edge 叢集的自動部署解決方案 fleetMonitoring 強化，減少與 Rancher 本身的連接性，反而更加使用 Prometheus operator 來提供服務。管理人員可以直接創建相關的 CRD 提供服務，而這些資訊也都會被 Rancher UI 給一併呈現 其中 (4) 裡面還提供的 cluster-level 的客製化設定，就不需要向過往一樣要開很多個 project-level 的 prometheus 來處理，這方面輕鬆不","keywords":"","version":null},{"title":"閱讀筆記: 「SO_REUSEPORT 提昇 Nginx 效能」","type":0,"sectionRef":"#","url":"/2021/11/25/reading-note-13","content":"連結: https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/ 今天要來跟大家分享一個單一節點如何提高應用程式吞吐量與服務能力的方式 這個方式主要探討的是應用程式對於網路連線的 I/O 模型，試想一個常見的使用範例。 一個主要的 Process 會去聽取一個固定的 port number (ex port 80)，並且通知後面眾多的 worker 來幫忙處理這些封包連線，而這些 worker 的工作就是處理連線。 整個架構中是一個 1 v.s N 的狀況， 一個負責 Listen ，N個負責處理連線內容 而今天要分享的則是想要讓架構變成 N v.s N 的狀況， 會有 N 個 Process, 每個 Process 配上一個 Worker。 而這 N個 process 同時共享一樣的 Port (ex, port 80) 這種情況下可以減少多個 worker 共享一個 listen socket 時的各種保護機制，取而代之的則是每個 listen socket 配上一個專屬的 worker 來處理。 要達成這樣的架構非常簡單，只要透過 SO_REUSEPORT 這個 socket option 告 訴 Kernel 當前這個 PORT 可以重複使用。 當封包送到 kernel 後則是由 kernel 幫你分配封包到所有使用相同地址的 Listen Socket (Process) 根據 nginx 官方文章的測試，這種架構下對於 RPS (Request per second) 有顯著的提升，有興趣的可以看看下列兩篇文章","keywords":"","version":null},{"title":"閱讀筆記: 「SCP 工具的注意事項」","type":0,"sectionRef":"#","url":"/2021/11/30/reading-note-16","content":"連結: https://lwn.net/Articles/835962/ LWN.net 這幾天有一個文章是關於基於安全性考量下，改用其他工具取代 scp, 譬如使用 sftp 以及 rsync. 有常常使用 scp 這個工具的人可以看一下這篇文章討論的原因，以及如果要改成使用 rsync, 可以有什麼樣的參數使用","keywords":"","version":null},{"title":"閱讀筆記: 「使用 Open Policy Agent 來保護 Ingress 的誤用」","type":0,"sectionRef":"#","url":"/2021/12/01/reading-note-17","content":"連結: https://www.cncf.io/blog/2020/09/29/enforce-ingress-best-practices-using-opa/ 不知道大家有沒有聽過 Open Policy Agent (OPA) 這個 CNCF 專案? 有滿多專案的背後都使用基於 OPA 的語言 Rego 來描述各式各樣的 Policy，譬如可以使用 conftest 來幫你的 kubernetes yaml 檢查語意是否有符合事先設定的 Policy。 本篇文章則是跟大家分享如何使用 OPA 來針對 Ingress 資源進行相關防呆與除錯，一個最基本的範例就是如何避免有多個 Ingress 使用相同的 hostname 卻指向不同的 backend service. 過往可能都是靠人工去維護 ，確保沒有一致的名稱，但是透過 OPA 的概念我們可以再佈署 Ingress 到 Kubernetes 前先進行一次動態的比對，確保當前設定符合所有 Policy，得到所謂的 Approved 後才能夠佈署進去。 有興趣的人可以看看這篇文章，甚至學習一下 OPA 的使用方式","keywords":"","version":null},{"title":"閱讀筆記: 「本地開發 Kubernetes 的各種選擇」","type":0,"sectionRef":"#","url":"/2021/11/26/reading-note-14","content":"連結: https://www.dex.dev/dex-videos/development-clusters 不知道大家第一次接觸 kubernetes 的時候都是使用哪套解決方案來打造你的 K8s 叢集？ 亦或是作為一個開發者，你平常都怎麼架設 K8s 來本地測試? 這篇文章提到了作為一個 Local Kubernetes Cluster 幾個選擇，並且點出了三個需要解決的問題 Container Registry, 作為一個開發環境，應該不會想要每次測試都要將 Container Image 給推到遠方，譬如 dockerHub, Quay，這樣整體效率低落Builder, 如何有效率的幫忙建置你的應用程式，並且與 Kubernete 整合，讓開發者可以更專心於本地開發，而不要擔心太多 k8s 之間的設定https://www.dex.dev/dex-videos/development-clustersRuntime, 底層使用哪套 Container Runtime, 譬如 docker/containerd/cri-o 註: 我個人對第三點其實沒太多感覺，不覺得本地測試這個會影響太多 後面列舉了當前知名的相關專案，譬如 KIND, K3D, MicroK8S, Minikube 以及 Docker for desktop. 並且簡單的比較了一下這些本地開發的差異。 不知道大家平常本地開發時，都會用哪一套? 我個人是比較常使用 KIND 來測試，畢竟輕量化且同時支援多節點，環境也乾淨，測試起來也方便。","keywords":"","version":null},{"title":"閱讀筆記: 「Kubernetes 多租戶實作的挑戰」","type":0,"sectionRef":"#","url":"/2021/11/24/reading-note-12","content":"連結: https://faun.pub/kubernetes-multi-tenancy-a-best-practices-guide-88e37ef2b709?gi=6e43dc5ed7a 這邊跟大家分享一篇關於 Kubernetes 多租戶的相關文章，該文章中探討到底多租戶的定義，以及實現上的難易程度 多租戶可分成軟性與硬性兩種隔離， Kubernetes namespace 可以視為軟性隔離，而硬性隔離則是希望能夠更強力的隔離所有資源，文章中提到了 vClusters 的概念，連結放在最後作者認為多租戶的 Kubernetes Cluster 實際上也會帶來一些限制，讓某些功能變得不方便使用。 a. 基於 namespace 的租戶隔離方式就只能大家都同樣一個 k8s 版本，同時有一些支援 RBAC 設定的 Helm Chart 可能就不方便使用。作者這邊反思提出一個問題，為什麼真的需要多租戶的 Kubernetes 叢集，不能夠用多個單一租戶的 Kubernetes 叢集來取代? a. 真的有這樣的實例，但是其實成本過高且沒效率。 b. 如果公司內每個開發人員都需要一個自已的 k8s來操作測試，規模一大的話你每個月的成本非常可觀，因此如果可以有一個多租戶的 k8s，就可以解決這些問題多租戶實作上的挑戰，作者這邊列出幾個問題，包含使用者管理，資源分配以及如何隔離 a.基本上每個組織本身都已經有管理使用者的解決方案，譬如 AD/LDAP 等，如果要將這些使用者的認證授權與 kubernetes 整合，推薦使用 dex 這個支持 OpneID/OAtuth2 的解決方案，幫你將 Kubernetes 與外&gt; 部資料系統整合 b. 底層資源的共享，避免單一租戶過度使用導致其他租戶不能使用。資源包含了運算資源，網路頻寬等。作者列出透過 Resource Quotas 等可以幫忙限制運算資源，但是並沒有說出網路頻寬這部份該怎麼處理。&gt; 這部份我認為需要導入更多的network qos解決方案來限制，應該會需要cni以及外部交換機路由器等來幫忙 c. 最後則是互動上的隔離，要如何確保這些多租戶不會互相影響彼此，甚至攻擊彼此。這部份可能要從 NetworkPolicy 來處理網路流量，同時透過 vCluster的方式來提供相對於 namespace層級更強烈的隔離，確 保彼此不會互相影響。最後，作者列出了一些關於多租戶的可能解決方案，包含了 kiosk, loft等 結論來說就是，今天你如果有多租戶的需求，請先問自己，你需要什麼等級的多租戶管理，再來則是三個重點問題要先想清楚，你要怎麼處理 1) 如何管理使用者/租戶 2) 系統資源要如何分配與限制 3) 如何真正有效的隔離這些租戶 如果有這方面的需求，可以先看看別的開源軟體怎麼實作，再來思考是否滿足需求，如果要自己實現，有哪些好的設計值得參考!","keywords":"","version":null},{"title":"閱讀筆記: 「Container Image 的儲存挑戰」","type":0,"sectionRef":"#","url":"/2021/12/03/reading-note-18","content":"連結: https://medium.com/flant-com/cleaning-up-container-images-with-werf-ec35b5d46569 不知道大家有沒有遇過本地儲存空間滿了，再也抓不了 docker image 的慘痛經驗呢？ 本文就想要探討的是遠方 Container Image 上的管理問題，隨者時間演進，愈來愈多的版本產生，那作為管理者，我們要怎麼去&gt; 看待這些 image，放任他們無限擴張嘛？ 這些資源背後都代表一個儲存空間，也就意味額外的成本開銷。 作者想要解決的問題是，如何設計一套自動機制去刪除用不到的 image tag，保留會用到的，為了解決這個問題，要先定義什麼叫做 &quot;用得到的 image tag&quot;. 本文列舉了四種需要保留 image tag的情況 1) Production 環境正在使用的 image tag, 如果刪除了，遇到 ImagePullPolicy:Always 的情況那可真的麻煩了 2) 遇到緊急情況，應用程式需要退版，因此保留的 image tag 可不能只有當前版本，過往穩定版本也都要保留 3) 從開發角度來看需要的 image tag, 譬如我們開了一個 PR，這個 PR 有一個對應的 image tag, 再這個 PR 還沒有結束前，這個 image tag 應該都要保留讓開發者去驗證與使用 4) 最後則是特定版本號或是code name等專屬名稱 作者使用 werf 這套 k8s 建置佈署工具來幫忙，這工具除了常見的 build/deploy 外，還可以刪除遠方的 container image。 因此作者整合一套演算法，將其與 werf 整合，讓整個 CI/CD 的過程中能夠自動去產生新 的 image，並且根據需求去移除用不到的 image. 有興趣的記得點選下列原文來學習更多","keywords":"","version":null},{"title":"閱讀筆記: 「Kubernetes manageFields 討論」","type":0,"sectionRef":"#","url":"/2021/11/22/reading-note-11","content":"連結: https://github.com/kubernetes/kubernetes/issues/90066?fbclid=IwAR3d3oXBtTz2ChxmqXQmLGIrghUxN3Tz67EYWZiuzNfltqVedAlFheg3qLA 如果你機會跑過 kubernetes 1.18 版本，一定要試試看最基本的 kubectl get pods -o yaml，看看是不是內容裡面多出了非常多 f:{} 系列的檔案，導致整個 Yaml 變得非常冗長，閱讀不易，甚至想要抓取到最原始&gt; 的內容都非常麻煩。 Kubernetes 官方 Github 上還有相關的 issue 再討論這個欄位，詢問是否有辦法能夠清除。不少人都提出了一些希望的用法來處理 Issue: https://github.com/kubernetes/kubernetes/issues/90066目前看下來最簡單的做法還是透過 kubectl plugin, kubectl-neat 來幫忙完成，可以透過 krew 這個 kubectl 管理工具來安裝管理https://github.com/itaysk/kubectl-neat此工具可以將 Server 上得到 Yaml 的內容給整理最後得到最初的檔案 至於到底什麼是 managedFiles? 這個由欄位的出現是因為 1.18 以後，已經將 Server Side Apply 更新策略預設啟用而導致的，而 Server Side Apply 則是一種用來管理 Declarative 設定檔案的方式，對使用者來&gt; 說基本上完全無感，因為一切都還是透過 kubectl apply 來使用，只是到底如何判斷 當前檔案內容與系統上內容誰先誰後，誰對誰錯，甚至當有人透過 kubectl edit 去編輯內容的時候，到底該怎麼更新。","keywords":"","version":null},{"title":"閱讀筆記: 「使用 k3s Rancher Vault and ArgoCD 來實作 GitOps」","type":0,"sectionRef":"#","url":"/2021/11/28/reading-note-15","content":"連結: https://adam-toy.medium.com/implementing-gitops-on-kubernetes-using-k3s-rancher-vault-and-argocd-f8e770297d3a 這邊跟大家分享一篇 GitOps 實作心路歷程，這篇文章中總共使用下列工具 AWS, 所有環境都基於 AWS 此 cloud providerK3S, 一套由 Rancher 開發的輕量級 Kubernetes 發行版本Rancher, 管理 K3S 介面Cert-Manager, 與 Let's Encrypt 連動，管理相關憑證Vault, Secret 管理工具ArgoCD GitOps 使用工具，連動 Git Repo 與 K8sTerraform, IaaC 的一種工具 這篇文章從頭開始介紹如何整合上述工具，並且完成一個簡易的範例，透過這些範例也讓你理解每個元件對應的功能，如何使用，共重要的是從一個大範圍的視角來看，這些元件的地位，更可以幫助你瞭解整體架構 有興趣的可以閱讀全文","keywords":"","version":null},{"title":"Bash 下要如何處理 Signal","type":0,"sectionRef":"#","url":"/2021/12/19/bash-trap","content":"連結: https://linuxconfig.org/how-to-propagate-a-signal-to-child-processes-from-a-bash-script 基本 Bash 介紹文，探討 trap 的用法，如何於不同的情況下正確攔截 SIGNAL，同時如果 script 中運行的程式有無背景執行 會有什麼差異，推薦給對 Bash 不熟的讀者閱讀重新複習","keywords":"","version":null},{"title":"閱讀筆記: 「三個加強 Kubernetes 服務穩定性的經驗」","type":0,"sectionRef":"#","url":"/2021/12/10/reading-note-21","content":"連結: https://medium.com/kudos-engineering/increasing-resilience-in-kubernetes-b6ddc9fecf80 今天這篇文章作者跟大家分享一些如何加強 Kubernetes 服務穩定的方式，這篇文章這邊做個簡單摘要一下 發生問題: 作者的 k8s 是基於 Google Kubernetes Service (GKE)的叢集，運作過程中有時候會發現部分節點當掉，最後導致部分的服務不能正確使用。這邊作者團隊從兩個角度出發去改善 研究為什麼節點會一直當掉，與 Google Supporte Team 來回信件最後有找到問題點強化 Kubernetes 服務的韌性，就算有部分節點壞掉也要讓服務能夠繼續運行 ，本文主要的一些觀點也都是基於這邊發展 強化方式修正 Deployment 的數量，並且加上 Anti-Affinity，讓這些 Deployment 的副本能夠散落到不同的節點上，避免所有 Pod 都塞到同個節點，最後該節點出問題導致 Pod 全部出問題。所有需要被 Service 存取的服務都加上 Readess Probe 來確保這些服務都準備好後才會收到服務，避免一些請求被送過來確又不能正確處理加入 Pre-Stop 的使用，再裡面透過 sleep 10的方式，讓 Pod 要被刪除能夠將手上的封包請求給處理完畢 (請看註解補充) 註: 我個人認為第三點其實不太需要，比較漂亮的作法應該是實作 Singal Handler 去處理 SIGTERM 的訊號，收到此訊號後就不要再接受任何 Request 並且把剩下的工作處理完畢，當然如果這部份處理的時間過長，超過預設的 GracePeriod (30sec)，就會被 SIGKILL 給強制刪除。 要解決這個問題可能就要從應用程式下手去看如何改善，或是透過修改 Pod Spec 來提昇 GracePeriodTemination 的長短","keywords":"","version":null},{"title":"閱讀筆記: 「Kubernetes Resource Limit/Request 誤用造成的錯誤」","type":0,"sectionRef":"#","url":"/2021/12/06/reading-note-19","content":"連結: https://itnext.io/how-to-set-kubernetes-resource-requests-and-limits-a-saga-to-improve-cluster-stability-and-a7b1800ecff1 今天這篇文章探討的則是 resources 底下的 request/limit 問題。 本文作者之前遇到一個非常規律的服務警告問題，花了非常多時間與步驟去查詢，最後才發現是 Pod 裡面 Resource 的設定不夠嚴謹與完善。 舉例來說， resources: limit: cpu: 1000m request: cpu: 100m 今天假設有一個服務描述，我對 cpu 的最低要求是 0.1顆，但是極限是 1顆 且有一個節點本身有 3 顆 CPU，這種情況下，我們對該服務設定多副本運行(10個). 那根據 request 的要求，10個副本頂多只需要 1 顆 cpu，所以非常輕鬆的可以將 10 個服務運行起來，但是如何今天遇到尖峰流量 ，每個 pod 都瘋狂使用 CPU會發生什麼事情？ 每個副本的極限都是 1 顆，因此 10 個副本就可以衝到 10 顆 CPU..而系統上只有 3顆，這就會造成 CPU 完全不夠使用，最後導致每個應用程式都在搶 CPU 使用，如果沒有特別設定相關的 nice 值來處理，可能會造 成關鍵 process 無法回應(案例中就是kubelet)。 這案例中 limit/request = 10x，作者認為這數字太大，它覺得合理的大概是 2x ~ 5x，並且最重要的是要定期去檢視系統上資源的用量， limit 要設定的合理，如果本身有很大量需求，建議還要搭配 node select, affinity/anti-affinity 讓每個 pod 最好找到適合的配置方式，然後也要避免尖峰流量到來時，系統資源被吃光甚至影響到 kubelet/kube-proxy 等底層服務的運作。","keywords":"","version":null},{"title":"建置 Container Image 中的 Anti-Patterns","type":0,"sectionRef":"#","url":"/2021/12/12/docker-build-anti-pattern","content":"ref: https://jpetazzo.github.io/2021/11/30/docker-build-container-images-antipatterns/ 本篇文章分享的是建置 Container 中的 Anti-Patterns，不講哪些好反而探討哪些不好。 文內列舉了不同的主題，包含 Big images All-in-one mega imagesData sets Small imagesRebuilding common basesBuilding from the root of a giant monorepoNot using BuildKitRequiring rebuilds for every single changeUsing custom scripts instead of existing toolsForcing things to run in containersUsing overly complex toolsConflicting names for scripts and images 以下針對內文幾個部分摘錄一下為什麼作者認為是個不好的模式 Small Images Image 小本身不是什麼問題，但是有時候過度追求容量會使得一些常用有幫助的工具沒有辦法於容器內執行，這可能會導致未來要除錯時要花費更多的時間去處理，可能要研究如何重新安裝該工具等。 作者有強調這個議題是非常看環境與需求的，有些情況可能團隊根本不需要進入到容器內去執行 shell 來處理，有些可能會需要到容器內執行 ps, netstat, ss 等指令來觀察不同狀態。 作者推薦可以使用 gcr.io/distroless/static-debian11 這個 image 做為基礎然後將其之間的 busybox 給複製環境中，至少確保有基本工具可以使用 Not using BuildKit BuildKit 是 docker build 的新版建置方式，相對於舊版方式來說 Buildkit 提供了更多功能，譬如平行建置，跨平台建置甚至效能上也會比過往的更好。 為了讓舊有使用者可以無痛轉移，所以 BuildKit 完全相容既有的 Dockerfile 的語法，所以切換方面是完全無腦的。 目前新版的 Docker Desktop 基本上已經預設採用 BuildKit 來進行建置，不過某些系統譬如 Linux 的環境下，還是需要透過設定環境變數來啟用這個功能，譬如 DOCKER_BUILDKIT=1 docker build . 等方式來建置。 此外透過 BuildKit 建置的產生結果跟過往不同，所以只要看建置結果的輸出就可以判別自己是否使用 BuildKit。 剩下的8個項目就留給有興趣的讀者自行閱讀","keywords":"","version":null},{"title":"Dockerfile 內 Shell/Exec 的用法差異","type":0,"sectionRef":"#","url":"/2021/12/21/docker-shell-exec","content":"連結: https://emmer.dev/blog/docker-shell-vs.-exec-form/ Docker 基本介紹文，不知道常寫 Dockerfile 的讀者能不能分清楚 Dockerfile 內 Shell 與 Exec 兩種格式的差異 RUN, CMD, ENTRYPOINT 等指令都同時支援這兩種格式 Shell 格式就是 RUN command arg1 arg2 arg3 這種直接描述的格式，而 Exec 則是用 [] 包起來，每個參數單獨敘述，譬如 RUN [&quot;command&quot;, &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot;] 等。 本篇文章推薦 RUN 指令採取 Shell 格式而 CMD/ENTRYPOINT 都應該採用 EXEC 格式。 如果自己不清楚差異以及沒有想法為什麼平常自己這麽寫的話可以參考全文","keywords":"","version":null},{"title":"Infrastructure 各種踩雷經驗","type":0,"sectionRef":"#","url":"/2021/12/13/infra-mistakes","content":"連結: https://matduggan.com/mistakes/ 本文是作者踩過的各種 Infrastructure 雷，希望讀者能夠避免這些雷。 總共有幾大類，分別 Don't migrate an application from the datacenter to the cloudDon't write your own secrets systemDon't run your own Kubernetes clusterDon't Design for Multiple Cloud ProvidersDon't let alerts grow unboundedDon't write internal cli tools in python 其中第六點簡短扼要，大概就是「沒有人知道如何正確地去安裝與打包你的 python apps, 你真的要寫一個內部的 python 工具就給我讓他完全跨平台不然就給我改用 go/rust, 不要浪費生命去思考到底該如何安裝那些東西」 這讓我想到你的 Python 跟我的 Python 每次都不一樣，有已經不支援的 python 2.x, 還有各種可能會互相衝突的 python 3.x....","keywords":"","version":null},{"title":"又一個 0-day...","type":0,"sectionRef":"#","url":"/2021/12/11/log4j-cve","content":"相關介紹: https://www.lunasec.io/docs/blog/log4j-zero-day/ 歷史講古: https://blog.cloudflare.com/inside-the-log4j2-vulnerability-cve-2021-44228/","keywords":"","version":null},{"title":"工作20 餘年的軟體架構經驗談","type":0,"sectionRef":"#","url":"/2021/12/22/software-experience","content":"連結: https://coolshell.cn/articles/21672.html本篇文章是作者工作 20 年來的經驗分享文，內容非常好，提出了很多不同的觀點，由於是篇中文文章，所以就不幫忙節錄重點，直接列 文章中的十一個原則標題。 每個原則都非常精彩，文章底下的討論也滿有趣的。 原则一：关注于真正的收益而不是技术本身 原则二：以应用服务和 API 为视角，而不是以资源和技术为视角 原则三：选择最主流和成熟的技术 原则四：完备性会比性能更重要 原则五：制定并遵循服从标准、规范和最佳实践 原则六：重视架构扩展性和可运维性 原则七：对控制逻辑进行全面收口 原则八：不要迁就老旧系统的技术债务 原则九：不要依赖自己的经验，要依赖于数据和学习 原则十：千万要小心 X – Y 问题，要追问原始需求 原则十一：激进胜于保守，创新与实用并不冲突","keywords":"","version":null},{"title":"是時候停止使用 python 3.6","type":0,"sectionRef":"#","url":"/2021/12/25/stop-python-36","content":"連結: https://pythonspeed.com/articles/stop-using-python-3.6/ 本部落格是一個基於 Python &amp; Container 的系列介紹文，而本篇文章是其中一篇探討容器化 Python 要注意的事項。 標題非常簡單：「是時候停止使用 python 3.6了」 Python 3.6 的 EOL 時間就剛好是 2021/12，這也意味到 2022 年後 python 3.6 就不再享受官方的任何維護與更新。 文章中提到建議趕快升級，同時也要注意不要每次都拖到最後一刻才升級相關軟體，應該要把升級軟體的流程給整合到日常流程中，定期掃描定期更新， 否則只會不停的被不同版本的 EOL 追趕而已。","keywords":"","version":null},{"title":"系統設計文，探討交友app背後的設計理念","type":0,"sectionRef":"#","url":"/2021/12/29/system-dating-app","content":"連結: https://medium.com/.../dating-application-system-design... 本篇是一個系統設計文，探討設計一個如 Tinder 的應用程式該如何去思考整體架構 Tinder 這種交友應用程式有幾個特點 透過 FB 走 OAuth 登入左滑右滑配對機制聊天對話通知功能 其中 (3) 這個特色是說當使用者開啟 app 之後系統要根據一系列的條件們去推薦可能的對象，條件包含很多從 FB 中抓到的個人資料，喜好等地理位置，通常這類型的交友軟體都可以設定希望對象與自己的距離，譬如 10km 內， 50 km 內。 同時這類型的交友軟體支援多語言，支援多語言基本上就是意味多地區，簡單的說法可以說支援全世界不同地區的使用者共同使用， 因此基於效能考量上，通常不會使用單獨使用一個地區的伺服器來提供全球的服務，取而代之的則劃分地區讓每個地方都有一個稍微近的伺服器可以使用。 所以整體架構上還需要考量這類型的分散式架構設計，特別是有一些交友軟體還支援切換地點的功能，使用者可以切換到不同地區去匹配 不同地區的使用者，這意味該使用者的資料也會需要同步到不同地區的伺服器之間，因此資料部分也需要特別注意處理。 接下來就是當使用者希望配對範圍 100km 的使用者，該功能到底該如何實作，要如何將實體的地理位置劃分出來並且有辦法根據該敘述， 100km 內的使用者進行配對。 文章內有針對這部分進行詳細解釋，如何拆分不同的小區塊然後如何後續處理，有興趣的可以參閱全文","keywords":"","version":null},{"title":"閱讀筆記: 「透過 Kubefarm 來自動化幫實體機器打造基於 Kubernetes in Kubernetes 的 Kubernetes 環境」","type":0,"sectionRef":"#","url":"/2022/01/07/reading-notes-2","content":"標題: 「透過 Kubefarm 來自動化幫實體機器打造基於 Kubernetes in Kubernetes 的 Kubernetes 環境」 類別: Kubernetes 連結: https://kubernetes.io/blog/2021/12/22/kubernetes-in-kubernetes-and-pxe-bootable-server-farm/ 摘要: 本篇文章要介紹 Kubefarm 這個專案，該專案的目的是希望能夠於大量的實體機器上去創建各式各樣的 Kubernetes 叢集供不同團隊使用 為了讓整體的運作更加自動化，作者先行介紹何謂 Kubernetes in Kubernetes 這個專案，如何透過 Kubeadm 的方式於一個現存的 Kubernetes 專案 去部署 control-plane 並且透過這個 control-plane 去控管其他的 kubernetes 叢集，基本上達到的效果就如同各種 kubernetes service 服務一樣，使用者完全看不到 control-plane 的元件。 雖然透過這個方式可以很輕鬆地去創建新的 Kubernetes 叢集來使用，但是使用上覺得還是不夠方便，特別是這些實體機器還是會有不少手動的過程要處理， 為了讓整體流程更加自動化，作者團隊又基於 Kubernetes in Kubernetes 這個專案為基礎再開發更上層的專案，稱為 Kubefarm，一個如農場般可以快速於實體機器創建各式各樣 kubernetes 叢集的解決方案 Kubefarm 由三個專案組成，分別是 Kubernetes in Kubernetes, LTSP (PXE-Server) 以及 Dnsmasq-Controller 透過這三者專案的結合，實體機器會自動取得 DHCP 的 IP 地址並且透過 PXE 系統自動化安裝 OS，待一切都安裝完畢後又會自動地加入到現存的 Kubernetes 叢集中 整篇文章滿長的，是一過非常有趣的用法與研究，如果團隊是大量實體非虛擬化機器的讀者可以研究看看別人遇到什麼問題以及透過何種思路去解決的。","keywords":"","version":null},{"title":"閱讀筆記: 「DNS 5 秒 Dealy 的緣由」","type":0,"sectionRef":"#","url":"/2021/12/08/reading-note-20","content":"連結: https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts 今天跟大家分享一個 UDP 於 Linux Kernel 內的 Race Condition 問題。這問題我以前於 Linux Kernel 3.14 也有採過一樣的雷，但是到今日都還沒有一個很漂亮的解決方案，這邊就快速的跟大家介紹一下這個問題&gt; 是什麼，以及跟 k8s 有什麼關係 發生前提 使用 UDP 這種沒有重送機制的協定Kernel 有開啟 conntrack 此功能 發生條件 相同的 Client 短時間內透過 UDP (也許是不同 thread) 送出兩個 UDP 封包到外面，對於 Linux Kernel 來說，會希望透過 conntrack 來追蹤每一條連線，但是底層建立的時候會有一些會有一些機制，因此當兩個封 包同時進入的時候，有可能就會因為先後順序導致第二個封包被丟棄 可能發生問題 DNS 的請求封包預設情況下會同時透過 UDP 送出 A &amp; AAAA 兩個封包，而這兩個封包如果很巧的採到這個情況，然後你的 A 封包就沒有辦法順利解出 DNS，最後就要等五秒的 timeout 來重新發送 下偏這篇文章就是 weave works 遇到 DNS 5秒 timeout 的問題，然後仔細的將我上面所寫的總結給解釋清楚，每一個步驟發生什麼事情，什麼是 conntrack 以及暫時的 workaround 是什麼 之後會在跟大家分享目前一些解決方法怎麼做","keywords":"","version":null},{"title":"2021-年度回顧","type":0,"sectionRef":"#","url":"/2021/12/29/2021-review","content":"疫情肆虐下的 2021 年即將結束，按照慣例來個年度回顧紀錄。 這一整年完全遠端工作，算是人生經驗中非常不同的一年，台灣副業今年的發展也稍微多一點，不過整體演講數量就有明顯下降 2021 回顧 粉絲頁 - 矽谷牛的耕田筆記發文: 199 篇，發文數量比預估(183)篇還要多，整體來說學到很多東西不過也花了很多時間在閱讀與分享。Ithome 鐵人賽發文: 30 篇，這次的主題圍繞於 Rancher &amp; GitOps 的介紹，最後也很榮幸的獲得了佳作。演講紀錄: 3 篇。整年度的演講數量下降，跨時區的限制實在不方便每次半夜四點起來參與台灣社群，比較特別是的參與 HiEXPERT 2021 DevOps 領航者論壇 的討論分享。部落格原創文章: 6 篇，想寫的很多，時間不夠多...線上課程: 2 門課程，今年本來想要把 Networking 系列一口氣弄完的，沒有預期的順利，時間忙碌。 線上課程包的線上演講: 2 次，這是個一整年的計畫，每個月給課程內的學生有一次分享，還有十次才結束。 書籍出版: 1 本，人生第一本實體書籍，因應鐵人賽的結果很順利的就體驗了出書的過程。運動回歸: 因應疫情荒廢一年多的運動習慣直到有兩劑疫苗後才重新復活，直至年底整個運動表現有整體回溫，不過年底膝蓋有點小傷所以又要好好休養一下 硬舉: 210 kg臥推: 125 kg深蹲: 170 kg肩推: 75 kg 新技能學習: 今年開始認真學習並且繼續鑽研的技術是舉重，先從抓舉開始練習並且養成瑜伽的習慣來加強活動度與柔軟度企業教育訓練: 1 次，年尾很驚訝的收到一個企業教育訓練的機會，看看有沒有機會讓這個也變成常態服務長時間出遊: 3 次，下半年去了芝加哥, Reno 以及西雅圖度假放鬆，體驗不同生活。 2022 展望 保持運動習慣，想把體重給降回到 7 字頭了書籍，課程: 量力而為原創文章: 花更多時間到這一塊粉絲頁: 繼續多閱讀多分享，藉由閱讀逼迫自己學習一些平常碰不到的東西生活順遂: 工作之餘還是要定期放鬆旅遊","keywords":"","version":null},{"title":"[Cloud Design Pattern] - Ambassador 模式","type":0,"sectionRef":"#","url":"/2021/12/27/cloud-pattern-ambassador","content":"連結: https://docs.microsoft.com/.../archit.../patterns/ambassador 微軟文件中的系列好文，探討雲端方面的各種設計模式，而本篇探討的是 Ambassador 模式 想法: 想要提供更多進階的網路功能到應用程式上，譬如 TLS、circuit、breaking、routing 或 metering。應用程式不太方便修改來符合上述功能。部署一個跟原應用程式相鄰的應用程式來處理這些網路功能。 應用程式過於古老，團隊沒有辦法進行深度修改或是團隊中的應用程式使用過多的語言與框架完成，很難簡易的將這些功能給導入到既有的應用程式中 這時候部署一個全新的應用程式就可以再不修改既有應用程式的前提下來提供這些進階的網路功能。 這個模式普遍被稱為 ambassador 模式，而本篇文章就是針對該模式進行一個科普概念。 文章最後還要探討使用這種模式的一些注意事項，譬如網路的延遲會因為多一個應用程式而提升，所以使用上也要評估看看是否合適。 也有簡單的列出什麼情況適合使用 ambassador 什麼情況不適合。","keywords":"","version":null},{"title":"閱讀筆記: 「2021年回顧，因為 DB 的效能的爭論所以我女友跟我分手了....」","type":0,"sectionRef":"#","url":"/2022/01/10/reading-notes-4","content":"標題: 「2021年回顧，因為 DB 的效能的爭論所以我女友跟我分手了....」 類別: usecase 連結: https://ottertune.com/blog/2021-databases-retrospective/ 摘要: 2021 對於 DB 行業來說發生了許多風風雨雨，作者列出幾個觀察到的現象並且給予一些評論 「Dominance of PostgreSQL」 愈多愈多的人開發一個新的應用程式時首選都是 PostgreSQL 這個穩定可信賴且一直不停加入新功能的資料庫。 2010 年時 PostgreSQL 的開發團隊決定採取更為激進的方式來每年都要釋出一個主要版本的演進，其相容性的能力使得 PostgreSQL 能夠跟很多系統整合。 譬如前端介面如 Amazon Aurora, YugaByte, Yellowbrick 甚至 Google 都於 2021/10 宣布要讓 Cloud Spanner 支援 PostgreSQL 作者也嘗試從 Database Subreddit 上去爬文搜尋，基於過去一年所有發文去統計每個資料庫的出現次數，以結論來看 PostgreSQL -&gt; MySQL -&gt; MongoDB -&gt; Oracle -&gt; SQLite 等 這個過程不是非常嚴謹的統計分析，只是一個簡單的方式去觀察該論壇上大家都喜歡討論什麼資料庫而已。 「Benchmark Violence」 Benchmark 一直以來都是各個廠商展示軍火的地方，想辦法利用這些數據去說服大眾自己才是最棒的，作者列出去年三個激烈的 benchmark 討論 Databricks vs. SnowflakeRockset vs. Apache Druid vs. ClickHouseClickHouse vs. TimescaleDB 作者也有參與上述血流成河的 Benchmark 的戰場，但是這些爭論的過程中作者失去了不少朋友，甚至連女朋友也一起離開了，作者回過頭來看這一切都 不值得，此外由於現在雲端的 DBMS 也有許多可最佳化的參數，要直接去比較彼此的優劣其實沒有這麼簡單。 後面還有「Big Data, Big Money」以及「In Memoriam」 兩個不同的議題，有興趣的可以點選全文閱讀","keywords":"","version":null},{"title":"閱讀筆記: 「透過 Crossplane 與 ArgoCD 來達成應用程式與基礎建設的 GitOps 部署方式」","type":0,"sectionRef":"#","url":"/2022/01/19/reading-notes-7","content":"標題: 「透過 Crossplane 與 ArgoCD 來達成應用程式與基礎建設的 GitOps 部署方式」 類別: cicd 連結: https://medium.com/containers-101/using-gitops-for-infrastructure-and-applications-with-crossplane-and-argo-cd-944b32dfb27e 作者表示過往很多教學文章當探討到 Kubernetes 部署議題的時候，通常都不會去探討如何部署 Kubernetes 而是專注於應用程式的部署， 理由非常直觀，文章就是要專注於 Deployment 的議題，能夠讓讀者更容易地去閱讀與參考，另外一個背後的原因其實是因為 Kubernetes 部署的方式 太多種，常見的方式使用 Terraform 透過 IaC 的概念來管理，而應用程式都使用 Helm/Kustomize 完全不同的方式來管理 而作者今天想要探討的是如何透過 ArgoCD 來建設一個 GitOps 的環境，並且於上面使用 Crossplan 這個解決方案來處理各種底層基礎建設的需求，如此一來 就可以統一透過 Helm/Kustomize 的方式來描述這些基礎建設 Crossplan 很類似 Terraform 但是有者一些些微的差異 Crossplan 本身是 Kubernetes 的應用程式，所以本身的描述方式就是 Kubernetes 的 YAML 方式可以使用 kubectl, Helm/Kustomize 等方式來部署這些描述並且讓 Crossplan 來幫忙創建描述的基礎建設 由於整個 Crossplan 可以視為一個 Kubernetes 應用程式，所以直接使用 ArgoCD 的方式來部署 有興趣的可以閱讀全問","keywords":"","version":null},{"title":"閱讀筆記: 「PwnKit, 長達 12 年可以讓一般使用者輕鬆變成 Root 的 CVE」","type":0,"sectionRef":"#","url":"/2022/01/27/reading-notes-10","content":"標題: 「PwnKit, 長達 12 年可以讓一般使用者輕鬆變成 Root 的 CVE」 類別: others 連結: https://blog.qualys.com/vulnerabilities-threat-research/2022/01/25/pwnkit-local-privilege-escalation-vulnerability-discovered-in-polkits-pkexec-cve-2021-4034 CVE-2021-4034 講述的是 pkexec 此工具的 vulnerability，其影響範圍非常廣大，主要原因有 滿多系統預設都有安裝這個工具，而該工具預設有 SUID 的權限2009 後的版本就內建此安全性問題，所以請趕緊更新系統上的 pkexec任何使用者可以輕鬆地直接透過此漏洞變成 root 的身份，譬如你今天取得一個 nobody 的角色，你也是有辦法變成 root 的。 漏洞細節文章中有解釋非常多，主要是記憶體位置的處理沒有處理，當運行參數為空的時候，程式會意外地去讀取到後面的 envp 這塊用來存放環境變數的區塊，搭配 pkexec 後續的程式邏輯就有機會觸發本次 CVE 的安全性問題。 所以請趕緊更新系統上的 pkexec，確保該版本已經更新，否則任何一個使用者都可以輕鬆變成 root。 Ubuntu 使用者可參考 https://ubuntu.com/security/CVE-2021-4034","keywords":"","version":null},{"title":"閱讀筆記: 「Kubernetes 內透過 DNS-01 處理 wildcard TLS 的兩三事」","type":0,"sectionRef":"#","url":"/2022/01/21/reading-notes-8","content":"標題: 「Kubernetes 內透過 DNS-01 處理 wildcard TLS 的兩三事」 類別: introduction 連結: https://medium.com/linkbynet/dns-01-challenge-wildcard-tls-certificates-on-kubernetes-d2e7e3367328 很多人都會使用 Kubernetes 的 Ingress 讓外部可以存取的部署的應用程式，同時會透過 Ingress 搭配 Cert Manager 來處理 TLS 的憑證 大部分情況下都會使用 HTTP-01 的方式來進行域名擁有性的認證，而某些情況可能不方便透過 HTTP 來驗證的話就會採取 DNS-01 的方式透過 DNS 創建一個 TXT 的資訊來驗證域名的擁有權，本篇文章則是作者分享 DNS-01 的一些心得分享 文章開頭有介紹使用的 Stack，包含透過 Terraform 來架設模擬環境，並且使用 Scaleway DNS 作為 DNS ProviderCert-Manager 部分的 DNS Provider 要採用 webhook 的方式來動態處理請求，當 cert-manager 偵測到有新的 TLS 憑證 需求時就會透過 webhook 的方式去創建遠方的 DNS TXT 紀錄，接者 Let's Encrypt 就會透過 TXT 來判斷你是否真的擁有個域名 對 DNS-01 使用有興趣的可以看看本篇文章","keywords":"","version":null},{"title":"閱讀筆記: 「如何使用 jq 讓你的 kubectl更為強大」","type":0,"sectionRef":"#","url":"/2022/02/02/reading-notes-11","content":"標題: 「如何使用 jq 讓你的 kubectl更為強大」 類別: tools 連結: https://medium.com/geekculture/my-jq-cheatsheet-34054df5b650 作者認為 kubectl 本身提供的 label-selector, go-templates, jsonpath, custom-colume 等各式各樣功能使這個工具變得強大，能夠找到符合自己需求的各式各樣資源 然而上述的這些指令使用起來還是會覺得卡卡的，並沒有辦法滿足所有條件，而且不同選項的語法也完全不同，所以對於操作者來說其實不太便利。 順利的是 kubectl 可以透過 -o json 以 json 的格式輸出結果，這時候就可以搭配 jq 這個指令來使用，相對於前述的各種用法，作者更加推薦使用 jq 來處理，因為 jq 是一個更為廣泛的工具， 除了 kubectl 可以搭配外，很多時候都可以搭配 jq 來處理其他情況，因此掌握 jq 的語法實務上會有更多用途。 文章幾乎都是以 kubectl 為範例來介紹 jq 內的各種用法，除了基本的 read/write/filter 之外，還有各式各樣 jq 的內建函式， 有興趣的都可以使用看看","keywords":"","version":null},{"title":"閱讀筆記: 「The pains of GitOps 1.0」","type":0,"sectionRef":"#","url":"/2022/01/17/reading-notes-6","content":"標題: 「The pains of GitOps 1.0」 類別: cicd 連結: https://codefresh.io/about-gitops/pains-gitops-1-0/ 作者認為很多文章都闡述 GitOps 對於部署帶來的好處，但是軟體世界沒有十全十美的東西，所以作者就探討了 12 個其認為 GitOps 的缺點 註: 本篇文章是 2020 年底的文章，所以文章探討的內容也許當年沒有很好的解決方式，但是現在已經有了比較好的處理方式。我個人覺得文章的某些部分有點太牽強，已經假設 GitOps 是個萬能解法，什麼問題都要靠這個。就這個問題是不管有沒有 GitOps 都會存在的問題，有點為了反對而反對，與其說 GitOps 的缺點不如說沒有解決的問題。 這邊就節錄幾個文章中探討的議題，剩下有興趣的可以閱讀全文 GitOps covers only a subset of the software lifecycle 作者認為 GitOps 的精神「我想要將 Git 專案內的所描述的狀態給同步到叢集中」這點只能處理應用程式部署的問題，但是其他的流程 譬如編譯程式碼，運行單元測試，安全性檢查，靜態掃描等過程都沒有辦法被 GitOps 給處理。 作者不滿的點主要是很多 GitOps 的工具好像都會宣傳自己是個全能的解決方案，能夠處理所有事情，但是實際上卻沒有辦法。 實際上其專注的點就是應用程式部署策略為主的部分，其餘部分還是團隊要有自己的方式去處理 Splitting CI and CD with GitOps is not straightforward 過往很多團隊都會將 CI/CD 給整合到相同的 pipeline 中去處理，通常是最後一個階段就會將應用程式給部署到目標叢集，然而有外部 Controller 實作的 GitOps 解決方案會使得 CI/CD 兩者脫鉤，好處來說就是 pipeline 不需要去處理部署，只需要專心維護 Git 內的資訊，後續都讓 Controller 來處理。 然後某些團隊本來的 CI/CD 流程會於部署完畢後還會進行一些測試或是相關操作，這部分會因為 GitOps 將部署給弄走導致整個流程不太好處理，畢竟要如何讓 GitOps 部署完畢後又要可以觸發其他的工作也是額外要處理的事情 There is no standard practice for GitOps rollbacks 雖然 GitOps 的核心是透過 Git Commit 去控制當前部署的版本，那發生問題時到底該怎麼處理，如何去 rollback? 作者舉兩種範例 讓 GitOps 去指向之前的 Git Commit針對 Git 使用 Git revert 等相關操作來更新最新的內容 作者認為沒有一個標準來告訴使用者該怎麼使用以及處理 Observability for GitOps (and Git) is immature 作者認為目前現有的 GitOps 工具都沒有辦法提供下列答案 目前生產環境是否有包含 Feature XBug X,Y 是否只有存在於 Staging 環境？ 還是生產環境也有？ 註: 有什麼概念是天生就可以有這些東西的..? GitOps 有點無妄之災","keywords":"","version":null},{"title":"閱讀筆記: 「多年工作經驗總是搞砸電話面試， why ?」","type":0,"sectionRef":"#","url":"/2022/01/03/reading-notes","content":"標題: 「多年工作經驗總是搞砸電話面試， why ?」 類別: 其他 連結: https://kevin.burke.dev/kevin/phone-screens-broken/ 本篇是一個面試經驗探討文，作者闡述自己雖然已經有十年多的工作經驗，但是部分面試工作上還是沒有很辦法的去展現自己的能力 特別是那些用電話面試的經驗，而此文就是關於電話面試的小小抱怨文 滿多的電話面試都會搭配 Coderpad 這個網站來要求面試者線上進行程式撰寫，而該網站就要求你使用線上編輯器並且將所有的程式碼都統一到一個檔案中 ，同時也不一定有辦法去撰寫相關的測試規則，這對於作者來說非常不喜歡。 作者平常習慣開啟多個視窗進行開發，一邊撰寫程式一邊透過測試來驗證當前撰寫的程式是否往正確的方向前進，同時也花費大量時間去調整自己喜歡的工具來輔助所有 程式碼的撰寫。而上述所有習慣都沒有辦法於 Coderpad 的單一編輯器上去完成。 此外面試過程中還會被問各種問題，譬如為什麼這個變數這樣命名，為什麼blablabla... 對於作者來說，有些概念要到快完成時才會最佳化，就很明顯不符合電話面試這種要一次完美的特色。 最後作者也分享了一下關於電話面試的問題想法，相較於問一些實際上工作根本用不到的演算法問題，不如問一些更貼切真正工作會用到的經驗與概念，譬如 給面試者看一段 opensource 專案產生的 stack trace, 問問面試者能不能從這個 trace 看得出來大概可能是什麼問題如果你開啟一個 database transaction 過久，有可能會發生什麼問題?寫檔案到硬碟如果每次都只有寫一個 byte, 這可能會帶什麼樣的壞處?給定一個函數，請面試者描述會如何針對這個函式去寫測試案例 這讓我想到多年前也有接過電話面試直接問我 Linux 用幾個bit實作權限，但是面試官本身也非這個專頁，是哪個權限也沒辦法回答，也不能給清楚的 context，就如同教科書一樣一個問題一個答案，沒辦法針對面試者的疑問去解惑...","keywords":"","version":null},{"title":"閱讀筆記: 「Meta 如何打造一個供多團隊使用的 SLI/SLO 設定與觀測平台」","type":0,"sectionRef":"#","url":"/2022/01/04/reading-notes-1","content":"標題: 「Meta 如何打造一個供多團隊使用的 SLI/SLO 設定與觀測平台」 類別: usecase 連結: https://engineering.fb.com/2021/12/13/production-engineering/slick/ 本篇文章是 Meta 公司的技術分享文，探討內部如何搭建一個觀測 SLO 的大平台，讓不同的應用程式團隊都可以更方便地去觀察是否有達到其設定的 SLO。 文章內容有點長，這邊稍微節錄一些重點，非常推薦大家花點時間看完全文 Meta 的產品多，同時規模又大，背後又有數千的工程師不停地部署新版本，因此維運團隊必須要有一個好的方式來維運這些服務，包含預期狀態，當前狀態以及有能力去分析問題。團隊決定從 SLI/SLO 為基準點去設定預期狀態以及量測所有服務的效能。團隊決定打造一個名為 SLICK 的系統來視覺化與控管所有服務的 SLI/SLO沒有 SLICK 以前，每個服務的團隊都有各自的處理與儲存方式，所以都要花費很多時間去研究每個開發團隊的文件與用法，整體工作效率會下降過去的系統也沒有維護超過一週以上的資料，所以後續團隊也沒有辦法針對這些部分去分析。 透過 SLICK 可以讓整個 Meta 達到 每個服務都可以用一個統一的方式去定義 SLO可以維持資料長達兩年且資料的細度達到每分鐘等級有個標準化的視覺方式來呈現 SLI/SLO定期地將當前服務狀態發送到內部群組，讓團隊可以用這些報告來檢視服務的穩定度並進行改善 文章後半部分包含 SLICK 用法介紹，包含 UI 的呈現樣子，定期的報告內容以及相關的 CLI 介紹SLICK 的架構，團隊是如何設計 SLICK 這個服務，用到哪些元件以及這些元件之間個溝通流向兩個使用 SLICK 來改善穩定度的案例，這兩個案例都有簡單的去識別化，主要是介紹這些團隊發生什麼問題，如何透過 SLICK 來改善以及改善後的效能。","keywords":"","version":null},{"title":"閱讀筆記: 「使用 OpenKruise v1.0 提供更進階的 workload 部署與升級」","type":0,"sectionRef":"#","url":"/2022/01/09/reading-notes-3","content":"標題: 「使用 OpenKruise v1.0 提供更進階的 workload 部署與升級」 類別: tool 連結: https://www.cncf.io/blog/2021/12/23/openkruise-v1-0-reaching-new-peaks-of-application-automation/ Openkruise 1.0 版本釋出，該專案是 CNCF 沙盒層級的專案，主要是由阿里巴巴開發與維護，不久前的 Kubeconf 中阿里巴巴的議題也有 分享到有將此專案部署到期內部的 Kubernetes 管理平台 該專案主要是強化 Kubernetes 內應用程式的自動化，包含部署，升級，維運等面向，此外其架構是基於 Operator 去設計的，因此任何的 Kubernetes 叢集都可以安裝這個功能。 相對於原生的 Deployment 等部署方式， Openkruise 提供了 強化 workloads 的部署方式，包含支援原地更新，金絲雀等不同的升級策略。Sidecar 容器的管理，可以更方便地去定義想要自動掛到不同 workloads 上的 sidecar 容器。提供更多方便維運的功能，譬如可以針對 container 進行重啟，可以針對不同節點進行先行下載 container image，將一個應用程式給部署到多個 namespace 甚至還可以 去定義 Pod 裏面所有 containers 的啟動優先順序，如果 Pod 內的容器彼此之間有依賴性時就可以透過這個功能讓整個啟動過程更加順暢。 有興趣的可以研究看看此專案","keywords":"","version":null},{"title":"閱讀筆記: 「GitHub 上常常看到的奇妙 commit 到底是什麼？」","type":0,"sectionRef":"#","url":"/2022/02/11/reading-notes-15","content":"標題: 「GitHub 上常常看到的奇妙 commit 到底是什麼？」 類別: others 連結: https://people.kernel.org/monsieuricon/cross-fork-object-sharing-in-git-is-not-a-bug 每過一段時間都可以於 GitHub 上面看到一些看起來很嚇人的 Commit，最經典莫過於 Linux Kernel 中的各種內容，譬如檔案被砍光，README 加入一些驚嚇言論 不知道情的使用者可能會想說這個內容是真正的 Github Repo 上的東西，鐵定是真正被認可而合併進去的，所以相信不疑。 殊不知這一切其實都只是 Git 的底層設計使得一些有心人可以打造出一些以假亂真的內容，文章中就有列出兩個關於 Linux Kernel 的有趣 Commit. 文章內詳細的去解釋整個來龍去賣以及底層 Git 的設計，包含 blob, tree, commit 之間的關係，並且說明為什麼有心人可以輕鬆的產生這些以假亂真的 Commit。 舉個範例來說，Linux Kernel 的整個 Git 專案大概有 3GB 的大小，然後被 Fork 的次數高達 40000 次，請問從實作方面來考量，你會希望 每個 Fork 有一份屬於自己的 Git 專案?仰賴 Git 的底層設計，針對差異性去記錄每個 Fork 專案？ 如果是選項(1)的話，那這樣至少要準備 120TB 的資料，從儲存方面來說完全不是一個可接受的實作方式，因此自然而然的都會是基於(2)的方式去實作 因此該 Linux Kernel 的 Git 專案實際上裡面記錄了所有的 Fork 資料，而每個人針對自己的 Fork 去進行更新等行為也都會記錄到 Linux Kernel 本身的專案上。 換句話說， 那 40000 個 Fork 出來的專案實際上還是共用同一份 Git 專案，因此每個人的 Commit 都只要該 Hash 被知道，其他人都有機會去檢視與瀏覽。 而 GitHub 的 UI 又允許使用者以 Commit Hash 的方式去瀏覽每一個 Commit 的內容，因此就可以跑到主要專案去輸入自己 Fork 產生的 Commit 來產生以假亂真的 Commit 內容。 對於整體有興趣的可以觀看全文","keywords":"","version":null},{"title":"閱讀筆記: 「透過一點小技巧讓你的 Makefile 有一個更好的 Help說明」","type":0,"sectionRef":"#","url":"/2022/02/09/reading-notes-14","content":"https://daniel.feldroy.com/posts/autodocumenting-makefiles 標題: 「透過一點小技巧讓你的 Makefile 有一個更好的 Help說明」 類別: tools 連結: https://daniel.feldroy.com/posts/autodocumenting-makefiles 本篇文章使用 python 搭配 Makefile 的內建語法來輕鬆幫你的 Makefile 加上各種 Help 訊息，整個概念滿簡單的 每個 Target 後面都補上一個基於 ## 的註解說明使用 define/endef 來定義一個 python3 的內容，該 python3 會從 stdin 中去判別該 target 是否含有 ## 的字串，有的話就組合起來，並且輸出加入一個 help 的 target，將內建變數 MAKEFILE_LIST 給丟到上述的 python3 去執行 有興趣的可以看看，整個寫法非常簡單有趣。","keywords":"","version":null},{"title":"閱讀筆記: 「 Kubernetes 四種不同開發環境的比較」","type":0,"sectionRef":"#","url":"/2022/02/16/reading-notes-17","content":"標題: 「 Kubernetes 四種不同開發環境的比較」 類別: Kubernetes 連結: https://loft-sh.medium.com/kubernetes-development-environments-a-comparison-f4fa0b3d3d8b 根據 VMware 2020 的一個研究報告指出，如何存取 Kubernetes 叢集是影響開發者生產效率的最大要素，所以本篇文章就是就會針對如何去評估與挑選一個適合開發者的 Kubernetes 叢集與存取方式。 作者將 Kubernetes 叢集分成四大類，分別是 Local Cluster: 開發者會基於自己本地的電腦來創造一個本地的 Kubernetes 叢集Individual Cloud-Based Cluster: 開發者基於雲端環境來創建一個專屬於該開發者的 Kubernetes 叢集Self-Service Namespace: 使用基於 namespace 的方式來讓多位開發者共享一個 Kubernetes 叢集Self-Service Virtual Cluster: 讓 Kubernetes 來創建更多小 Kubernetes 叢集並且讓每個使用者有獨立專屬的 Kubernetes 叢集 為了比較這四種不同的叢集，作者定義了幾個不同的面向，針對這幾個面向來評比，分別是 Developer Experience: 對於開發者來說要如何開始使用叢集，包含架設的複雜度，使用的難易度以及需要的相關背景多寡Admin Experience: 對於公司的管理人員來說需要花多少心力來管理該從開發者環境，除了基本的管理還要考慮增加新使用者帶來的負擔Flexibility/Realism: 該開發環境與正式生產環境的架構相似度如何，此外開發者是否有足夠的彈性去客製化該叢集的所有設定Scalability: 該環境是否能夠根據開發需求來擴充？ 特別是針對部分需要大量使用資源的應用程式開發是否有辦法處理。Isolation/Stability: 開發者彼此之間的隔離程度如何，彼此之間的工作是否會影響彼此？ 有資安問題的時候是否會連環爆？Cost: 該解決方案的成本多寡，成本就是真正的金錢考量。 文章一開始就有列出一個結論表，對於這個議題有興趣的歡迎閱讀","keywords":"","version":null},{"title":"閱讀筆記: 「NPM 的 colors modules 打亂一堆人...」","type":0,"sectionRef":"#","url":"/2022/01/11/reading-notes-5","content":"標題: 「NPM 的 colors modules 打亂一堆人...」 類別: other 連結: https://research.swtch.com/npm-colors NPM 上一個著名的 Module Color 以及 Faker 的作者這幾天生氣氣的修改這兩個 module，於 Color 內塞入了無限循環並且印出各種亂碼 然後所有使用這兩個 module 的工具一旦更新就會發現自己的 Terminal 輸出整個爆炸，完全看不懂了。 這篇文章是 Golang 的作者 Russ Cox 分享關於這件事情的一些看法，簡單來說每個開放原始碼的授權都有提到並沒有保固這種事情，所以任何現代化的模組管理者 設計時都必須要考量到這種版本更新的可能性，並且盡可能地去減少。 文章中以 aws-cdk 作為範例， aws-cdk 最初描述時是使用 ^1.4.0 的方式來參考各種 ^1.4.0 版本的 color，結果 color 的作者就直接爆氣來一個炸彈，aws-cdk 直接更新然後建置，最後 產生出一個令人崩潰的版本。 作者認為任何一個系統要更新的時候應該都需要緩慢與穩健的去逐步更新，並且這些更新都要經過一段時間的觀察與測試來降低各種可能放到生產系統出包的可能性。 以下節錄自文章中的重點 「High-fidelity builds solve both the testing problem and the gradual rollout problem. A new version of colors wouldn’t get picked up by an aws-cdk install until the aws-cdk authors had gotten a chance to test it and push a new version configuration in a new version of aws-cdk. At that point, all new aws-cdk installs would get the new colors, but all the other tools would still be unaffected, until they too tested and officially adopted the new version of colors.」","keywords":"","version":null},{"title":"閱讀筆記: 「 談談遷移應用程式到 Kubernetes 內的失敗經驗談」","type":0,"sectionRef":"#","url":"/2022/02/14/reading-notes-16","content":"標題: 「 談談遷移應用程式到 Kubernetes 內的失敗經驗談」 類別: Kubernetes 連結: https://medium.com/@marcong_54227/unsuccessful-experience-of-migrating-applications-to-kubernetes-a896823d9b95 作者團隊於 2019 年要開發一個全新的 API 應用程式，當時部門的 IT 團隊計畫要將既有的 VM-Based 應用程式給轉換到 Container-Based，最後團隊使用了 RedHat 的系統，並且使用 OpenShift 做為容器管理平台。 從結果來看，該專案從 2020/05 一路到 2021/05 花了整整一年也沒有順利的將應用程式轉移到 OpenShift 中，其中的一些重點有 初期建設時 RedHat 有展示過如何使用 Java 基於 Fuse 開發應用程式，但是作者團隊全部都是 .Net 的經驗，因此團隊花了很多時間來學習如何使用 Fuse2020/06 時因為團隊的進度緩慢，所以 IT 團隊尋找外部的軟體顧問，尋求如何將 .Net 從 VM 轉移到 OpenShift團隊內的開發者都不擅長學習新技術，對於學習新技術這一塊非常不行。外部團隊幫忙建置了 CI/CD 系統，然後團隊內從 2020/09 開始進行程式開發與轉移，可惜直到 2021/05 依然沒有半個產品成功的用 OpenShift 作為正式生產環境與此同時，外部團隊也撰寫了幾個 .NET 示範應用程式來展示容器化的注意事項，然而團隊本身對 Container 的知識非常薄落，所以團隊人員沒有辦法參考這些範例程式來改善自己開發過程 最後團隊內又針對不同團隊給予不同的想法 Application TeamServer TeamNetwork TeamIT Management Team 譬如 Application Team 的開發人員都只滿足自身的技能，而且拒絕學習新技能，誇張的是一年過後團隊內的人也沒有辦法撰寫 dockerfile 或是使用 docker build. 後續還有一些針對不同團隊的想法與總體建議，整體來說非常真實，一個血淋淋的轉換案例。","keywords":"","version":null},{"title":"閱讀筆記: 「 取代 Docker Desktop 的高效率開發環境」","type":0,"sectionRef":"#","url":"/2022/02/18/reading-notes-18","content":"標題: 「 取代 Docker Desktop 的高效率開發環境」 類別: Container 連結: https://medium.com/partoo/replacing-docker-desktop-with-a-more-efficient-development-environment-582c61c50984 作者認為 Docker Desktop 是一個非常好的開發環境工具，能夠簡化很多設定讓開發者更容易的開發應用程式，但是對於 Windows/Mac 的使用者來說 Docekr Desktop 實際上也是先運行一個基於 Linux 的 VM 並且於其中運行 Docker Container。這種架構實際上帶來了一些使用上的缺陷，包含 FileSystem 的處理效能非常不好，不論是使用 cahced 或是 gRPC-fuse 檔案系統還是沒有辦法得到很好的效能。資源使用有問題不如預期，作者設定希望最多使用 6GB 結果最後卻使用到了 15GB，幾乎吃光系統所有記憶體官方幾乎沒有文件去探討該 VM 的存取方式(雖然滿多人會用 nsenter 進入)，所以很難把一些本地檔案給直接放到 VM 內來提昇儲存相關的問題，變成所有的儲存都只能用 docker volume 來處理。 作者的公司 Partoo 採取了 VM + Vagrant + Ansible 的方式來創建開發者環境，讓每個加入團隊的開發者都可以輕鬆簡單的建設期開發環境 並且文章中也探討如何於本地端使用 Vistual Studio Code, PyCharm 來編輯 VM 的檔案並且順利的進行應用程式開發。 從效率來看，相對於直接使用 Dodcker Desktop 來看，作者團隊的測試程式基於自己的 VM 提升了將近 30% 的效能，主要的問題就是儲存系統與 Docker Container 之間的掛載關係差異。","keywords":"","version":null},{"title":"閱讀筆記: 「Paypal 如何調整 Kubernetes 讓其規模達到四千節點，20萬個 Pod」","type":0,"sectionRef":"#","url":"/2022/02/23/reading-notes-20","content":"標題: 「Paypal 如何調整 Kubernetes 讓其規模達到四千節點，20萬個 Pod」 類別: usecase 連結: https://medium.com/paypal-tech/scaling-kubernetes-to-over-4k-nodes-and-200k-pods-29988fad6ed 摘要: Paypal 過去一直都使用 Apache Mesos 來運行其大部分的服務，而其最近正在針對 Kubernetes 進行一個評估與測試，想瞭解如果需要轉移到 Kubernetes 會有哪些問題需要挑戰與克服。 本篇文章著重的是效能問題，原先的 Apache Mesos 可以同時支持一萬個節點，因此 Kubernetes 是否可以拿到相同的效能 而本文節錄的就是擴充 Kubernetes 節點中遇到的各種問題以及 Paypal 是如何修正與調整讓 Kubernetes 可能容納盡可能更多的節點。 Cluster Topology 三個 Master 節點與三個獨立的 ETCD 叢集，所有服務都運行於 GCP 上。工作節點與控制平面的服務都運行於相同的 GCP Zone 上。 Workload 效能測試方面是基於 k-bench 去開發的測試工具，基於平行與依序等不同方式來創建 Pod/Deployment 兩種資源。 Scale 測試初期先以少量的節點與少量的 Pod 開始，接者發現還有提升的空間就會開始擴充 Pod 與節點的數量。測試的應用程式是一個要求 0.1m CPU 的無狀態應用程式。最初的工作節有點 4 個 CPU，根據測試可以容納大概 40 Pod 左右。 接者就是不停地擴充數量，先從一千個節點開始，接者調整Pod 的數量直到 32,000 個 Pod。最後擴充到 4,100 個節點並且配上 200,000 個 Pod. 過程後期有調整節點的 CPU 數量讓其能夠容納更多的 Pod 數量 文章接下來開始針對 API Server, Controller Manager, Scheduler, ETCD 元件遇到的問題並且如何解決，中間提到了不少參數，這部分應該是大部分使用者都比較不會去研究與使用的參數 因此我認為本篇文章非常值得閱讀。 ETCD 的部分遇到很嚴重的效能問題，作者團隊觀察到大量的 Raft 溝通失敗個訊息，觀測到跟硬碟寫入速度有關，然而 GCP 沒有辦法單純增加效能，必須要同時提升硬碟空間，所以使用上彈性不變。 不過就算採用 1TB 的 PD-SSD ，當 4 千個節點同時加入到 Kubernetes 時依然會遇到效能上的問題，團隊最後決定使用本地端的 SSD 來想辦法改善寫入速度，結果又遇到 ext4 的一些設定 過程很多問題也很多解決方式。 結論來說: k8s 複雜","keywords":"","version":null},{"title":"閱讀筆記: 「macOS 的 fsync 實作其實跟大家想像的完全不同 」","type":0,"sectionRef":"#","url":"/2022/02/21/reading-notes-19","content":"標題: 「macOS 的 fsync 實作其實跟大家想像的完全不同 」 類別: others 連結: https://mobile.twitter.com/marcan42/status/1494213855387734019?t=TyXUEg-2LcbNiKkv0JVOsg&amp;s=09 以下節錄自該留言串 「As it turns out, macOS cheats. On Linux, fsync() will both flush writes to the drive, and ask it to flush its write cache to stable storage. But on macOS, fsync() only flushes writes to the drive. Instead, they provide an F_FULLSYNC operation to do what fsync() does on Linux.」 簡單來說就是 Linux 的 fsync 會執行兩次動作，最終讓當次修改給寫回到底層儲存設備，而 macOS 的版本卻不會真的確認寫回硬碟，所以這個導致很多仰賴 fsync 的應用程式就會有一些不預期的行為，這也是為什麼首篇發文的內容 「Well, this is unfortunate. It turns out Apple's custom NVMe drives are amazingly fast - if you don't care about data integrity. If you do, they drop down to HDD performance. Thread.」","keywords":"","version":null},{"title":"閱讀筆記: 「Golang 原始碼的的版本控制歷史」","type":0,"sectionRef":"#","url":"/2022/03/02/reading-notes-23","content":"標題: 「Golang 原始碼的的版本控制歷史」 類別: others 連結: https://research.swtch.com/govcs 本篇文章是 rsc 來仔細介紹 golang 的發展歷史，主要是針對整個開發過程的版本控制轉移以及一些有趣的 Commmit 舉例來說，如果你去看 golang 的 commit 會發現第一筆 commit 是 1972 年的內容，並且該 commit 增加了一個 src/pkg/debug/macho/testdata/hello.b 的檔案 而以實際狀況來說，前面四筆都是假的 commit，第五筆 commit 才是 golang 開發的第一筆 commit，這之間的緣由牽扯到版本控制的轉變。 以 Golang 來說，其經歷了四次轉變化，從最初的 Subversion 到 Perforce 到 Mercurial 到 Gerrit 其中 golang 正式對外公開是發生於 Mercurial 的過程中，而這些假的 commit 也是這個時間點由 rsc 自己產生的，當作一個復活節彩蛋的概念 有興趣的可以閱讀全文","keywords":"","version":null},{"title":"閱讀筆記: 「大家總是喜歡誇大自己的工作時數」","type":0,"sectionRef":"#","url":"/2022/02/28/reading-notes-22","content":"標題: 「大家總是喜歡誇大自己的工作時數」 類別: other 連結: https://drmaciver.substack.com/p/people-dont-work-as-much-as-you-think 本篇是一個心態探討文，作者探討了到底一天的有效工作時間有多少，並且認為人們宣稱的工作時間通常都不准，很多時候只是一種想要讓人看到自己工作很久的心態而已。 從作者的經驗來說，作者認為軟體開發，作家，顧問等行業都會有這類型的特性，一個有效率的工作者其實根本不可能一整天連續工作八小時。 一開始作者先列舉自己一天可以達到的事情有哪些(可達到不代表全部都會做) 1-2 小時的重度耗腦力工作，如果事情沒有太複雜有時候可以到三小時左右。不太動腦 code review 或是閱讀不會花費太多精力，整體來說算可以輕鬆進行4 小時的面對面教學與指導4-6 小時左右的常規工作，譬如重構程式碼4-6 小時左右的沈浸式工作，譬如研究一個困難的 Bug。這類型的工作通常需要當天身心狀況良好同時也沒有人會一直打斷你，通常一年中發生的次數不太高。 總體來說，作者一天認真工作的時間大概就是四小時左右，當然有需求時當然可以提升整體的工作量，不過作者認為這會影響身心健康，所以不是很喜歡。作者也認為「愈需要創作的工作其實真正工作的時間愈少」 很多工程師看起來工時很長其實有時候上班也都在滑 Twitter, Reddit 等各種網站，真正的工作時長其實都不如自己宣稱的。因為每個人的精力有限，不可能長時間執行高度專心的工作，很多時候花一堆時間去拼工作量結果得到的是每件事情都辦不好， 與其如此不如找到一個適合自己的工作方式，讓自己能夠有效率地去做好每件事情。 這邊要特別注意的是 請評估自己每天到底可以工作多少小時，然後不要嘗試工作超時不要因為自己的精力與工作時數多寡而感到羞愧，好好面對自己不要為了那些會打破你上述規則的工作而加班 如果今天有人要丟工作給你，請友善禮貌地回答「我很願意幫忙你，不過我目前手頭上正在忙 XYZ，你想要做的事情優先度有比較高嗎？」 最後作者針對工程師列出一個不同類型工作的適合工作時數 1-2 小時的專注工作，如 coding, debugging1-2 小時的無腦工作，如參加會議， code review1-2 小時的&quot;可被找到&quot;的時間，譬如 Slack 上可以被找到，可以回答各種問題結論來說一天工作不會超過四小時","keywords":"","version":null},{"title":"閱讀筆記: 「20年工程師生涯教會我的 20 件事情」","type":0,"sectionRef":"#","url":"/2022/03/07/reading-notes-25","content":"標題: 「20年工程師生涯教會我的 20 件事情」 類別: others 連結: https://www.simplethread.com/20-things-ive-learned-in-my-20-years-as-a-software-engineer/ 本篇文章是作者談談自己工程生涯近 20 年的經驗，聊聊 20 個作者覺得很值得跟大家分享的一些想法與經驗。 開頭作者就說，所有的建議與經驗分享最重要的就是脈絡，沒有了脈絡所有的建議可能都沒有任何用處，甚至可能會對你有害 所以作者先表明自己的一些經驗，包含 前半職業生涯作為一個小型公司與新創的軟體工程師。顧問人生同時加入規模非常大的公司創立 Simple Thread 這間公司從兩人團隊一路成長至今 這邊節錄幾個經驗，整體來說我認為全部都滿不錯的，都推薦當作心法去看待，甚至也可以作為一些討論的主題。 I still don’t know very much: 不知道你有沒有很常聽到別人說「你怎麼會不知道 BGP 怎麼運作？」「你怎麼沒有聽過 RUST ?」，軟體世界的有趣點不論你怎麼學習，每天都有會展新的技術被發展出來 你於你專精的領域努力了十年，對於其他人的領域你會發現還是有很大一片空白需要學習。 所以請認知這一點，軟體世界龐大本來就很容易有不知道的事情，然後保持謙虛的去面對這一切，不要用一種別人都是白癡的態度來質疑別人 The best software engineers think like designers 最好的軟體工程師其思路不單純只是滿足功能，而是如同一個設計師一樣，會去思考設計的這個軟體各種使用，包含外部 API的設計，使用者介面，甚至要去考慮 使用者會怎麼使用，使用者為什麼會想要用等，將使用需求放到開發的第一順位才能夠打造一款真的是很適合使用者使用的軟體。 Look for technological sharks 那些發展許久至今仍活躍的技術能夠於現在這個迭代快速的世代存活下來必定有其價值與原因，如果今天想要替換掉這些技術一定要有很好的理由與評估，不要單純冒然基於 新技術好像比較好就去替換。這些舊有的技術與工具也許用起來沒有現在這麼潮，但是其穩定性與良好的表現絕對能夠讓你晚上好好睡覺 Every system eventually sucks, get over it 軟體開發中沒有一個正確的架構，所有的人無時無刻都在處理相關的技術債，所有開發的介面過一段時間都會因為情境不同而需要改動與重寫，你撰寫的測試永遠都不足夠。 但是這些概念並不能當作一個不往前邁進的理由，相反的就是因為知道沒有一個完美的架構，所以才要更有系統地去持續精進整個架構，永遠都有值得改善與變好的地方 透過不停的循環這個開發流程才能夠讓整個軟體愈來愈好。 文章中還有其他 16 個非常實用的建議，非常推薦大家閱讀閱讀","keywords":"","version":null},{"title":"閱讀筆記: 「ClickHouse 與 Elasticsearch 的比較」","type":0,"sectionRef":"#","url":"/2022/02/25/reading-notes-21","content":"標題: 「ClickHouse 與 Elasticsearch 的比較」 類別: other 連結: https://developer.aliyun.com/article/783804 這篇文章內容非常精彩，從不同層面去探討 Elasticsearch 與 ClickHouse 這兩套解決方案的差異，探討了包含 分散式架構儲存架構，包含寫入資料的設計，底層 Segment 與 DataPart 的差異以及 Schemaless 特性帶來的影響查詢架構，包含底層引擎是如何計算使用者的輸入效能測試，針對不同的場景來比較效能 文章很長且滿多技術坑，適合對於 Elasticsearch 有維運與管理有經驗的使用者","keywords":"","version":null},{"title":"閱讀筆記: 「如何判別到底要不要使用 Service Mesh」","type":0,"sectionRef":"#","url":"/2022/03/13/reading-notes-27","content":"標題: 「如何判別到底要不要使用 Service Mesh」 類別: Network 連結: https://medium.com/google-cloud/when-not-to-use-service-mesh-1a44abdeea31 本篇文章是一個經驗探討文，想要探討近年來非常熱門的網路網格(Service Mesh) 到底導入時要怎麼抉擇與判斷。 Service Mesh 如果用得正確與適當，能夠為團隊帶來很多優勢，可以讓團隊更專注於軟體的服務上，讓 Service Mesh 幫忙提供各種方便的功能。 但是如果使用錯誤則可能只會造成整體架構更加複雜，同時也沒有解決什麼真的重點問題，一切只是疊床架屋的空殼而已。 採用 Service Mesh 要儘早 作者認為到底要不要導入 Service Mesh 是一個專案初期就要決定的事情，即使 Istio 網站有特別教學如何將專案從 non-MTLS 給轉移到基於 Istio MTLS 的過程 但是作者說真的跑過這些流程就知道絕對不是官網寫的三言兩語這麼簡單，有太多額外的事情要考慮，譬如上層安裝的服務，網路分層設計等，這些會因為有沒有 Service Mesh 而有不同的決定 不要當 Yes Man 作者體驗過最多的案例就是每個團隊看到下列問題都是不停的說 YES，然後最後就直接無腦導入 Service Mesh 我們需不需要強化資安 使用 mTLS 能不能強化資安 mTLS 是不是很難管理 Service Mesh 是不是可以讓 mTLS 便於管理 連續四個 YES 下來就直接無懸念的導入 Service Mesh，殊不知 因此作者接下來就列出幾個要導入 Service Mesh 前需要仔細思考過的問題 是否有計畫於當下或是未來使用到 Serivce Mesh 的所有功能 Service Mesh 的功能除了 mTLS 外還有各式各樣跟流量有關的管理，譬如 A/B Testing, 金絲雀部署等。 透過 Service Mesh 能夠讓應用程式不需要實作這些功能而依然可以享有這些功能的好處。 所以作者認為團隊中的所有人都要仔細的注意，到底你們即將採用的 Service Mesh 有哪些功能可以用，這樣可以避免應用程式重複開發相同功能。 作者也提到不需要第一天就決定好要採用什麼功能，但是至少要仔細理解自己採用的解決方案到底有什麼功能，然後未來改善架構的時候可以即時的想起來這功能有提供 團隊中是否有人對於 Service Mesh 有足夠的理論或是實戰理解？ 作者看到的非常多團隊很多人根本不理解 Kubernetes 以及 Service Mesh 但是就想要導入 Service Mesh。 由於對 Service Mesh 完全不理解，連其實作的概念都不同，所以當問題發生的時候就什麼都不能做，因為根本不懂也不知道該從何下手 請花時間學習與理解你要使用的專案，以確保你有足夠的背景知識去使用與除錯 除了問題之外，作者也認為要導入 Service Mesh 到生產環境並不是單純的建構一個 Hello World 這麼簡單，還有很多事情要考慮，譬如 自動化監控與追蹤除錯與已難雜症排除 整篇文章非常的棒，有興趣的可以詳細閱讀","keywords":"","version":null},{"title":"閱讀筆記: 「Facebook 內的文化特別之處」","type":0,"sectionRef":"#","url":"/2022/03/18/reading-notes-29","content":"標題: 「Facebook 內的文化特別之處」 類別: usecase 連結: https://chinese.catchen.me/2022/02/unique-engineering-culture-of-facebook.html 作者作為一個 Meta 工作七年的員工，分享了一些認為 Facebook 頗有特色的文化，這些特色文化並沒有辦法直接斷言是好是壞，一切都是看用什麼角度去看待。 工程師對產品結果負責 年度績效評估時要如何去評估一個工程師的績效一直都是個不簡單的問題，作者提到對於 Meta 內部的高級工程師(不確定正確級別代號是什麼)來說，其績效並不是單單的只去看技術用的好不好，程式寫的好不好 更多的反而是這個產品是否有真正的商業成長結果。 作者認為這種鼓勵從下而上解決問題的思路能夠讓產品的發展更佳有效率與有意義，舉例來說 假如今天工程師的績效是完全基於技術方面的呈現，而專案負責人(PM)的績效可能是該專案對於使用者的黏著度，兩者績效不一致的情況下很容易發展出不同的開發與演進策略 工程師為了達到自己的績效其發展的路線就不一定可以為產品帶來更好的使用者黏著度，反之亦然，為產品帶來更好使用者黏著度的改善並不一定可以讓工程師看到很好的表現績效。 但是一旦當工程師與 PM 的目標一致，整體的合作就會更加融洽也目標，這也是為什麼 Meta 的工程師非常了解自己產品的指標跟數據，還會花時間去分析產品數據與使用者分析報告，透過自己的理解來思考到底要怎麼去改善產品的方向，而不是完全等待 PM 發號司令。 基礎架構被視為一個產品販售 Meta 內部的基礎架構某程度也被視為是一個產品，公司內的其他工程團隊則是該產品的潛在用戶，所以開發該產品的團隊本身也要努力的去推銷這個產品，去說服為什麼要使用這個架構，使用這個架構能夠帶來什麼樣的好處。 作者以早期的 Reat, React Native 為範例，早期該產品於公司內推廣也是四處碰壁，並非外部所想像的一推廣就廣受歡迎與嘗試。 由於基礎架構被視作是產品，所以如果其「商業」表現不如預期的話，該專案也是會被砍掉的，這類型的模式搭配上述的概念其實非常有趣 所有的專案都想要長期存活都必須要證明其有價值，就算是內部架構也要證明其對內部其他工程團隊有價值 這種方式也降低了「只專注技術而不考慮使用者需求的」的開發方式，這讓我想到大家最愛講的「Service Mesh」.... 帶來什麼效應不確定，但是很潮就是享用...，","keywords":"","version":null},{"title":"閱讀筆記: 「Terraform 生態下的五個相關輔助工具」","type":0,"sectionRef":"#","url":"/2022/03/21/reading-notes-30","content":"標題: 「Terraform 生態下的五個相關輔助工具」 類別: terraform 連結: https://betterprogramming.pub/5-essential-terraform-tools-to-use-everyday-e910a96e70d9https://medium.com/geekculture/my-jq-cheatsheet-34054df5b650 隨者 IaC 的概念落地開花，愈來愈多團隊嘗試使用 Terraform 來管理各式各樣的 infrastructure，作者本篇文章分享五個自己每天使用的 Terraform 輔佐工具，分別是 TFSwitchTFLintTerraform-docsCheckovInfracost TFSwitch: 如果環境中目前因為歷史因素沒有辦法統一轉移到相同版本的 Terraform 使得你必須要用不同版本的 Terraform 來處理不同的專案的話，可以透過 TFSwitch 來幫助你快速地切換版本 TFLint: 就如同大部分的 Lint 工具一樣， TFLint 針對 Terraform 的工具，特別是跟特定 CloudProvider 整時候會有更多的錯誤偵錯，將該工具整合到 CI/CD pipeline 中更可以幫助團隊避免合併一個有問題的 Terraform code. Terraform-docs: 這是一套能夠將你的 Terraform module 直接產生對應 Markdown 格式文件的工具，如果本身有撰寫 Terraform Module 的團隊都可以使用這工具試試看，看看產生的文件是否可以滿足基本需求 Checkov: 這是一套支援 Terraform 的靜態程式碼掃描工具，可以用來檢查是否有可能的安全性漏洞與不良好的設定，目前預設大概有 750+ 以上的預設規則， Infracost: 這工具會的目的就如同專案名稱一樣，根據創造的雲端資源幫你估計這些資源的實際花費，對於要控管成本的團隊來說，可以提供一個粗略的金額概念，畢竟如網路流量等相關付費還是要實際上線才知道，但是可以快速地針對不同的 infra 直接列出大概的金額差異，搭配得宜對於整體工作流程還是有幫助的。","keywords":"","version":null},{"title":"","type":0,"sectionRef":"#","url":"/2022/02/04/reading-notes-12","content":"標題: 「如何用 2297 個 Linux Kernel Patches 來重新整理所有的 header file 並提升整個 Kernel 建置時間高達 78%」 類別: 其他 連結: https://www.phoronix.com/scan.php?page=news_item&amp;px=Linux-Fast-Kernel-Headers 摘要: Linux Kernel 的長期貢獻者 Ingo Molnar 花了一年多的時間整理 Kernel 內的 Header 架構，一口氣提交了 2297 個 patches，其中影響 的檔案數量有 25,288 個，並且加入了 178,024 行數，移除了 74,720 行。 這一系列的改動直接重新整理 Linux Kernel 內將近 10,000 個不同的 header 檔案，這次的整理將過去 30 年累積的各種你呼叫我，我呼叫你這 種「Dependency Hell」問題給一起處理掉，結果論來說提升了整體建置時間 50% ~ 80 %","keywords":"","version":null},{"title":"閱讀筆記: 「為什麼 3A 大作的遊戲室都不愛喜歡使用 STL」","type":0,"sectionRef":"#","url":"/2022/03/23/reading-notes-31","content":"標題: 「為什麼 3A 大作的遊戲室都不愛喜歡使用 STL」 類別: usecase 連結: https://mobile.twitter.com/m_ninepoints/status/1497768472184430600 熟悉 C++ 語言的讀者必定聽過 STL，而本篇推特系列文則是用來解釋為什麼 3A 大作的遊戲室都不愛使用 STL，這邊節錄一些概念與想法 STL 的實作通常會因為不同平台與編譯器而會有不同變化，但是對於遊戲業者來說很多時候會希望能夠針對所有平台能夠有一個統一一致的實作跨平台的統一實作對於開發者來說可以帶來很多好處，譬如除錯可以更有效率，不需要針對每個平台獨立研究問題STL 由於不同平台的實作方式都不同，所以發生問題或是要客製化都會變得稍嫌麻煩，變成還要根據平台去研究與處理，整體來說就是煩某些情況下 STL 的實作是犧牲效能來完成的，但是效能反而是業者所需要的 覺得本篇推文非常有興趣，對這系列有興趣的可以看看大家的討論","keywords":"","version":null},{"title":"閱讀筆記: 「Linux 5.17 將使用 BLAKE2s 來替代 SAH1 來達到更安全更快速的隨機亂數產生器」","type":0,"sectionRef":"#","url":"/2022/01/24/reading-notes-9","content":"標題: 「Linux 5.17 將使用 BLAKE2s 來替代 SAH1 來達到更安全更快速的隨機亂數產生器」 類別: other 連結: https://www.phoronix.com/scan.php?page=news_item&amp;px=Linux-5.17-RNG Linux Kernel 內亂數子系統的維護者近期遞交了一個將 SAH1 給全面替換為 BLAKE2s 的相關 Patch 相對於 SHA1 來說， BLAKE2s 本身更為安全，同時計算速度也更快，這邊也可以參考下列這篇 2017 的文章https://valerieaurora.org/hash.html 來探討不同 HASH 演算法的一些狀態，雖然沒有及時更新到 2022 的狀態，但是如果 2017 都 不安全的東西現在就更不應該使用，譬如文章中提到 SAH1 於 2017 就被 Google 用(6500 CPU或是110 GPU)的實驗來證實有衝突，建議停止使用。","keywords":"","version":null},{"title":"閱讀筆記: 「視覺化系統內 iptables 規則」","type":0,"sectionRef":"#","url":"/2022/02/07/reading-notes-13","content":"標題: 「視覺化系統內 iptables 規則」 類別: tools 連結: https://github.com/Nudin/iptable_vis 這是一個非常有趣的小工具，目標就是視覺化系統中的 iptables 規則，把 chain 之間的關聯給視覺化呈現出來 整個專案非常小，該專案主要會透過 awk 來解析系統中的規則並且產生特定輸出，接者使用別的軟體將該輸出給轉換成圖檔 有興趣的都可以使用看看","keywords":"","version":null},{"title":"閱讀筆記: 「透過 Helm 與 Terraform 來自動 Re-new Cloudflare origin CA」","type":0,"sectionRef":"#","url":"/2022/03/11/reading-notes-26","content":"標題: 「透過 Helm 與 Terraform 來自動 Re-new Cloudflare origin CA」 類別: usecase 連結: https://awstip.com/auto-renew-cloudflare-origin-ca-with-terraform-and-helm-d28be3f5d8fa?source=linkShare-91a396987951-1645539866&amp;gi=a18b2bbd9604 本篇文章是過工具介紹文，探討如何基於 Helm 與 Terraform 這兩個不同層級的工具來處理 Cloudflare 的憑證。 Why Cloudflare 根據 W3Techs 的調查顯示， 81.2% 的網站都使用 Cloudflare 來提升讀取速度或安全防護。 透過 CDN 的概念與機制，網站可以讓全球使用者有更快的讀取速度，此外也愈來愈多的網站會透過 Cloudflare 來處理如機器人, DDOS 之類的流量攻擊，畢竟要自己架設網站處理這些攻擊非常困難 因此讓 Cloudflare 這類型的網站來幫忙過濾與處理能夠讓團隊更專注於本身的業務開發與維運 Kubernetes 想要在 Kubernetes 內妥善管理所有使用的憑證其實也是一個麻煩事情，除了要能夠設置正確來創立憑證外，能夠於到期前自動 re-new 也是一個不可或區的功能。 Kubernetes 內跟憑證有關的最知名專案我想就是 Cert-Manager，而 Cloudflare 也基於此專案撰寫了相關的 Kubernetes Controller，如 Origin CA 等 因此本文使用的功能與示範都會基於 cert-manager 與 Cloudflare 的架構。 目的 本文的目的是希望能夠將過往手動的繁瑣步驟給自動化，讓 Kubernetes 可以獲得 Cloudflare 提供的好處，如憑證與相關域名等。 內文是基於 Terraform 作為出發點，然後透過 Kubernetes Provider 的方式來與之互動，一步一步的安裝各種資源最後成功於叢集內獲得相關域名的 SSL 憑證以及其他資源","keywords":"","version":null},{"title":"閱讀筆記: 「一個用來管理 Kubernetes 開源工具的開源工具」","type":0,"sectionRef":"#","url":"/2022/03/25/reading-notes-32","content":"標題: 「一個用來管理 Kubernetes 開源工具的開源工具」 類別: tools 連結: https://github.com/alexellis/arkade 作者因應過去於 Kubernetes 的教學與開源過程中，必須要一直不停地去安裝各式各樣必備的工具而感到厭煩，譬如每次都要安裝 kubectl, kind, kubectx 等各種常見工具 而每個工具又會有不同的版本，每次都要專寫相關的安裝流程都很麻煩，因此作者萌生出開發一個能夠安裝這些工具的開源工具, arakde. 該工具用起來非常簡單，同時也支援不同版本的工具，除了基本 CLI 工具外也支援 Helm App 的安裝，我個人認為光工具本身就非常好用了，譬如可以透過該指令輕鬆的安裝不同版本的下列工具 divehelmghjqk3dkindkubectlk9skailopaterraform ... 如果你常常需要撰寫文件去分享安裝各種文件的需求，也許可以考慮使用看看此工具來簡化流程","keywords":"","version":null},{"title":"閱讀筆記: 「kubectl delete 的行為跟 docker delete 完全不同」","type":0,"sectionRef":"#","url":"/2022/04/06/reading-notes-36","content":"標題: 「kubectl delete 的行為跟 docker delete 完全不同」 類別: kubernetes 連結: https://www.acritelli.com/blog/kubectl-delete-sigkill/ 熟悉 Linux 系統的人想必都了解 Signal 的概念，特別是幾個常見的如 SIGTERM, SIGKILL 等， 作者的團隊嘗試透過 SIGKILL 的行為來驗證與測試團隊內部署的 Kubernetes Pod，特別是當遇到 ungraceful shutdown 的情境時這些 Pod 會如何運作。 團隊嘗試透過 kubectl delete 的方式來刪除這些 Pod，但是實驗過程中發現 --grace-period 這個參數的運作行為與團隊的預期行為不同。 kubectl delete 得說明文件中特別指出 --grace-period=-1: Period of time in seconds given to the resource to terminate gracefully. Ignored if negative. Set to 1 for immediate shutdown. Can only be set to 0 when --force is true (force deletion). 作者看文這段文字說明後滿腦問號，提出兩個問題 grace-period 設定為 1 的 immediate shutdown 是直接送出 SIGKILL 嗎？ 還是說會有一秒的間隔時間才發送 SIGKILL?grace-period 設定為 0 是代表沒有間隔，所以也是馬上送出 SIGKILL 嗎? 還是說其只是單純將資源從 k8s API 中移除而沒有等待而已？ 作者認為文件沒有辦法解決這些問題，所以設計了一些實驗來測試 --grace-period=1 的實驗結果是 送出 SIGTERM等待一秒送出 SIGKILL 作者對於這個行為感到不解，認為 &quot;immediate shutdown&quot; 應該就是要馬上關閉呀，怎麼可以送 SIGTERM 給 Pod 讓 Pod 有機會可以優雅的結束一切？ 因為對於這行為的認知不同導致作者團隊的測試行為沒有辦法順利完成。 接下來測試 --grace-period=0 &amp; --force=true 文件中說明這樣設定會立刻將該資源從 API Server 內給刪除並且避開 graceful 的階段。 最後測試的結果是 發送 SIGTERM等待 30 秒發送 SIGKILL 作者表示又糊塗了，沒想到設定 grace-period=0 竟然中間還有 30 秒的時間，這完全與預料的不同，更麻煩的是文件也沒有講得非常清楚到底什麼是正確的行為， 此外還提到 Docker 就支援真正的 immediate shutdown，直接送 SIGKILL。 另外作者發現 K8s GitHub 中的也有人提出類似的 issue，對於這些 graceful 的行為感到不解同時文件說明不夠精準。 這件事情很難說誰正確誰不正確，畢竟不同的系統架構下的設計方式與條件都不同，不過的確 K8s 的指令文件有時候是真的不是精準，需要仔細測試才可以理解到底運作行為為何","keywords":"","version":null},{"title":"閱讀筆記: 「Package Maintainers 應該要具備的資安概念」","type":0,"sectionRef":"#","url":"/2022/03/16/reading-notes-28","content":"標題: 「Package Maintainers 應該要具備的資安概念」 類別: other 連結: https://sethmlarson.dev/blog/security-for-package-maintainers 本篇文章的作者是 python3 urllib3 套件的主要維護者，由於近年來軟體供應鏈的資安議題逐漸受到重視，特別是這些已經被廣泛使用的套件，一套受到入侵與修改，其影響危害程度難以想像 作者撰寫本篇文章分享自己的想法希望能夠讓所有套件維護者有一些基本的資安觀念，同時也讓所有使用 OSS 的使用者一起學習 捐贈給開源貢獻者 作者提到本文提到的所有資安意識都需要花時間精力去實作與維護，如果你的組織使用了大量的開源專案但是本身在意資安卻又不想要花時間研究資安，那就花點小錢捐贈給這些開源貢獻者，讓這些貢獻者有更多的動力去幫你維護這些套件。 如果該開源專案對於你公司來說有非常舉足輕重的角色，那甚至可以考慮雇用一兩個該專案的主要維護者讓他們定期分配時間來維護，對公司來說只是花一些小錢但是卻能夠有更有信心的使用這些開源專案，以免哪天這些專案一炸整個公司產品全炸 文章主要分成兩大類，分別是 如何保護好你的個人帳戶如何保護好你的套件倉庫 Securing Your Accounts 對於一個套件維護者來說，你的個人帳號由於有超級大的權力，所以該帳號的資安管理必須是最高層級的注意，一但這個帳號被攻破，攻擊者就可以很輕鬆的去發布新版本，加入惡意程式碼等各種行徑，而通常使用者如果本身使用時沒有很好的限制版本，譬如採用大於1.1.0 這種比較寬鬆的用法就會不自覺升級而使用到危險版本 作者強調就算你本身不是套件維護者，這些帳戶保護方式對你來說也是非常實用的，良好的資安保護永遠不吃虧 接下來作者列出幾個大項，分別是 Email Securiy Is Your Top Priority Email 地址很重要，很多情況下這些地址都會是重設密碼的一個途徑，所以妥善保存 Email 地址是非常重要的，所以作者推薦使用那些大公司服務如 Gmail 與 Outlook。 如果你真的想要使用私人域名作為你的聯絡信箱，你就要保證你不會有忘記付費的那天，不會剛好有人買走你的域名然後順利的取走你的 Email 地址。 2FA 2FA 要求提供一個除了密碼以外的認證方式，常見的有手機或是一些硬體裝置，使用妥當的話基本上攻擊者很難登入你的帳號。 作者認為所有跟程式碼有關的帳號密碼最終都需要有一個不使用 SMS(簡訊) 的 2FA 機制保護，譬如 NPM 最近才宣布其 Top 500 的套件管理都需要強迫使用 2FA，作者希望 Python 有天也可以跟上這種趨勢。 Password Managers Hardware Keys Why Not SMS 2FA Where Do I Put My Recovery Codes What to do if your account is compromised? 上述範例我就不列出來了，每個項目都沒有很長，非常推薦大家閱讀，甚至可以讓團隊的所有工程師都有這些基本概念","keywords":"","version":null},{"title":"閱讀筆記: 「Kubernetes 紀錄片 」","type":0,"sectionRef":"#","url":"/2022/03/04/reading-notes-24","content":"標題: 「Kubernetes 紀錄片 」 類別: others 連結: https://thenewstack.io/a-kubernetes-documentary-shares-googles-open-source-story/ 來自歐洲的 Honeypot.io 與 RedHat, Google 以及 CNCF 合作完成一個長達一小時的 Kubernetes 紀錄片，該紀錄片分成上下兩集，探討 Kubernetes 的發展及過程 被戲稱就像是給開發人員的 Netflix 影片 如果英聽還行的話，非常推薦當個小品影片去聽聽看 Kubernetes 的今生前後","keywords":"","version":null},{"title":"閱讀筆記: 「Dockerfile 中透過 COPY --chomd 比透過 RUN chomd 可以省下更多空間」","type":0,"sectionRef":"#","url":"/2022/04/04/reading-notes-35","content":"標題: 「Dockerfile 中透過 COPY --chomd 比透過 RUN chomd 可以省下更多空間」 類別: containers 連結: https://blog.vamc19.dev/posts/dockerfile-copy-chmod/ 本篇文章是作者探討自己建制 Image 中所發現的一些有趣事實。 作者使用一個大概 70MB 的 image，並且安裝與運行大概 90MB 左右的額外空間，結果最後整個 image 高達 267 70MB 因此作者就花了一些時間來研究整體原因並且嘗試理解到底發生什麼事情 作者首先檢視自己的 Dockerfile，其內容簡單不複雜，包含如 COPY 一個 Binary (該 Binary 80 MB 左右) RUN xxxxx 等常見用法。 詳細檢視所有的 layer 資訊後，作者發現 RUN 這個指令竟然產生了 94.4MB 的全新 layer，而就是這個 layer 導致整體空間變成 267 MB. 作者的 RUN 指令執行 透過 apt-get 安裝四個套件透過 chmod 將前述 COPY 來的檔案給予執行的權限創建資料夾 作者檢查過安裝的套件，大概只有 6MB 左右，但是整個 layer 很明確就是多了 94.4 MB 出來，因此經過測試與研究後，作者觀察到 當移除第二步(修給檔案給予執行的權限)後整個空間瞬間變得很小，整體 image 最後的大小就符合預期的 174MB。 所以整個問題就出來了，為什麼單純執行一個 RUN chmod 就會使得整個 image layer 變大? 簡單來說 image 的底層是基於 OverlayFS，而 OverlayFS 的一大特色就是 CoW, Copy on Write，作者起初覺得 我只是透過 chmod 去修改該 Binary 一個屬性而以，本身並沒有寫入檔案到檔案系統中，怎麼會產生這麼大的檔案變化？ 仔細研究 OverlayFS 的文件後終於水落石出，原來除了寫入檔案外，修改檔案的某些 metadata 也會觸發 CoW 的機制 When a file in the lower filesystem is accessed in a way the requires write-access, such as opening for write access, changing some metadata etc., the file is first copied from the lower filesystem to the upper filesystem (copy_up). 至於為什麼修改個 metadata 也要觸發 CoW 主要是跟安全性有關，文章中有關於這部分的額外連結，有興趣的可以參考","keywords":"","version":null},{"title":"閱讀筆記: 「透過 Kubernetes Event-Driver Autoscaler(KEDA) 來根據各種指標動態擴充容器」","type":0,"sectionRef":"#","url":"/2022/04/13/reading-notes-38","content":"標題: 「透過 Kubernetes Event-Driver Autoscaler(KEDA) 來根據各種指標動態擴充容器」 類別: kubernetes 連結: https://medium.com/@casperrubaek/why-keda-is-a-game-changer-for-scaling-in-kubernetes-4ebf34cb4b61 Kubernetes 內已經有 HPA 的物件可以讓 K8s 根據一些基本的指標來動態調整 Pod 的數量，而 KEDA 這款 CNCF 的孵化專案則是完全強化 HPA 的效果 KEDA 的概念很簡單，就是應用程式應該要可以有更多的指標來幫忙動態擴充，除了最基本的 CPU/Memory 等基本指標外， KEDA 來支援了下列各種不同指標，讓 k8s 可以使用更為廣泛的指標，譬如 Redis 內某個 Queue 的長度K8s 內其他 Pod 的數量PostgreSQL Query 的結果Elasticsearch Query 的結果各種雲端服務，如 Azure Event Hubs, AWS CloudWatch, GCP Pub/SubKafka ... 等各種不同指標 使用方式很單純，一切的規則都是基於 K8s 的 CRD 來描述與管理，因此團隊可以使用 YAML 的方式來定義這些擴充的規則 文章中有基於 CPU/Memory 的基本介紹使用，同時文章中也有官方的連結來介紹各種不同指標的示範用法","keywords":"","version":null},{"title":"閱讀筆記: 「你真的有正確使用 SSH 嗎?」","type":0,"sectionRef":"#","url":"/2022/04/15/reading-notes-39","content":"標題: 「你真的有正確使用 SSH 嗎?」 類別: tools 連結: https://smallstep.com/blog/use-ssh-certificates/ SSH 基本上是每個系統管理員都熟悉不過的工具，而本文的作者指出 SSH 使用上有一些小缺陷，譬如 使用者體驗很差，每一個新使用 SSH 的人如果不熟悉其概念，每次連線到新機器都會看到一次 Yes/No 的選擇，就因為不熟大部分的人都會直接選擇 Yes 來通過， 背後實際發生什麼事情都不清楚，只知道會動就好大規模的管理 SSH 非常麻煩且花費時間，Hostname 如果前後有出現重複的還會出現問題，需要重新處理 known_hosts 等相關資料透過 Key 的管理聽起來很安全，但是其架構使得使用者通常不太會換 key，會一直重富使用固定的那把 Key 來避免重新處理一切問題 舉了一些問題後，作者點出能夠真正駕馭 SSH 的應該是採取 SSH Certificate 而非使用 SSH Public Key 來進行身份驗證。 作者團隊開發了些許工具來幫助其他人能夠更輕鬆的使用 SSH Certificate 但是卻發現這類型的工具卻沒有受到歡迎與採用，因此也針對這個現象 進行問卷調查，想瞭解這類型的工具為什麼不受青睞，原因包含 根本沒聽過 SSH CertificateCertificate 以及 PKI 架構對人們來說不容易理解，很難理解其好處轉換中間有一些陣痛期，所以與其花時間去學習這些不如就繼續使用本來的 Public Key 機制 文章後半開始介紹 SSH Public Key 與 SSH Certificate 的差異 Public Key 的概念非常簡單，就是透過一組 Private/Public Key 並且將 Public Key 給寫入到目標節點帳戶上的 ~/.ssh/authorrized_keys. 節點數量爆炸多的情況下要如何有效率的去管理這些檔案則是一個非常麻煩但是又不能不處理的事情，這也是作者為什麼要推廣 SSH Certificate 的原因之一 SSH Certificate 的方式移除了關於 SSH Public Key 不停重複上傳與設定的情境，相反的則是將自身的 Public Key 給綁到 Certificate 內，同時也包含如過期時間，名稱等其他資料。 目標 Certificate 本身會由一個 CA 簽署，而每台 Server 都需要去修改 /etc/ssh/sshd_config 來指定相關的 CA Key 讓該 SSH 能夠信任。 文章後半部分介紹更多關於 SSH Certificate 的好處以及用法","keywords":"","version":null},{"title":"閱讀筆記: 「如何於 Docker 環境中運行 rootless 模式」","type":0,"sectionRef":"#","url":"/2022/03/28/reading-notes-33","content":"標題: 「如何於 Docker 環境中運行 rootless 模式」 類別: container 連結: https://thenewstack.io/how-to-run-docker-in-rootless-mode/ 雖然可以使用非 root 的方式去安裝 Docker 服務，但是 Docker 本身服務中還有其他各種元件需要透過 root 身份去運行，譬如 dockerd, containerd, runc 等元件， 而本篇文章則是探討要如何以真正 rootless 的方式來運行一個 docker container 。 使用 rootless container 有一些要注意的事情，譬如 port number 沒有辦法使用 1024 以下，所以如果你的服務有需要被外界存取時要使用大於 1024 的 port number。 此外 AppArmor, host network mode 這些都不支援，因此使用上會有一些情境要注意。 安裝其實滿簡單的， Docker 官網有提供 rootless 的安裝檔案，安裝後需要針對一個使用者 ID 進行處理，這個處理主要是因為要將 container 內的 root 使用者給轉換到系統上的非 root 使用者，所以才會有相關的 userID 要設定。 當然如果真的要完全追求 rootless 的容器解決方案可以考慮使用 Podman 來使用，其本身的設定就是針對 rootless 去開發的，使用上會相對於 docker 來說更為簡單。","keywords":"","version":null},{"title":"閱讀筆記: 「軟體工程師你真的工作的很開心嗎??」","type":0,"sectionRef":"#","url":"/2022/03/30/reading-notes-34","content":"標題: 「軟體工程師你真的工作的很開心嗎??」 類別: others 連結: https://stackoverflow.blog/2022/03/17/new-data-what-makes-developers-happy-at-work/ 疫情這兩年影響全球，其中對於勞動力來說更有甚巨的變化，而科技業更是其中的佼佼者，是所有行業中離職率最高的行業。 2021 的離職率相對於 2020 來說提升了 4.5%。 StackOverflow 基於想要理解科技領域的這個趨勢及原因，所以發起了一個調查想研究工程師工作是否開心，並且將 基於 350 位來自全球開發者的回應統整為報告於三月份釋出。 結果來看 70.3% 的工程師覺得工作開心14.4% 表示不開心15.3% 沒感覺 以最令人感到開心的地區排名來看，前五名分別為 西班牙 (90%)印度 (79%)德國 (70%)美國 (69%)英國 (68%) 那到底哪些因素會影響開心與否？ 報告中列舉了相關的指標 前五個最令人感到開心的因素有 薪資(60%感到開心)work-life 平衡工作彈性是否有足夠生產力職涯發展機會 而令人感到不開心的五個最重要指標其實就是上述指標的反轉，依序為 工作覺得毫無效率與生產力工作生活不平衡沒有職涯發展與機會薪水太低工作無彈性 調查的全部指標除了上述五個之外還有 工作是否能夠帶來影響力是否能夠獨力解決問題有一個瞭解我工作的主管 ... 等 其實面試找工作也就是針對這些條件進行排序，與其跟風看大家去什麼公司就想去什麼公司，不如好好跟自己對話，瞭解自己對於工作的目的以及追求是什麼，才有辦法找到一個自己喜歡且舒適的工作環境","keywords":"","version":null},{"title":"閱讀筆記: 「升級 Kubernetes 1.22 的注意事項」","type":0,"sectionRef":"#","url":"/2022/04/11/reading-notes-37","content":"標題: 「升級 Kubernetes 1.22 的注意事項」 類別: kubernetes 連結: https://blog.runx.dev/will-that-kubernetes-v1-22-upgrade-break-my-application-cc339dc2e2c7 隨者各大公有雲逐步支援 Kubernetes 1.22，相關使用者可能都會開始進入升級的準備階段，而每次 Kubernetes 升級除了單純 思考 Kubernetes 本身的升級順利與否外，也要確認正在運行的所有 Kubernetes 資源與相關工具是否也能夠順利運行，這使得整個準備工作變得複雜與龐大。 從 Kubernetes 的角度來看，每次的升級除了基本的穩定性與相關功能修正外，最重要的還有 Kubernetes API 的改變，該改變影響巨大，譬如所有 Manifest 的內容，譬如眾多透過 YAML 所描述的各種資源 API 的改變都會提早通知所有社群，於先前的版本先將該 API 標為 deprecated 接者後續版本才會正式移除，譬如 networking.k8s.io/v1beta1 於 1.19 被標示為 deprecated 然後正式於 1.22 移除。 正式的版本 networking.k8s.io/v1 則從 1.19 正式啟用，讓管理者有大概有三個版本的時間轉移。 因此升級前一定要先架設一個測試環境，嘗試部署所有現存的資源來確保升級不會出現不預期的錯誤。 作者整理出關於 1.22 升級要注意的版本變化，如下(幾乎都是從 v1beta 變成 v1) Webhook: admissionregistration.k8s.io/v1beta1 → admissionregistration.k8s.io/v1CRD: apiextensions.k8s.io/v1beta1 → apiextensions.k8s.io/v1APIService: apiregistration.k8s.io/v1beta1 → apiregistration.k8s.io/v1TokenReview: authentication.k8s.io/v1beta1 → authentication.k8s.io/v1SubjectAccessReview: authorization.k8s.io/v1beta1 → authorization.k8s.io/v1CertificateSigningRequest: certificates.k8s.io/v1beta1 → certificates.k8s.io/v1Lease: coordination.k8s.io/v1beta1 → coordination.k8s.io/v1Ingress: extensions/v1beta1, networking.k8s.io/v1beta1 → networking.k8s.io/v1IngressClass: networking.k8s.io/v1beta1 → networking.k8s.io/v1RBAC resources: rbac.authorization.k8s.io/v1beta1 → rbac.authorization.k8s.io/v1PriorityClass: scheduling.k8s.io/v1beta1 → scheduling.k8s.io/v1Storage resources: storage.k8s.io/v1beta1 → storage.k8s.io/v1","keywords":"","version":null},{"title":"閱讀筆記: 「DevOps 的 2022 學習之路」","type":0,"sectionRef":"#","url":"/2022/04/20/reading-notes-41","content":"標題: 「新一代 Helm Chart 的管理套件 helmwave」 類別: tools 連結: https://medium.com/wriketechclub/new-wave-for-helm-b9800733587f Helm 作為現在包裝與安裝 Kubernetes 應用服務的主流方式，單單使用 Helm 很多時候不能滿足部署需求，譬如公司的業務是由多套 Helm Chart 同時組成的，這時候可能會有幾種做法 使用 Helm Dependency 的方式來產生一個 Umbrella charts 讓你可以安裝一個 Helm 實際上會把相關的服務一起搞定透過 Helmfile 等相關工具以更上層的概念來管理你的應用，用多套 Helm Chart 來管理與部屬你的應用程式 而作者長期使用 Helmfile 來管理各種 Helm 的安裝方式，而今天作者終於發現一個相對於 Helmfile 來說更容易使用，而且整體使用方式更為簡潔的解決方案，helmwave. Helmwave 的官方介紹很簡單， Helmwave is like docker-compoose for helm. 其本身的實作更為簡潔，直接使用 Helm Library 於整個實作中，所以下載單獨的 binary 即可，不需要如同 helmfile 一樣還要於系統中先安裝 helm 等相關工具。 文章中透過範例來示範如何滿足 服務需要安裝多套 Helm chart有兩個不同環境， prod 與 stage 有不同的 values 要使用 整個使用的方式跟 docker-compose 有點類似，可以透過 helmwave up, helmwave down 的概念來啟動與停止服務，只不過所有的服務都是基於 k8s + helm-charts 來完成。 有使用 helmfile 的人可能會對這類型的工具比較有感覺，也許可以看看其差異性是否真的有如作者所提這麼好 標題: 「DevOps 的 2022 學習之路」 類別: others 連結: https://medium.com/faun/devops-roadmap-2022-340934d360f9 本篇文章是作者根據自己的觀察與經驗，列出 2022 需要繼續學習與觀察的 13 項技能與概念，希望讓每個 DevOps(SRE) 相關領域的人有一個方向去精進自己。 Network Technologies 網路的概念短時間內很難被顛覆，所以掌握基本的 L4/L7, HTTP2/, HTTP3/(QUIC), DNS, BGP, Load-Balancing 等基本網路概念絕對不吃虧，作為一個熟悉架構的專家，能夠描述環境中的封包流向是不可缺少的能力。 OS, particularly Linux Linux 很重要，請學習系統上的各種基本概念， CPU/Memory 基本概念, Init, cgroup 等 CI/CD Jenkins 作為老牌的解決方案，能夠使用其實也很好，不過要注意的是現在有愈來愈多的環境嘗試使用其他的 pipeline 來搭建，所以有時間的話也可以學習一下其他的解決方式，讓自己能夠有能力去面對各種需求 Containerlization/Virtualization 除了最知名的 Docker 環境外，也嘗試看看 containerd, podman 等不同專案，同時也考慮如何將 container security 的概念給導入到日常生活中 Container Orchestration K8s 幾乎變成容器管理維運的 de facto 標準，單純的 k8s 叢集還不足以面對所有正式環境的問題，所以還需要搭配各個面向的概念將其整合才可以打造出一個適合團隊的 k8s 叢集。 Observability at Scale 除了最基本常見的 Prometheus 之外，也看一下其他基於 Prometheus 所打造更適合大規模的架構，如 Thanos, Cortex, VictoriaMetrics 等 此外可以試試看 Continuous Profiling 等持續觀察系統效能的工具，如 Parca, Pyroscope, hypertrace 以及順便試試看導入 Open Telemetry。 Platform team as a Product team 稍微有規模的團隊可能會慢慢的感覺到 Platform 逐漸轉型成為一個 Product 的概念，只不過該 Product 的面向對象是內部開發與測試人員而並非外部使用者。 整體目標就是打造一個更好的協同平臺，讓開發與測試人員能夠更有效地去滿足日常工作需求，同時 Platform team 除了維護產品之外也要教授使用人員讓他們有能力去使用該平台來滿足需求 而不是所有問題都要一直讓 Platform 的人來幫忙處理，這種模式小團隊可行，但是當團隊過大時就沒有辦法處理。 Security Programming Infrastructure as Code Cloud Technical Writing Site Reliability Engineering 剩下的內容就留給有興趣的人自行到文章去觀看，每個類別都有舉出幾個趨勢與值得關注的專案，其中特別注意的是 Technical Writing 這項技能非常重要 遠端工作的趨勢使得透過文字交流的機會比過往多很多，所以如何寫出一個有效不會浪費彼此時間的設計文件，架構，開發文件等則是一個很重要的技能，所以即使是個開發人員也要努力練習 將腦中的想法有系統地呈現出來","keywords":"","version":null},{"title":"閱讀筆記: 「istio 下因為YAML 與 Go template 結合產生的 CVE」","type":0,"sectionRef":"#","url":"/2022/04/27/reading-notes-44","content":"標題: 「istio 下因為YAML 與 Go template 結合產生的 CVE」 類別: others 連結: https://paper.seebug.org/1882/ 熟悉 Kubernetes 的使用者一定對於各式各樣的資源格式感到不陌生，譬如描寫一個 Pod 需要準備些關於 containers 的基本資料，其餘還有 Label, Annotation 等 各種資料需要填寫。 Kubernetes 內透過 apimachinery 的方式來驗證每個欄位是不是合法，譬如最常見的就是創建資源名稱時有時候會因為等出現格式不符合，準確來說是 Pod 的方式來驗證每個欄位是不是合法，譬如最常見的就是創建資源名稱時有時候會因為等出現格式不符合，準確來說是 透過 DNS RFC 1123 來驗證 Pod 是否合法。 部分的數值資料可能會於 Controller 中額外去檢查，至於自定義的 CRD(Customer Resource Definition) 則是創建時可以透過 openAPIV3Schema 去定義每個欄位的合法數值。 今天這篇文章要介紹的問題是跟 istio 環境的問題，當使用者創建一個名為 Gateway 的資源到叢集中時， istio 會去讀取該 Gateway 資料並且轉換為 Service/Deployment 兩個底層資源。 作者仔細研究發現創建 Service 時會從 Gateway 中的 Annotation 找到名為 &quot;networking.istio.io/service-type&quot; 的資料，並用其作為 Serivce 的 type. 然而 Annotation 的數值沒有並沒有任何檢查機制，所以使用者可以於該欄位 &quot;networking.istio.io/service-type&quot; 填入各種數值，因此作者就嘗試撰寫一個非常長的 Annotation，譬如 annotations: networking.istio.io/service-type: |- &quot;LoadBalancer&quot; apiVersion: apps/v1 kind: Deployment metadata: name: pwned-deployment namespace: istio-ingress spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.3 ports: - containerPort: 80 securityContext: privileged: true 結果非常順利的， isio 最終創造了一個作者故意描述的 deployment，而該 deployment 還特別的設定 privileged: true 的選項並且透過這次的測試證明該 YAML 的檢查問題導致使用者有機會插入任何想要的資源到環境中 對本文有興趣的可以觀看一下","keywords":"","version":null},{"title":"閱讀筆記: 「使用 serverless 5年後的心酸經驗談」","type":0,"sectionRef":"#","url":"/2022/04/29/reading-notes-45","content":"標題: 「使用 serverless 5年後的心酸經驗談」 類別: usecases 連結: https://dev.to/brentmitchell/after-5-years-im-out-of-the-serverless-compute-cult-3f6d 本文作者想要分享自己過去五年來使用 Serveless 的經驗談，從不同角度切入導入 Serveless 後的痛點。 作者的 serverless 環境是基於 AWS 環境，使用了包含 API GAtewayCognitoLambdaDynamoDBDAXSQS/SNS/EventBridge 作者提及了幾個痛點，包含 TestingAccount ChaosSecurityNo Fundamental EnforcementDNS Migration FailuresMicroservice HellAPI Respones 回傳不一致 這篇文章最有趣的點不是文章本身，而是底下的留言討論，雖然有少數留言是支持作者但是大部分的人都是秉持反對的意見來看這篇文章。 我自己的角度是這篇文章提出非常多問題，但是這些問題我看不太出來跟 Serveless 的關係是什麼，更多的是公司的文化，工程品質與開發工具有關 譬如作者說團隊內有很多非資深工程師會因為 serveless 的易用而依賴自己的想法去攥寫，譬如光 Auth 就有十種不同方式。 但是仔細思考這個問題，似乎 server-based 的架構也會有這問題，完全是公司的文化與規範問題。 其他問題還有很多寫 serveless 的人都沒有 HTTP 的深厚底子，所以 200,400,500 想回就回，然後回傳格式也都沒有統一固定 這些東西其實跟 serverless 也沒有直接關係，更多依然是 Code Review 的問題，工程師品質的問題。 所以有時候看文章除了單純閱讀外，也要思考一下作者講的東西自己是否認同，同時也可以看下留言處，來自不同文化與團隊的留言往往能夠帶來更大的啟發，也是閱讀網路文章上我覺得非常有價值的地方","keywords":"","version":null},{"title":"閱讀筆記: 「強化 Kubernetes 叢集的必備工具」","type":0,"sectionRef":"#","url":"/2022/04/18/reading-notes-40","content":"標題: 「強化 Kubernetes 叢集的必備工具」 類別: kubernetes 連結: https://medium.com/mycloudseries/must-haves-for-your-kubernetes-cluster-to-be-production-ready-dc7d1d18c4a2 作者本篇文章想要分享一個其用來讓一個 Kubernetes 變得能夠真正上戰場的相關工具，因此文章中特別強調是 Production-Ready 的情況。 一個 Production Ready 的 K8s 叢集必須對於下列每個大項目都要有相關處理方式，譬如 Reliability and AvailabilitySecurityNetwork, Monitoring &amp; ObservabilityBackup/RecoveryCost OptimizationCluster Visualization Reliability and Availability: 該領域的兩個指標代表的意義不太一樣，但是對於一個提供服務的叢集來說都一樣重要 這邊作者列舉了幾個工具譬如 K8s 內建的 HPAAWS 的 karpenter，讓你針對基於節點為單位來擴充Cluster-AutoscalerGoldilocks Backup/Recovery 有不少人團隊都對於對於叢集的備份與還原感到頭痛，目前最知名的開源專案莫過於 Velero，其支援不同的儲存設備如 Cloud Storage 等來存放，讓不同環境的 k8s 使用者都有辦法去備份其叢集內的資料 Cost Optimization 對於雲端架構來說，基本上雲端業者的內建功能已經可以針對如 VM, 底層架構等各種服務去列舉出各自的花費金錢，將此概念套入到 Kubernetes 本身大抵上只能理解到 Master Node, Worker Node 等之類的花費， 因此透過 Kubecost 之類的專案來將成本的洞察範圍擴充到 Kubernetes 內部，以 namespace, pod 等各種 k8s 的資源為單位來列舉實際花費的金額，能夠讓團隊更有效地去管理相關花費","keywords":"","version":null},{"title":"閱讀筆記: 「新手閱讀，我踩過的 Terraform 各種雷」","type":0,"sectionRef":"#","url":"/2022/05/06/reading-notes-48","content":"標題: 「新手閱讀，我踩過的 Terraform 各種雷」 類別: terraform 連結: https://medium.com/contino-engineering/10-things-i-wish-i-knew-before-learning-terraform-f13637a01aa6 本篇文章作者分享自己學習與使用 Terraform 多年來遇過的各種雷，也希望藉由這類型的文章可以讓每個踏入 Terraform 的人都不要走冤枉路 Make sure you have a terraform block in your configuration TF 檔案中可以透過 Terraform 區塊來描述關於 Terraform 本身的一些限制，譬如版本條件，相關的 provider 來源以及版本。 這個區塊非常重要但是本身是一個 optional 選項，所以不寫其實不影響整體功能，但是沒有去限制使用的版本範圍其實就跟任何的軟體環境一樣非常危險， 很容易踩到「昨天還可以，今天就不行的」通靈現象，所以作者希望每個人都好好的將 Terraform 區塊描述清楚，確定當前支援的版本是哪個確保該 TF 能夠用正確的版本於任何環境執行 Statefile 實際上本身是純文字格式，作者想要提醒的是 State 檔案作為 Terraform 同步上最重要的檔案，其本身是一個純文字明碼的格式，這意味你運行過程中的任何帳號密碼其實都是純文字的格式存放於該檔案中。 所以 State 檔案的保存非常重要，需要用很嚴肅的資安態度來保護這個檔案，否則該檔案被人取得則你 TF 中的各種資訊都會被對方取得。 作者直接於文章中展示一個範例，該範例會創建一個 AWS aws_secretsmanager_secret_version，而該物件的 secret_id, secret_string 都會以明碼的方式被存放於 State 檔案中。 Have verbose variables and outputs blocks TF 中的所有變數都可以用非常簡易的方式去宣告，但是如果妥善地利用這些內建的功能將可以使得變數的使用變得更加方便，特別是當該變數要跨 Module 使用時，呼叫者可以透過更輕易的方式 去理解該變數的格式與用法。 其中最為重要的則是 validation 的內容，作者以 AWS image_id 為範例，該變數基本上就是一個字串，所以使用者可以傳遞任何變數到該欄位去使用，但是如果搭配 validation，就可以讓 TF Apply 提早 先觀察到這些變數是否合法，能夠降低與避免不必要的失敗。 所以針對每個變數都好好的撰寫相關敘述與驗證，能夠讓團隊使用上減少無謂的猜想與溝通。 Integrate your environment with a pipeline early Terraform 的入門非常容易，但是當你想要將 Terraform 導入到團隊中並且與其他人共同合作時，整個使用上的複雜度會大幅度增加。 作者認為如果真的要導入 Terraform 到整個團隊中，則要盡快且盡可能地將 Terraform 導入到現有的 pipeline 架構中，譬如 Terraform Cloud 服務 能夠幫你妥善的管理這些 Lock/State 並且透過 Terraform Apply 來執行變化。 作者還有第二篇探討剩下的用法，包含 Keep your code together as much as possible Have clear lines of demarcation on responsibility Use multiple environment files for the same code Familiarise yourself with HCL’s functions and meta-arguments Terraform is not a golden bullet 有興趣的讀者建議兩篇文章都閱讀一下","keywords":"","version":null},{"title":"閱讀筆記: 「容器的除錯之路，遇到 Permission Denied 該怎麼辦」","type":0,"sectionRef":"#","url":"/2022/05/04/reading-notes-47","content":"標題: 「容器的除錯之路，遇到 Permission Denied 該怎麼辦」 類別: container 連結: https://live-rhes.pantheonsite.io/sysadmin/container-permission-denied-errors 作者提到大部分遇到 Container 權限問題時，最無腦的一招就是 --privileged 直接硬上權限，但是其實大家都不知道自己到底缺少什麼權限，盲目地使用 --privileged 的確可以解決問題 但是實務上卻是犧牲 Security 換來的，因為不知道缺少什麼而直接硬開，其實就是硬生生的將幾乎所有保護功能都關閉。 本篇文章就來探討當遇到權限問題時有可能是什麼造成的，以及應該如何精準地去設定這些權限而不是用一招 --privileged 跳過。 此外由於作者本身就是 Podman 開發團隊，因此文章之後的介紹與範例都會基於 Podman 來完成， 錯誤定位 如果你的容器問題透過 --privileged 也不能解決，那至少你的問題跟本篇文章的關聯性不大，或是說你的問題其實根本不是安全性方面的設定問題，只有當妳確認你的問題 可以因為 --privileged 而解決時本篇文章的內容才會對你有幫助 Is SELinux the issue?Is AppArmor the issue?Test capabilitiesTest SECCOMPTest masked kernel filesystem 除了上述五個安全性設定外，作者也針對 namespace 探討可能會出現的問題，包含 Is user namespace the issue?Is network namespace the issue?Is pid namespace the issue? 最後就是不免俗的推薦大家使用看看 rootless container，畢竟大部分的應用程式其實都沒有要寫入系統的需求，理論上來說應該都要可以運行於 rootless 的模式 整篇文章整理的非常的好，每個類別都有指令操作來介紹概念，對於這些資安控管不熟的人來說可以說是一個溫習的好機會","keywords":"","version":null},{"title":"閱讀筆記: 「三座獨立 k8s cluster 還是一個跨三個地區的 k8s cluster ?」","type":0,"sectionRef":"#","url":"/2022/04/25/reading-notes-43","content":"標題: 「三座獨立 k8s cluster 還是一個跨三個地區的 k8s cluster ?」 類別: kubernetes 連結: https://itnext.io/3-reasons-to-choose-a-wide-cluster-over-multi-cluster-with-kubernetes-c923fecf4644 講到多套 kubernetes 的情況下，目前大部分的文章都會推薦用三套獨立的 kubernetes 叢集而非架設一套同時管理三個地點的 kubernetes 叢集。 本篇文章作者從不同的面向分享為什麼要選擇一個 kubernetes 管全部，而不是要架設三套 kubernetes 叢集。 Latency 一套 kubernetes 最令人詬病且很難處理的就是 Latency 的問題，作者提到 Latency 的問題會影響 ETCD ETCD 被影響後會影響整個叢集的運作，甚至連應用程式相關的處理都會變慢。 作者提到其實這個問題能夠採取兩個步驟來解決 重新安排 etcd 的節點位置，或是使用 non-etcd 的解決方案透過 node labels 讓要使用 etcd 的服務跟 etcd 盡量靠近 註: 我是覺得這說法不能解決問題，一般應用程式要是被分散到不同地區你的存取還是有機會跨地區，除非要很認真地針對不同地區去設計 label，讓應用程式的部屬都只會固定同個地區，但是要這樣搞跟我直接搞三套不覺得後者會比較累。 Security 作者一直強調使用 mesh VPN 來打通底層所有網路封包處理，讓你一個大 k8s 管理多個地區，就不用擔心底層網路問題 單套 k8s 的好處有什麼？作者認為有 No Complicated tooling 作者提到 2021 年的 KubeConf 有各種管理多套 k8s 叢集的工具，如 KubeEdge, OpenShift Edge, Akri, Baetyl, Kubermatic, Rancher, KubeFed... 等，如果用一套大 k8s 就可以不使用這些工具，直接減少與這類型複雜工具的依賴性 一套 k8s 叢集可以讓你使用最簡單也是最習慣的方式來管理所有環境 No extra overhead 每套 K8s 環境中都會有如監控，日誌， registry 等各種工具，多套 k8s 的架構就是每個叢集都要安裝一份，但是如果採用一個大 k8s 的架構就只要維護一份即可 所以可以減少很多不必要的重複安裝。 Ultimate Flexibility 這段其實不很理解，為什麼作者這麼想要推廣 mesh VPN ... 註: 這篇文章底下有留言說探討到說 RBAC 等相關權限問題是個很大的問題，你一套 k8s 很難處理這些，事情沒有想像的這麼簡單","keywords":"","version":null},{"title":"閱讀筆記: 「成為軟體架構師的閱讀之路」","type":0,"sectionRef":"#","url":"/2022/05/02/reading-notes-46","content":"標題: 「成為軟體架構師的閱讀之路」 類別: others 連結: https://haitham-raik.medium.com/books-for-great-software-architect-34c81fc70e12 作者認為網路上有很多文章分享想要成為一個軟體架構師應該要閱讀哪些書籍來補充知識，但是這些文章都沒有提供一個好的閱讀路徑，沒有告訴你說 這些書有什麼樣的前置條件，這群書有什麼樣的閱讀順序等，這很容易造成讀者沒有系統的四處閱讀，容易導致無聊與沮喪。 作者根據自己的經驗整理特這些書籍，並且從中找到一個閱讀順序，透過這些閱讀順序可以讓你掌握每本書籍的前置知識同時也能夠有更好的知識去思考書本所談論的內容。 作者認為軟體架構實際上還可以根據領域進行二次細分，包含 應用架構整合架構資料架構 不同專項其內榮與知識都不同，因此閱讀時的路徑也會不同。所以本篇文章實際是個系列文，總共會有四篇 本篇是一個探討大綱的文章，探討一下基本概念，而後續系列文則是會針對上述三個不同面向去深度探討該怎麼閱讀 要認真踏入軟體架構前，必須要先掌握基本概念，如相關技術與工具，而作者認為學習這些基本概念的路徑就是所謂的 Design Path. Design Path 中將會學習到 Domain-Driver Design(DDD)Object-Oriented Design PatternsBasic agile Development conecptsModeling using UMLRespoinsiblity-driven design(RDD)..等 針對這 Design Path，作者推薦依照順序閱讀下列書籍 Applying UML and Patterns, by LarmanHead First Design Patterns, by Freemanbject Design: Roles, Responsibilities and Collaboration, by IvarDomain-Driven Design Tackling Complexity in the Heart of Software, by Eric 掌握好 Design Path 後，下一個就是 Architecture Fundamentals 的技術掌握，該過程要學習關於架構的基本概念，原則，模式與實踐方式，閱讀書籍如下 Fundamentals of Software Architecture, by Mark RichardsClean Architecture, by Robert MartinDocumenting Software Architecture, by Paul Clements","keywords":"","version":null},{"title":"閱讀筆記: 「提升 DevOps 技術的免費書籍」","type":0,"sectionRef":"#","url":"/2022/05/09/reading-notes-49","content":"標題: 「提升 DevOps 技術的免費書籍」 類別: others 連結: https://vladimir-mukhin.medium.com/free-books-that-will-boost-your-devops-game-to-the-next-level-5940482b0f96 本篇文章的重點很簡單 閱讀書籍提升對於 DevOps 領域的掌握度所有書籍都是免費 這邊節錄文章中列出的所有書籍 Kubernetes Up &amp; Running — Dive into the Future of Infrastructure Kubernetes 從 2014 發行以來的八個年頭席捲全世界，作為一個 DevOps 不論你當下的環境適不適合使用 Kubernetes，你都必須要瞭解到底這個容器管理平台的魅力是什麼 為什麼可以打趴眾多競爭者成為所有容器管理平台的主要首選。 本書從開發者(Dev)以及維運者(Ops)的角度來看到底 Kubernetes 是如何提升整體工作的效率，速度與整體的靈活度 Designing Distributed Systems — Patterns and Paradigms for Scalable, Reliable Services 這本由 Brendan Burns 所攥寫的書籍探討了分散式系統架構上幾個常見的設計模式，事實上這些設計模式有些都可以於 Kubernetes 的設計與用法中反覆發現 所以花點時間去研究一下大師所分享的分散式系統模式的設計理念，對於未來去學習理解新系統，或是設計一套系統都會有所幫助 97 Things Every Cloud Engineer Should Know — Collective Wisdom from the Experts 這本有紅帽所發行的免費書籍，書中收集了眾多資深雲端工程師的經驗，列舉了 97 個每個雲端工程師都應該要知道的事情，這 97 項包含很多東西，譬如 資料，自動化，網路，公司文化，個人發展，軟體開發以及雲端預算評估等眾多常見議題 Linux — Notes for Professionals Production Kubernetes — Building Successful Application Platforms Git — Notes for Professionals Automate The Boring Stuff with Python — Practical Programming For Total Beginners 剩下的書本也都非常有趣，大家有需要時可以閱讀下列書籍","keywords":"","version":null},{"title":"閱讀筆記: 「基於 eBPF 的 ServiceMesh」","type":0,"sectionRef":"#","url":"/2022/05/11/reading-notes-50","content":"標題: 「基於 eBPF 的 ServiceMesh」 類別: networking 連結: https://isovalent.com/blog/post/2021-12-08-ebpf-servicemesh 本篇文章是 2021末 由 Cilium 背後的 isovalent 公司團隊所發表的文章，主要探討一個全新的 Service Mesh 的架構可能帶來的好處，整篇文章以 Cillium + eBPF 為背景去探討 我認為如果對於 eBPF 沒有全面理解的情況下，其實只能讀懂這篇文章想要帶來的果，沒有辦法去理解到底整體實作與運作原理，同時因為 eBPF 本身的用途除了網路(Cilium)之外有愈來愈多的底層除錯工具都是透過 eBPF 的概念來實作的，因此學習 eBPF 的概念其實帶來的好處很多，有空的都推薦大家花點時間去學習。 本文主要分成幾個部分 什麼是 Service Mesh 以及目前的主流做法聊一下 Linux 網路傳輸的歷史發展基於 eBPF 的 Service Mesh 架構不同架構下的差異以及可能的隱性成本 隨者分散式應用程式架構的興起，如何針對這些散落各地的應用程式提供關於網路連線方面的資訊一直以來都是維運上的問題，過往最簡單的方式就是針對各種開發環境導入相關框架 每個應用程式都需要修改來整合這些框架，但是隨者整個架構發展與要求愈來愈多，譬如開發環境有不同程式語言，甚至有不可修改的第三方應用程式，除了網路監控外還想要導入認證授權，負載平衡等各種功能 要求每個應用程式開發者引用這些框架已經沒有辦法漂亮的滿足所有需求，因此一個能夠無視應用程式本體的透明性框架架構就變成眾人追捧與渴望的解決方案。 現今大部分的 Service Mesh 就是採取這種透明性的架構，透過額外 Proxy 來攔截應用程式的封包進行後續管理與監控，使得 應用程式開發者專注自己的商業邏輯開發第三方不可修改應用程式也可以導入這些進階網路功能 以 kubernetes 來說，目前主流都是透過 sidecar 的概念，讓每個應用程式旁邊都放一個 Proxy 的應用程式，同時基於 Pod 內 Containers 可以使用 localhost 互通的方式來處理連線。 應用程式本身都透過 localhost 打到 Proxy，而所有對外連線都讓 Proxy 幫忙處理，因此所有的進階功能都實作於該 Proxy 上。 Isovalent 認為這種方式功能面上可行，但是認為如果導入 Sidecar 其實有很多隱性成本 根據測試不管哪種 Service Mesh/Proxy 的解決方案都會使得真正連線的 Latency 提高 3~4 倍，這主因是 Linux Kernel 的架構導致，所有的網路封包 都必須要於 Linux Kernel Network Stack 來回繞行很多次，封包這種東西來回本身又會牽扯到 Context Switch, Memory Copy 等各種成本，所以整體 Latency 的提升是不可避免的。系統的額外資源需求，每個 Pod 都需要一個額外的 Proxy 來處理，以一個 500 節點，同時每個節點都有 30 Pod 來說，整個環境就要額外部署 15,000 的 Proxy 的 Container，每個 Container 消耗 50MB 就至少要額外 750G 的記憶體， 同時也要注意隨者 Pod/Node 等數量增加，每個 Proxy 可能就需要更多的記憶體來維護這些 Mesh(網格) 之間的資訊，因此使用的 Memory 量只會愈來愈多。 所以 Cillium/Isovalent 想要引入基於 eBPF 的架構來打造一個不同架構的 Service Mesh。透過 eBPF 的架構使得整個 Service Mesh 的發生點是發生於 Kernel 階段，而非一個獨立的 Uses Proxy。 這邊帶來的改變有 基於 eBPF 的特性，其本身就有辦法針對系統上所有 Socket 去執行特定的函式，所以 Cillium 就可以偷偷去修改應用程式的網路流量，不論是修改封包內容，偵錯與監控等都可以達到不需要如同之前一樣每個 Pod 都部署一個獨立的應用程式，取而代之的是撰寫通用的 eBPF 程式來提供各種功能由於所有的事情都發生於 Kernel，甚至可以達到基於 Socket-level 的封包處理，所以封包不需要繞來繞去，整個處理的路徑非常的短，因此產生的 Latency 非常的小 非常對於這系列戰爭有興趣的人花點時間去把 eBPF 的概念補齊，接下來針對這系列的大戰與討論就能夠有更多的背景去理解","keywords":"","version":null},{"title":"閱讀筆記: 「Mizu, 一套用來檢視 Kubernetes Traffic 的視覺化工具」","type":0,"sectionRef":"#","url":"/2022/05/25/reading-notes-53","content":"標題: 「Mizu, 一套用來檢視 Kubernetes Traffic 的視覺化工具」 類別: tools 連結: https://getmizu.io/docs/ Mizu 是一個專門針對 Kubernetes 開發的流量分析工具，該工具透過簡單好用的 UI 讓你檢視叢集內的流量走向，其支持的協定有 HTTP, REST, gRPC, Kafka, AMQP 以及 Redis 等相關的應用程式封包。 雖然說透過大部分的 Service Mesh 也都可以提供一樣的功能，但是 Mizu 的特色就是其輕量的架構設計，就是針對流量分析而已，所以如果團隊目前沒有現成的解決方案時， 可以考慮試試看 Mizu 這套輕量級的解決方案。 Mizu 本身由兩個元件組成，分別是 CLI 以及 Agent，當你安裝 Mizu 的 Kubernetes 內時，其會安裝兩個元件 透過 Daemonset 安裝 Agent 到所有節點透過 Pod 安裝一個 API Server Agent 會針對需求去抓取節點上特定的 TCP 封包(目前也只有支援 TCP 流量，這意味如 ICMP, UDP, SCTP 等就沒有辦法)，此外要特別注意這類型的解決方案為了能夠 抓取到節點上的所有流量，通常都會讓這類型的 Agent 都設定為 hostnetwork: true，如此一來該 Agent 才有辦法觀察到節點上的所有網卡來進行流量擷取。 有些 k8s 環境會透過如 OPA(Gatekeeper) 等機制去控管要求所有的 Pod 不准使用 hostnetwork，有這些規範的可能就要注意一下整合上的問題。 有興趣的可以稍微玩看看，看看這類型的工具是否可以幫忙除錯","keywords":"","version":null},{"title":"閱讀筆記: 「Datree, Kubernetes Configuration 檢查工具」","type":0,"sectionRef":"#","url":"/2022/05/20/reading-notes-51","content":"標題: 「Datree, Kubernetes Configuration 檢查工具」 類別: tools 連結: https://opensource.com/article/22/4/kubernetes-policies-config-datree 如同各類程式語言的測試框架， Kubernetes 的部署文件(YAML)實際上也是可以導入 CI 的概念，那到底 YAML 檔案有什麼東西需要檢驗？ 最基本的概念大致上可以分成三種 YAML 語法的檢查Kubernetes YAML 的語意檢查Kubernetes YAML 的設定規範檢查 除了基本的 YAML 部署外，還要考慮一下團隊是採用何種方式來管理 Kubernetes App，譬如原生 YAML, Helm, Kustomize 等各種不同方法。 (1) 的話其實最基本的方式就是使用 yq 指令，其本身就可以檢查基本的 YAML 語法，如果是 Helm 的使用者也可以透過 Helm template 的方式來嘗試渲染，渲染的過程也會幫忙檢查 YAML 的合法性。 (2) 的話其實也有其他如 kubeval 等類型的工具去幫忙檢驗 YAML 內容是否符合 Kubernees Scheme，這邊要特別注意的還有版本問題，畢竟每次升級都會有很多 API Version 被調整 (3) 的話講究的是規範，譬如要求所有 workload 都必須要描述 CPU/Memory 的Request/Limit，或是要求所有容器都要以 non-root 的身份運行， 這部分有如 kube-score，或是基於 REGO 的 conftest 等工具可以檢測。 而今天分享的這個工具 datree 基本上就是一個人包辦上述三個工具，該工具基本上有兩種模式使用 local 使用，就如同上述所有工具一樣，你可以把所有策略與規則都放到本地環境，搭配 git hook, CI pipeline 等概念去執行datree 還提供了一個中央管理 Policy 的伺服器，每個運行 datree 的環境都可以與該團隊維護的 server 連動，讓你透過網頁的方式去設定想要驗證的 k8s 版本以及想要檢測的規範有哪些。 基本上這類型的工具愈來愈多，找到一個適合團隊的工具將其整合到 CI 中，讓團隊的 Kubernetes YAML 都能夠符合團隊規範，同時也透過 CI 的流程盡可能提早地找出問題","keywords":"","version":null},{"title":"閱讀筆記: 「Tetragon, 基於 eBPF 的 Kubernetes 資安管理工具」","type":0,"sectionRef":"#","url":"/2022/05/23/reading-notes-52","content":"標題: 「Tetragon, 基於 eBPF 的 Kubernetes 資安管理工具」 類別: others 連結: https://isovalent.com/blog/post/2022-05-16-tetragon Cillium 的開發團隊 isovalent 最近公布其內部一直使用的資安相關專案， Teragon (可愛的蜜蜂戰士)。 Teragon 底層是基於 eBPF 的技術，其目的就是讓你的 Kubernetes 於資安方面可以獲得超級強大的能力，包含 詳細的視覺化功能，讓你可以一目瞭然到底系統中各項資源的發生過程動態強化，可以讓你透過 Kubernetes CRD, OPA, Json 等各種格式來描述相關規範，然後動態無縫的套入到你的 Kubernetes 叢集中 探討 Teragon 前，要先理解以前目前已知的相關解決方案有哪些，而這些解決方案又有什麼樣的優缺點，包含 App InstrumentationLD_PRELOADptraceseccompSELinux/LSMKernel Module 上述六個方式都有各自的特點，這邊簡單敘述 App Instrumentation O 效率高，可以看到非常細部的資訊 X 程式碼需要修改，不夠透明 X 單純的視覺化，不能套入資安規則來防護應用程式 X 應用程式為主，不能理解整個系統的狀況 LD_PRELOAD (動態切換載入的 Library ) O 效率高 O 應用程式不需要修改 X 如果是 Static Llinking 的應用程式那就沒有用了 X 幾乎沒有什麼觀察性可言 ptrace (透過 kernel 提供的功能來檢視使用的 syscall) O 透明，應用程式不用修改 X 效能負擔比較高 X 應用程式有辦法偵測到自己目前被 ptrace 給監控 X 整體範圍只能針對 syscall(系統呼叫) seccomp (可以過濾應用程式呼叫的 syscall) O 有效率，應用程式不需要修改 X 規則只能針對 syscall 去阻擋 X 沒有很好的視覺化方式 SELinux/LSM (Kernel 內建的 security 框架，可以針對存取去控制) O 有效率，應用程式不需要修改 O 可防 TOCTTOU 攻擊 X 針對 Contaienr/Kubernetes 的整合很有限 X 不容易擴充 X 要針對攻擊類型去設定 Kernel Module O 有效率，應用程式不需要修改 O 不用修改 Kernel 就可以擴充功能 X 不是每個環境都允許使用者去載入 kenrel Module X Module 有問題會打爆你的 Kernel X 沒辦法無縫升級，意味你升級功能的過程中必須要將kernel module給 uninstall ，然後重新安裝 上列六個解決方案有的只能檢視相關流程，有的只能設定規則去防護，但是就是沒有一個工具可以全面處理，而基於 eBPF 實作的 Tetragon 則是一個 能夠提供兩項功能的全新解決方案。 首先資安防護方面， Tetragon 採取的是更底層的概念，不去探討特定的 CVE 操作手法，取而代之的是從幾個常見的攻擊方式來防禦。 假如有任何應用程式有不預期的下列行為，就可以直接將該 Process 移除 使用到不該使用的 capability使用到不該使用的 linux namespace使用到不該使用的 binary看到不該出現的 Pid ... 這些規則都可以透過 Kubernetes CRD 來描述，當這些規則被送到 Kubernetes 後，相關的 Controller 就會將規則給轉換後續讓 eBPF 來處理 此外因為 eBPF 以及 kprobe 的架構，Tetragon 能夠看到非常多 kernel 的資源存取與操作，譬如 syscall(系統呼叫)Virtual FSTCP/IPnamespaceStorageNetwork Tetragon 收集上列不同資訊的資料後進行二次處理，透過精美的網頁來顯示系統中的各種資訊，這些資訊可以提供包含 哪些 Pod 一直存取 /etc/passwd, 採用何種方式存取 /etc/passwd特定 Pod 中對外的網路流量資訊，從封包內容到用什麼指令去存取都可以看光光... eBPF 的應用愈來愈多，而目前看起來 isovalent 更是 Kubernetes 生態系中的領頭羊，雖然不確定未來是否能夠被廣泛採用，但是至少這方面還沒有看到其他解決方案有這麼積極的基於 eBPF 來開發 有餘力的話花點時間學習一下 eBPF 的概念可以加強自己對於這類型文章的速度與理解度","keywords":"","version":null},{"title":"閱讀筆記: 「goss, 一個簡易且迅速的 server 驗證工具」","type":0,"sectionRef":"#","url":"/2022/06/01/reading-notes-56","content":"標題: 「goss, 一個簡易且迅速的 server 驗證工具」 類別: others 連結: https://github.com/aelsabbahy/goss 今天要介紹的是一個驗證工具 goss，該工具的目的非常簡單，讓系統管理員可以透過 YAML 的方式幫機器上的服務撰寫 Unit Testing 什麼情況會需要使用這類型工具？ 舉例來說，當你今天部署了一個全新機器(手動/自動後)，你安裝了下列軟體 sshdnginxdocker.... 同時你也根據需求事先創建了一些使用者，接者你想要驗證這些軟體與相關設定是否設定完成 最直覺的方式就是手動檢查，一個一個服務與設定人工檢查 而 goss 這套軟體的目的就是讓你用 YAML 的方式去撰寫你想要驗證的所有服務，可以用來驗證包含 使用者 (uid, gid, home, shell)Package: 系統是否有透過 rpm, de, pacman, apk 等安裝套件File: 檢查檔案資料夾是否存在Addr: 用來檢查 $IP:$Port 是否可以被存取Port: 用來檢查 $Port 是否有開啟DNS: 用來檢查是否可以解析特定 DNS Process: 檢查特定 Process 是否有開啟Mount: 檢查是 Mount Point 以及相關參數Kernel Param: 檢查 Kernel 參數...等 Goss 除了基本用法外，也有人基於其概念往上疊加 dgoss，用來驗證 Docker 的運行狀態，還有類似的 dcgoss，針對 docker-compose 來使用。 當然目前也很多人會透過 Ansible 的方式來自動化部屬，而 Ansible 本身其實也有相關的測試框架可以用來測試部署結果，所以到底要用哪類型的工具 來驗證 Server 等級的狀態就根據團隊需求與現有流程而定，比較沒有一個獨大的工具用法。","keywords":"","version":null},{"title":"閱讀筆記: 「如何提供專業 Code Review 意見」","type":0,"sectionRef":"#","url":"/2022/05/27/reading-notes-54","content":"標題: 「如何提供專業 Code Review 意見」 類別: others 連結: https://medium.com/@yar.dobroskok/how-to-review-the-code-like-a-pro-6b656101eb89 作者開門見山提到，如果團隊中沒有任何 code review 文化的話，請直接忽略這篇文章。 當團隊真的有 code review 的經驗時，才有機會透過本篇文章分享的一些概念來改善整個 code review 的流程，高效率低耗時。 作者認為一個好品質的 code review 能夠幫助團隊帶來下列好處 避免合併一些充滿 bug, 難讀, 無效率的程式碼到專案中開發者可以互相分享彼此的知識獲得關於實作上的各種意見確保團隊內的 coding style 一致 為了讓上述概念可以充分的導入到團隊專案中，作者分享了一些自己常用的概念與招式 事先準備一份 Checklist一個好的 review 流程就是要有一份檢查清單，這份清單上面描述的是每次程式碼合併都“必須”要符合的規則，同時也是團隊很重視的規則 這份清單沒有絕對標準，主要是根據團隊去思考哪些東西是最重要的，舉例來說 Branch, Commit 內容與名稱是否符合規範Code 是否有足夠的可讀性Codesytle 以及命名規範是否符合團隊文化資料夾/檔案結構是否符合團隊文化是否有包含相關測試文件是否有一起準備 這份清單的重點是只要列入那些被視為是非常必須且重要的項目就好，不然整個清單落落長其實意義也不高 盡可能的自動化上述檢查準備好前述清單後，下一個步驟就是想辦法將上述清單規範給自動化，譬如 透過 linters 來檢查 codesytle運行一些如 SonarQube, Codacy 等工具來幫忙檢查是否有潛在的低效率或是有漏洞的程式碼透過相關框架運行自動化測試並且得到相關的覆蓋率報表 當有辦法自動化這些操作後，下一個步驟就是要思考什麼時候執行？ 針對一些快速檢查，譬如 linter, beautifer 等工具，可以考慮整合到 pre-commit hook/ pre-push Git hook 等時間點運行 這樣就可以讓開發者快速檢查簡單錯誤針對一些比較花時間的檢查，譬如分析工具，測試以及相關建置流程這些都可以放到 CI pipeline 去運行 一切都準備完畢後就可以將其整合到整個 git 工具中，譬如只有當 CI pipeline 通過的 PR 才有被人 review 的需求，如果連自動化測試都沒有辦法通過，那就是開發者的 責任要去將其完成，一切準備就緒後才要開始最後一步 人工介入 review * 開始人工 review 時，因為前述自動化的過程已經幫忙檢查非常多的事項，所以這時候要專注的就是運作邏輯。 能的話作者建議 review 與其慢慢看 code 猜想不如直接跟開發者一起討論 review，可以避免來回溝通花費的無效時間 此外開發者也可以更清楚地去解釋所有實作的背後理由與考量。 作者也推薦採用 IDE 來進行 code review，很多 IDE 強大的功能都能夠幫助開發者更有效率地去檢視程式碼，譬如快速找到宣告點，被呼叫點以及整個資料結構的面貌等 這些都可以省下不少時間 最後最重要的是每次 PR 的大小不能太大，這點其實也是 Linux Kernel 內一直奉行的原則，過大的修改有太多檔案要看，同時也有更多可能潛在的不相容問題要注意 這對開發者與 reviewer 來說都是個沈重的負擔，因此能的話將修改以拆分成數個有意義的 PR 分別檢視會使得整體流程更講有效率，同時也可以避免 檔案太多時可能看不下去就直接無腦 +2 的蓋章行為","keywords":"","version":null},{"title":"閱讀筆記: 「使用 StressChaos 的經驗來學習 Pod Memory 使用情況」","type":0,"sectionRef":"#","url":"/2022/06/06/reading-notes-58","content":"標題: 「使用 StressChaos 的經驗來學習 Pod Memory 使用情況」 類別: others 連結: https://chaos-mesh.org/blog/how-to-efficiently-stress-test-pod-memory/ 本篇文章是來自於 Chaos Mesh 內的官方文章，主要是想要探討為什麼使用 Chaso Mesh 來測試記憶體狀況時結果實際狀況與設定的狀況不一致 文章一步一步的探討所有問題最後同時也整理了一些關於 Kubernetes 內的 Memory 相關機制 文章開頭，作者先部署了一個簡單的 Pod(只有一個 container)，該 Pod 針對 Memory 的部分設定 request: 200Mi, limits: 500Mi 結果作者到該 Container 中透過 free 與 top 的指令，都觀察到顯示的記憶體使用量高達 4G，這部分明顯與設定的 limits 500Mi 有所衝突 因此這邊產生了第一個點要特別注意 Kubernetes 是透過 cgroup 來計算與控管 Pod 的 Memory 用量，然而 free/top 等指令並沒有跟 cgroup 整合，因此就算跑到 container 中執行這兩個 指令看到的輸出其實都是 host 相關的，如果想要知道真正 container 相關的數量，還是要使用 cgroup 相關的指令來取得，譬如 cat /sys/fs/cgroup/memory/memory.usage_in_bytes 文章還有特別提到 Kubernetes 會針對 Request/Limit 的設定方式來將你的 Pod 分為三個等級，分別是 BestEffort, Burstable 以及 Guaranteed 其中當系統因為 OOM 不足要開始找受害者下手時，被設定為 Guaranteed 的應用程式則會是最低優先度，只有真的找不到其他受害者時才會來處理 Guaranteed 類型的 Pod。 最後則是更細部的去探討 Kubernetes 關於 Memory 的使用與管理 對於 Kubernetes 來說， 當系統上 Memory 數量不足時，可能會觸發 Evict 的行為，開始將部分運行的 Pod 給踢出該節點，而如同前面所述， Kubernetes 是依賴 Cgroup 來處理的，因此 /sys/fs/cgroup/memory/memory.usage_in_bytes 自然而然就成為其決策的重要參數 其中要注意的是 /sys/fs/cgroup/memory/memory.usage_in_bytes 代表的並不是 &quot;剛剛好系統上正在被使用的 Memory 數量&quot;，其數值則是由 &quot;resident set&quot;, &quot;cache&quot;, &quot;total_inactive_file&quot; 等三個面向組合而成，因此 Kubernetes 實際上會先從 /sys/fs/cgroup/memory/memory.usage_in_bytes 與 /sys/fs/cgroup/memory/memory.stat 取得相關參數，其中後者可以得到如 total_inactive_file 的數量 最後透過下列算式 working_set = usage_in_bytes - total_inactive_file 來得到一個名為 working_set 變數，該變數實際上也可以由 kubectl top 獲取，這也是 kubernetes 用來判斷是否執行 evict 的主要指標。 一個節點還有多少可用的 Memory 則是透過 memory.available = nodes.status.capacity[memory] - working_set 所以每個節點的總共量扣掉 workign_set 就是當前可用量，一旦當前可用量低於門檻時，也就是 k8s 執行 evict 之時 官網文件中其實有滿仔細的去描述這些操作行為 有興趣的可以花點時間全部看完https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/","keywords":"","version":null},{"title":"閱讀筆記: 「/proc/meminfo 與 free 指令的內容比較」","type":0,"sectionRef":"#","url":"/2022/06/03/reading-notes-57","content":"標題: 「/proc/meminfo 與 free 指令的內容比較」 類別: others 連結: https://access.redhat.com/solutions/406773 本篇文章要探討的是到底 /proc/meminfo 與 free 這個指令所列出來的 memory 相關資訊到底該怎麼匹配 雖然文章有特別強調主要是針對 RedHat Enterprise Linux 5,6,7,8,9，但是我認為大部分的 Linux 發行版的差異不會太大，畢竟整體都是來自於 Kernel 內的實作，我認為還是值得閱讀與理解。 對於大部分的系統管理員來說，勢必都有聽過 free 這個指令，該指令可以列出系統上當前的 memory 使用狀況，舉例來說通常會有 Total, Used, Free, Shared, Buffers, Cached 之類的欄位(不同版本可能會有些許差異)。 不熟悉的人可能會認為系統上的記憶體就只有“全部“,&quot;使用中&quot;,&quot;閒置&quot; 等三種類型，而實際上的記憶體處理遠比這些複雜，這也是為什麼 free 的輸出欄位會比較多的原因 除了 Free 指令外， Kernel 本身還有提供一個特殊的檔案位置讓使用者可以讀取當前的 memory 狀況，該位置為 /proc/memifno，其會提供如 MemTotal, MemFree, Buffers, Cached 等相關欄位 本文並不會針對每個欄位去探討實際上的意義，取而代之的是簡單的比對，透過幾個列表讓你清楚的知道 free 指令輸出的每個欄位要如何與 /proc/meminfo 去比較，要如何轉換等 特別要注意的是文章內有仔細地針對不同 RedHat Enterprise Linux 版本去分別探討，所以如果是 RedHat 系列的使用者更要好得閱讀並確保能夠理解自己當前使用版本的狀況","keywords":"","version":null},{"title":"閱讀筆記: 「如何寫出有意義的討論訊息 」","type":0,"sectionRef":"#","url":"/2022/05/30/reading-notes-55","content":"標題: 「如何寫出有意義的討論訊息 」 類別: others 連結: https://conventionalcomments.org/ 本篇文章非常短，大意就是探討透過文字討論事項時如何讓這些訊息更有意義，能夠讓目標受眾可以更快的理解該訊息的意義 假如今天有人想要反應「This is not worded correctly.」的概念，作者認為相對於直接撰寫「該文字措辭不當」，可以適當的加上一些是先定義好的前綴形容詞 譬如 「suggestion: This is not worded correctly. Can we change this to match the wording of the marketing page?」 「nitpick (non-blocking): This is not worded correctly.」 透過這些有共識的形容詞可以讓團隊之間的溝通速度快，減少誤解與猜測的可能性，讓整體的溝通效率更高，譬如 「suggestion: Let's avoid using this specific function… If we reference much of a function marked Deprecated, it is almost certain to disagree with us, sooner or later.」 「issue (ux,non-blocking): These buttons should be red, but let's handle this in a follow-up.」 透過這些形容詞能夠提醒目標受眾該討論的一些概念，同時也能夠讓對方更有想法下一步驟可以怎麼做。 作者就自己的習慣列舉了幾個下列前綴形容詞 Praise: 正面的去稱讚該事項Nitpick: 大部分都是一些非常小然後不會影響整體功能的小問題，譬如個人偏好等相關討論Suggestion: 針對當前目標有想要改進的部分，而且重點是要很明確且清楚的描述到底問題是什麼，以及為什麼需要這個改進。Issue: 強調當前主題下的潛在問題，如果確定該問題已經存在，搭配 Suggestion 來描述，否則可搭配 Question 來確認該問題是否存在Todo: 針對簡單且非必要的一些修改，主要是讓受眾能夠區分這些討論的重要性，能夠專注於當前更重要的事項Question: 如果對於當前主題有一些不確定的問題，就使用 Question 讓其他人認知到你有問題要發問，能夠幫助你更快的得到解答。 說到底這類型的討論都是一個習慣，就如同 coding style 一樣，所有共事者有一個共識原則，大家合作起來會更加有效率有方便 文中說的方法也不是唯一的辦法，但是團隊內有一個準則文化絕對會帶來好處的","keywords":"","version":null},{"title":"閱讀筆記: 「SRE 的工作介绍」","type":0,"sectionRef":"#","url":"/2022/07/11/reading-notes-65","content":"標題: 「SRE 的工作介绍」 類別: others 連結: https://www.kawabangga.com/posts/4481 本篇是簡體中文的文章，所以就不針對文章進行導讀。 文內我覺得有趣的部分是從各個面向來探討 SRE 這個職位的可能內容，畢竟一直以來 DevOps, SRE 等相關概念的詢問就沒有停止過，而事實上每個公司的 DevOps 工程師, SRE 工程師的工作內容也都不盡相同。 也因為不盡相同，本來就很難一言概括到底 SRE 的可能內容，因此個人算是滿推崇本篇文章的分析與整理方式，從不同角度出發去列舉各種可能的工作內容，譬如文章中分成三大類 架構服務平台業務導向 三種不同類型的 SRE 面對的對象不同，專注的事物也不同，我猜想很多人看完架構類型的論述可能第一個想法就跟以前的 SA/NA 網管有什麼不同？ 也許從 SRE 的 R 出發，去探討你今天到底想要針對什麼樣的目標去提供高可用性，也許更能夠方便探討彼此對於 SRE 的需求與認知","keywords":"","version":null},{"title":"閱讀筆記: 「分散式系統上的常見網路謬誤」","type":0,"sectionRef":"#","url":"/2022/06/13/reading-notes-60","content":"標題: 「分散式系統上的常見網路謬誤」 類別: others 連結: https://architecturenotes.co/fallacies-of-distributed-systems/ 本篇文章是探討分散式系統上很常被開發者所忽略的網路情況，這些情境都容易被忽略與考慮，但是每個點實際上都會影響整個系統的效能與功能 這些常常被忽略的網路情況包含 The network is reliableLatency is zeroBandwidth is infiniteThe network is secureTopology doesn't changeThere is one administratorTransport cost is zeroThe network is homogeneous The network is reliable 開發分散式系統的時候，一定要去考慮網路壞掉的情況，切記網路中的任何傳輸都不是 100% 穩定的。千萬不要假設所有封包與傳輸都沒有問題，必要時還要考慮重新連線，重新傳輸的情況。 Latency 網路時間還有一個要注意的就是延遲時間，通常 Client/Server 如果都是同一個系統內的服務時，這類型的時間可能非常短，如 ms 等級。 但是當 client 可能是來自真實使用者的手機裝置時，就要將 latency 這些因素給考慮進去，不能假設所有的 API 與網路請求都是秒回的情況。 更常見的還有導入 CDN 等方式透過地理性的位置來減少 client/server 之間要傳輸的距離。 文章內針對剩下的類別都有簡單的圖文並茂來解釋，淺顯易懂，有興趣的可以參閱全文","keywords":"","version":null},{"title":"閱讀筆記: 「面試人生 - 設計一個簡易的分散式 Job Scheduler」","type":0,"sectionRef":"#","url":"/2022/06/27/reading-notes-63","content":"標題: 「面試人生 - 設計一個簡易的分散式 Job Scheduler」 類別: others 連結: https://medium.com/@raxshah/system-design-design-a-distributed-job-scheduler-kiss-interview-series-753107c0104c 本篇文章是一個面試技術文，探討開發一個類似 Job Scheduler 的專案時應該要如何去設計整體系統來完成需求，整體的架構基於 KISS 的原則，就是簡單為主。 整個流程原則基本上是 理解所有功能需求，包含功能面以及非功能面瞭解可能的資料，根據規模大小與功能需求去推估出整體的規模大小根據上述需求去規劃整體架構，其中規模大小有時候可以幫忙歸納出 ”讀寫“彼此的比例，這個會影響架構設計 功能面常見類型如 針對使用者提供何種操作，譬如遞交一個 Job, 列出所有 Job(當前，歷史)每個 Job 的運行時間限制(ex, 5min)，同時 Job 可以重複運行或是只運行一次等不同用法Job 本身也有優先度的設計，可以插隊等 非直接功能面如 可動態擴充規模來支援不同量級的需求不論發生任何錯誤問題，使用者提交過的 Job 資訊都不能遺失非同步設計，使用者遞交 Job 後就可以繼續別的工作， Job 完成後會主動通知使用者 有了功能面的需求，接下來就是數量大小的需求，譬如該架構要可以達到每秒 1000 個 Job(1000QPS), 從這些需求下去估算大概需要多少 CPU 以及多少 Memory，同時這些數量還可以滿足功能面的需求，譬如每個 Job 可以運行最多五分鐘。 所以也許會得到需要 10,000 台的 (16C) 機器，以及 100 台(16GB) 的機器來提供服務 基本的運算可以快速的理解該需求到底需不需要分散式的架構來處理，本文的範例資料量就很明顯是 scale up 沒有辦法完成的。 接下來就基於分散式的架構去設計相關架構，包含如 Load BalancerBackendDBJob schedulerJob ExecutorQueueFile system 逐步的規劃這些架構，並且探討彼此元件之間的溝通方式，這些方式是如何互相組合來滿足功能面/非功能面的需求 詳細需求可以參考全文","keywords":"","version":null},{"title":"Docusaurus 使用 blog mode 後連結一直反白的問題","type":0,"sectionRef":"#","url":"/2023/10/08/docusaurus-link-active","content":"根據文件將 ladning page 移除並且使用 blog 做為首頁後，發現上方的連結永遠都會顯示反白，仔細檢查後發現連結被加上一個 navbar__link--active 的屬性。 仔細研究後發現官方有相關 Issue，根據 issue 所述針對 items 內補上 activeBaseRegex: '^/$', 即可。 最後呈現 { to: '/', label: '短篇筆記', position: 'left', activeBaseRegex: '^/$', }, ","keywords":"","version":null},{"title":"[MacOS ]隨手筆記 Sed 與 Rename 的使用","type":0,"sectionRef":"#","url":"/2023/10/09/sed-rename","content":"刪除特定一行 sed '/^keywords:/d' input &gt; output 刪除符合字串後的所有行數 sed '/^keywords/,$d' input &gt; output 搭配 Find 達到大量修改所有檔案 統一刪除所有檔案 find . -type f -exec sed -i '' '/^authors:/d' {} + Append 一行新的，換行要特別注意處理 find . -type f -exec sed -i '' '/^title/a\\ authors: hwchiu\\ ' {} + 假設環境中有大量檔案需要改名稱，透過 rename 這個工具可以快速達成 譬如以下範例會先用正規表達式找尋所有符合的檔案名稱，接者將所有 read-notes 都改名為 reading-notes rename 's/read-notes/reading-notes/' *read-notes-* ","keywords":"","version":null},{"title":"閱讀筆記: 「為什麼有些工程師不相信 Best Practices 」","type":0,"sectionRef":"#","url":"/2022/06/09/reading-notes-59","content":"標題: 「為什麼有些工程師不相信 Best Practices 」 類別: others 連結: https://blog.devgenius.io/why-some-developers-dont-believe-in-best-practices-8c03ea4f7e88 工程師想必對於 DRY, KISS(Keep It Simple, Stupid), YAGNI(You Ain’t Gonna Need It) 這些廣為流傳的開發原則並不陌生，這些原則都是過去許許多多優秀工程師透過自己的經驗而濃縮的開發準則。 但是作者觀察到有滿多工程師對於這些 開發原則/命名標準/最佳實驗經驗 等採取一個不信任態度，甚至覺得導入這些東西都是浪費時間。 因此本文章是作者探討什麼樣的工程師可能會不太願意去學習與導入這些廣為流傳的開發原則與經驗 Small projects and experience 作者開宗明義地說，小專案的成功經驗基本上是沒有辦法導入到大專案的開發的，小專案的特型譬如 1) 合作人員很少 2)專案時間少 這類型的特性只得技術債或是欠缺設計的程式架構不太會影響整個專案，畢竟專案太小，時間太短，後續的人不一定有機會真的觀察到這些潛在的問題。 而小專案還有一個很大的特性就是後續維護的人很有可能跟當初的開發者不同，所以對於開發者來說非常難去感受後續維護的痛苦與需求。 上述特性有可能會使得開發者覺得自己的開發經驗非常足夠且堪用，因此就會基於這樣的經驗來抵抗其他更多被推廣且推崇的開發原則與最佳實戰經驗。 因此對於一些只開發過小專案且沒有後續維護的工程師來說，其有可能就不會想要學習這些原則與經驗，畢竟自己的工作流程根本沒有機會感受到好處。 Reward and incentive 簡單來說就是「劣幣逐良幣」的概念，從工程師開發的角度來看，寫一個「可能比較好維護，未來比較不會有問題，高品質的程式碼」如果實務上不會帶來任何好處，甚至可能「績效表現比較不好」的情況下，那為什麼工程師要努力寫出高品質的程式碼？ 軟體團隊除了開發者之外，還會有相關的專案管理人員，產品管理人員以及最終使用者。 對於非技術人員來說，其在意的點更有可能專注於「程式開發速度，產品上線速度」，至於專案的後續維護性，開發靈活性等長時間才會看到的問題都不是他們所在意的點。 這種情況下，如何評價一個工程師的能力很有可能就變成「能夠多快的滿足專案需求，而不是寫出的程式碼品質」，所以對於工程師來說，快速寫出功能不考慮後續其他維護等長期問題反而更可能受到團隊重視，因為「功能出的快」。 所以如果團隊沒有辦法好好的去重視「高品質程式碼等考慮長期問題的開發原則」就有可能會導致工程師不會想要好好的去撰寫好的程式碼，長期下來這些工程師就可能會開始拒絕學習各種開發原則與最佳實踐的經驗，畢竟導入這些東西沒有任何實質上的幫助，反而初期可能會降低功能的上線速度。 Ignorance 「知彼知己，百戰百勝」，沒有花時間去學習這些開發原則的前後脈絡與適用情景就沒有辦法很順利的導入到開發專案中，所以實際上這些導入都要工程師花時間去學習與理解，然後嘗試導入與使用。 然而並不是每個工程師都願意花時間去學習，畢竟平常工作就是寫寫程式碼，寫寫文件，結果來說能動就好。花這些時間去學習這些東西 作者認為很多工程師「其實都不知道自己不知道什麼」，這導致他們很難去學習新技術與新概念，畢竟未知領域帶來的好處與優勢不是他們工作經驗中有機會去體驗到的，就如同前面所述，對一個擅長開發短期專案就拋棄給別人維護的人來說，其很難體會到各種長期維護與技術債的問題。 practices. Best practices, and poor practices get the same results initially 另外一個非常常見的問題就是「導入開發原則與好的開發經驗」與否對於初期開發來說很有可能沒有很任何明顯的差異。 從短期目標來看，兩者開發角度產生的結果不會差異太大，但是對於只有幾個月的小專案來說，後者甚至能夠更快的完成需求然後拍拍屁股結束閃人。 架構複雜性，技術債以及其他的爛程式碼可能產生的後續問題都需要時間發酵，所以團隊事主如果沒有辦法以長期觀念來看到程式開發的話，上述的問題就沒有辦法被重視，就如同前面所述，開發者就會「速度為主，品質為輔」的概念來開發，至於後續維護痛苦與否就是後續接手人的事情。 剩下其他論點就可以到原文去觀賞 Professionalism Short-term approach","keywords":"","version":null},{"title":"閱讀筆記: 「Cloudflare 06/21 災後報告」","type":0,"sectionRef":"#","url":"/2022/06/23/reading-notes-62","content":"","keywords":"","version":null},{"title":"Process​","type":1,"pageTitle":"閱讀筆記: 「Cloudflare 06/21 災後報告」","url":"/2022/06/23/reading-notes-62#process","content":"雖然嶄新的 MCP 架構其目的就是要提供更好更強的可用性，但是將舊架構給升級到新架構的過程中還是不夠完善。整體的更新流程直到最後一步驟才算是真正的接觸到全新 MCP 架構，這使得如果中間更新流程有錯必須要到最後才會觀察到 MCP 資料中心的網路炸了。 改善的方式則是未來的這些流程與自動化必須要加入更多關於 MCP 架構的測試來確保整體部署不會遇到預期外的結果。 ","version":null,"tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"閱讀筆記: 「Cloudflare 06/21 災後報告」","url":"/2022/06/23/reading-notes-62#architecture","content":"路由器的錯誤設定使得正確的路由規則沒有辦法順利的被傳達下去，最終使得網路封包無法如預期般地到達這些資料中心。 所以修復過程中就是要找出這些錯誤的設定並且修正，最終使得這些 BGP 能夠將正確的路由政策給轉發下去。 ","version":null,"tagName":"h2"},{"title":"Automaiton​","type":1,"pageTitle":"閱讀筆記: 「Cloudflare 06/21 災後報告」","url":"/2022/06/23/reading-notes-62#automaiton","content":"當前的自動化流程中有非常多的部分可以改進，這些改進有機會完全或是部分的去減緩問題發生時的影響程度。 有兩個目標是想要透過改善自動化機制達成的 減少問題發生時的影響範圍減少問題發生時的修復時間 結論 CDN 不通先上社群看同業有沒有哀嚎，大概就可以知道是不是自己的問題了? ","version":null,"tagName":"h2"},{"title":"閱讀筆記: 「DevOps is a failure」","type":0,"sectionRef":"#","url":"/2022/06/29/reading-notes-64","content":"標題: 「DevOps is a failure」 類別: others 連結: https://leebriggs.co.uk/blog/2022/06/21/devops-is-a-failure 本篇文章作者從不同的角度來聊聊 DevOps 這個詞所代表的含義與實作意義 第一段作者先閒聊一下自己與 DevOps 詞的歷史，接者直接拋出一個作者長期好奇的觀點 「每個人都一定聽過 DevOps 是一個需要 Dev + Ops 共同參與的文化，但是作者自己參與的 DevOps 相關會議與討論，與會者大部分都是 Ops 人員，而不是那些真正參與開發的 Dev 人」 困惑時期 接者作者聊聊自身多年前的經驗，當時的開發團隊宣稱該團隊是「true devops」，同時不跟作者的維運團隊討論各種維運需求，這過程讓作者非常困惑，為什麼對方會說自己是 true devops 而又不找自己探討維運需求 作者後來與該開發團隊深聊後終於理解對方的意思，原來該開發團隊身兼開發與維運，該團隊使用 boto3 加上一些腳本來管理應用程式的生命週期，同時該團隊招募的 「full stack engineer」除了基本的後端技術外，也要對 AWS 有不少的熟悉與經驗。 對方的舉動更加困惑了作者，畢竟公司當時採取類似 Netflix 的方式來打造一個平台來讓所有開發者可以更輕鬆的去管理這些東西，而該開發團隊的舉動完全是反其道而行，到底為什麼要這麼做？？ Pulumi 時期 當作者加入 Pulumi 時期時，作者開始使用一些知名工具如 GitLab, Terraform, Kubernetes 等工具來打造一個適合開發者的好用平台，然而每次想要將該平台給推廣給開發者時總是屢屢碰壁，總是會聽到如「你們的東西我不熟悉，我們還是習慣自己打造的工具」等類似說詞給打發掉。 作者接下來不斷嘗試說服開發團隊來使用自己打造的超級平台，鼓勵他們參加 DevOps 相關活動等各種方式，最終得到的還是類似「我們會按照我們自己的方式去嘗試～謝囉」之類的回覆 回顧 回顧過往，作者發現錯的是自己，一直以來相信的 DevOps 願景「讓 Ops 停止說 No, 讓 Dev 停止說&quot;yo~ 今天部署吧&quot;」 其實並不真實，作者認為 2022 的今天， DevOps 真正的含義是 「維運端的人努力說服開發人員按照維運人員的想法去做事情」 綜觀所有號稱跟 DevOps 有關的工具，你會發現幾乎都跟維運有關，每個跟 DevOps 有關的職缺列舉的技能也是滿滿的跟維運有關，對作者來說， DevOps 工程師跟過往的 System Admin 根本沒有太大分別，差異只有把「實體機房建置，上架機器」 v.s 「雲端機器建置，創立VM」而已。 文章內後半部分還有一些作者的想法，有興趣的可以閱讀完畢 本篇文章的想法滿有趣的，譬如作者提到想要幫開發團隊建立一個維運平台卻屢屢碰壁。 Ops 可能會覺得 Dev 一直不停重複打造工具沒有效率，不如使用自己打造的好平台 Dev 可能會覺得 Ops 不懂自己的需求，不如自己根據需求打造 同樣的敘述放到不同的規模，譬如 dev -&gt; 5 人的專職開發團隊 dev -&gt; 50 人的專職產品團隊 後者的角度也許會覺得團隊人數夠多，可以自己處理自己的需求，不需要仰賴公司提供一個萬能平台來處理一切，同時跨 team 合作可能還會使得很多事情效率低落，溝通成本過大。 歡迎留言探討你的想法","keywords":"","version":null},{"title":"閱讀筆記: 「啟動 container 直接 kernel panic 的 bug」","type":0,"sectionRef":"#","url":"/2022/06/15/reading-notes-61","content":"標題: 「啟動 container 直接 kernel panic 的 bug」 類別: others 連結: https://bugs.launchpad.net/ubuntu/+source/linux-aws-5.13/+bug/1977919 本篇文章探討的是一個關於 Ubuntu kernel(5.13+) bug 產生的各種悲劇，已知受害的雲端業者包含 linux-oracle linux-azure linux-gcp linux-aws 等常見大廠。 簡單來說，預設設定下只要簡單跑一個 container 譬如docker run -it ubuntu bash 就可以直接觸發 kernel panic，直接讓你系統死亡強迫重啟 整個 bug 結論來說就是，一連串的操作最後有機會導致使用到一個 null pointer，然後 kernel 就炸拉... 相關的修復可以參閱這個連結，裡面有大概提到問題發生點以及修復方式。https://kernel.ubuntu.com/git/ubuntu/ubuntu-impish.git/commit/?id=6a6dd081d512c812a937503d5949e4479340accb","keywords":"","version":null},{"title":"Why Cloudflare","type":0,"sectionRef":"#","url":"/page/5","content":"標題: 「透過 Helm 與 Terraform 來自動 Re-new Cloudflare origin CA」 類別: usecase 連結: https://awstip.com/auto-renew-cloudflare-origin-ca-with-terraform-and-helm-d28be3f5d8fa?source=linkShare-91a396987951-1645539866&amp;gi=a18b2bbd9604 本篇文章是過工具介紹文，探討如何基於 Helm 與 Terraform 這兩個不同層級的工具來處理 Cloudflare 的憑證。 Why Cloudflare 根據 W3Techs 的調查顯示， 81.2% 的網站都使用 Cloudflare 來提升讀取速度或安全防護。 透過 CDN 的概念與機制，網站可以讓全球使用者有更快的讀取速度，此外也愈來愈多的網站會透過 Cloudflare 來處理如機器人, DDOS 之類的流量攻擊，畢竟要自己架設網站處理這些攻擊非常困難 因此讓 Cloudflare 這類型的網站來幫忙過濾與處理能夠讓團隊更專注於本身的業務開發與維運 Kubernetes 想要在 Kubernetes 內妥善管理所有使用的憑證其實也是一個麻煩事情，除了要能夠設置正確來創立憑證外，能夠於到期前自動 re-new 也是一個不可或區的功能。 Kubernetes 內跟憑證有關的最知名專案我想就是 Cert-Manager，而 Cloudflare 也基於此專案撰寫了相關的 Kubernetes Controller，如 Origin CA 等 因此本文使用的功能與示範都會基於 cert-manager 與 Cloudflare 的架構。 目的 本文的目的是希望能夠將過往手動的繁瑣步驟給自動化，讓 Kubernetes 可以獲得 Cloudflare 提供的好處，如憑證與相關域名等。 內文是基於 Terraform 作為出發點，然後透過 Kubernetes Provider 的方式來與之互動，一步一步的安裝各種資源最後成功於叢集內獲得相關域名的 SSL 憑證以及其他資源","keywords":"","version":null},{"title":"GitOps covers only a subset of the software lifecycle","type":0,"sectionRef":"#","url":"/page/7","content":"標題: 「The pains of GitOps 1.0」 類別: cicd 連結: https://codefresh.io/about-gitops/pains-gitops-1-0/ 作者認為很多文章都闡述 GitOps 對於部署帶來的好處，但是軟體世界沒有十全十美的東西，所以作者就探討了 12 個其認為 GitOps 的缺點 註: 本篇文章是 2020 年底的文章，所以文章探討的內容也許當年沒有很好的解決方式，但是現在已經有了比較好的處理方式。我個人覺得文章的某些部分有點太牽強，已經假設 GitOps 是個萬能解法，什麼問題都要靠這個。就這個問題是不管有沒有 GitOps 都會存在的問題，有點為了反對而反對，與其說 GitOps 的缺點不如說沒有解決的問題。 這邊就節錄幾個文章中探討的議題，剩下有興趣的可以閱讀全文 GitOps covers only a subset of the software lifecycle 作者認為 GitOps 的精神「我想要將 Git 專案內的所描述的狀態給同步到叢集中」這點只能處理應用程式部署的問題，但是其他的流程 譬如編譯程式碼，運行單元測試，安全性檢查，靜態掃描等過程都沒有辦法被 GitOps 給處理。 作者不滿的點主要是很多 GitOps 的工具好像都會宣傳自己是個全能的解決方案，能夠處理所有事情，但是實際上卻沒有辦法。 實際上其專注的點就是應用程式部署策略為主的部分，其餘部分還是團隊要有自己的方式去處理 Splitting CI and CD with GitOps is not straightforward 過往很多團隊都會將 CI/CD 給整合到相同的 pipeline 中去處理，通常是最後一個階段就會將應用程式給部署到目標叢集，然而有外部 Controller 實作的 GitOps 解決方案會使得 CI/CD 兩者脫鉤，好處來說就是 pipeline 不需要去處理部署，只需要專心維護 Git 內的資訊，後續都讓 Controller 來處理。 然後某些團隊本來的 CI/CD 流程會於部署完畢後還會進行一些測試或是相關操作，這部分會因為 GitOps 將部署給弄走導致整個流程不太好處理，畢竟要如何讓 GitOps 部署完畢後又要可以觸發其他的工作也是額外要處理的事情 There is no standard practice for GitOps rollbacks 雖然 GitOps 的核心是透過 Git Commit 去控制當前部署的版本，那發生問題時到底該怎麼處理，如何去 rollback? 作者舉兩種範例 讓 GitOps 去指向之前的 Git Commit針對 Git 使用 Git revert 等相關操作來更新最新的內容 作者認為沒有一個標準來告訴使用者該怎麼使用以及處理 Observability for GitOps (and Git) is immature 作者認為目前現有的 GitOps 工具都沒有辦法提供下列答案 目前生產環境是否有包含 Feature XBug X,Y 是否只有存在於 Staging 環境？ 還是生產環境也有？ 註: 有什麼概念是天生就可以有這些東西的..? GitOps 有點無妄之災","keywords":"","version":null},{"title":"個人經驗","type":0,"sectionRef":"#","url":"/page/11","content":"","keywords":"","version":null},{"title":"交通​","type":1,"pageTitle":"個人經驗","url":"/page/11#交通","content":"矽谷灣區最著名的一個特色就是塞車，上班塞車，下班塞車，永遠都在塞車，所以過往很多人都會採取彈性上班的方式來避免車流，或是採取大眾運輸工具的方式來通勤。 以我個人為例，過往每天上班的通勤時間大概平均兩個小時，主要分成開車與大眾運輸 大眾運輸: 腳踏車配上火車通勤，由於火車時刻表固定，所以上下班時間比較沒有彈性開車: 開車的話大概可以將時間縮短到單趟 40 分鐘左右，但是整個高速公路都是一片紅，其實開起來很悶 遠端工作後，每天可以省下大約兩個小時的時間，這部分可以拿來上班工作，也可以拿來處理私人事情，時間上更加彈性 ","version":null,"tagName":"h2"},{"title":"工作​","type":1,"pageTitle":"個人經驗","url":"/page/11#工作","content":"遠端在家工作後，最大的幾個變化就是 工作自律 基本上一整天就是在電腦前面，所以沒有開會的時候，其實都是自己去控制自己的工作節奏，這部分好壞的意見都有聽過，大家也會彼此分享如何讓自己工作更加專心，而不會被家裡的舒適環境給影響。 會議全面遠端化且數量上升 過往一些小事情要討論時，可能就直接現場約一約，到小會議室直接討論。然而目前則是因為遠端會議，所以全部都要事先預約時間來確保對方目前在電腦前。有時候一個小事情卻要花更多的時間來定案，我個人認為效率是不如過往的。 此外之前也出現如 Zoom Fatigue 這樣的說法，過多的會議內容導致大家開會疲倦，分心 就我個人的心得來看，自己能夠維持好工作的節奏，該完成的工作事項能完成就好，此外被打擾中斷工作的機會也更少了，整體而言利大於弊， ","version":null,"tagName":"h2"},{"title":"互動​","type":1,"pageTitle":"個人經驗","url":"/page/11#互動","content":"員工之間的會面都是透過線上會議，缺少實體見面的互動，這部分也是見仁見智。有些人不喜歡交流，覺得 「上班當同事，下班不認識」是其工作原則，那應該會滿喜歡這種模式的。但是也有人喜歡實體交流，每天吃個一起吃個午餐，聊聊一些工作以外的事情，分享彼此的事情也都是一種生活。 這部分我就沒有太大意見，有好有壞，偶而還是會透過 slack 與同事聊聊幹話，分享最近的生活，只是少了聲音的互動。 ","version":null,"tagName":"h2"},{"title":"福利​","type":1,"pageTitle":"個人經驗","url":"/page/11#福利","content":"遠端工作以後，公司內本來的福利近乎全滅，什麼咖啡，零食，飲料，冰箱內各種飲品，甚至免費午餐等都不再擁有，一切都要依賴自己處理。 對於一個自住的邊緣人來說，變成每天都要煩惱該怎麼處理今天的飲食，「每餐叫外送，荷包先消瘦」是一個顧慮點，自己煮飯又是一個額外的挑戰，買菜/備料/烹飪/善後四個步驟往往也花費不少時間在處理，如果將這個時間與通勤時間組合起來，未必可以省下時間。 我個人平常就習慣煮飯，已經練就如何快速料理與一次準備多日便當，所以這個情況比較不會有太大問題。 倒是我有聽過朋友因為沒有公司零食的迫害，遠端工作期間被迫減肥，也算是一個附加好處 一個美好的夢想就是辦公室可以縮編，「公司省租金，員工可加薪」 美國現況 疫情以來，各大公司陸陸續續宣布相關的遠端工作政策，而規範的工作日期也隨者疫情的擴散而延後，下面就整理目前我知道部分公司的工作內容，此外政策不一定適用於所有員工，必要性的情況下，部分員工還是要定期前往辦公室。 Apple: 員工可以遠端工作直到 2021 初期 Google: 員工至少可以遠端工作到 2021/07 Facebook: 員工至少可以遠端工作到 2021/07 Twitter: 員工可以選擇永遠遠端工作 Fujitsu: 員工可以選擇永遠遠端工作 Microsoft: 可以遠端工作直到 2021/02 Amazon: 員工至少可以遠端工作到 2021/01 Netflix: 員工遠端工作直到全面接受疫苗 此外，也有人對於遠端工作提出了一些負面說詞，譬如 Netflix 董事長則認為遠端工作沒有帶來正面效果，反而使得討論工作更加困難。 這部分真的就是沒有定案，完全見仁見智，而 Google 最近的調查則顯示只有 10% 不到的員工想要回到過往天天進辦公室的日子， 以下節錄自 Google 官方推特 除了公司本身政策的變動外，居住地遷移最近也是活動頻繁，譬如 Oklahoma 洲則有 Tulsa Remote 計畫，向所有遠端工作者提供一萬美元的獎勵，只要你願意搬來這邊即可。 根據報導指出，舊金山的兩房公寓每個月可能略低於四千美金，而 Tusla 則不到一千美金，再租金花費方面可以說是節省不少開銷。 心得 就我過去五年的台灣工作經驗，我認為台灣公司要跟進遠端工作政策實屬不易，幾個原因如下 勞基法規定下，公司都要提出員工上下班出勤時間的紀錄，這部分會用來評估是否有加班超時 對於遠端工作者來說，要如何打卡來滿足這些紀錄是一個行政上的問題，而不是技術上的問題 更多時候是公司行政團隊願不願意嘗試探討可能性，並嘗試看看沒人監督，老闆放不下 這種情況下，我認為信任是最基本的基礎，老闆信任員工可以遠端工作依然保持良好效率，員工也真的能夠滿足一定的效率來證明制度可行。過往就有聽過台灣發生過老闆接受遠端工作，結果員工私下兼差來賺錢，最後兩邊信任破壞，一切回歸辦公室制度。 Reference https://www.cnbc.com/2019/01/10/vermont-will-pay-you-10000-to-move-there-and-work-remotely---.htmlhttps://www.wsj.com/articles/facebook-to-shift-permanently-toward-more-remote-work-after-coronavirus-11590081300https://www.wsj.com/articles/google-to-keep-employees-home-until-summer-2021-amid-coronavirus-pandemic-11595854201https://www.cnn.com/2020/07/27/tech/google-work-from-home-extension/index.htmlhttps://www.bbc.com/news/business-53303364https://www.cnbc.com/2020/09/23/google-ceo-sundar-pichai-considering-hybrid-work-from-home-models.html ","version":null,"tagName":"h2"},{"title":"來源","type":0,"sectionRef":"#","url":"/tags/food","content":"本身算是 Costco 愛好者，每個禮拜的蛋白質基本上都是從 Costco 取得 常買的有 雞胸肉雞腿排牛奶蛋白丁蝦仁鮭魚 這篇文章主要用來記錄冷凍鮭魚排的資訊，每一份的大小以及熱量等資訊 來源 Costco 基本上有滿多的鮭魚可以購買，有在冷藏處比較新鮮的挪威鮭魚，也有在冷凍庫的冷凍鮭魚。 由於我大部分都是製作便當，因此任何的食材在烹飪完畢後都會冷藏並且隔天透過微波爐來加熱飲食 因此在選擇上我都還是以冷凍鮭魚為主，只有特別想要當天吃才會選擇冷藏鮭魚 本篇主要以冷凍鮭魚為主,其包裝如下 大小 根據 包裝背後的標示，本包裝大概含有 2KG 的鮭魚，然而實際上裡面的包裝物並不是真的如其所說有 20 份這麼多，其實每片鮭魚大小都頗大的。 實際上拿出一片使用電子秤來進行實測，測出來的重量大概是 300g 左右 因此一整包的份量大概會落在7片左右，畢竟每片鮭魚的大小都會有點差距，不過大概可以記住一片300g就好 營養 本篇就直接使用該包裝背後所提供的成份表來計算每片鮭魚排的營養素 將常見的營養素基於 100 以及 300 公克列舉出來 100公克\t300公克熱量\t149 KCal\t447 KCal 蛋白質\t17.9 g\t53.7 g 粗脂肪\t8.5 g\t25.5 g 飽和脂肪\t2.2 g\t6.6 g 碳水化合物\t0.1 g\t0.3 g 鈉\t59 mg\t177 mg 這樣來看如果今天一餐吃一片冷凍鮭魚排的話，，攝取的蛋白質大概是 54 g 左右，熱量450卡，如果單純考慮價錢與蛋白質的比例的話，還是雞胸/雞腿的比例比較好，不過換換口味吃個鮭魚也不錯 烹調 基本上好好的煎就沒有什麼問題了，唯一要注意的是要確認內部比較厚的部分需要比較長的時間才會熟透，可以切半去料理會比較快。","keywords":"","version":null},{"title":"Why Cloudflare","type":0,"sectionRef":"#","url":"/tags/helm","content":"標題: 「透過 Helm 與 Terraform 來自動 Re-new Cloudflare origin CA」 類別: usecase 連結: https://awstip.com/auto-renew-cloudflare-origin-ca-with-terraform-and-helm-d28be3f5d8fa?source=linkShare-91a396987951-1645539866&amp;gi=a18b2bbd9604 本篇文章是過工具介紹文，探討如何基於 Helm 與 Terraform 這兩個不同層級的工具來處理 Cloudflare 的憑證。 Why Cloudflare 根據 W3Techs 的調查顯示， 81.2% 的網站都使用 Cloudflare 來提升讀取速度或安全防護。 透過 CDN 的概念與機制，網站可以讓全球使用者有更快的讀取速度，此外也愈來愈多的網站會透過 Cloudflare 來處理如機器人, DDOS 之類的流量攻擊，畢竟要自己架設網站處理這些攻擊非常困難 因此讓 Cloudflare 這類型的網站來幫忙過濾與處理能夠讓團隊更專注於本身的業務開發與維運 Kubernetes 想要在 Kubernetes 內妥善管理所有使用的憑證其實也是一個麻煩事情，除了要能夠設置正確來創立憑證外，能夠於到期前自動 re-new 也是一個不可或區的功能。 Kubernetes 內跟憑證有關的最知名專案我想就是 Cert-Manager，而 Cloudflare 也基於此專案撰寫了相關的 Kubernetes Controller，如 Origin CA 等 因此本文使用的功能與示範都會基於 cert-manager 與 Cloudflare 的架構。 目的 本文的目的是希望能夠將過往手動的繁瑣步驟給自動化，讓 Kubernetes 可以獲得 Cloudflare 提供的好處，如憑證與相關域名等。 內文是基於 Terraform 作為出發點，然後透過 Kubernetes Provider 的方式來與之互動，一步一步的安裝各種資源最後成功於叢集內獲得相關域名的 SSL 憑證以及其他資源","keywords":"","version":null},{"title":"2021 回顧","type":0,"sectionRef":"#","url":"/tags/annual-review","content":"疫情肆虐下的 2021 年即將結束，按照慣例來個年度回顧紀錄。 這一整年完全遠端工作，算是人生經驗中非常不同的一年，台灣副業今年的發展也稍微多一點，不過整體演講數量就有明顯下降 2021 回顧 粉絲頁 - 矽谷牛的耕田筆記發文: 199 篇，發文數量比預估(183)篇還要多，整體來說學到很多東西不過也花了很多時間在閱讀與分享。Ithome 鐵人賽發文: 30 篇，這次的主題圍繞於 Rancher &amp; GitOps 的介紹，最後也很榮幸的獲得了佳作。演講紀錄: 3 篇。整年度的演講數量下降，跨時區的限制實在不方便每次半夜四點起來參與台灣社群，比較特別是的參與 HiEXPERT 2021 DevOps 領航者論壇 的討論分享。部落格原創文章: 6 篇，想寫的很多，時間不夠多...線上課程: 2 門課程，今年本來想要把 Networking 系列一口氣弄完的，沒有預期的順利，時間忙碌。 線上課程包的線上演講: 2 次，這是個一整年的計畫，每個月給課程內的學生有一次分享，還有十次才結束。 書籍出版: 1 本，人生第一本實體書籍，因應鐵人賽的結果很順利的就體驗了出書的過程。運動回歸: 因應疫情荒廢一年多的運動習慣直到有兩劑疫苗後才重新復活，直至年底整個運動表現有整體回溫，不過年底膝蓋有點小傷所以又要好好休養一下 硬舉: 210 kg臥推: 125 kg深蹲: 170 kg肩推: 75 kg 新技能學習: 今年開始認真學習並且繼續鑽研的技術是舉重，先從抓舉開始練習並且養成瑜伽的習慣來加強活動度與柔軟度企業教育訓練: 1 次，年尾很驚訝的收到一個企業教育訓練的機會，看看有沒有機會讓這個也變成常態服務長時間出遊: 3 次，下半年去了芝加哥, Reno 以及西雅圖度假放鬆，體驗不同生活。 2022 展望 保持運動習慣，想把體重給降回到 7 字頭了書籍，課程: 量力而為原創文章: 花更多時間到這一塊粉絲頁: 繼續多閱讀多分享，藉由閱讀逼迫自己學習一些平常碰不到的東西生活順遂: 工作之餘還是要定期放鬆旅遊","keywords":"","version":null},{"title":"個人經驗","type":0,"sectionRef":"#","url":"/tags/life","content":"","keywords":"","version":null},{"title":"交通​","type":1,"pageTitle":"個人經驗","url":"/tags/life#交通","content":"矽谷灣區最著名的一個特色就是塞車，上班塞車，下班塞車，永遠都在塞車，所以過往很多人都會採取彈性上班的方式來避免車流，或是採取大眾運輸工具的方式來通勤。 以我個人為例，過往每天上班的通勤時間大概平均兩個小時，主要分成開車與大眾運輸 大眾運輸: 腳踏車配上火車通勤，由於火車時刻表固定，所以上下班時間比較沒有彈性開車: 開車的話大概可以將時間縮短到單趟 40 分鐘左右，但是整個高速公路都是一片紅，其實開起來很悶 遠端工作後，每天可以省下大約兩個小時的時間，這部分可以拿來上班工作，也可以拿來處理私人事情，時間上更加彈性 ","version":null,"tagName":"h2"},{"title":"工作​","type":1,"pageTitle":"個人經驗","url":"/tags/life#工作","content":"遠端在家工作後，最大的幾個變化就是 工作自律 基本上一整天就是在電腦前面，所以沒有開會的時候，其實都是自己去控制自己的工作節奏，這部分好壞的意見都有聽過，大家也會彼此分享如何讓自己工作更加專心，而不會被家裡的舒適環境給影響。 會議全面遠端化且數量上升 過往一些小事情要討論時，可能就直接現場約一約，到小會議室直接討論。然而目前則是因為遠端會議，所以全部都要事先預約時間來確保對方目前在電腦前。有時候一個小事情卻要花更多的時間來定案，我個人認為效率是不如過往的。 此外之前也出現如 Zoom Fatigue 這樣的說法，過多的會議內容導致大家開會疲倦，分心 就我個人的心得來看，自己能夠維持好工作的節奏，該完成的工作事項能完成就好，此外被打擾中斷工作的機會也更少了，整體而言利大於弊， ","version":null,"tagName":"h2"},{"title":"互動​","type":1,"pageTitle":"個人經驗","url":"/tags/life#互動","content":"員工之間的會面都是透過線上會議，缺少實體見面的互動，這部分也是見仁見智。有些人不喜歡交流，覺得 「上班當同事，下班不認識」是其工作原則，那應該會滿喜歡這種模式的。但是也有人喜歡實體交流，每天吃個一起吃個午餐，聊聊一些工作以外的事情，分享彼此的事情也都是一種生活。 這部分我就沒有太大意見，有好有壞，偶而還是會透過 slack 與同事聊聊幹話，分享最近的生活，只是少了聲音的互動。 ","version":null,"tagName":"h2"},{"title":"福利​","type":1,"pageTitle":"個人經驗","url":"/tags/life#福利","content":"遠端工作以後，公司內本來的福利近乎全滅，什麼咖啡，零食，飲料，冰箱內各種飲品，甚至免費午餐等都不再擁有，一切都要依賴自己處理。 對於一個自住的邊緣人來說，變成每天都要煩惱該怎麼處理今天的飲食，「每餐叫外送，荷包先消瘦」是一個顧慮點，自己煮飯又是一個額外的挑戰，買菜/備料/烹飪/善後四個步驟往往也花費不少時間在處理，如果將這個時間與通勤時間組合起來，未必可以省下時間。 我個人平常就習慣煮飯，已經練就如何快速料理與一次準備多日便當，所以這個情況比較不會有太大問題。 倒是我有聽過朋友因為沒有公司零食的迫害，遠端工作期間被迫減肥，也算是一個附加好處 一個美好的夢想就是辦公室可以縮編，「公司省租金，員工可加薪」 美國現況 疫情以來，各大公司陸陸續續宣布相關的遠端工作政策，而規範的工作日期也隨者疫情的擴散而延後，下面就整理目前我知道部分公司的工作內容，此外政策不一定適用於所有員工，必要性的情況下，部分員工還是要定期前往辦公室。 Apple: 員工可以遠端工作直到 2021 初期 Google: 員工至少可以遠端工作到 2021/07 Facebook: 員工至少可以遠端工作到 2021/07 Twitter: 員工可以選擇永遠遠端工作 Fujitsu: 員工可以選擇永遠遠端工作 Microsoft: 可以遠端工作直到 2021/02 Amazon: 員工至少可以遠端工作到 2021/01 Netflix: 員工遠端工作直到全面接受疫苗 此外，也有人對於遠端工作提出了一些負面說詞，譬如 Netflix 董事長則認為遠端工作沒有帶來正面效果，反而使得討論工作更加困難。 這部分真的就是沒有定案，完全見仁見智，而 Google 最近的調查則顯示只有 10% 不到的員工想要回到過往天天進辦公室的日子， 以下節錄自 Google 官方推特 除了公司本身政策的變動外，居住地遷移最近也是活動頻繁，譬如 Oklahoma 洲則有 Tulsa Remote 計畫，向所有遠端工作者提供一萬美元的獎勵，只要你願意搬來這邊即可。 根據報導指出，舊金山的兩房公寓每個月可能略低於四千美金，而 Tusla 則不到一千美金，再租金花費方面可以說是節省不少開銷。 心得 就我過去五年的台灣工作經驗，我認為台灣公司要跟進遠端工作政策實屬不易，幾個原因如下 勞基法規定下，公司都要提出員工上下班出勤時間的紀錄，這部分會用來評估是否有加班超時 對於遠端工作者來說，要如何打卡來滿足這些紀錄是一個行政上的問題，而不是技術上的問題 更多時候是公司行政團隊願不願意嘗試探討可能性，並嘗試看看沒人監督，老闆放不下 這種情況下，我認為信任是最基本的基礎，老闆信任員工可以遠端工作依然保持良好效率，員工也真的能夠滿足一定的效率來證明制度可行。過往就有聽過台灣發生過老闆接受遠端工作，結果員工私下兼差來賺錢，最後兩邊信任破壞，一切回歸辦公室制度。 Reference https://www.cnbc.com/2019/01/10/vermont-will-pay-you-10000-to-move-there-and-work-remotely---.htmlhttps://www.wsj.com/articles/facebook-to-shift-permanently-toward-more-remote-work-after-coronavirus-11590081300https://www.wsj.com/articles/google-to-keep-employees-home-until-summer-2021-amid-coronavirus-pandemic-11595854201https://www.cnn.com/2020/07/27/tech/google-work-from-home-extension/index.htmlhttps://www.bbc.com/news/business-53303364https://www.cnbc.com/2020/09/23/google-ceo-sundar-pichai-considering-hybrid-work-from-home-models.html ","version":null,"tagName":"h2"},{"title":"Cluster Topology","type":0,"sectionRef":"#","url":"/tags/kubernetes/page/2","content":"標題: 「Paypal 如何調整 Kubernetes 讓其規模達到四千節點，20萬個 Pod」 類別: usecase 連結: https://medium.com/paypal-tech/scaling-kubernetes-to-over-4k-nodes-and-200k-pods-29988fad6ed 摘要: Paypal 過去一直都使用 Apache Mesos 來運行其大部分的服務，而其最近正在針對 Kubernetes 進行一個評估與測試，想瞭解如果需要轉移到 Kubernetes 會有哪些問題需要挑戰與克服。 本篇文章著重的是效能問題，原先的 Apache Mesos 可以同時支持一萬個節點，因此 Kubernetes 是否可以拿到相同的效能 而本文節錄的就是擴充 Kubernetes 節點中遇到的各種問題以及 Paypal 是如何修正與調整讓 Kubernetes 可能容納盡可能更多的節點。 Cluster Topology 三個 Master 節點與三個獨立的 ETCD 叢集，所有服務都運行於 GCP 上。工作節點與控制平面的服務都運行於相同的 GCP Zone 上。 Workload 效能測試方面是基於 k-bench 去開發的測試工具，基於平行與依序等不同方式來創建 Pod/Deployment 兩種資源。 Scale 測試初期先以少量的節點與少量的 Pod 開始，接者發現還有提升的空間就會開始擴充 Pod 與節點的數量。測試的應用程式是一個要求 0.1m CPU 的無狀態應用程式。最初的工作節有點 4 個 CPU，根據測試可以容納大概 40 Pod 左右。 接者就是不停地擴充數量，先從一千個節點開始，接者調整Pod 的數量直到 32,000 個 Pod。最後擴充到 4,100 個節點並且配上 200,000 個 Pod. 過程後期有調整節點的 CPU 數量讓其能夠容納更多的 Pod 數量 文章接下來開始針對 API Server, Controller Manager, Scheduler, ETCD 元件遇到的問題並且如何解決，中間提到了不少參數，這部分應該是大部分使用者都比較不會去研究與使用的參數 因此我認為本篇文章非常值得閱讀。 ETCD 的部分遇到很嚴重的效能問題，作者團隊觀察到大量的 Raft 溝通失敗個訊息，觀測到跟硬碟寫入速度有關，然而 GCP 沒有辦法單純增加效能，必須要同時提升硬碟空間，所以使用上彈性不變。 不過就算採用 1TB 的 PD-SSD ，當 4 千個節點同時加入到 Kubernetes 時依然會遇到效能上的問題，團隊最後決定使用本地端的 SSD 來想辦法改善寫入速度，結果又遇到 ext4 的一些設定 過程很多問題也很多解決方式。 結論來說: k8s 複雜","keywords":"","version":null},{"title":"困惑時期","type":0,"sectionRef":"#","url":"/tags/dev-ops","content":"標題: 「DevOps is a failure」 類別: others 連結: https://leebriggs.co.uk/blog/2022/06/21/devops-is-a-failure 本篇文章作者從不同的角度來聊聊 DevOps 這個詞所代表的含義與實作意義 第一段作者先閒聊一下自己與 DevOps 詞的歷史，接者直接拋出一個作者長期好奇的觀點 「每個人都一定聽過 DevOps 是一個需要 Dev + Ops 共同參與的文化，但是作者自己參與的 DevOps 相關會議與討論，與會者大部分都是 Ops 人員，而不是那些真正參與開發的 Dev 人」 困惑時期 接者作者聊聊自身多年前的經驗，當時的開發團隊宣稱該團隊是「true devops」，同時不跟作者的維運團隊討論各種維運需求，這過程讓作者非常困惑，為什麼對方會說自己是 true devops 而又不找自己探討維運需求 作者後來與該開發團隊深聊後終於理解對方的意思，原來該開發團隊身兼開發與維運，該團隊使用 boto3 加上一些腳本來管理應用程式的生命週期，同時該團隊招募的 「full stack engineer」除了基本的後端技術外，也要對 AWS 有不少的熟悉與經驗。 對方的舉動更加困惑了作者，畢竟公司當時採取類似 Netflix 的方式來打造一個平台來讓所有開發者可以更輕鬆的去管理這些東西，而該開發團隊的舉動完全是反其道而行，到底為什麼要這麼做？？ Pulumi 時期 當作者加入 Pulumi 時期時，作者開始使用一些知名工具如 GitLab, Terraform, Kubernetes 等工具來打造一個適合開發者的好用平台，然而每次想要將該平台給推廣給開發者時總是屢屢碰壁，總是會聽到如「你們的東西我不熟悉，我們還是習慣自己打造的工具」等類似說詞給打發掉。 作者接下來不斷嘗試說服開發團隊來使用自己打造的超級平台，鼓勵他們參加 DevOps 相關活動等各種方式，最終得到的還是類似「我們會按照我們自己的方式去嘗試～謝囉」之類的回覆 回顧 回顧過往，作者發現錯的是自己，一直以來相信的 DevOps 願景「讓 Ops 停止說 No, 讓 Dev 停止說&quot;yo~ 今天部署吧&quot;」 其實並不真實，作者認為 2022 的今天， DevOps 真正的含義是 「維運端的人努力說服開發人員按照維運人員的想法去做事情」 綜觀所有號稱跟 DevOps 有關的工具，你會發現幾乎都跟維運有關，每個跟 DevOps 有關的職缺列舉的技能也是滿滿的跟維運有關，對作者來說， DevOps 工程師跟過往的 System Admin 根本沒有太大分別，差異只有把「實體機房建置，上架機器」 v.s 「雲端機器建置，創立VM」而已。 文章內後半部分還有一些作者的想法，有興趣的可以閱讀完畢 本篇文章的想法滿有趣的，譬如作者提到想要幫開發團隊建立一個維運平台卻屢屢碰壁。 Ops 可能會覺得 Dev 一直不停重複打造工具沒有效率，不如使用自己打造的好平台 Dev 可能會覺得 Ops 不懂自己的需求，不如自己根據需求打造 同樣的敘述放到不同的規模，譬如 dev -&gt; 5 人的專職開發團隊 dev -&gt; 50 人的專職產品團隊 後者的角度也許會覺得團隊人數夠多，可以自己處理自己的需求，不需要仰賴公司提供一個萬能平台來處理一切，同時跨 team 合作可能還會使得很多事情效率低落，溝通成本過大。 歡迎留言探討你的想法","keywords":"","version":null},{"title":"發生前提","type":0,"sectionRef":"#","url":"/tags/linux/page/2","content":"連結: https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts 今天跟大家分享一個 UDP 於 Linux Kernel 內的 Race Condition 問題。這問題我以前於 Linux Kernel 3.14 也有採過一樣的雷，但是到今日都還沒有一個很漂亮的解決方案，這邊就快速的跟大家介紹一下這個問題&gt; 是什麼，以及跟 k8s 有什麼關係 發生前提 使用 UDP 這種沒有重送機制的協定Kernel 有開啟 conntrack 此功能 發生條件 相同的 Client 短時間內透過 UDP (也許是不同 thread) 送出兩個 UDP 封包到外面，對於 Linux Kernel 來說，會希望透過 conntrack 來追蹤每一條連線，但是底層建立的時候會有一些會有一些機制，因此當兩個封 包同時進入的時候，有可能就會因為先後順序導致第二個封包被丟棄 可能發生問題 DNS 的請求封包預設情況下會同時透過 UDP 送出 A &amp; AAAA 兩個封包，而這兩個封包如果很巧的採到這個情況，然後你的 A 封包就沒有辦法順利解出 DNS，最後就要等五秒的 timeout 來重新發送 下偏這篇文章就是 weave works 遇到 DNS 5秒 timeout 的問題，然後仔細的將我上面所寫的總結給解釋清楚，每一個步驟發生什麼事情，什麼是 conntrack 以及暫時的 workaround 是什麼 之後會在跟大家分享目前一些解決方法怎麼做","keywords":"","version":null},{"title":"The network is reliable","type":0,"sectionRef":"#","url":"/tags/network","content":"標題: 「分散式系統上的常見網路謬誤」 類別: others 連結: https://architecturenotes.co/fallacies-of-distributed-systems/ 本篇文章是探討分散式系統上很常被開發者所忽略的網路情況，這些情境都容易被忽略與考慮，但是每個點實際上都會影響整個系統的效能與功能 這些常常被忽略的網路情況包含 The network is reliableLatency is zeroBandwidth is infiniteThe network is secureTopology doesn't changeThere is one administratorTransport cost is zeroThe network is homogeneous The network is reliable 開發分散式系統的時候，一定要去考慮網路壞掉的情況，切記網路中的任何傳輸都不是 100% 穩定的。千萬不要假設所有封包與傳輸都沒有問題，必要時還要考慮重新連線，重新傳輸的情況。 Latency 網路時間還有一個要注意的就是延遲時間，通常 Client/Server 如果都是同一個系統內的服務時，這類型的時間可能非常短，如 ms 等級。 但是當 client 可能是來自真實使用者的手機裝置時，就要將 latency 這些因素給考慮進去，不能假設所有的 API 與網路請求都是秒回的情況。 更常見的還有導入 CDN 等方式透過地理性的位置來減少 client/server 之間要傳輸的距離。 文章內針對剩下的類別都有簡單的圖文並茂來解釋，淺顯易懂，有興趣的可以參閱全文","keywords":"","version":null},{"title":"Adapter Pattern","type":0,"sectionRef":"#","url":"/docs/techPost/2013/adapter-pattern","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Adapter Pattern","url":"/docs/techPost/2013/adapter-pattern#introduction","content":"舉例來說，今天有一家廠商開發數位電視A，並有且對應的API可以讓使用者去操縱 於是就有一家廠商根據這個API開發了對應的遙控器B。  這時候一切都很正常work,但是不久之後，該廠商又開發了一台更新型的電視C， 這時候提供的API卻跟原來的完全不一樣，這時候原本的遙控器就完全沒有辦法去操控這台新的電視C 這時候解決方法如下 重新製作一個新的遙控器，然後這個新的遙控器可以支援新舊兩款電視。對新的電視B製造一個轉接器，能夠再新舊API運作，使得舊有的遙控器能夠順利使用。  如果重新製作一個遙控器，每次有新的API出現，就要重新改寫遙控器，此外在維護上面也複雜。 因此這邊採用轉接器的方式製作。  再智慧電視B的前面多一層轉接器，這轉接器提供舊有一致的API給搖控器使用，底層使用新的API與電視B溝通，如此一來 遙控器本身依然可以正常運作的去操控新舊兩款電視。 ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Adapter Pattern","url":"/docs/techPost/2013/adapter-pattern#example","content":" //television A public class TelevisionA implements Television{ public old_control{ .... } .... } //television C public class TelevisionB{ public new_control{ ...} .... } //television adapter public class TVBdapter implements Television{ TelevisionB tb; public old_control{ tb.new_control() ....} ... } //controller public class controller{ televisionList = {TelevisionA, TVBdapter }; for(Television tv : televisionList){ tv.old_control(...) } .... }  ","version":"Next","tagName":"h2"},{"title":"C++ 語言中的 new","type":0,"sectionRef":"#","url":"/docs/techPost/2013/c-new","content":"","keywords":"","version":"Next"},{"title":"new operator​","type":1,"pageTitle":"C++ 語言中的 new","url":"/docs/techPost/2013/c-new#new-operator","content":"平常最熟悉的new 就是屬於這種角色 string* myStr = new string(&quot;haha&quot;); int* myInt = new int(123);  這種new就是所謂的new operator,是C++語言內建的，類似sizeof 沒有辦法overriding或是改變其行為。 每次呼叫new operator實際上會有三件事情在背後運作。 Allocate Memory for ObjectCall Constructor to init that memoryreturn a pointer which point to this Object 所以今天呼叫string* myStr = new string(&quot;aa155495&quot;);會做下列事情 1. void* memory = operator new (sizeof(string)); 2. call string::string() on memory; 3. string* ptr = static_cast&lt;string*&gt;(memory); 4. return ptr  第一步就是要先去memory中要空間，這部分就是透過 operator new 來完成。 第二部就是在要到的空間上，呼叫對應物件的建構式，這部分就是透過 placement new 來完成。 接者就是取得一個該型態的指標，並且回傳。 ","version":"Next","tagName":"h2"},{"title":"operator new​","type":1,"pageTitle":"C++ 語言中的 new","url":"/docs/techPost/2013/c-new#operator-new","content":"不同於 new operator ,operator new 是一個運算符號，就類似+-*/[]&lt;&gt;這種，所以可以overridding. 當呼叫operator new時，會嘗試從heap中去取得對應大小的空間，如果成功則返回，否則會去呼叫new_handler來處理 並且繼續重覆該事情直到得到exception為止。 所以有operator new 呼叫完畢只會有兩種情況 申請空間成功截取到bad_alloc exception 如果要overridding 的話，可以採下列方式 class A { A(); ~A(); void* operator new(size_t size){ cout&lt;&lt;&quot;hello&quot;; return ::operator new(size); } };  這邊我們重載operator new，讓他會先輸出hello，之後就呼叫最原本的operator new來幫忙操作。 此外，也有對應的operator delete與之呼應。 ","version":"Next","tagName":"h2"},{"title":"placement new​","type":1,"pageTitle":"C++ 語言中的 new","url":"/docs/techPost/2013/c-new#placement-new","content":"第三個new是用來定位用的，在特定的位置上去呼叫特定物件的建構式 int main() { void* S = operator new(sizeof(A)); A* p = (A*)s; A* ptr = new(p) A(1234); )  這邊先透過operator new來取得一塊空地，然後取得一個該房子的控制權(指標) 然後要在這塊空地上蓋房屋，因此就在new(p) 這個空地上，蓋上了 A這個房屋，然後以1234去呼叫建構式。 蓋完房子後會在回傳一個指標指向該地區。 ","version":"Next","tagName":"h2"},{"title":"Python 動態載入模組","type":0,"sectionRef":"#","url":"/docs/techPost/2013/dynamicimport","content":"Python 動態載入模組 最近在弄irc機器人，希望這個機器人能夠靈活一些，因此把所有功能都弄成module 機器人在掛上這些module來完成各種能力，心中的設想架構如下 --------ircbot |-------config.json | | |-------server.py | | |-------modules | |-----googleSearch | | | |---googleSearch.py | |-----wikiSearch | | | |---wikiSearch.py | |-----echoServer | |---echoServer.py config.json 是主要的設定檔，其中包含了要載入哪些module server.py是主要的server程式，可以透過指令重新去載入config.json 當server載入設定檔後，會動態的去把modules資料夾底下的py都載入，這樣未來當有新功能要增加的時候，只要修改config.json，然後發送指令 叫server重新載入就可以獲得新功能，而不需要整個server重開。 參考Telling import where to look - am I stuck with sys.path?以及python3.0中重载模块後 整理如下 首先要先創造出一個pseudo-package，sys.modules是一個dict的物件，由module name mapping 到 對應的module 這邊先創造一個tuple，key='plugins' value = type為sys的物件，叫做plugins 接下來把我們要載入的module路徑都加入道剛剛創立的物件之中， 利用path這個串列，把路徑一一加入進去 這樣在pseudo-package plugins底下，已經看得到我們的module了! 最後再利用importlib.import_module('plugins.'+moduleNmae) 來把這些module全都載入 值得注意的是，由於module被載入一次後，即使你修改了code,利用這個方法重新載入，依然沒有辦法改變其行為 所以必須要使用reload這個function重新載入值，並讀取新的內容，因為module載入一個很大開銷的動作，因為每次都要尋找文件、編譯成 bytecode、轉成執行碼，因此這個行為必須要透過reload強制重做才可以達成。 最後就可以呼叫每個module的方法來達成動態載入的功能了 範例如下 def dynamicLoadModules(self): sys.modules['plugins'] = self.plugins = type(sys)('plugins') self.plugins.__path__ = [] for path in self.config['MODULES']: path = os.path.join(sys.path[0],'modules',path) self.plugins.__path__.append(path) ##dynamic load modules self.modules = [] self.modules = [ importlib.import_module('plugins.'+module) for module in self.config['MODULES']] for module in self.modules: #用此來重新載入module http://eriol.iteye.com/blog/1113588 reload(module) for module in self.modules: module.run() ","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/facade-pattern","content":"","keywords":"","version":"Next"},{"title":"Compare​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/facade-pattern#compare","content":"使用前  使用後  以上圖為例子，再最原始的情況下，每個Client連線進來後都必須要跟後方三個Manager進行溝通，這樣的話對於Client方面會很複雜，同時整個系統的密合度太高。 如果透過 Facade pattern 設計一個介面，把與內部的溝通都隱藏起來，然後外部的client只要與這個介面溝通即可，未來若是內部有任何變動，只要針對Facade的介面去修改，Client端不必去修改也能夠正常運作。 ","version":"Next","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/facade-pattern#examples","content":" #original system public class MapManager{ public Point getLocation(String name){....} public Point setLocation(String name, Point point) {....} ... } public class ItemManager{ public ArrayList&lt;Item&gt; getItemList(String name) {....} public void deleteItem(String name, Item item) {....} public void addItem(String name, Item item) {....} ... } public class SkillManager{ public ArrayList&lt;Skill&gt; getSkill(String name) {....} public void learnSkill(String name, String skillname) {...} public void forgetSkill(String name, String skillname) {...} ... } #Facade interface public class PersonManager{ private MapManger mapManager; private ItemManager itemManager; private SkillManager skillManager; public PersonManager (){ mapManager = MapManager::GetSingleTon(); itemManager = ItemManager::GetSingleTon(); skillManager = SkillManager::GetSingleTon(); } public loadData(String name,Point point,ArrayList&lt;Item&gt; itemList,ArrayList&lt;Skill&gt; skillList){ point = mapManager.getLocation(name); itemList =ItemManager.getItemList(name); skillList = skillManager.getSkill(name); } } # Client public class Client { public Point point; public ArrayList&lt;Item&gt; itemList; public ArrayList&lt;Skill&gt; skillList; public Client(String name){ PersonManager pm = new PersonManager(); PersonManager.loadData(name,point,itemList,skillList); } }  ","version":"Next","tagName":"h2"},{"title":"FloodLight--Module","type":0,"sectionRef":"#","url":"/docs/techPost/2013/floodlight-module","content":"FloodLight--Module Floodlight把module分成core跟application兩個方向為主 core的部分提供的都是比較核心的功能，譬如PacketIN,PacketOUt,或是拓樸的更動...等 而application則是用這些core的功能來達到一些進階的功能 如防火牆、最短路徑搜尋....等 這邊就簡單研究一下Core Module中的相關功能。 #Core Module# ##FloodlightProvider## 相關檔案 FloodlightProvider.javaIFloodlightProviderService.javaController.java (主要功能實現) ###FloodlightProvider.java### 實作 IFloodlightModule 並且override五個function getModuleServices() 回傳一個含有 IFloodlightProviderService.class 的 Collection Container。 getServiceImpls() 回傳一個型態為 key = &lt;Class&lt;? extends IFloodlightService&gt; value = IFloodlightService的map container,已包含一個成員，其value 為 Controler()。 在FloodlightModuleLoader中會執行initModules此function,會把所有要用到的module都產生一份物件，並且透過floodlightModuleContext.addService把這些物件都收集起來，這樣所有的module就可以透過floodlightModuleContext.getServiceImpl(Class&lt;T&gt; service)的方式來取得其他的module物件。 getModuleDependencies() 回傳這個module會用到的相關module,在FloodlightModuleLoader中會執行使用BFS的方式來取得所有用到的module,接者才會去執行module相關的初始化行為。 init() module的初始，是個internal的初始化，這個步驟中可以去取得其他module所提供的service以及初始化本module會使用到的data structure。 會在initModules中依序執行每個module的init。 startup() module的初始，是個external的初始，此時可以與其他module進行相關操作，如取得其他module的相關結構，是module相關設定的最後一步驟。 在startupModules中會依序執行毎個module的startup()。 註:初始化的順序是依據BFS跑出來的結果的，沒有任何規律。 ###IFloodlightProviderService.java### 繼承自IFloodlightService並且提供一些基本功能，這邊列出幾個重要功能。 bcStore 是一個FloodlightContextStore&lt;Ethernet&gt; 的成員，用來存取Ethernet的值，包含了MAC address,VlanID,Ehternet Type。 存取時會用到IFloodlightService 所定義的一個變數CONTEXT_PI_PAYLOAD addOFMessageListener(OFType type, IOFMessageListener listener) 註冊成為OpenFlow message Listener。 OFType 共有 (HELLO ERROR ECHO_REQUEST ECHO_REPLY VENDOR FEATURES_REQUEST FEATURES_REPLY GET_CONFIG_REQUEST GET_CONFIG_REPLY SET_CONFIG ACKET_IN FLOW_REMOVED PORT_STATUS PACKET_OUT FLOW_MOD PORT_MOD STATS_REQUEST STATS_REPLY BARRIER_REQUEST BARRIER_REPLY QUEUE_GET_CONFIG_REQUEST QUEUE_GET_CONFIG_REPLY) IOFMessageListener 則是一個Interface,必須實做該介面並override下列function。public Command receive(IOFSwitch sw, OFMessage msg, FloodlightContext cntx); removeOFMessageListener(OFType type, IOFMessageListener listener) 取消註冊，與(2)對應。 Map&lt;OFType, List&lt;IOFMessageListener&gt;&gt; getListeners(); 取得所有Listener。 Map&lt;Long, IOFSwitch&gt; getSwitches()取得所有連上的switch與其ID public void addOFSwitchListener(IOFSwitchListener listener); 加入一個Switch Listener，這樣此module就可以聽取相關Switch 相關 event。 IOFSwitchListener 是一個Interface,必須實做該介面並且override下列function。public void addedSwitch(IOFSwitch sw);public void removedSwitch(IOFSwitch sw);public void switchPortChanged(Long switchId);public String getName(); removeOFSwitchListener(IOFSwitchListener listener); 取消註冊，與(7)對應 public boolean injectOfMessage(IOFSwitch sw, OFMessage msg); 把某個OpenFlow Message送回controller重新處理一次。 public void handleOutgoingMessage(IOFSwitch sw, OFMessage m,FloodlightContext bc); 還不是很確定 public void addInfoProvider(String type, IInfoProvider provider); 加入一個Info Provider。 IInfoProvider是個介面，必須override下列function。public Map&lt;String, Object&gt; getInfo(String type);當rest API發出 特定type的請求時，就會呼叫到getInfo(type)。 public void removeInfoProvider(String type, IInfoProvider provider); 取消一個Provider,與(10)對應。 ###Controller.java### ##DeviceManager## ##LinkDiscoveryManager## ##TopologyService## ##RestApiServer## ##ThreadPool## ##MemoryStorageSource## ##FlowCatch## ##PacketStreamer## #Application Modules# ##Forwarding## ##Firewall## #Reference#For Developers - Floodlight Controller - OpenFlowHub - OpenFlow news and projects","keywords":"","version":"Next"},{"title":"Floodlight-modules-dependency","type":0,"sectionRef":"#","url":"/docs/techPost/2013/floodlight-modules-dependency","content":"","keywords":"","version":"Next"},{"title":"FIFO​","type":1,"pageTitle":"Floodlight-modules-dependency","url":"/docs/techPost/2013/floodlight-modules-dependency#fifo","content":"今天完全不考慮每個module之間的dependency，依照module被載入的順序來決定處理封包的順序 那我們就把所有進來的封包依照 LLDP-&gt;DEVICE-&gt;Forwarding-&gt;VirtualNetwork 這樣的順序去處理。 這邊要注意的是 Forwarding會把封包用最短路徑的方式傳送到destinationVirtualNetwork會根據mac address建立一個layer 2的virtual network 假如依照FIFO的方式來處理封包  封包先經過 Forwarding決定如何轉送，並且把相關的flow entry送給對應的switch。 封包在經過 VirtualNetwor來決定如何處理，但此時已經沒有任何意義了，因為即使這邊發現封包的流向是不同VN要阻擋，但是先前的forwarding已經通知switch如何轉送，因此VirtualNetwor就變成雞肋了。 所以根據這個情形可以發現如果採用FIFO的模式，就必須要很仔細的設定每個module之間的關係，這樣當module數量過多時，會很麻煩，所以這不是一個很好的辦法。 ","version":"Next","tagName":"h2"},{"title":"Priority​","type":1,"pageTitle":"Floodlight-modules-dependency","url":"/docs/techPost/2013/floodlight-modules-dependency#priority","content":"如果每個module都能夠設定一個優先度，然後依照優先度去排序得到一個運行的順序，那這樣每次撰寫新的module 只要設定一個優先度就好，不需要苦力的調整全部的順序。 這邊思考了一下，如果優先度採用數字的方式來比較，那一旦module變多的時候是否也要每個module都要做些調整，所以這部分一開始設定的時候就要想遠一點，避免未來的調整。 這邊介紹一下在floodlight中是如何決定module的運行順序的。 首先每個module必須要先override下列兩個function isCallbackOrderingPrereq(String type, String name) isCallbackOrderingPostreq(String type, String name)  第一個function代表 哪些module的哪些event要在我之前執行 第二個function代表 哪些module的哪些event要在我之後執行 每個module之間就依靠這些function來決定 誰先誰後，因此假設今天四個module彼此的宣告如下 以下的type 都假定為 PACKET_IN。 ","version":"Next","tagName":"h2"},{"title":"LLDP​","type":1,"pageTitle":"Floodlight-modules-dependency","url":"/docs/techPost/2013/floodlight-modules-dependency#lldp","content":"不在乎誰在我前面不在乎誰在我後面 ","version":"Next","tagName":"h3"},{"title":"DEVICE​","type":1,"pageTitle":"Floodlight-modules-dependency","url":"/docs/techPost/2013/floodlight-modules-dependency#device","content":"LLDP 必須在我之前不在乎誰在我後面 ","version":"Next","tagName":"h3"},{"title":"Forwarding​","type":1,"pageTitle":"Floodlight-modules-dependency","url":"/docs/techPost/2013/floodlight-modules-dependency#forwarding","content":"LLDP跟DEVICE 必須在我之前不在乎誰在我後面 ","version":"Next","tagName":"h3"},{"title":"VirtualNetwork​","type":1,"pageTitle":"Floodlight-modules-dependency","url":"/docs/techPost/2013/floodlight-modules-dependency#virtualnetwork","content":"LLDP跟DEVICE 必須在我之前Forwarding 必須在我之後 接下來有兩大步驟 找尋terminal modules用terminal modules為起點跑DFS，建立modules的執行順序 Terminal module指的是其後面不會有任何module要執行的module，因此這種module可以拖延期執行次序，因為該module本身沒有限制一定要在哪裡執行。 ","version":"Next","tagName":"h3"},{"title":"Algorithm (pseudo)​","type":1,"pageTitle":"Floodlight-modules-dependency","url":"/docs/techPost/2013/floodlight-modules-dependency#algorithm-pseudo","content":"for(int i=0;i&lt;modules.size;i++) { isTerminal = true; for(int j=0;j&lt;modules.size;j++) { if( modules[j] go after modules[i]) { isTerminal = false; break; } } if(isTerminal) terminalQueue.add(modules[i]); }  每個modules都去問其他的module，根據每個module先前定義的優先權function，如果所有的modules都沒有要求要在我之後那我就是terminal modules，反之只要有一個modules必須要在我之後執行，則就跳開。 接者針對每個terminal module都去跑一個DFS，來建立執行順序。 for(int i=0;i&lt;terminalQueue.size();i++) { visit(terminalQueue[i]); } function visit(listener) { if(!visted.contain(listener) { visted.add(listener) for(int i=0;i&lt;modules.size();i++) { if( modules[i] go before listener) visit(modules[i]) } orderingQueue.add(listener); } }  每次進入visit後，就去問其他的modules，看有沒有modules要在我前面執行，然後遞迴下去 如果這個modules都問完了，就把他加入到執行queue裡面。 以剛剛的範例來說，Terminal modules只會有一個modules 就是forwarding 然後以forwarding為起點去跑DFS，則過程如下 listener:forwarding visted:forwarding ordering:empty action:choose Device (因為forwarding 有宣示 DEVICE要在我之前)  listener:Device visted:forwarding,Device ordering:empty action:choose LLDP (因為DEVICE 有宣示 LLDP要在我之前)  listener:LLDP visted:forwarding,Device,LLDP ordering:LLDP action:找不到符合條件的modules,所以把自己加入到ordering。  listener:Device visted:forwarding,Device,LLDP ordering:LLDP,Device action:找不到符合條件的modules,所以把自己加入到ordering。  listener:forwarding visted:forwarding,Device,LLDP ordering:LLDP,Device action:choose VirtualNetwork (因為 VirtualNetwork 有宣示 forwarding 要在我之後)  listener:VirtualNetwork visted:forwarding,Device,LLDP,VirtualNetwork ordering:LLDP,Device,VirtualNetwork action:找不到符合條件的modules,所以把自己加入到ordering。 (因為DEVICE跟LLDP已經visted了，所以不會繼續跑)  listener:forwarding visted:forwarding,Device,LLDP,VirtualNetwork ordering:LLDP,Device,VirtualNetwork,forwarding action:找不到符合條件的modules,所以把自己加入到ordering。 (因為其他都已經visted了，所以不會繼續跑) 按照這個流程跑完，可以發現執行順序就是 LLDP,Device,VirtualNetwork,forwarding 符合我們的預期，同時這種設計可以讓module針對多個modules去進行相依性的處理。 但是這方面如果沒有寫好，就會造成dead lock，當發生deadlock時，就會找不到terminal modules，此時floodlight就會丟出錯誤。算是有做個錯誤預防。 modules dependency的部分就大概到這邊，有機會在看看nox &amp; pox是如何處理這方面的。 ","version":"Next","tagName":"h3"},{"title":"Basic install","type":0,"sectionRef":"#","url":"/docs/techPost/2013/bulid-octopress","content":"","keywords":"","version":"Next"},{"title":"Install​","type":1,"pageTitle":"Basic install","url":"/docs/techPost/2013/bulid-octopress#install","content":"增加一個category_list 外掛,新增 plugins/category_list_tag.rb  module Jekyll class CategoryListTag &lt; Liquid::Tag def render(context) html = &quot;&quot; categories = context.registers[:site].categories.keys categories.sort.each do |category| posts_in_category = context.registers[:site].categories[category].size category_dir = context.registers[:site].config['category_dir'] category_url = File.join(category_dir, category.gsub(/_|\\P{Word}/, '-').gsub(/-{2,}/, '-').downcase) html &lt;&lt; &quot;&lt;li class='category'&gt;&lt;a href='/#{category_url}/'&gt;#{category} (#{posts_in_category})&lt;/a&gt;&lt;/li&gt;\\n&quot; end html end end end Liquid::Template.register_tag('category_list', Jekyll::CategoryListTag)  增加aside 修改 source/_includes/asides/category_list.html 加入下列,記得把category那行頭尾改成{}，這邊是因為使用{}的話，我頁面顯示會不如預期，所以為了顯示而修改  &lt;section&gt; &lt;h1&gt;Categories&lt;/h1&gt; &lt;ul id=&quot;categories&quot;&gt; &lt;% category_list %&gt; //change &lt;&gt; to {} &lt;/ul&gt; &lt;/section&gt; &gt;  修改 _config.yml,根據自己需求調整 default_asides: [asides/category_list.html, asides/recent_posts.html]  ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"Basic install","url":"/docs/techPost/2013/bulid-octopress#usage","content":"寫新文章的時候，底下會出現categories的標籤，在後面增加其類別即可 categories: [System]如果想要同時增加到多個類別，就用逗號隔開 categories: [System ， Life]  Comments 先到Disqus註冊帳號,其中會有個short_name,這個名稱記住下來，等等會用到  修改_config.yml ，這邊就把剛剛紀錄的short_name給設定上去 disqus_short_name: your_disqus_short_name disqus_show_comment_count: true 上傳重新整理看看  Backup Octopress source  git add * git commit -m &quot;message&quot; git push origin source  [Reference] Setup Octopress on Windows From Zero to 100 为octopress添加分类(category)列表 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/floodlightmodule-topology","content":"","keywords":"","version":"Next"},{"title":"Cluster​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#cluster","content":"Cluster再controller的觀點中，ㄧ個cluster就是一個SCC( strongly connective component)，cluster中的switch都有辦法到達彼此，因此不同cluster間的switch基本上不能互相傳送資料，除非中間經過ㄧ些傳統的switch( controller不知道有傳統switch的存在) 這個檔案定義ㄧ個cluster所需要的基本屬性  protected long id; // the lowest id of the nodes protected Map&lt;Long, Set&lt;Link&gt;&gt; links; // set of links connected to a node.  id:該cluster中最小ID的switch dpid.links:該cluster中所有的link,每個link包含 soruce switch dpidsource switch portdestination switch dpiddestination switch port ","version":"Next","tagName":"h2"},{"title":"ITopologyListener​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#itopologylistener","content":"Topology提供的callBack function. void topologyChanged(List&lt;LDUpdate&gt; linkUpdates);  當有拓樸發生變化的時候，就會呼叫此function,並且把所有變動的Link (LLDP)都當作參數傳入。 ","version":"Next","tagName":"h2"},{"title":"ITopologyService​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#itopologyservice","content":"Topology提供的主動API,主要可以讓其他的module來獲取topology上的相關資訊 isAttachmentPointPort根據switch's dpid、switch port 判斷是否有device連結到該swtich的port上面。 若存在就是回傳false,否則回傳true,不清楚為什麼要反轉。getOpenflowDomainId根據switch dpid，取得該swtich所屬cluster。getL2DomainId目前的版本中，做的事情同等於 getOpenflowDomainIdinSameOpenflowDomain判斷兩個switch是否屬於同一個cluster。getSwitchesInOpenflowDomain取得該swtich所屬cluster中的所有switch。inSameL2Domain目前的版本中，做的事情同等於 inSameOpenflowDomainisBroadcastDomainPort判斷該switch的某個port是否屬於broadcast tree的一部分。isAllowed不明，總是回傳true。isConsistent不是很清楚，感覺是判斷一個device的新後位置是否相同。待確認 deviceManager.java &amp;&amp; Device.java 都有使用到，看起來跟位置有相關。isInSameBroadcastDomain兩個swtich的port是否屬於同一個broadcast tree。getPortsWithLinks取得一個switch上的所有ports。getBroadcastPorts取得一個switch上所有屬於broadcast tree port的portisIncomingBroadcastAllowed判斷該switch的某個port是否能夠接受broadcast的封包，若該port不屬於broadcast tree就會丟棄該封包。getOutgoingSwitchPort意義不明getIncomingSwitchPort意義不明getAllowedOutgoingBroadcastPort尚未實作，意義不明getAllowedIncomingBroadcastPort尚未實作，意義不明getBroadcastDomainPorts取得broadcast tree的所有成員set&lt;dpid,port&gt;getTunnelPorts取得目前的TopologyInstance中是否是tunnelPorts 意義不明getBlockedPorts尚未實做完成，估計是取得所有被封住的ports。getPorts取得該switch上所有未被隔離且可以使用的port。 ","version":"Next","tagName":"h2"},{"title":"NodePair​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#nodepair","content":"(此class目前沒有被使用) ","version":"Next","tagName":"h2"},{"title":"OrderdNodePair​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#orderdnodepair","content":"(此class目前沒有被使用) ","version":"Next","tagName":"h2"},{"title":"NodePortTuple​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#nodeporttuple","content":"定義了一個Tuple,包含了一個swtich dpid以及對應的port  protected long nodeId; // switch DPID protected short portId; // switch port id  nodeId :switch dpidportId :switch port id ","version":"Next","tagName":"h2"},{"title":"TopologyInstance​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#topologyinstance","content":"網路拓璞真正的物件，每次同時只會有一個Instance存在，當網路拓樸改變時，就會創造一個新的TopologyInstance，大部分的service都是在這邊真正實作。 TopologyInstance主要分成兩個部分 Cluster的相關運算TopologyService的implement Cluster的運算則透過compute來完成，其中又有四大項目分別去跑。 compute identifyOpenflowDomainsaddLinksToOpenflowDomainscalculateShortestPathTreeInClusterscalculateBroadcastNodePortsInClusters identifyOpenflowDomains 找出在controller底下的所有cluster,並且把所有的swtich都區分到各自的cluster中。 addLinksToOpenflowDomains 把每個cluster中的所有link都記錄起來，以 Cluster物件存放所有link。 calculateShortestPathTreeInClusters 計算cluster中每個switch到彼此間的最短路徑。 for cluster in clusters for switch in cluster.getSwitch calculate shortest path for switch in the cluster  calculateBroadcastNodePortsInClusters計算每個cluster中的broadcast tree。 實際上就只是取該cluster中最小id switch的shortest path tree 當作該cluster的broadcast domain。 經過這四個步驟後，整個拓樸中 兩兩swtich間的路徑就決定好了，同時broadcast tree也建立完成，所以也不會有broadcast storm的問題。 ","version":"Next","tagName":"h2"},{"title":"TopologyManager​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-topology#topologymanager","content":"網路拓樸的管理者待續 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/floodlight-core-restapi","content":"","keywords":"","version":"Next"},{"title":"特定switch id​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlight-core-restapi#特定switch-id","content":"core/web/SwitchStatisticsResource.java 可以看到下列的程式碼  @Get(&quot;json&quot;) public Map&lt;String, Object&gt; retrieve() { HashMap&lt;String,Object&gt; result = new HashMap&lt;String,Object&gt;(); Object values = null; String switchId = (String) getRequestAttributes().get(&quot;switchId&quot;); String statType = (String) getRequestAttributes().get(&quot;statType&quot;); if (statType.equals(&quot;port&quot;)) { values = getSwitchStatistics(switchId, OFStatisticsType.PORT); } else if (statType.equals(&quot;queue&quot;)) { values = getSwitchStatistics(switchId, OFStatisticsType.QUEUE); } else if (statType.equals(&quot;flow&quot;)) { values = getSwitchStatistics(switchId, OFStatisticsType.FLOW); } else if (statType.equals(&quot;aggregate&quot;)) { values = getSwitchStatistics(switchId, OFStatisticsType.AGGREGATE); } else if (statType.equals(&quot;desc&quot;)) { values = getSwitchStatistics(switchId, OFStatisticsType.DESC); } else if (statType.equals(&quot;table&quot;)) { values = getSwitchStatistics(switchId, OFStatisticsType.TABLE); } else if (statType.equals(&quot;features&quot;)) { values = getSwitchFeaturesReply(switchId); } result.put(switchId, values); return result; }  這邊可以看到retrieve會先取得使用者輸入的swtichId以及對應的statType. 接者透過 getSwitchStatistics 這個function去取得資料 最後透過 result.put(switchId, values) 把該資料跟該dpid綁在一起，方便JSON的格式回傳 core/web/SwitchResourceBase.java 中可以看到關於 getSwitchStatistics的定義  protected List&lt;OFStatistics&gt; getSwitchStatistics(long switchId, OFStatisticsType statType) { IFloodlightProviderService floodlightProvider = (IFloodlightProviderService)getContext().getAttributes(). get(IFloodlightProviderService.class.getCanonicalName()); IOFSwitch sw = floodlightProvider.getSwitch(switchId); Future&lt;List&lt;OFStatistics&gt;&gt; future; List&lt;OFStatistics&gt; values = null; if (sw != null) { OFStatisticsRequest req = new OFStatisticsRequest(); req.setStatisticType(statType); int requestLength = req.getLengthU();  先透過floodlightProvider 取得該switchId所對應IOFSitch object.初始化查詢結果以及需要的容器 future, values如果該dpid對應的switch存在，則先產生一個OFStatisticsRequest的物件，等等就要透過這個物件去發送請求。  if (statType == OFStatisticsType.FLOW) { OFFlowStatisticsRequest specificReq = new OFFlowStatisticsRequest(); OFMatch match = new OFMatch(); match.setWildcards(0xffffffff); specificReq.setMatch(match); specificReq.setOutPort(OFPort.OFPP_NONE.getValue()); specificReq.setTableId((byte) 0xff); req.setStatistics(Collections.singletonList((OFStatistics)specificReq)); requestLength += specificReq.getLength(); } else if (statType == OFStatisticsType.AGGREGATE) { OFAggregateStatisticsRequest specificReq = new OFAggregateStatisticsRequest(); OFMatch match = new OFMatch(); match.setWildcards(0xffffffff); specificReq.setMatch(match); specificReq.setOutPort(OFPort.OFPP_NONE.getValue()); specificReq.setTableId((byte) 0xff); req.setStatistics(Collections.singletonList((OFStatistics)specificReq)); requestLength += specificReq.getLength(); } else if (statType == OFStatisticsType.PORT) { OFPortStatisticsRequest specificReq = new OFPortStatisticsRequest(); specificReq.setPortNumber(OFPort.OFPP_NONE.getValue()); req.setStatistics(Collections.singletonList((OFStatistics)specificReq)); requestLength += specificReq.getLength(); } else if (statType == OFStatisticsType.QUEUE) { OFQueueStatisticsRequest specificReq = new OFQueueStatisticsRequest(); specificReq.setPortNumber(OFPort.OFPP_ALL.getValue()); // LOOK! openflowj does not define OFPQ_ALL! pulled this from openflow.h // note that I haven't seen this work yet though... specificReq.setQueueId(0xffffffff); req.setStatistics(Collections.singletonList((OFStatistics)specificReq)); requestLength += specificReq.getLength(); } else if (statType == OFStatisticsType.DESC || statType == OFStatisticsType.TABLE) { // pass - nothing todo besides set the type above } req.setLengthU(requestLength);  這邊就是針對type的請求，使用不同格式的封包。 以flow為例子， flow request 會使用OFMatch去尋找所有match的flow,有mathc的flow才會回傳狀態資訊 setWildcards(0xffffffff): 這樣就能夠match 所有的flowspecificReq.setOutPort(OFPort.OFPP_NONE.getValue()): 這邊使用OFPP_NONE就是代表在match flow的時候，不會去看該flow entry的output port. specificReq.setTableId((byte) 0xff): 把tableId設定成0xff就是代表對所有的table都去詢問。最後設定一些相關資訊，並且更新整個request的長度req.setLengthU(requestLength)設定整個request packet的最後長度  try { future = sw.queryStatistics(req); values = future.get(10, TimeUnit.SECONDS); } catch (Exception e) { log.error(&quot;Failure retrieving statistics from switch &quot; + sw, e); } } return values; }  接下來把該 request的封包送給switch,然後使用一個future的物件來取得回傳結果,透過future去發送一個非同步的要求，如果10秒內沒有辦法把該任務給完成，就會發出例外直接停止。最後把結果給回傳回去。 ","version":"Next","tagName":"h2"},{"title":"所有switch​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlight-core-restapi#所有switch","content":"core/web/AllSwitchStatisticsResource.java 中可以觀察倒整個code  @Get(&quot;json&quot;) public Map&lt;String, Object&gt; retrieve() { String statType = (String) getRequestAttributes().get(&quot;statType&quot;); return retrieveInternal(statType); }  這邊可以看到，會先從statType中取得使用者要求的type,接者再呼叫retrieveInternal()來取得結果並回傳給使用者  public Map&lt;String, Object&gt; retrieveInternal(String statType) { HashMap&lt;String, Object&gt; model = new HashMap&lt;String, Object&gt;(); OFStatisticsType type = null; REQUESTTYPE rType = null; if (statType.equals(&quot;port&quot;)) { type = OFStatisticsType.PORT; rType = REQUESTTYPE.OFSTATS; } else if (statType.equals(&quot;queue&quot;)) { type = OFStatisticsType.QUEUE; rType = REQUESTTYPE.OFSTATS; } else if (statType.equals(&quot;flow&quot;)) { type = OFStatisticsType.FLOW; rType = REQUESTTYPE.OFSTATS; } else if (statType.equals(&quot;aggregate&quot;)) { type = OFStatisticsType.AGGREGATE; rType = REQUESTTYPE.OFSTATS; } else if (statType.equals(&quot;desc&quot;)) { type = OFStatisticsType.DESC; rType = REQUESTTYPE.OFSTATS; } else if (statType.equals(&quot;table&quot;)) { type = OFStatisticsType.TABLE; rType = REQUESTTYPE.OFSTATS; } else if (statType.equals(&quot;features&quot;)) { rType = REQUESTTYPE.OFFEATURES; } else { return model; }  根據使用者的type, 設定 type跟 rType兩種變數， 其中rType是用來做features request的。  IFloodlightProviderService floodlightProvider = (IFloodlightProviderService)getContext().getAttributes(). get(IFloodlightProviderService.class.getCanonicalName()); Set&lt;Long&gt; switchDpids = floodlightProvider.getAllSwitchDpids(); List&lt;GetConcurrentStatsThread&gt; activeThreads = new ArrayList&lt;GetConcurrentStatsThread&gt;(switchDpids.size()); List&lt;GetConcurrentStatsThread&gt; pendingRemovalThreads = new ArrayList&lt;GetConcurrentStatsThread&gt;(); GetConcurrentStatsThread t; for (Long l : switchDpids) { t = new GetConcurrentStatsThread(l, rType, type); activeThreads.add(t); t.start(); }  這邊會先透過floodlightProvider 取得所有的swtich dpid然後跑一個for迴圈，針對每個switch都去發送一個request,這邊採用了thread的方式來發送，並且把這個thread給記錄下來。  for (int iSleepCycles = 0; iSleepCycles &lt; 12; iSleepCycles++) { for (GetConcurrentStatsThread curThread : activeThreads) { if (curThread.getState() == State.TERMINATED) { if (rType == REQUESTTYPE.OFSTATS) { model.put(HexString.toHexString(curThread.getSwitchId()), curThread.getStatisticsReply()); } else if (rType == REQUESTTYPE.OFFEATURES) { model.put(HexString.toHexString(curThread.getSwitchId()), curThread.getFeaturesReply()); } pendingRemovalThreads.add(curThread); } } // remove the threads that have completed the queries to the switches for (GetConcurrentStatsThread curThread : pendingRemovalThreads) { activeThreads.remove(curThread); } // clear the list so we don't try to double remove them pendingRemovalThreads.clear(); // if we are done finish early so we don't always get the worst case if (activeThreads.isEmpty()) { break; } // sleep for 1 s here try { Thread.sleep(1000); } catch (InterruptedException e) { log.error(&quot;Interrupted while waiting for statistics&quot;, e); } } return model; }  這邊是用來收集所有thread的結果，並且統合後把結果回傳的地方最外圈是一個12秒的迴圈，然後裡面會針對之前記錄的所有thread去跑，如果該thread任務已經結束，狀態是TERMINATED就會把該thread記錄倒一個 記錄要被移除threadpendingRemovalThreads,然後從該thread取回請求結果，並記錄下來。從pendingRemovalThreads拿出所有thread，並且把對應於activeThread中的那份給刪除掉如果最後activeThread已經是空的，就代表所有結果都會來了，提前結束。  protected class GetConcurrentStatsThread extends Thread { ... @Override public void run() { if ((requestType == REQUESTTYPE.OFSTATS) &amp;&amp; (statType != null)) { switchReply = getSwitchStatistics(switchId, statType); } else if (requestType == REQUESTTYPE.OFFEATURES) { featuresReply = getSwitchFeaturesReply(switchId); } } ... } - 這個thread 會根據前面傳進來的type,去呼叫`SwitchResourceBase.java`中的`getSwitchStatistics`來取得結果。  ","version":"Next","tagName":"h2"},{"title":"淺談auto_ptr","type":0,"sectionRef":"#","url":"/docs/techPost/2013/cpp-auto-ptr","content":"淺談auto_ptr 在寫C++的時候，常常會使用new來獲取heap的空間，來取得heap的空間，如下。 void Test1(){ char* name = new char(100); //process something delete name; } char* GetHeap(char* name){ char* name = new char(100); return name; } 然而對程式設計師來說，最麻煩的過於一旦new出空間後，一定要執行delete把空間回收，以免發生memory leak的行為 memory leak: 某塊記憶體再也沒有檔案去reference,當妳new出空間後，沒有去delete回收空間時，很容易發生 void Test2(){ char* name = new char(100); //要空間 }//沒有回收，就會造成剛剛取得的空間會變成記憶體中的孤兒 所以在學習 new/delete時，總是被不斷的叮嚀，有new就要有delete，一定要成對出現。 在C++的標準中，提供了一個叫做auto_ptr的標準，就是專門用來處理這種情況， 只要當這個指標沒有繼續reference時，便會自動回收自己，讓程式設計師更方便，能夠遠離new/delete之苦。 可惜如此良好想法的設計，在目前C++標準中，卻是不建議被使用的，因為其某些特性，反而會使得程式碼變得難以捉模。 以下就來正式介紹 auto_ptr 俗稱智慧型指標，保證只要本身被摧毀，必定釋放其所指資源 是一種指向所屬物件擁有者的指標。所以當身為物件擁有者的auto_ptr被摧毀時，該物件也會被摧毀 auto_ptr要求，一個物件只能有一個擁有者，嚴禁一物二主(這點卻造成困擾) 本身不支援指標算數(++之類) 不允許一般指標慣用的賦值初始化方式，必須直接使用顯示轉換來初始化，因為在其實作中，有使用到explicit關鍵字。 void Test3(){ auto_ptr&lt;int&gt; ptr(new int[100]); //ok auto_ptr&lt;int&gt; ptr = new int[100]; //error //process something } #擁有權的轉移 auto_ptr 所界定的是嚴格的擁有權概念，由於一個auto_ptr會刪除所擁有的物件，不應該發生同時間有多個auto_ptr共同擁有一個物件 否則就會出現問題，程式設計師要特別小心避免寫出這樣的程式碼， auto_ptr&lt;int&gt; ptr1(new int[100]); //declare a auto_ptr pointer toint *ptr1=123; //change value cout&lt;&lt;*ptr1&lt;&lt;endl; auto_ptr&lt;int&gt; ptr2(ptr1); //initial ptr2 via ptr1 and ptr1 trans its ownership if(ptr1.get()==NULL) cout&lt;&lt;&quot;ptr1 has transfered ownership to ptr2&quot;&lt;&lt;endl; cout&lt;&lt;*ptr2&lt;&lt;endl; return 0; ------------------ output: 123 ptr1 has transfered ownership to ptr2 123 執行第二行的時候，ptr1會把所有權轉移給ptr2,所以此行一旦結束，ptr1就會是個null。 同樣的問題也會發生在assign的情況下 auto_ptr&lt;int&gt; ptr1(new int[100]); auto_ptr&lt;int&gt; ptr2; ptr2 = ptr1 ; //transfers ownership from ptr1 to ptr2 此外，如果ptr2被賦值前擁有另外一個物件，則被賦值後，便會釋放該物件，並呼叫destructor。 #include&lt;iostream&gt; #include&lt;memory&gt; using namespace std; class Student{ public: Student(int index):_index(index){ cout&lt;&lt;&quot;Student&quot;&lt;&lt;_index&lt;&lt;&quot; constructor&quot;&lt;&lt;endl; }; ~Student(){ cout&lt;&lt;&quot;Student&quot;&lt;&lt;_index&lt;&lt;&quot; destructor&quot;&lt;&lt;endl; } private: int _index; }; int main() { auto_ptr&lt;Student&gt; ptr1(new Student(1)); auto_ptr&lt;Student&gt; ptr2(new Student(2)); ptr2 = ptr1 ; //ptr2's object will release cout&lt;&lt;&quot;over&quot;&lt;&lt;endl; return 0; } ------------------ output: Student1 constructor Student2 constructor Student2 destructor over Student1 destructor #缺陷 由於auto_ptr本身就涵蓋擁有權，如果無意去轉換擁有權，絕對不要在參數中使用auto_ptr，也不要另auto_ptr作為返回值，否則會有很大的災難。以下是個範例 #include&lt;iostream&gt; using namespace std; void bad_print(auto_ptr&lt;int&gt; p) { if(p.get()==NULL){ cout&lt;&lt;&quot;NULL&quot;&lt;&lt;endl; } else cout&lt;&lt;*p&lt;&lt;endl; } int main() { auto_ptr&lt;int&gt; ptr(new int); *ptr=123; bad_print(ptr); *ptr=456; return 0; } ------------- output: 123 Segmentation fault (core dumped) 因為在參數中，使用了auto_ptr，所以當呼叫此function的時候，便會把所有權轉移到function中的臨時變數，然後當function結束後， 這個區域的臨時變數又會銷毀，而在main中的ptr,因為呼叫function後擁有權轉移，所以第二次執行賦值的動作，就會出現runtime error了， 因為此時ptr並沒有任何指向任何物件，所以導致此崩壞行為。 在這種情況下，我們可以考慮採用pass by reference的方式來傳遞變數，可惜對於auto_ptr來說反而會製造更多麻煩，更難去掌握擁有權轉移的順序， 因此能避免就盡量避免使用auto_ptr跟pass by reference。 如果今天真的有要當參數傳入的需求，這時候可以使用constant reference,如此一來可以使得auot_ptrs本身無法轉移所有權。 以下的例子就會編譯錯誤，可提醒設計師轉移權的問題。與一般不同的是，這邊的const代表的並不是不能修改指標所擁有的物件，而是不能更改 auto_ptr的擁有權，更像是T* const ptr; #include&lt;iostream&gt; using namespace std; void bad_print(const auto_ptr&lt;int&gt; p) { if(p.get()==NULL){ cout&lt;&lt;&quot;NULL&quot;&lt;&lt;endl; } else cout&lt;&lt;*p&lt;&lt;endl; } int main() { const auto_ptr&lt;int&gt; p(new int); *p = 123; bad_print(p); //COMPILE TIME ERROR *p = 456; return 0; } 總結 auto_ptrs之間不能共享擁有權 auto_ptr間沒辦法同時擁有一個物件，因此當把兩個auto_ptr指向對方時，就會使得本來的一方釋放其所擁有的物件，之後若是再透過該指標去操作，就會出現錯誤。 並不存在針對array設計的auto_ptr。 auto_ptr的內部設計是delete,而非delete[],所以不可以指向array。 auto_ptr並非萬能指標。 由於auto_ptr並非一個計數型指標(或者是上限為一的計數型指標)，因此在使用上有非常多的限制，如果設計師沒有完全瞭解其特性，很容易就會讓程式出錯。 千萬別在STL 容器中使用auto_ptr。 因為STL標準規定，C++標準容器容易必須要符合&quot;copy-constructible&quot; 跟 &quot;assignable.&quot; ，亦即容器中的元素必須都要能夠被複製跟賦值，然而auto_ptr的特性並不相容上述行為，所以切忌使用，否則容易出錯。","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/install-freebsd-92r","content":"Preface 假設你今天在VM上安裝FreeBSD，然後因為硬碟空間不夠，變透過VM的設定去擴充硬碟空間 那使用 gpart show你會得到下列資訊 34 62914493 ada0 GPT (232G) 34 128 1 freebsd-boot 162 39845760 2 freebsd-ufs (19G) 39845922 2097152 3 freebsd-swap (1G) 41943074 20971453 - free - (10G) 會發現新增加的10G並沒有直接增加到原本的系統中，而是一個free的狀態，需要手動去合併。 那這時候我們就要把原本的ufs跟新增的區塊給合併。 但是由於中間卡了一個swap的區塊，所以我們要先把該swap給砍掉， 然後重新建立一個swap的區域，接者再把兩個ufs的部分合併。 Note 由於我們要對root partition去進行操作，所以請先進入live cd的環境 先刪除本來的swap空間 gpart delete -i 3 ada0擴大本來的ufs gpart resize -i 2 -s 20G ada0用剩下的空間再創立一個swap gpart add -t freebsd-swap ada0 34 62914493 ada0 GPT (232G) 34 128 1 freebsd-boot 162 60817408 2 freebsd-ufs (29G) 60817570 2096957 3 freebsd-swap (1G) 使用 growfs /dev/ada0p2 來把空間真正的擴大","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/csharp-xml","content":"","keywords":"","version":"Next"},{"title":"XMLReader​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-xml#xmlreader","content":"使用XmlReader來逐步讀取XML，對於過大的XML檔案時，不會一口氣全部讀進memory 由於有實作IDisposable,所以使用using來自動釋放資源 reader會逐步讀取，每個tag分成 Element,Text,EndElement，以上述為例  =&gt; Element; hwchiu =&gt; Text;=&gt; EndElement; 其中對於某些node中，有其attribute的，可以使用MoveToAttribute(string name)或是 MoveToNextAttribute()來遍尋，範例如下  using(XmlReader reader = XmlReader.Create(&quot;student.xml&quot;)){ while (reader.Read()) { switch (reader.NodeType) { case XmlNodeType.Element: Console.Write(&quot;&lt;&quot;+reader.Name); while (reader.MoveToNextAttribute()) Console.Write(&quot; &quot; + reader.Name + &quot;=&quot; + reader.Value); Console.WriteLine(&quot;&gt;&quot;); break; case XmlNodeType.Text: Console.WriteLine(reader.Value); break; case XmlNodeType.EndElement: Console.Write(&quot;&lt;&quot; + reader.Name); Console.WriteLine(&quot;&gt;&quot;); } } }  Output &lt;Students&gt; &lt;Student StudentID=156521&gt; &lt;name&gt; hwchiu &lt;/name&gt; &lt;age&gt; 22 &lt;/age&gt; &lt;email&gt; sppsorrg11.csg01@nctu.edu.tw &lt;/email&gt; &lt;/Student&gt; &lt;Student StudentID=9717164&gt; &lt;name&gt; sppsorrg &lt;/name&gt; &lt;age&gt; 18 &lt;/age&gt; &lt;email&gt; sppsorrg11.cs97@nctu.edu.tw &lt;/email&gt; &lt;/Student&gt; &lt;/Students&gt;  ","version":"Next","tagName":"h2"},{"title":"XMLDocument​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-xml#xmldocument","content":"XMLDocument載入xml檔案時，會一次讀完，所以可以直接使用其方法來訪問各個節點 一樣以剛剛的student.xml為範例 先利用GetElementsByTagName取得所有node的list 集合 由於底下的&lt;name&gt;&lt;age&gt;&lt;email&gt;都是其child node,所以必須又要在取得一次NodeList, 如此反覆就可以取得所有資料  XmlDocument xml = new XmlDocument(); xml.Load(&quot;student.xml&quot;); XmlNodeList nodeList = xml.GetElementsByTagName(&quot;Student&quot;); foreach (XmlNode parentNode in nodeList) { if (parentNode is XmlElement) { XmlElement element = (XmlElement)parentNode; String id = element.GetAttribute(&quot;StudentID&quot;); XmlNodeList childList = element.ChildNodes; Console.WriteLine(&quot;StudentID=&quot;+id); foreach (XmlNode childNode in childList) { Console.Write(&quot;&lt;&quot; + ((XmlElement)childNode).Name + &quot;&gt; &quot;); Console.WriteLine(((XmlElement)childNode).ChildNodes.Item(0).Value); } } }   Output StudentID=156521 &lt;name&gt; hwchiu &lt;age&gt; 22 &lt;email&gt; sppsorrg11.csg01@nctu.edu.tw StudentID=9717164 &lt;name&gt; sppsorrg &lt;age&gt; 18 &lt;email&gt; sppsorrg11.cs97@nctu.edu.tw  LINQ 這邊使用LINQ來搜尋XML文件，所以必須要先使用using system.xml.Linq 為了讓Linq能夠順利運行，這邊使用的物件是XElement以及XNode 首先以XElement的方式來讀取檔案，接者使用linq的語法從中獲取我們想要的資訊 範例中先以root.Elements(&quot;Student&quot;)獲取所有Student的節點，接者在去比較其屬性中的 StudentID，來得到特定的資訊。 最後回傳的資訊是個IEnumerable&lt;XElement&gt;的型態，使用foreach來拜訪 這邊的XElement本身的值就是 &lt;Student StudentID=&quot;156521&quot;&gt; &lt;name&gt;hwchiu&lt;/name&gt; &lt;age&gt;22&lt;/age&gt; &lt;email&gt;sppsorrg11.csg01@nctu.edu.tw&lt;/email&gt; &lt;/Student&gt;  所以為了獲得其中的資訊，就必須要繼續拆解該節點，繼續以foreach的方式取得 每個XNode都代表者一行資訊如 &lt;name&gt;hwchiu&lt;/name&gt; 此時可以將XNode給轉型為XElement，就可以利用&lt;Name/Value&gt;的方式分別取得&lt;name&gt;跟hwchiu 若只是想要取得特別的資訊，可以直接透過Linq的語法來查詢，寫起來會更加簡潔及可讀。  XElement root = XElement.Load(&quot;student.xml&quot;); var student = from el in root.Elements(&quot;Student&quot;) where (string)el.Attribute(&quot;StudentID&quot;) == &quot;156521&quot; select el; foreach (XElement el in student) { foreach (XNode node in el.Nodes()) { Console.WriteLine(node); } }   Output &lt;name&gt;hwchiu&lt;/name&gt; &lt;age&gt;22&lt;/age&gt; &lt;email&gt;sppsorrg11.csg01@nctu.edu.tw&lt;/email&gt;  ","version":"Next","tagName":"h2"},{"title":"Components","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openflow-number-1","content":"","keywords":"","version":"Next"},{"title":"OpenFlow Ports​","type":1,"pageTitle":"Components","url":"/docs/techPost/2013/openflow-number-1#openflow-ports","content":"openflow ports 是openflow switch彼此交換使用的port.與 openflow switch 上真實的port不會完全對應.封包進來的稱做為ingress port.封包出去的稱為output port.必須支援三種type的port physical portslogical portsreserved ports ","version":"Next","tagName":"h2"},{"title":"Standard Ports​","type":1,"pageTitle":"Components","url":"/docs/techPost/2013/openflow-number-1#standard-ports","content":"定義為physical ports,logical ports and LOCAL reserved port(不包含其他的reserved port) ","version":"Next","tagName":"h3"},{"title":"Physical Ports​","type":1,"pageTitle":"Components","url":"/docs/techPost/2013/openflow-number-1#physical-ports","content":"由openflow switch定義跟硬體上真實的port有關，對於一般的switch來說，是 one-to-one的關係在openflow switch中，有些會使用virtualised的方式來管理ports,此時的physical ports就代表virtual slice ","version":"Next","tagName":"h3"},{"title":"Logical Ports​","type":1,"pageTitle":"Components","url":"/docs/techPost/2013/openflow-number-1#logical-ports","content":"由openflow switch定義跟硬體教無關，比physical port更高一階.被用來使用一些 non-openflow methods(e.g. link aggregation groups, tunnels, loopback interfaces)可封裝封包.可對應到不同的physical ports.metadata中會含有Tunnel-ID. ","version":"Next","tagName":"h3"},{"title":"Reserved Ports​","type":1,"pageTitle":"Components","url":"/docs/techPost/2013/openflow-number-1#reserved-ports","content":"由specification 1.3定義做一些通用的forwarding action (e.g. sending to controller,flooding, forwarding using non-OpenFlow methos)openflow switch 沒有要求要支援所有的reserved portsRequired:(待釐清，有點模糊) ALL:代表switch的所有port都可以用來forwarding 特定的封包，除了封包的ingress port跟被標記OFPPC_NO_FWD的portCONTROLLER:可用在ingress port &amp; output ports,用在output port時，會被封裝成pkcket-in message然後送往controller,當用在ingress port時，代表封包來自controllerTABLE:代表openflow pipeline的開始，只有當output action在packet-out message的action list中時才有效IN_PORT:代表封包的ingress port, 只能在output port使用，會讓封包由ingress port送出去.ANY:當沒有port被使用(wildcarded)時用在某些特殊的openflow command. Optional (openFlow-only switch 不支援) NORMAL:傳統的switch處理方式FLOOD:使用normal pipeline 來flooding. ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/nox-spanning-tree","content":"","keywords":"","version":"Next"},{"title":"init​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#init","content":"這邊做的是一些成員的初始化，包括一些set的初始。 ","version":"Next","tagName":"h2"},{"title":"install​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#install","content":"首先會呼叫update_lldp_send_period更新一次lldp發送的頻率 會根據port的數量去決定LLDP發送的頻率,目前FLOO_D_WAIT_TIME 預設是10秒，代表10秒內要把平均的送出LLDP出去。 接者會去註冊一些相關事件 self.register_for_datapath_join ( self.dp_join ) self.register_for_datapath_leave( self.dp_leave ) self.register_for_port_status( self.handle_port_status ) self.register_for_packet_in( self.handle_packet_in)  這段還不是很清楚，但是感覺是讓一些變數在各module之間互通使用的。self.bindings = self.resolve(pybindings_storage) 這段則是透過reactor把命令給延緩1秒後執行 意思就是這行結束一秒後，就會自己執行update_spanning_tree.self.post_callback(1, self.update_spanning_tree) ","version":"Next","tagName":"h2"},{"title":"dp_join​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#dp_join","content":"當有switch 與controller連線之後，便會呼叫此function來做處理。 如果Nox本身不認得該switch的話，就會去紀錄該switch有哪些port 如果該port的port number 小於 OFPP_MAX(65280)的話，就進行相關設定 設定該port的起始時間 預設該port是不能flood的，這樣可以避免新的PORT一出現就會使得spanning tree出問題。 然後發送一個port_modify的封包去把該port給設定成no_flood 最後紀錄該Port，並且重新調整LLDP的值 now = time.time() ports = {} for port in stats['ports']: ports[port[core.PORT_NO]] = port if port[core.PORT_NO] &lt;= openflow.OFPP_MAX: port['enable_time'] = now + FLOOD_WAIT_TIME port['flood'] = False hw_addr = &quot;\\0\\0&quot; + port[core.HW_ADDR] hw_addr = struct.unpack(&quot;!q&quot;, hw_addr)[0] self.ctxt.send_port_mod(dp, port[core.PORT_NO], ethernetaddr(hw_addr), openflow.OFPPC_NO_FLOOD, openflow.OFPPC_NO_FLOOD) self.datapaths[dp] = ports self.port_count += len(ports) self.update_lldp_send_period()  ","version":"Next","tagName":"h2"},{"title":"dp_leave​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#dp_leave","content":"當有swtich離開的時候，先檢查該switch是否存在 然後把整體的port_count給調整。 ","version":"Next","tagName":"h2"},{"title":"update_spanning_tree​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#update_spanning_tree","content":"先利用bindings去取得所有的link，然後把本身的一個callback function傳進去。 接者在FLOOD_PORT_UPDATE_INTERVAL(5 sec)的時間後，呼叫update_spanning_tree. self.bindings.get_all_links(self.update_spanning_tree_callback) self.post_callback(FLOOD_PORT_UPDATE_INTERVAL, self.update_spanning_tree)  ","version":"Next","tagName":"h2"},{"title":"update_spanning_tree_llback​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#update_spanning_tree_llback","content":"","version":"Next","tagName":"h2"},{"title":"handle_port_status​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#handle_port_status","content":"","version":"Next","tagName":"h2"},{"title":"handle_packet_in​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#handle_packet_in","content":"port是flood port或是該封包是LLDP 這兩種情況就直接把該封包傳給下一個module去處理。 除了此情況以外 檢查destination的mac address 是否學過檢查destination的ip type = ethernet 且 ip version = ipv4且 ip header = 20byte (5*4) if not packet.parsed: if packet.type == ethernet.LLDP_TYPE: return CONTINUE  try: dst_mac = (struct.unpack('!I', packet.arr[0:4])[0] &lt;&lt; 16) + struct.unpack('!H', packet.arr[4:6])[0] if dst_mac in self.mac_bypass: return CONTINUE type = struct.unpack('!H', packet.arr[12:14])[0] ipver = struct.unpack('!b', packet.arr[14:15])[0] if type == 0x800 and ipver == 0x45: dst_ip = struct.unpack('!I', packet.arr[30:34])[0] if dst_ip in self.ip_bypass: return CONTINUE except: pass   try: if self.datapaths[dpid][inport]['flood']: return CONTINUE else: log.warn(&quot;STOP&quot;) return STOP except KeyError: return STOP  ","version":"Next","tagName":"h2"},{"title":"update_lldp_send_period​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/nox-spanning-tree#update_lldp_send_period","content":"在nox中LLDP的發送情況是要在 FLOOW_WAIT_TIME的時間內 把所有的LLDP都送出去 所以PORT的數量愈多，每個LLDP的間隔就愈短。 預設值 FLOOW_WAIT_TIME = 10MIN_LLDP_SEND_PERIOD = 0.05 if self.port_count == 0: nox.netapps.discovery.discovery.LLDP_SEND_PERIOD = MIN_LLDP_SEND_PERIOD else: nox.netapps.discovery.discovery.LLDP_SEND_PERIOD = min( MIN_LLDP_SEND_PERIOD, (FLOOD_WAIT_TIME * 1.0) / 2 / self.port_count)  ","version":"Next","tagName":"h2"},{"title":"Python-Robot(1) IRC","type":0,"sectionRef":"#","url":"/docs/techPost/2013/irc-robot","content":"Python-Robot(1) IRC 要撰寫IRC 機器人其實不難，網路上到處都有範例，其實就是簡單的NP，字串來回處理而以。 Connect To IRC SERVERSend User InfomationJoin a channelRead data from irc channel and do responseSend something to irc channellooping (4~5) 第一步: 我們使用TCP建立連線 self.sock = socket.socket( socket.AF_INET, socket.SOCK_STREAM ) self.sock.connect( (self.config['HOST'],int(self.config['PORT']))) HOST:&quot;HOST&quot;:&quot;irc.freenode.org&quot;, PORT:6667 第二步: 發送機器人的資訊給IRC 頻道 self.sock.send ( 'NICK '+self.config['NICK']+'\\r\\n' ) self.sock.send ( 'USER '+self.config['IDENT']+' '+self.config['HOST']+' bla :'+self.config['REALNAME']+'\\r\\n') 第三步: 加入某個頻道，做為該頻道的機器人 self.sock.send ( 'JOIN '+self.config['CHANNELINIT']+'\\r\\n') 第四步+第五步: while True: data = self.sock.recv(4096) if(data.find('PING'))!=-1: ##response to server avoid be kicked self.sock.send('PONG ' + data.split()[1]+'\\r\\n') elif(data.find('PRIVMSG'))!=-1: for module in self.modules: response = module.run(data) self.sock.send(&quot; PRIVMSG &quot;+channel + &quot; :&quot;+response+&quot;\\r\\n&quot;) ..... 接下來就是一個無窮的從channel讀取資訊然後處理後再送回 這邊要注意的是IRC會定期送一個PING的資訊過來，必須要回一個PONG回去，否則該機器人不久後就會被踢下線 得到的data格式大概是如這種 ###:hwchiu!hwchiu@bsd4.cs.nctu.edu.tw PRIVMSG #hwchiu_test :hi 分別是使用者名稱、所在的機器位置、訊息類型、頻道(私人訊息的話就會是機器人本身)、以及說話內容 所以只要針對這些格式去處理，得到想要的資訊，就可以進行各種想要的功能了 這邊因為我有很多個功能module,所以每次收到訊息的時候，就把這些資料都送給所有的module 讓每個module自己去處理並且回應。","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/floodlightmodule-forwarding","content":"","keywords":"","version":"Next"},{"title":"Drop​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-forwarding#drop","content":"Create Openflow Flow-Modify Packet with no action ( no action means drop)Send Flow-Modify Packet to switch. 得到一個Openflow Flow-Modify類型的封包OFFlowMod fm =(OFFlowMod) floodlightProvider.getOFMessageFactory().getMessage(OFType.FLOW_MOD); 設定一個Actions,然後不增加任何action,這樣就會事drop的行為List&lt;OFAction&gt; actions = new ArrayList&lt;OFAction&gt;(); 設定Flow-Modify Packet的ㄧ些欄位，譬如HardTimeout,IdleTimeout...,這邊沒有設定Command預設就是flow_add fm.setCookie(cookie) .setHardTimeout((short) 0) .setIdleTimeout((short) 5) .setBufferId(OFPacketOut.BUFFER_ID_NONE) .setMatch(match) .setActions(actions) .setLengthU(OFFlowMod.MINIMUM_LENGTH);  把訊息藉由messageDamper送給switchmessageDamper.write(sw, fm, cntx); ","version":"Next","tagName":"h2"},{"title":"Flood​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-forwarding#flood","content":"Check the ingress port is allowed broadcast ( according broadcast tree)Create Packout packet with Flood actionSend Packout to switch. 根據BroadCast Tree判斷發送PacketIn Event的{swtich,port}是否能夠廣播，避免造成broadcast storm if (topology.isIncomingBroadcastAllowed(sw.getId(), pi.getInPort()) == false) { return; }  創造一個Packet Out的封包OFPacketOut po =(OFPacketOut) floodlightProvider.getOFMessageFactory().getMessage(OFType.PACKET_OUT) 創造actions,放入一個flood的action,根據ㄧ些property來決定要送到哪個logical port List&lt;OFAction&gt; actions = new ArrayList&lt;OFAction&gt;(); if (sw.hasAttribute(IOFSwitch.PROP_SUPPORTS_OFPP_FLOOD)) { actions.add(new OFActionOutput(OFPort.OFPP_FLOOD.getValue(), (short)0xFFFF)); } else { actions.add(new OFActionOutput(OFPort.OFPP_ALL.getValue(), (short)0xFFFF)); } po.setActions(actions); po.setActionsLength((short) OFActionOutput.MINIMUM_LENGTH);  把封包的資料一併傳下去，然後flood如果PacketIn是送bufferID而不是packetData的話，這邊是否要額外判斷??? byte[] packetData = pi.getPacketData(); poLength += packetData.length; po.setPacketData(packetData);  把訊息藉由messageDamper送給switchmessageDamper.write(sw, po, cntx); ","version":"Next","tagName":"h2"},{"title":"Forward​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlightmodule-forwarding#forward","content":"check we know the desination devicecheck source device and destination device are same cluseterfind all attach switchfind route between souce device and destination deviceuse forwardingBase's method to push a route to all swith which on route. 先取得source 跟 destination device每個device 是用IP、MAC、VLAN來做為區別的IDevice dstDevice = IDeviceService.fcStore.get(cntx, IDeviceService.CONTEXT_DST_DEVICE);IDevice srcDevice = IDeviceService.fcStore.get(cntx, IDeviceService.CONTEXT_SRC_DEVICE); 接下來根據pkacetIN進來的switch取得其所屬的cluster.Long srcIsland = topology.getL2DomainId(sw.getId()); 去探訪destination device所連接到的switch,看看是否有跟發生PacketIn的switch是在同一個Cluster, 是的話才有辦法轉送，否則就Flood出去 for (SwitchPort dstDap : dstDevice.getAttachmentPoints()) { Long dstSwDpid = dstDap.getSwitchDPID(); Long dstIsland = topology.getL2DomainId(dstSwDpid);` if ((dstIsland != null) &amp;&amp; dstIsland.equals(srcIsland)) on_same_island = true;  取得source / destination device所連接到的所有switch目前還不是很清楚怎樣的情形下，可以一個device連接到多個switch 也許用hub吧 SwitchPort[] srcDaps = srcDevice.getAttachmentPoints();SwitchPort[] dstDaps = dstDevice.getAttachmentPoints();` 利用routingEngine來取得兩個switch間的最短路徑 (dijstra)Route route = routingEngine.getRoute(srcDap.getSwitchDPID(),(short)srcDap.getPort(),dstDap.getSwitchDPID(), (short)dstDap.getPort(), 0); 接者透過ForwardingBase的pushRoute,會把路徑上所有的switch都發送一個Flow-Modify的封包pushRoute(route, match, wildcard_hints, pi, sw.getId(), cookie,cntx, requestFlowRemovedNotifn, false, OFFlowMod.OFPFC_ADD); 處理完這組switch後，繼續嘗試其他連接的switchiSrcDaps++;iDstDaps++; 結論 Forwarding是個很基本的module,原始的情況下就是把封包給forward或是flood的而已， 目前裡面的設計是希望能夠取得多個attach points,但是我目前嘗試各種拓樸，都沒有辦法讓一個device連接到多個switch,不知道是否要使用hub之類的東西來完成，這部分要再嘗試看看。 ","version":"Next","tagName":"h2"},{"title":"Freebsd_Quota","type":0,"sectionRef":"#","url":"/docs/techPost/2013/freebsd-quota","content":"","keywords":"","version":"Next"},{"title":"Setup Disk Quota in FreeBSD​","type":1,"pageTitle":"Freebsd_Quota","url":"/docs/techPost/2013/freebsd-quota#setup-disk-quota-in-freebsd","content":"","version":"Next","tagName":"h2"},{"title":"Build Kernel​","type":1,"pageTitle":"Freebsd_Quota","url":"/docs/techPost/2013/freebsd-quota#build-kernel","content":"因為預設的kernel中並沒有支援這個功能，所以要自己重編kernel,加入 options QUOTA 關於build kernel，參考這裡 ","version":"Next","tagName":"h3"},{"title":"Edit /etc/fstab​","type":1,"pageTitle":"Freebsd_Quota","url":"/docs/techPost/2013/freebsd-quota#edit-etcfstab","content":"修改/etc/fstab,對想要進行quota控制的FS進行參數調整 Device MountPoint FSType Options Dump Pass /dev/da0p2 / UFS rw,userquota,groupquota 1 1  接者重新開機，或是remount FS，使其重新讀取設定 ","version":"Next","tagName":"h3"},{"title":"對使用者或是群組 調整其上限​","type":1,"pageTitle":"Freebsd_Quota","url":"/docs/techPost/2013/freebsd-quota#對使用者或是群組-調整其上限","content":"這邊使用edquota這個指令來調整 -u: 加上要調整的使用者 -g: 加上要調整的群組 -t: 調整grace period 執行後會看到已EDITOR對應的文字編輯器開啟編輯，會出現類似下面 Quotas for user hwchiu: /usr : in use: 11216k, limits (soft = 0k, hard = 0k) inodes in use : 903, limits (soft, hard=0) 這邊就可以去調整軟硬限制，根據FILE SIZE或是INODES的數量 啟動quotacheck​ 使用quotacheck來掃描使用者的使用狀況 -a : 掃描/ect/fstab底下所有FS中檔案的使用情況 -v : 詳細過程 -u : 掃描使用者的檔案情況 -g : 掃描群組的檔案情況 就給他執行 quotacheck -avug 啟動quota​ 執行quotaon -a，執行quota限制的功能，沒有開啟的話，一切的設定就只是擺好看的 觀看​ 使用quota這個指令來觀看 quota: -u:使用者名稱 -g:群組名稱 -v:詳細 -h:以容易辨識的格式表達大小，如M、 Filesystem usage quota limit grace files quota limit grace /amd/gcs 305M 390M 410M 6414 40000 42000 /amd/mail 41M 97M 117M 1 2 3 ","version":"Next","tagName":"h3"},{"title":"Reference​","type":1,"pageTitle":"Freebsd_Quota","url":"/docs/techPost/2013/freebsd-quota#reference","content":"FreeBSD Handbook ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/floodlights-dijkstra","content":"","keywords":"","version":"Next"},{"title":"Step1​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlights-dijkstra#step1","content":"初始化相關容器由cluster取得所有的link，先設定所有switch node的cost都是無限大root該switch node的cost是0把root加入到queue內。  HashMap&lt;Long, Link&gt; nexthoplinks = new HashMap&lt;Long, Link&gt;(); HashMap&lt;Long, Integer&gt; cost = new HashMap&lt;Long, Integer&gt;(); int w; for (Long node: c.links.keySet()) { nexthoplinks.put(node, null); cost.put(node, MAX_PATH_WEIGHT); } HashMap&lt;Long, Boolean&gt; seen = new HashMap&lt;Long, Boolean&gt;(); PriorityQueue&lt;NodeDist&gt; nodeq = new PriorityQueue&lt;NodeDist&gt;(); nodeq.add(new NodeDist(root, 0)); cost.put(root, 0);  ","version":"Next","tagName":"h2"},{"title":"Step 2​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlights-dijkstra#step-2","content":"從queue裡面拿出cost最小的node取得到達該node的cost做個錯誤檢查如果該node已經檢查過了，就忽略。把該node加入到seen裡面  while (nodeq.peek() != null) { NodeDist n = nodeq.poll(); Long cnode = n.getNode(); int cdist = n.getDist(); if (cdist &gt;= MAX_PATH_WEIGHT) break; if (seen.containsKey(cnode)) continue; seen.put(cnode, true);  ","version":"Next","tagName":"h2"},{"title":"Step 3​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlights-dijkstra#step-3","content":"取得該node連接的所有link 每條link都會存放兩次，src跟destnation會相反根據 isDstRooted，每條link都只取src or dest (因為每條link會出現兩次，所以switch一定不會漏掉)檢查該node是否已經看過了取得該該的cost計算到該neighbor的cost = 本來node的cost + link的cost如果cost比以前學過得更低，那我們就採用這個新的路徑 更新最新的cost資料 更新nexthoplinks的資料，記錄到此node所需要的link是哪條然後把該node重新加入到queue裡面  for (Link link: c.links.get(cnode)) { Long neighbor; if (isDstRooted == true) neighbor = link.getSrc(); else neighbor = link.getDst(); // links directed toward cnode will result in this condition if (neighbor.equals(cnode)) continue; if (seen.containsKey(neighbor)) continue; if (linkCost == null || linkCost.get(link)==null) w = 1; else w = linkCost.get(link); int ndist = cdist + w; // the weight of the link, always 1 in current version of floodlight. if (ndist &lt; cost.get(neighbor)) { cost.put(neighbor, ndist); nexthoplinks.put(neighbor, link); log.info(&quot;neibhbor = {}&quot;,neighbor.toString()); //nexthopnodes.put(neighbor, cnode); NodeDist ndTemp = new NodeDist(neighbor, ndist); // Remove an object that's already in there. // Note that the comparison is based on only the node id, // and not node id and distance. nodeq.remove(ndTemp); // add the current object to the queue. } } }  ","version":"Next","tagName":"h2"},{"title":"Step 4​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/floodlights-dijkstra#step-4","content":"利用nexthoplinks去創見一個broadcast tree.並且把該tree回傳做為該node的shortest path tree.  BroadcastTree ret = new BroadcastTree(nexthoplinks, cost); return ret; }  ","version":"Next","tagName":"h2"},{"title":"OpenvSwitch - hmap","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openvswitch-hmap","content":"","keywords":"","version":"Next"},{"title":"insert​","type":1,"pageTitle":"OpenvSwitch - hmap","url":"/docs/techPost/2013/openvswitch-hmap#insert","content":"static inline void hmap_insert_fast(struct hmap *hmap, struct hmap_node *node, size_t hash) { struct hmap_node **bucket = &amp;hmap-&gt;buckets[hash &amp; hmap-&gt;mask]; node-&gt;hash = hash; node-&gt;next = *bucket; *bucket = node; hmap-&gt;n++; }  先利用此hash與mask找到對應的 bucket, 值得注意的是這邊拿到的也是一個 pointer to pointernode的 next 指向 bucket所指向的第一個node，然後bucket則改成指向node，結論就是會把這個node從前面插入 #define HMAP_FOR_EACH_WITH_HASH(NODE, MEMBER, HASH, HMAP) \\ for (ASSIGN_CONTAINER(NODE, hmap_first_with_hash(HMAP, HASH), MEMBER); \\ NODE != OBJECT_CONTAINING(NULL, NODE, MEMBER); \\ ASSIGN_CONTAINER(NODE, hmap_next_with_hash(&amp;(NODE)-&gt;MEMBER), \\ MEMBER))  這是一個用來搜尋的 marco,使用到了 ASSIGN_CONTAINER 以及 OBJECT_CONTAINING兩個marco呼叫 ASSIGN_CONTAINER 取得在hmap中含有特定 hash的第一個 hmap_nodeOBJECT_CONTAINING 回傳一個NULL物件每次都透過 hmap_next_with_hash 取得相同hash下的下一個node static inline struct hmap_node * hmap_next_with_hash__(const struct hmap_node *node, size_t hash) { while (node != NULL &amp;&amp; node-&gt;hash != hash) { node = node-&gt;next; } return CONST_CAST(struct hmap_node *, node); } static inline struct hmap_node * hmap_next_with_hash(const struct hmap_node *node) { return hmap_next_with_hash__(node-&gt;next, node-&gt;hash); }  ","version":"Next","tagName":"h2"},{"title":"OpenVSwitch - Basic Install","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openvswitch-install","content":"","keywords":"","version":"Next"},{"title":"Environment​","type":1,"pageTitle":"OpenVSwitch - Basic Install","url":"/docs/techPost/2013/openvswitch-install#environment","content":"System: Ubuntu 12.04 TLSOpenVSwtich : v.20 openvswitchController: Floodlight controller #Install# ","version":"Next","tagName":"h2"},{"title":"OpenVSwitch​","type":1,"pageTitle":"OpenVSwitch - Basic Install","url":"/docs/techPost/2013/openvswitch-install#openvswitch","content":"按照文件中的INSTALL 即可安裝完成， ./configure (如果要安裝kernel module的話，./configure --with-linux=/lib/modules/uname -r/build 來建置) make make modules_install /sbin/modprobe openvswitch 用 lsmod | grep openvswitch 檢查kernel module 是否載入 mkdir -p /usr/local/etc/openvswitch ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema (創造ovs-db) ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach ovs-vsctl --no-wait init ovs-vswitchd --pidfile --detach ","version":"Next","tagName":"h2"},{"title":"Network Environment​","type":1,"pageTitle":"OpenVSwitch - Basic Install","url":"/docs/techPost/2013/openvswitch-install#network-environment","content":"主機板網路孔*1 + 4 port 網卡eth0 藉由internet與controller連接 (out-bound)用ovs 創造一個虛擬的interface br0,把eth1, eth2, eth3, eth4加入 (in-bound)eth1 &amp; eth4 分別連上兩台hosthost1: 192.168.122.100host2: 192.168.122.101  ","version":"Next","tagName":"h2"},{"title":"Operation​","type":1,"pageTitle":"OpenVSwitch - Basic Install","url":"/docs/techPost/2013/openvswitch-install#operation","content":"ovs-vsctl add-br br0ovs-vsctl add-port br0 eth1ovs-vsctl add-port br0 eth2ovs-vsctl add-port br0 eth3ovs-vsctl add-port br0 eth4ovs-vsctl set-controller br0 tcp:x.x.x.x:6633 ","version":"Next","tagName":"h2"},{"title":"Controller side​","type":1,"pageTitle":"OpenVSwitch - Basic Install","url":"/docs/techPost/2013/openvswitch-install#controller-side","content":"Switch 00:00:90:e2:ba:49:58:84 connected.Watch x.x.x.x:8080/ui/index.html to see switch ","version":"Next","tagName":"h2"},{"title":"Ovs side​","type":1,"pageTitle":"OpenVSwitch - Basic Install","url":"/docs/techPost/2013/openvswitch-install#ovs-side","content":"ovs-vsctl show  Bridge &quot;br0&quot; Controller &quot;tcp:127.0.0.1:6633&quot; is_connected: true Port &quot;eth2&quot; Interface &quot;eth2&quot; Port &quot;eth4&quot; Interface &quot;eth4&quot; Port &quot;br0&quot; Interface &quot;br0&quot; type: internal Port &quot;eth3&quot; Interface &quot;eth3&quot; Port &quot;eth1&quot; Interface &quot;eth1&quot;  ","version":"Next","tagName":"h2"},{"title":"Environment","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openvswitch-overview","content":"Environment Three PCs.One for openvswitch (with a 4-port ethernet card).Two for hosts.OVS version 1.9 kernel module insmod datapath/linux/openvswitch.ko When we load the openvswitch's kernel module, it will register four generic netlink event includingdatapath, vport, flow and packet. In the datapath.c, we can see those four generic netlink type. Take the vport for example, there're four command we can excute via this netlink type. If we want the kernel to create a new port, we can send the vport type netlink with the command OVS_VPORT_CMD_NEW, and the command handler (doit) ovs_vport_cmd_new will be excuted to create the new vport. ovs-vswitchd ovsdb-server ...ovs-vswitchd --pidfile --detach First, the ovsdb-server will start a database daemon, In addition, there're some user-space tool will work with it, like ove-vsctl, ovs-ofctl..etc.The user-space process ovs-vswitchd play a importmant role about openflow in OpenvSwitch. It will parse the openflow protocol and handle it (you can use the keyword ofproto to find the resource about it) ovs-vswitchd: Process openflow messagesManage the datapath (which actually in kernel space)Maintain two flow table (exactly flow &amp; wildcard flow) Adding bridge ovs-vsctl add-br br0 When we excute the ovs-vsctl, it will send a command to ovsdb and the DB will store this information. After that, the ovsdb will pass the command to ovs-vswitchd, and the ovs-vswitch send the netlink with datapath type to the kernel. Since we have installed the kernel module before, the datapath will receive the netlink and excute the corresponding command handler. In this case, it will excute ovs_dp_cmd_new. Finally, the datapath will be created and it will be managed by ovs-vswitchd. datapath: Maintain one flow table (exactly flow) This study is based on the OVS v1.9Act as the software switch (look up flow, forward the packet) Adding vports ovs-vsctl add-port br0 eth1 Like the above discussion about datapath, ovs-vswitchd send the netlink to the kernel. In the command handler ovs_vport_cmd_new. 1.Find the the struct net_device object in the kernel by the user typing interface name (eth1) 2.Modify the receive_handler of that net_device to the OpenvSwitch's packet handler. Set-Controller ovs-vsctl set-controller br0 tcp:xxx.xxx.xxx.xxx:6633 Set the controller setting and it will be done in ovs-vswitchd. ##In the following example, we use a simple case to explain how the ping works Target command hostA ping hostB We devide the picture into two parts by the red line. Upper Part This part show the physical view of thie case.The middle PC has installed the Ubuntu 12.04 and OVS 1.9.The left PC connect to the OVS's nic eth1The right PC connect to the OVS's nic eth2 Lower Part This part show the system view of the switch PC (middle one)We use the dash-line to separate the user-space and kernel-space. Analysis After the OVS receives the ICMP packet from the left PC. What will happen about OVS? The NIC eth1 receives the ICMP packet.Call the receive_handler to handler this ICMP packet.Do flow_lookup, it will look up the flow table maintained by the kernel-space. All the flow entry in this table is exactly flow entry, which means there're no any wildcard. This architecture will speed up the look-up since we don't need to consider the wildcard field. In the OpenvSwtich, it use the struct sw_flow_key to present a exactly flow.If we find the flow entry in the flow table, excute its flow actiojn.Otherwise, we need the help from controller. so the datapath.ko will send this flow to the user-space via the f unction upcall(actually, it's a netlink message) What will happen when the ovs-vswitch receive the flow from the kernel-space. Both exactly matching flow and wildcard matching flow are stored in the user-space (by Openflow protocol).Since the exactly matching has high priority than wildcard matching, we need to lookup the exactly macthing flow table first.Look up the flow entry in the user-space by exactly matching, if we find it, send two netlink message to the kernel (we will discuss these two nelitnkj message later)Otherwise, look up the flow entry by wildcard matching, if we find it, generate a corrsponding exactly flow entry and send two netlink message to the kernel.If we can't find any flow entry in the flow-table, we issue a Packet_In to the controller. After the kernel-space receive those two netlink message which sending from user-space. Excute the flow_actiojn about that flow entry.Insert that exactly mactching flow into the kernel's flow-table. That will create the cache for that connection and crease the processing time for nect packets. Summary There is a limitation about the size of flow table in kernel, it use the cache (exactly macthing) to speed up the look-up.For the recently activity connection, those packets can be handled quickly.The flow-table in the user-space is the same as the what the controller see. It support the wildcard matching. We can reduce the size of flow entries by wildcard matching but it will bring the overhaed for look-up MISC You can use the ovs-dpctl dump-flows to dump the flow table of kernel-spaceThis article is based on the OVS v1.9 and the architecture has some change after v1.11","keywords":"","version":"Next"},{"title":"Install the FTP server on FreeBSD","type":0,"sectionRef":"#","url":"/docs/techPost/2013/pure-ftpd","content":"","keywords":"","version":"Next"},{"title":"public​","type":1,"pageTitle":"Install the FTP server on FreeBSD","url":"/docs/techPost/2013/pure-ftpd#public","content":"pulbic中，讓匿名帳號變成other的權限，然後把w權限給拔掉，這樣對於目錄中有任何異動的行為(刪除、移動、改名)都無法使用。 讓virtualgroup的人也有完整的權限去處理，這樣ftp-vip就有完整權限。 chown root:virtualgroup /home/ftp/publicchmod 775 /home/ftp/public ","version":"Next","tagName":"h2"},{"title":"upload​","type":1,"pageTitle":"Install the FTP server on FreeBSD","url":"/docs/techPost/2013/pure-ftpd#upload","content":"upload中，匿名帳號要可以下載跟創立資料夾，以及下載非ftp擁有的檔案。 由於先前有設定AntiWarez，因此檔案擁有者是ftp的就會無法下載， ftp-vip是group的權限，因此什麼都可以做。 給予其w的權限，這樣才可以創立資料夾,然後匿名帳號天生就不可以刪除文件。 chown ftp:virtualgroup /home/ftp/uploadchmod 775 /home/ftp/upload ","version":"Next","tagName":"h2"},{"title":"hidden​","type":1,"pageTitle":"Install the FTP server on FreeBSD","url":"/docs/techPost/2013/pure-ftpd#hidden","content":"由於目錄的r代表的能否看到這些檔案即ls指令，而x代表可否進入該資料夾即cd。 因此我們把r拔掉即可達成。 chown root:virtualgroup /home/ftp/hiddenchmod 775 /home/ftp/hidden TLS 詳細參考 /usr/local/share/doc/pure-ftpd/README.TLS 安裝的時候要勾選TLSconfig /usr/local/etc/pure-ftpd.conf TLS (0,1,2)0: 不支援加密傳輸1: 加密、不加密都支援傳輸2: 不支援非加密傳輸選擇2的話，就一定要用ftpes才能連線，選擇1的話，使用ftp or ftpes都可以連線 create a self-signed certificate 預設的憑證位置是 /etc/ssl/private/pure-ftpd.pem編譯的時候可以透過make configure CERTFILE=your pem location來修改位置mkdir -p /etc/ssl/privateopenssl req -x509 -nodes -newkey rsa:1024 -keyout /etc/ssl/private/pure-ftpd.pem -out /etc/ssl/private/pure-ftpd.pemchmod 600 /etc/ssl/private/*.pem Restart pure-ftpd /usr/local/etc/rc.d/pure-ftpd restart ","version":"Next","tagName":"h2"},{"title":"Python -- split()","type":0,"sectionRef":"#","url":"/docs/techPost/2013/split","content":"Python -- split() 在python中也可以利用split的方式把字串按照特定的字元切開 str.split([sep[, maxsplit]]) sep代表用來切割的符號，而maxsplit代表最多切多少個字串。 值得注意的是，sep可以吃多個字元，但是必須是連續字元，如下舉例 a = 'a,b,!c,!d e f :g' a.split(',') a.split(',!') a.split(',! :') 輸出['a', 'b', '!c', '!d e f :g']['a,b', 'c', 'd e f :g']['a,b,!c,!d e f :g'] 第一組以','作為分割符號，結果很明顯 第二組以',!'作為連續分割符號，所以a,b就切出來，c再切出來 第三組以',! :'作為連續分割符號，但是因為字串中沒有符合的，所以就根本沒有切到 但是這樣的功能，對於我上列的字串，假如我想要以',! :'這四個作為分割符號，希望可以切割成 'a','b','c','d','e','f','g' 這種格式，那要如何辦到? 把所有的符號都替換成單一符號 a.replace('!',',').replace(' ',',').replace(':',',')用re提供的split來達成 import re re.split(',|!| |:',a) 這兩種方法都可以達成一樣的效果，個人覺得第二種比較直覺，也比較容易一眼就懂 以上述範例來看，使用這兩種方法後，會得到如下 ['a', 'b', '', 'c', '', 'd', 'e', 'f', '', 'g']假設該字串存在變數needRemoveEmpty中 可以發現會有empty的值存在，這時候如果要去除這些值可以採用這些做法 採用remove的方式，逐一把empty給清除 while True: try: needRemoveEmpty.remove(&quot;&quot;) except ValueError: break 採用重新創立的方式 for entry in needRemoveEmpty: if entry: newList.append(str(entry)) 採用filter的方式 newList=filter(lambda x: len(x)&gt;0, needRemoveEmpty) ","keywords":"","version":"Next"},{"title":"SA - Shell Script(2)","type":0,"sectionRef":"#","url":"/docs/techPost/2013/sa-homework2","content":"SA - Shell Script(2) 作業二的部分是要寫一個script,真對參數變化然後使用gunplot去進行繪圖 程式要求要有下列參數 o : output file namet : typec : collorn : number of point should used 每個參數都要做錯誤檢查，這邊我使用了 getopts 來做參數的取得，並且把對應的值都存起來 然後再一個一個判斷是否有錯誤。 再gnuplot的部分，因為要求X軸必須是反向的，即(-10,-9,-8....0),這部分我採用的是利用一個暫存檔來做 先使用tail的方式取得最後 n筆資料，然後再透過 awk 把項目加上負號並且印出 如tail -r -n $pointNumber ${inputFile:=&quot;/tmp/sysmonitor&quot;} | awk '{ print -NR&quot; &quot;$1}' &gt; $tempInput後來有同學說可以再gnuplot中可以使用using這個方式來辦到這個結果。 gnuplot的部分，先把所有設定檔寫入暫存檔中，然後再直接透過gnuplot去執行該檔案，最後再刪除這些暫存檔。 #!/bin/sh print_usage() { echo &quot;Usage cpuplot [-h] [-o out_file_name] [-t type] [-c color] -n &lt;60-600&gt;&quot; } print_help() { echo &quot;-o set the output file name. (default: out.png)&quot; echo &quot;-t set the graph type. (one of ‘filledcurve’, ‘lines’. default: ‘filledcurve’)&quot; echo &quot;set graph color. (in hexadecimal form, default: #1E90FF)&quot; echo &quot;set the number of point should use. (must be set. should be in range[60-600]&quot; echo &quot;Read LOGFILE environment variable. If it is not set, use /tmp/sysmonitor&quot; } # Parse the arguments while getopts &quot;ho:t:c:n:&quot; opt do case &quot;$opt&quot; in h) print_help; exit 1 ;; o) outName=$OPTARG ;; t) graphType=$OPTARG ;; c) graphColor=$OPTARG ;; n) pointNumber=$OPTARG ;; *) exit 1 ;; esac done # check graph type, which must be filledcurve or lines. if [ &quot;$graphType&quot; ] ; then if [ &quot;$graphType&quot; != &quot;filledcurve&quot; ] &amp;&amp; [ &quot;$graphType&quot; != &quot;lines&quot; ] ; then echo &quot;type should be one of 'filledcurve' and 'lines'.&quot; fi if [ &quot;$graphType&quot; == &quot;filledcurve&quot; ] ; then graphType=&quot;filledcurve y1=0&quot; fi fi # check graph color, wich must fit #[0-9a-f]{6} if [ &quot;$graphColor&quot; ] ; then tmp=`echo $graphColor | grep '^#[0-9a-f]\\{6\\}' ` if [ -z &quot;$tmp&quot; ] ; then echo &quot;color format error.&quot; exit fi fi # check point number range in 60 ~ 600 if [ -z $pointNumber ] || [ &quot;$pointNumber&quot; -lt 60 ] || [ &quot;$pointNumber&quot; -gt 600 ] ; then print_usage exit fi # check input files's location inputFile=`printenv LOGFILE` #generate a reverse data tempInput=&quot;input2&quot; `tail -r -n $pointNumber ${inputFile:=&quot;/tmp/sysmonitor&quot;} | awk '{ print -NR&quot; &quot;$1}' &gt; $tempInput` #generate a temp plt file tempFile=&quot;temp.plt&quot; `touch $tempFile` echo &quot;set term png&quot; &gt;&gt; $tempFile echo &quot;set out '${outName:=&quot;out.png&quot;}'&quot; &gt;&gt; $tempFile echo &quot;set title 'CPU Usage'&quot; &gt;&gt; $tempFile echo &quot;unset key&quot; &gt;&gt; $tempFile echo &quot;set grid front&quot; &gt;&gt; $tempFile echo &quot;set xlabel 'time from now(sec)'&quot; &gt;&gt; $tempFile echo &quot;set ylabel 'CPU Usage(%)'&quot; &gt;&gt; $tempFile echo &quot;plot [-$pointNumber:0] [0:100] '$tempInput' with ${graphType:=&quot;filledcurve y1=0&quot;} linetype rgb '${graphColor:=&quot;#1E90FF&quot;}' &quot; &gt;&gt; $tempFile `gnuplot $tempFile` if [ -f $tempInput ] ; then `rm $tempInput` fi if [ -f $tempFile ] ; then `rm $tempFile` fi ","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/csharp-json","content":"","keywords":"","version":"Next"},{"title":"Create JSON Format Data​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#create-json-format-data","content":"","version":"Next","tagName":"h2"},{"title":"Serializing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#serializing","content":" public class Student { public Dictionary&lt;string,string&gt; name {get;set;} public string birthday { get; set; } public string studentID { get; set; } public List&lt;string&gt; email {get;set;} } Student student = new Student { name = new Dictionary&lt;string,string&gt; { {&quot;firstName&quot;,&quot;Hung-Wei&quot;}, {&quot;lastName&quot;,&quot;Chiu&quot;} }, birthday = &quot;19900317&quot;, studentID = &quot;0156521&quot;, email = new List&lt;string&gt; { &quot;sppsorrg@gmail.com&quot;, &quot;hwchiu@cs.nctu.edu.tw&quot; } }; string a = JsonConvert.SerializeObject(student, Newtonsoft.Json.Formatting.Indented); Console.WriteLine(a);   Output { &quot;name&quot;: { &quot;firstName&quot;: &quot;Hung-Wei&quot;, &quot;lastName&quot;: &quot;Chiu&quot; }, &quot;birthday&quot;: &quot;19900317&quot;, &quot;studentID&quot;: &quot;0156521&quot;, &quot;email&quot;: [ &quot;sppsorrg@gmail.com&quot;, &quot;hwchiu@cs.nctu.edu.tw&quot; ] }  ","version":"Next","tagName":"h2"},{"title":"LINQ TO JSON​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#linq-to-json","content":"這種類型下，有非常多的方法可以使用 JTokenWriterAnonymous TypeDynamic ObjectJObject and JProperty 這邊只介紹使用Anonymous Type的方式  JObject o = JObject.FromObject(new { name = new Dictionary&lt;string, string&gt; { {&quot;firstName&quot;,&quot;Hung-Wei&quot;}, {&quot;lastName&quot;,&quot;Chiu&quot;} }, birthday = &quot;19900317&quot;, studentID = &quot;0156521&quot;, email = new List&lt;string&gt; { &quot;sppsorrg@gmail.com&quot;, &quot;hwchiu@cs.nctu.edu.tw&quot; } }); Console.WriteLine(o.ToString());  Output: { &quot;name&quot;: { &quot;firstName&quot;: &quot;Hung-Wei&quot;, &quot;lastName&quot;: &quot;Chiu&quot; }, &quot;birthday&quot;: &quot;19900317&quot;, &quot;studentID&quot;: &quot;0156521&quot;, &quot;email&quot;: [ &quot;sppsorrg@gmail.com&quot;, &quot;hwchiu@cs.nctu.edu.tw&quot; ] }  ","version":"Next","tagName":"h2"},{"title":"Read JSON Format Data​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#read-json-format-data","content":"","version":"Next","tagName":"h3"},{"title":"Serializing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#serializing-1","content":" string json = @&quot; { 'name': { 'firstName': 'Hung-Wei', 'lastName': 'Chiu' }, 'birthday': '19900317', 'studentID': '0156521', 'email': [ 'sppsorrg@gmail.com', 'hwchiu@cs.nctu.edu.tw' ] }&quot;; Student student = JsonConvert.DeserializeObject&lt;Student&gt;(json); Console.WriteLine(student.name[&quot;firstName&quot;]); Console.WriteLine(student.name[&quot;lastName&quot;]); Console.WriteLine(student.birthday); Console.WriteLine(student.studentID); Console.WriteLine(student.email[0]); Console.WriteLine(student.email[1]);  Output: Hung-Wei Chiu 19900317 0156521 sppsorrg@gmail.com hwchiu@cs.nctu.edu.tw  ","version":"Next","tagName":"h3"},{"title":"LINQ TO JSON​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#linq-to-json-1","content":"在讀取方面，使用JObect.Parse來解析JSON字串，接下來在讀取資料方面，有很多種用法 LINQ QuerySelectTokendynamic Object 這邊就直接用最簡單的方法去列印JSON的資料  string json = @&quot; { 'name': { 'firstName': 'Hung-Wei', 'lastName': 'Chiu' }, 'birthday': '19900317', 'studentID': '0156521', 'email': [ 'sppsorrg@gmail.com', 'hwchiu@cs.nctu.edu.tw' ] }&quot;; JObject rss = JObject.Parse(json); Console.WriteLine(rss[&quot;name&quot;][&quot;firstName&quot;]); Console.WriteLine(rss[&quot;name&quot;][&quot;lastName&quot;]); Console.WriteLine(rss[&quot;birthday&quot;]); Console.WriteLine(rss[&quot;studentID&quot;]); Console.WriteLine(rss[&quot;email&quot;][0]); Console.WriteLine(rss[&quot;email&quot;][1]);  Output: Hung-Wei Chiu 19900317 0156521 sppsorrg@gmail.com hwchiu@cs.nctu.edu.tw  ","version":"Next","tagName":"h3"},{"title":"Modify JSON Format Data​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#modify-json-format-data","content":"","version":"Next","tagName":"h3"},{"title":"Serializing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#serializing-2","content":"這邊我沒有找到好的辦法，目前可能是要先deserialize給寫到物件，再對該物件進行操作，最後在serialize給寫回JSON去。 ","version":"Next","tagName":"h3"},{"title":"LINQ TO JSON​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/csharp-json#linq-to-json-2","content":"這部分直接對JObject去進行修改就可以了 rss[&quot;studentID&quot;]=&quot;9717164&quot;  還有很多詳細的用法，包刮檔案讀取、JArray、JValue...etc 詳細的就看官方文件 ","version":"Next","tagName":"h3"},{"title":"Linux-Kernel-PacketCapture","type":0,"sectionRef":"#","url":"/docs/techPost/2013/linux-capture-packets","content":"Linux-Kernel-PacketCapture 最近突然對抓封包挺有興趣的，正好以前修網際網路規約時，有trace過linux中TCP/IP相關的code 所以這次就來嘗試看看自己分析封包。 第一個方式就是重編kernel,直接在kernel中寫CODE，但是這樣的缺點就是重編kernel太消耗時間了， 每次修改都要等個十多分鐘，實在不是很有效益，所以這個方案直接放棄 而我採用的方法是利用kernel module的方式，自己先在kernel中加入自定義function,接收來自上層的封包，然後再透過修改kernel module的方式來分析取得的封包，這樣的話，我只有一開始需要重編kernel，後續都直接修改kernel module，編譯速度飛快，效率就高得許多。 使用的資訊版本如下linux:2.6.32function name: myPacket #Step1 在送出封包的function中，加入我們自定義function的使用,所以目標就是位於/net/core/dev.c中的dev_queue_xmit這個function，修改如下 先加入一個function pointer,參數是sk_buff,回傳int在function中，判斷function pointer是否有值，有的話就執行該function,並把sk_buff傳入 int(*myPacket)(struct sk_buff*)=0; int dev_queue_xmit(struct sk_buff *skb) { if(myPacket) { myPacket(skb); } ...ignore } #Step2 接者我們要讓kernel module知道有myPacket這個function存在所以在/net/core/sock.c這個檔案中 以extern的方式宣告該function pointer 過EXPORT_SYMBOL這個marco來把這個function 給export到外部的kernel module讓其使用 extern int(myPacket)(strcut sk_buff)=0; EXPORT_SYMBOL(myPacket); #Step3kernel重編!! #Step4 最後，就開始撰寫我們的kernel module 先寫一個簡單kernel module自定義一個處理function myPacketAnalyzeextern 剛剛的myPacket function pointer讓myPacket 給指向自定義的myPacketAnalyze撰寫myPacketAnalyze我想要看看該封包的ip header information #include &lt;linux/module.h&gt; #include &lt;linux/kernel.h&gt; #include &lt;linux/skbuff.h&gt; #lnclude &lt;linux/ip.h&gt; extern int(*myPacket)(strcut sk_buff*)=0; int myPacketAnalyze(struct sk_buff* skb) { struct iphdr *iph; iph = ip_hdr(skb); printk(&quot;version = %d\\n&quot;,iph-&gt;version); printk(&quot;header_len = %d\\n&quot;,iph-&gt;ihl); printk(&quot;tos = %d\\n&quot;,iph-&gt;tos); printk(&quot;total_len = %hu\\n&quot;,iph-&gt;tot_len); printk(&quot;id = %hu\\n&quot;,iph-&gt;id); printk(&quot;frag = %hu\\n&quot;,(iph-&gt;frag_off)&lt;&lt;13); printk(&quot;frag_off = %hu\\n&quot;,iph-&gt;frag_off&amp;0x1111111111111); printk(&quot;protocol = %d\\n&quot;,iph-&gt;protocol); printk(&quot;ttl = %d\\n&quot;,iph-&gt;ttl); printk(&quot;souce_addr = %u.%u.%u.%u\\n&quot;,NIPQUAD(iph-&gt;saddr)); printk(&quot;dest_addr = %u.%u.%u.%u\\n&quot;,NIPQUAD(iph-&gt;daddr)); } int init_module(void) { myPacket = myPacketAnalyze; return 0; } void cleanup_module(void) { myPacket = 0; } #Step4 這邊簡單介紹一下IP HEADER Version:4bit,代表者IP的版本，目前是4or6 代表ipv4 ipv6。 Header Length:4bit 代表header的長度，單位是4BYTE，最小值是5(20BYTE)，若IP HEADER中有其他options，則值會更大。 Type of Service (tos):8bit，代表QOS跟TOS，可用來調整優先權。 Total Lngth: 16bit, 代表ip header的長度(header + data),單位是byte Identifier(ID): 16bit,會跟flag &amp; fragment offset 一起使用，對封包進行fragment的操作。 flag: 3bit，目前使用兩個bit,分別代表Don't Fragments(DF)跟More Fragments(MF), 用來告知此封包的分段資料。 Fragment Offset:13bit,這個被分段封包在整個完整封包中的位置。 Time to live: 8bit,控制封包傳送節點的次數，每通過一個router就減一，當TTL為0時，就丟棄該 Protocol: 8bit,代表此封包使用的協定。 enum { IPPROTO_IP = 0, /* Dummy protocol for TCP */ IPPROTO_ICMP = 1, /* Internet Control Message Protocol */ IPPROTO_IGMP = 2, /* Internet Group Management Protocol */ IPPROTO_IPIP = 4, /* IPIP tunnels (older KA9Q tunnels use 94) */ IPPROTO_TCP = 6, /* Transmission Control Protocol */ IPPROTO_EGP = 8, /* Exterior Gateway Protocol */ IPPROTO_PUP = 12, /* PUP protocol */ IPPROTO_UDP = 17, /* User Datagram Protocol */ IPPROTO_IDP = 22, /* XNS IDP protocol */ IPPROTO_DCCP = 33, /* Datagram Congestion Control Protocol */ IPPROTO_RSVP = 46, /* RSVP protocol */ IPPROTO_GRE = 47, /* Cisco GRE tunnels (rfc 1701,1702) */ IPPROTO_IPV6 = 41, /* IPv6-in-IPv4 tunnelling */ IPPROTO_ESP = 50, /* Encapsulation Security Payload protocol */ IPPROTO_AH = 51, /* Authentication Header protocol */ IPPROTO_BEETPH = 94, /* IP option pseudo header for BEET */ IPPROTO_PIM = 103, /* Protocol Independent Multicast */ IPPROTO_COMP = 108, /* Compression Header protocol */ IPPROTO_SCTP = 132, /* Stream Control Transport Protocol */ IPPROTO_UDPLITE = 136, /* UDP-Lite (RFC 3828) */ IPPROTO_RAW = 255, /* Raw IP packets */ IPPROTO_MAX }; Source IP: 來源端IP位置 Destination IP:收端IP位置 Options: 控制項，可有可無，包含LSR、SSR、RR、TS。 寫完kernel module並且編譯掛上module後，我首先想先觀察看看ping的封包，於是我執行下列指令 ping 140.113.235.81 接者到/var/log/message去看訊息，看看印出來的資訊如何 version =4 header_len =5 tos = 0 total_len = 21504 id =0 frag = 0 frag_off = 64 protocl = 1 ttl = 64 souce_addr = 140.113.214.84 dest_addr = 140.113.235.81 可以看到version=4,代表ipv4，protocol = 1 就是icmp的封包 而因為沒有options，所以header_len是5 其中最令我那悶的是那封包長度，竟然是兩萬多byte..... 現在還想不透為什麼 經由wireshark幫忙檢查驗證後，發現是我的寫法寫錯了，myPacketAnalyze給重新寫過 int myPacketAnalyze(struct sk_buff* skb) { struct iphdr *iph; iph = ip_hdr(skb); printk(&quot;version = %d\\n&quot;,iph-&gt;version); printk(&quot;header_len = %d\\n&quot;,iph-&gt;ihl); printk(&quot;tos = %d\\n&quot;,iph-&gt;tos); printk(&quot;total_len = %hu\\n&quot;,ntohs(iph-&gt;tot_len)); printk(&quot;id = %hu\\n&quot;,ntohs(iph-&gt;id)); printk(&quot;frag = %hu\\n&quot;,(nthos(iph-&gt;frag_off))&gt;&gt;13); printk(&quot;frag_off = %hu\\n&quot;,(ntohs(iph-&gt;frag_off))&amp;0x1111111111111); printk(&quot;protocol = %d\\n&quot;,iph-&gt;protocol); printk(&quot;ttl = %d\\n&quot;,iph-&gt;ttl); printk(&quot;souce_addr = %u.%u.%u.%u\\n&quot;,NIPQUAD(iph-&gt;saddr)); printk(&quot;dest_addr = %u.%u.%u.%u\\n&quot;,NIPQUAD(iph-&gt;daddr)); } 輸出為 version =4 header_len =5 tos = 0 total_len = 84 id =0 frag = 2 frag_off = 0 protocl = 1 ttl = 64 souce_addr = 140.113.214.84 dest_addr = 140.113.235.81 原因是我忘了去使用ntohs去轉換byte order,所以 84: 00000000 01010100 21504:01010100 00000000 轉換後的結果就比較正常，且令人信服 接下來想嘗試看看修改TCP|IP header的資訊，然後利用簡單的TCP server/client來測試相關，之後有弄再補上。","keywords":"","version":"Next"},{"title":"X Window","type":0,"sectionRef":"#","url":"/docs/techPost/2013/xwindow-on-freebsd-91r","content":"","keywords":"","version":"Next"},{"title":"Install​","type":1,"pageTitle":"X Window","url":"/docs/techPost/2013/xwindow-on-freebsd-91r#install","content":"portmaster x11/xorg安裝 滑鼠跟鍵盤的驅動 sysutils/haldevel/dbushald_enable=&quot;YES&quot; &gt;&gt; /etc/rc.confdbus_enable=&quot;YES&quot; &gt;&gt; /etc/rc.conf ","version":"Next","tagName":"h2"},{"title":"Config​","type":1,"pageTitle":"X Window","url":"/docs/techPost/2013/xwindow-on-freebsd-91r#config","content":"Xorg -configure (產生 X11預設設定檔)測試設定檔OK與否 Xorg -config /root/xorg.conf.new cp /root/xorg.conf.new /etc/X11/xorg.conf 如果有要針對一些硬體、顯示、滑鼠去作調整，就針對這個xorg.conf去編輯即可 ","version":"Next","tagName":"h2"},{"title":"Run​","type":1,"pageTitle":"X Window","url":"/docs/techPost/2013/xwindow-on-freebsd-91r#run","content":"startx 這時候會看到三個視窗，就代表X11安裝成功了 X Window Manager Window Manager(WM) 可以看作一個特別的X client, 提供了類似windows的介面給使用者使用， - 背景、主題、桌布  虛擬桌面視窗特性 移動、放大、縮小... 再X server跟X client之間的溝通都會被導到WM來處理。 比較之名的有 GnomeKDEXFCE....等 ","version":"Next","tagName":"h2"},{"title":"Install​","type":1,"pageTitle":"X Window","url":"/docs/techPost/2013/xwindow-on-freebsd-91r#install-1","content":"Xfce x11-wm/xfce KDE x11/kde4 ","version":"Next","tagName":"h2"},{"title":"Config​","type":1,"pageTitle":"X Window","url":"/docs/techPost/2013/xwindow-on-freebsd-91r#config-1","content":"接下來要編輯xinitrc檔案，讓我們執行Xorg的時候會去執行WM 預設的檔案位置 /usr/local/lib/X11/xinit/xinitrc每個帳號的位置 ~/.xinitrcecho &quot;/usr/local/bin/startkde4&quot; &gt; ~/.xinitrc如此一來，當執行startx的時候，會先讀取家目錄底下的.xinitrc,然後就去執行對應的WM ","version":"Next","tagName":"h2"},{"title":"Run​","type":1,"pageTitle":"X Window","url":"/docs/techPost/2013/xwindow-on-freebsd-91r#run-1","content":"startx ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/wireshark-with-openflow-plugin-in-fedora-14","content":"Preface 參考這篇文章http://networkstatic.net/installing-wireshark-on-linux-for-openflow-packet-captures/ 安裝wireshark source - wget http://wiresharkdownloads.riverbed.com/wireshark/src/wireshark-1.8.8.tar.bz2 (http://wiresharkdownloads.riverbed.com/wireshark/src/ 自己挑選一個版本下載) - bunzip2 wireshark-1.8.8.tar.bz2 - tar -xvf wireshark-1.8.8.tar - cd wireshark-1.8.8 - ./autogen.sh - ./configure - make (這邊錯誤通常是少了某些套件，根據錯誤訊息再去安裝即可) - make install - sudo ldconfi - ./wireshark ``` # 編譯openflow plugin ## Options - hg clone https://bitbucket.org/barnstorm/of-dissector - cd of-dissector/src - apt-get install scons - 修改 packet-openflow.c ``` c Change from: static void dissect_dl_type(....) { .... const char* description = try_val_to_str(dl_type, etype_vals); .... } To: static void dissect_dl_type(....) { .... const char* description = match_strval(dl_type, etype_vals); .... } - scons install - export WIRESHARK=/path_to_wireshark_source/ - scons install - cp openflow.so /usr/lib/wireshark/libwireshark1/plugins/openflow.so Options 2 - git clone git://openflow.org/openflow.git - cd openflow - ./boot.sh - ./configure - make - sudo make install - cd utilities/wireshark_dissectors/openflow - 修改 packet-openflow.c Change from: void proto_reg_handoff_openflow() { openflow_handle = create_dissector_handle(dissect_openflow, proto_openflow); dissector_add(TCP_PORT_FILTER, global_openflow_proto, openflow_handle); } To: void proto_reg_handoff_openflow() { openflow_handle = create_dissector_handle(dissect_openflow, proto_openflow); dissector_add_uint(TCP_PORT_FILTER, global_openflow_proto, openflow_handle); } 安裝openflow plugin make ( pwd = utilities/wireshark_dissectors/openflow)make install Use 開啟wireshark即可觀看openflow protocol囉","keywords":"","version":"Next"},{"title":"INSTALL","type":0,"sectionRef":"#","url":"/docs/techPost/2013/zedgraph","content":"INSTALL 目前最新的版本是 5.16安裝方法請使用 Package Manager Console來安裝， PM&gt; Install-Package ZedGraph 之後就會幫你安裝完成，此時你的資料夾底下會有個packages的資料夾，裡面就放有此套件的dll。 USAGE ZedGraph本身有提供自定義的使用者元件，所有的繪圖都是在該元件上運作，所以必須要先加載該元件 元件=&gt;選擇項目=&gt;瀏覽=&gt;pkakage/zedgraph.dll 順利的話，就可以直接在工具箱中拖曳該元件囉 使用的方法，官方網站(有點舊)上面有許多的教學跟範例 仔細研讀後就大概會用了 以官方範例畫一張曲線圖來說明 private void CreateGraph( ZedGraphControl zgc ) { // get a reference to the GraphPane GraphPane myPane = zgc.GraphPane; // Set the Titles myPane.Title.Text = &quot;My Test Graph\\n(For CodeProject Sample)&quot;; myPane.XAxis.Title.Text = &quot;My X Axis&quot;; myPane.YAxis.Title.Text = &quot;My Y Axis&quot;; // Make up some data arrays based on the Sine function double x, y1, y2; PointPairList list1 = new PointPairList(); PointPairList list2 = new PointPairList(); for ( int i = 0; i &lt; 36; i++ ) { x = (double)i + 5; y1 = 1.5 + Math.Sin( (double)i * 0.2 ); y2 = 3.0 * ( 1.5 + Math.Sin( (double)i * 0.2 ) ); list1.Add( x, y1 ); list2.Add( x, y2 ); } // Generate a red curve with diamond // symbols, and &quot;Porsche&quot; in the legend LineItem myCurve = myPane.AddCurve( &quot;Porsche&quot;, list1, Color.Red, SymbolType.Diamond ); // Generate a blue curve with circle // symbols, and &quot;Piper&quot; in the legend LineItem myCurve2 = myPane.AddCurve( &quot;Piper&quot;, list2, Color.Blue, SymbolType.Circle ); // Tell ZedGraph to refigure the // axes since the data have changed zgc.AxisChange(); } 該第三方套件提供的自定義元件叫做ZedGraphControl而我們要畫圖則是在GraphPane上去操作 所以首先要先從取得GraphPane 接者先定義該圖表的 標題，橫軸標題，縱軸標題 接者我們要加入資料到該圖表中，對於一張2D的圖表來說 使用PointPairList這個物件來存放XY的座標位置 最後使用LineItem來把相關的點給連起來 由於資料加入圖表中，我們的座標軸要根據資料產生變化，此時就呼叫 AxisChange()來改變。","keywords":"","version":"Next"},{"title":"Bluez 5.x","type":0,"sectionRef":"#","url":"/docs/techPost/2014/bluze-5x","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Bluez 5.x","url":"/docs/techPost/2014/bluze-5x#introduction","content":"從官方網站的說明 BlueZ is official Linux Bluetooth protocol stack. It is an Open Source project distributed under GNU General Public License (GPL). BlueZ kernel is part of the official Linux kernel since version 2.4.6. 從這邊可以看得出來，Bluez是一套在linux系統專，專門負責bluetooth裝置連線的軟體，因此滿多linux-based的系統都會使用此套軟體作為與Bluetooth裝置連接的工具。 在版本方面，目前最新的版本是5.24(2014/10/25)。然而在Ubuntu 14.04的官方套件中，依然使用4.101的版本，這邊差了一個大的版本號。下列列舉一下 4.x與5.x版本的較大的差異性 Interface的部分完全改掉，在5.x中已經沒有了AudioSink等Profile相關的interface5.x中原生不再支援a2dp、hsp等profile，必需要依靠第三方套件支援。 ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Bluez 5.x","url":"/docs/techPost/2014/bluze-5x#installation","content":"安裝部分就參考文件內的說明進行configure以及make、make install即可，可以根據configure的需求去調整。由於a2dp部分bluez原生不再支援，這邊要使用第三方套件PulseAudio來處理，注意的是要5.x版本後才有支援bluez5。因此這邊就到PulseAudio的官方網站去下載，記得在configure的部分要指定--enable-bluez5，這樣才會編譯出與bluez5相關的套件。安裝過程可能會遇到一些lib缺少的問題，這邊就依照所缺少的去安裝即可。 ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"Bluez 5.x","url":"/docs/techPost/2014/bluze-5x#usage","content":"run the pulseaudio as daemonuse the dbus-send command to connect the BT device 這邊可以使用bluetoothctl指令來操作，包含與裝置的配對、連線等。power onagent ondefault-agentscan onpair ###connect ## 播放音樂方面，由於bluez 5.x沒有辦法支援alsa，因此沒有辦法透過mplay來播放，必須要透過pulseaudio來播放音樂 pacmd --help ","version":"Next","tagName":"h2"},{"title":"Config Qos on Ovs with Floodlight","type":0,"sectionRef":"#","url":"/docs/techPost/2014/config-qos-on-ovs-with-floodlight","content":"","keywords":"","version":"Next"},{"title":"Step​","type":1,"pageTitle":"Config Qos on Ovs with Floodlight","url":"/docs/techPost/2014/config-qos-on-ovs-with-floodlight#step","content":"Create two different QOS queue on the openvswitch with different limitation.Use the Restful API to add a flow entry with the action enqueue.Chagne the flow action when you want to change the QoS behavoir. ","version":"Next","tagName":"h2"},{"title":"Detail​","type":1,"pageTitle":"Config Qos on Ovs with Floodlight","url":"/docs/techPost/2014/config-qos-on-ovs-with-floodlight#detail","content":"Create topologysudo mn --mac --controller=remoteAdding two queues q0(limited 800M) and q1(limited 50M) on s1-eth1 ovs-vsctl -- set port s1-eth1 qos=@newqos -- --id=@newqos create qos type=linux-htb \\ queues=0=@q0,1=@q1 -- --id=@q0 create queue other-config:min-rate=200000000 \\ other-config:max-rate=800000000 -- --id=@q1 create queue other-config:min-rate=50000 \\ other-config:max-rate=50000000  Adding flow entry using q0 and forwarding packet from port2 to port1. curl -d '{&quot;switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;name&quot;:&quot;flow-mod-1&quot;, &quot;cookie&quot;:&quot;0&quot;, &quot;priority&quot;:&quot;32768&quot;,&quot;ingress-port&quot;:&quot;2&quot;,&quot;active&quot;:&quot;true&quot;, &quot;actions&quot;:&quot;enqueue=1:0&quot;}' http://127.0.0.1:8080/wm/staticflowentrypusher/json  Type &quot;iperf h2 h1&quot; in mininet mininet&gt; iperf h2 h1 *** Iperf: testing TCP bandwidth between h2 and h1 *** Results: ['745 Mbits/sec', '746 Mbits/sec']  Modify the flow entry with different queue. curl -d '{&quot;switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;name&quot;:&quot;flow-mod-1&quot;, &quot;cookie&quot;:&quot;0&quot;, &quot;priority&quot;:&quot;32768&quot;,&quot;ingress-port&quot;:&quot;2&quot;,&quot;active&quot;:&quot;true&quot;, &quot;actions&quot;:&quot;enqueue=1:1&quot;}' http://127.0.0.1:8080/wm/staticflowentrypusher/json  Type &quot;iperf h2 h1&quot; in mininet mininet&gt; iperf h2 h1 *** Iperf: testing TCP bandwidth between h2 and h1 *** Results: ['50.2 Mbits/sec', '50.6 Mbits/sec']  ","version":"Next","tagName":"h2"},{"title":"DebugCounter in Floodlight","type":0,"sectionRef":"#","url":"/docs/techPost/2014/debugcounter-in-floodlight","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"DebugCounter in Floodlight","url":"/docs/techPost/2014/debugcounter-in-floodlight#introduction","content":"DebugCounter是一個Floodlight本身就有提供的module，功能非常的簡單，就如同其名一樣，做一個counter，供debug的時候使用 實際上，自己要寫一個counter在module中是非常容易的事情，那為什麼還需要使用DebugCounter來處理? 唯一的好處就是可以透過DebugCounter所提供的REST API將此Counter的資訊給暴露出去，讓外面的應用程式可以透過REST API來存取。這樣的話就不需要自己寫一個REST API來處理了。是採用Hierarchy的架構來存放的， modules/level1/level2/level3, ex: mymodule/10.0.0.1/Pkt/In，最多只支援到底下三層的紀錄本身會使用thread-local counter來紀錄，可透過定期或是手動的方式將這些thread-local counter給整合到一個global counter中。 ","version":"Next","tagName":"h2"},{"title":"Implementation​","type":1,"pageTitle":"DebugCounter in Floodlight","url":"/docs/techPost/2014/debugcounter-in-floodlight#implementation","content":"IDebugCounterService.java 此檔案定義DebugCounter所需功能的界面IDebugCounter.java 則定義了每個counter應該要有的功能，如counter++DebugCounter.java 則實現了IDebugCounterService與IDebugCounter的功能web/ 這邊的檔案則是實現了RESTAPI的處理 ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"DebugCounter in Floodlight","url":"/docs/techPost/2014/debugcounter-in-floodlight#usage","content":"先透過Floodlight modules system取得IDebugCounterService的instance。呼叫IDebugCounterService的registerCounter來註冊，並會回傳一個IDebugCounter的物件，之後都透過此物件來進行counter的處理IDebugCounter.updateCounterNoFlush()就可以將該counter給遞增，並且不馬上寫回global counter，會較有效率。 ","version":"Next","tagName":"h2"},{"title":"Examle​","type":1,"pageTitle":"DebugCounter in Floodlight","url":"/docs/techPost/2014/debugcounter-in-floodlight#examle","content":"先取得 IDebugCounterService的instance。 protected IDebugCounterService debugCounters; this.debugCounters = fmc.getServiceImpl(IDebugCounterService.class);  宣告一個IDebugCounter，並且註冊 public IDebugCounter cntIncoming; cntIncoming = debugCounters.registerCounter(PACKAGE, &quot;incoming&quot;, &quot;All incoming packets seen by this module&quot;, CounterType.ALWAYS_COUNT);  根據情況來將該counter給+1，以此counter是希望紀錄所有incoming packets，所以收到PacketIn的時候就去遞增該counter public Command receive(IOFSwitch sw, OFMessage msg, FloodlightContext cntx) { switch (msg.getType()) { case PACKET_IN: cntIncoming.updateCounterNoFlush();  ","version":"Next","tagName":"h2"},{"title":"Floodlight LLDP problem","type":0,"sectionRef":"#","url":"/docs/techPost/2014/floodlight-lldp-problem","content":"Floodlight LLDP problem 問題來源:Floodlight LLDP problem 問題描述: 從floodlight去觀察，會發現有Link的{source,dest}都是相同的dpid但是不同的port 想法思路: 從該link的結果可以先猜測應該是LLDP從port 2送出去後不知道為什麼從port 3給接收到了. 那我想到的時候，中間兩個switch使用傳統的learning switch把這個LLDP給一路廣播下去，使得LLDP繞了回來，我覺得可能是後面兩個switch還沒有連上controller的時候會把自己運作成傳統的switch，因此我就詢問對方的網路環境. 在確認對方網路並非是mininet的後，就請對方先把ovs給設定成secure mode，在這種mode下，若是沒有跟controller連線，ovs就不會有任何的行為. 最後對方表示一切都正常了，所以發生原因應該就是中間的switch尚未變成openflow swtich前會把LLDP給透過傳統的方式給轉發下去，導致LLDP繞了回去.","keywords":"","version":"Next"},{"title":"Strategy Pattern","type":0,"sectionRef":"#","url":"/docs/techPost/2013/strategy-pattern","content":"Strategy Pattern 舉個例子，今天我寫了一個壓縮軟體，這個軟體會針對不同的輸入來採用不同的壓縮方法處理。 最基本的架構就是 然後該 compress function 可能長這樣 public Object compress(Object input){ //Part 1 if(input.type == TYPE1){ // do something } else if (input.type == TYPE2){ // do something } else if (input.type == TYPE3){ // do something } //Part 2 if(input.getEncoding() == TYPEA){ // do something } else if(input.getEncoding() ==TYPEB){ // do something } else if(input.getEncoding() ==TYPEC || input.getEncoding() ==TYPED){ // do something } } 這種程式架構再維護上過於麻煩，會有下列問題 程式碼冗長演算法邏輯部分難懂維護困難 如果今天選項夠多(part更多)的話，整個程式會變得很難處理，每次要增加一個新的算法，就要到很多地方去增加對應的code， 在處理上容易出錯且維護也不易。 使用 Stragegy pattern的話，架構如下 設計ㄧ個介面Algorithm, 然後每個算法都實現這個介面，自己去完成自己的算法邏輯 當程式要處理壓縮的時候，就根據輸入物件來產生與之對應的演算法物件，然後去處理。 這樣每個算法都獨立來看，邏輯清楚明瞭，而且要新增加ㄧ個算法的話，只要再寫一個新的物件實現自共同的介面即可。 public class CompressProgram{ public void process(){ //do something Algorithm algorithm = getAlgorithmByType(input); Compressor compressor = new COmpressor(); Object result = compressor.doCompress(input,algorithm); } private Algorithm getAlgorithmByType(Object input){ if(input.type ==TYPEA){ return new AlgorithmA(); } else if(input.type ==TYPEB){ return new AlgorithmB(); } else if(input.type ==TYPEC){ return new AlgorithmC(); } } } public class Compressor{ public Object doCompress(Object input,Algorithm algorithm){ //do something return algorithm.compress(input); } } public abstrace class Algorithm{ abstract public Object compress(Object input); } public class AlgorithmA extends Algorithm{ public Object compress(Object input){ // do something for TypeA } } public class AlgorithmB extends Algorithm{ public Object compress(Object input){ // do something for TypeB } } public class AlgorithmC extends Algorithm{ public Object compress(Object input){ // do something for TypeC } } ","keywords":"","version":"Next"},{"title":"Mininet with different network subnet","type":0,"sectionRef":"#","url":"/docs/techPost/2014/mininet-and-network-subnet","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Mininet with different network subnet","url":"/docs/techPost/2014/mininet-and-network-subnet#introduction","content":"我們使用 mn --topo single,3 --mac創造一個有不同subnet的拓墣，使用--mac使得所有host的MAC Address更加簡單，能夠使得此實驗變得容易。 我們目標是要讓這三個不同subnet的host都能夠互相溝通。 網路架構如下圖 在此實驗中，我們並不採用任何controller來控制所有封包，單純就手動下flow entry來處理所有的封包，一旦了解了這中間的道理，要自己撰寫ＡＰＰ處裏此情況就不會太難了。 ","version":"Next","tagName":"h2"},{"title":"Solutions​","type":1,"pageTitle":"Mininet with different network subnet","url":"/docs/techPost/2014/mininet-and-network-subnet#solutions","content":"首先，mininet創造出來網路後，預設會讓所有的host都屬於相同的network subnet 10.0.0.0/24，因此在實驗開始前，我們要先修改其餘host的設定，改變其network subnet。 在mininet的環境中執行下列指令h2 ifconfig h2-eth0 20.0.0.1h3 ifconfig h3-eth0 30.0.0.1 接下來，我們先執行h1 ping h3，這時候我們會看到有錯誤訊息 connect: Network is unreachable。這個原因是因為對於host1來說，host2是不一樣的network subnet，此時會將該封包轉送到本身subnet的gateway來處理，但是該host不知道gateway在哪裡，因此我們要幫他們加上route for default gateway。 在mininet的環境中執行下列指令h1 route add default gw 10.0.0.254 h1-eth0h2 route add default gw 20.0.0.254 h2-eth0h3 route add default gw 30.0.0.254 h3-eth0 接下來，我們繼續執行h1 ping h3，此時會得到下列的訊息 mininet&gt; h1 ping h3 PING 30.0.0.1 (30.0.0.1) 56(84) bytes of data. From 10.0.0.1 icmp_seq=1 Destination Host Unreachable From 10.0.0.1 icmp_seq=2 Destination Host Unreachable From 10.0.0.1 icmp_seq=3 Destination Host Unreachable From 10.0.0.1 icmp_seq=4 Destination Host Unreachable  到這步驟後，因為我們還沒有寫入任何的flow entry，所以網路不通是正常的。在處理ICMP 封包前，我們必須要先處理ARP的封包。 這邊我們先在mininet那邊持續的執行h1 ping h3。同時，我們開啟第二個視窗，執行tcpdump -vvv -i s1-eth1，我們會得到下列的訊息 tcpdump: WARNING: s1-eth1: no IPv4 address assigned tcpdump: listening on s1-eth1, link-type EN10MB (Ethernet), capture size 65535 bytes 20:07:04.639862 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 10.0.0.254 tell 10.0.0.1, length 28 20:07:05.639859 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 10.0.0.254 tell 10.0.0.1, length 28 20:07:06.639895 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 10.0.0.254 tell 10.0.0.1, length 28 20:07:07.639856 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 10.0.0.254 tell 10.0.0.1, length 28  由這邊可以發現，Host 1透過arp在詢問其gateway相關資訊，但是麻煩的是，在此網路中，我們並沒有真的一個Device的ip是該gateway，為了解決這個問題，我們有兩個選擇 弄一個Host出來，當作gateway去處理弄個arp proxy來處理，這部分在OpenDayLight中預設有提供此module，讓controller假裝自己是gateway來處理此問題。 由於本實驗並沒有採用任何controller，因此我們要手動修改switch，讓她覺得自己是gateway，能夠回arp reply給Host。 在mininet的環境中執行下列指令s1 ifconfig s1:0 10.0.0.254s1 ifconfig s1:1 20.0.0.254s1 ifconfig s1:2 30.0.0.254 我們令s1這個interface擁有三個ip，這些ip都代表每個network subnet的gatewayip，接下來為了讓switch自己幫我們處理所有arp request for gateway，我們加入下列flow entry到s1中 在mininet的環境中執行下列指令sh ovs-ofctl add-flow s1 &quot;table=0,priority=65535,arp,arp_tpa=10.0.0.254 actions=LOCAL&quot;sh ovs-ofctl add-flow s1 &quot;table=0,priority=65535,arp,arp_tpa=20.0.0.254 actions=LOCAL&quot;sh ovs-ofctl add-flow s1 &quot;table=0,priority=65535,arp,arp_tpa=30.0.0.254 actions=LOCAL&quot; 上面這三個flow entry會把所有arp request for gateway的封包都導入本地的OS去處理，因此這些封包就會進入到s1:0,s1:1,s1:2去處理，並且回覆一個arp reply。這些arp reply都會再度的進到ＯＶＳ內，為了處理這些封包，我們要根據他的destination ip address把它給送回去對應的Ｈost。 sh ovs-ofctl add-flow s1 &quot;table=0,priority=1,arp,nw_dst=10.0.0.1,actions=output:1&quot;sh ovs-ofctl add-flow s1 &quot;table=0,priority=1,arp,nw_dst=20.0.0.1,actions=output:2&quot;sh ovs-ofctl add-flow s1 &quot;table=0,priority=1,arp,nw_dst=30.0.0.1,actions=output:3&quot; 這些完畢後，arp封包就能夠正常處理了，接下來為了處理ICMP，我們要再做一些設定，在此實驗中，我們同時測試multiple table的功用，因此我們決定把ICMP routing的部分放到第二個table去處理。 首先，我們先在table 0加入一個flow entry，把剛剛沒有被arp處理掉的封包都送到table 1去處理。 sh ovs-ofctl add-flow s1 &quot;table=0,priority=0,actions=resubmit(,1)&quot; 接者，在table 1，因為switch的身份很類似router，因此我們要修改所有封包的destination MAC Address。 sh ovs-ofctl add-flow s1 &quot;table=1,icmp,nw_dst=10.0.0.1,actions=mod_dl_dst=00:00:00:00:00:01,output:1&quot;sh ovs-ofctl add-flow s1 &quot;table=1,icmp,nw_dst=20.0.0.1,actions=mod_dl_dst=00:00:00:00:00:02,output:2&quot;sh ovs-ofctl add-flow s1 &quot;table=1,icmp,nw_dst=30.0.0.1,actions=mod_dl_dst=00:00:00:00:00:03,output:3&quot; 最後執行h1 ping h3，就會順利的通了，以下整理一下flow table中的所有flow entry #Those two flow will handle the arp-request for the gateway, it will send the arp-request to s1 table=0,priority=65535,arp,arp_tpa=10.0.0.254 actions=LOCAL table=0,priority=65535,arp,arp_tpa=20.0.0.254 actions=LOCAL table=0,priority=65535,arp,arp_tpa=30.0.0.254 actions=LOCAL table=0,priority=1,arp,nw_dst=10.0.0.1,actions=output:1 table=0,priority=1,arp,nw_dst=20.0.0.1,actions=output:2 table=0,priority=1,arp,nw_dst=30.0.0.1,actions=output:3 table=0,priority=0,actions=resubmit(,1) #table1 - forward/route table=1,icmp,nw_dst=10.0.0.1,actions=mod_dl_dst=00:00:00:00:00:01,output:1 table=1,icmp,nw_dst=20.0.0.1,actions=mod_dl_dst=00:00:00:00:00:02,output:2 table=1,icmp,nw_dst=30.0.0.1,actions=mod_dl_dst=00:00:00:00:00:03,output:3  ","version":"Next","tagName":"h2"},{"title":"OpenFlow link capacity","type":0,"sectionRef":"#","url":"/docs/techPost/2014/link-capacity","content":"","keywords":"","version":"Next"},{"title":"環境建置​","type":1,"pageTitle":"OpenFlow link capacity","url":"/docs/techPost/2014/link-capacity#環境建置","content":"Controller: FloodlightNetwork environment: mininet or OVS on PC Expr 1​ mn --controller=remote,ip=127.0.0.1, --topo tree,1curl http://127.0.0.1:8080/wm/core/switch/all/features/json { &quot;00:00:00:00:00:00:00:01&quot;: { &quot;actions&quot;: 4095, &quot;buffers&quot;: 256, &quot;capabilities&quot;: 199, &quot;datapathId&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;length&quot;: 176, &quot;ports&quot;: [ { &quot;advertisedFeatures&quot;: 0, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 192, &quot;hardwareAddress&quot;: &quot;32:38:53:8a:27:42&quot;, &quot;name&quot;: &quot;s1-eth1&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 1, &quot;state&quot;: 0, &quot;supportedFeatures&quot;: 0 }, { &quot;advertisedFeatures&quot;: 0, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 192, &quot;hardwareAddress&quot;: &quot;8a:5d:09:2f:cf:06&quot;, &quot;name&quot;: &quot;s1-eth2&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 2, &quot;state&quot;: 0, &quot;supportedFeatures&quot;: 0 }, { &quot;advertisedFeatures&quot;: 0, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 0, &quot;hardwareAddress&quot;: &quot;7e:00:4e:66:4d:45&quot;, &quot;name&quot;: &quot;s1&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 65534, &quot;state&quot;: 0, &quot;supportedFeatures&quot;: 0 } ], &quot;tables&quot;: -2, &quot;type&quot;: &quot;FEATURES_REPLY&quot;, &quot;version&quot;: 1, &quot;xid&quot;: 7 } }  從回傳的訊息中可以看到，除了lo以外的currentFeatures都是192，192就是2^7+2^6,所以對應到ofp_port_features就是OFPPF_10GB_FD以及OFPPF_COPPER Expr 2​ 這次使用了traffic control link可調整頻寬的link來使用，看看是否會有所變化mn --controller=remote,ip=140.113.214.95,port=6633 --topo tree,1 --link tc,bw=100.0curl http://127.0.0.1:8080/wm/core/switch/all/features/json { &quot;00:00:00:00:00:00:00:01&quot;: { &quot;actions&quot;: 4095, &quot;buffers&quot;: 256, &quot;capabilities&quot;: 199, &quot;datapathId&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;length&quot;: 176, &quot;ports&quot;: [ { &quot;advertisedFeatures&quot;: 0, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 192, &quot;hardwareAddress&quot;: &quot;32:38:53:8a:27:42&quot;, &quot;name&quot;: &quot;s1-eth1&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 1, &quot;state&quot;: 0, &quot;supportedFeatures&quot;: 0 }, { &quot;advertisedFeatures&quot;: 0, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 192, &quot;hardwareAddress&quot;: &quot;8a:5d:09:2f:cf:06&quot;, &quot;name&quot;: &quot;s1-eth2&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 2, &quot;state&quot;: 0, &quot;supportedFeatures&quot;: 0 }, { &quot;advertisedFeatures&quot;: 0, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 0, &quot;hardwareAddress&quot;: &quot;7e:00:4e:66:4d:45&quot;, &quot;name&quot;: &quot;s1&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 65534, &quot;state&quot;: 0, &quot;supportedFeatures&quot;: 0 } ], &quot;tables&quot;: -2, &quot;type&quot;: &quot;FEATURES_REPLY&quot;, &quot;version&quot;: 1, &quot;xid&quot;: 7 } }  可以看到完全沒有變化，不管有沒有設定tc link,這個currentFeatures的值依然是固定在10G，因此就很好奇會不會是這個featureRequest本身並沒有實作出來，因此換一個網路環境再試試看 Expr 3​ 這次就不使用mininet而是直接用一台實體PC配上OVS來跑跑看curl http://127.0.0.1:8080/wm/core/switch/all/features/json { &quot;00:00:a0:36:9f:00:ed:04&quot;: { &quot;actions&quot;: 4095, &quot;buffers&quot;: 256, &quot;capabilities&quot;: 199, &quot;datapathId&quot;: &quot;00:00:a0:36:9f:00:ed:04&quot;, &quot;length&quot;: 272, &quot;ports&quot;: [ { &quot;advertisedFeatures&quot;: 1711, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 672, &quot;hardwareAddress&quot;: &quot;a0:36:9f:00:ed:06&quot;, &quot;name&quot;: &quot;eth3&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 3, &quot;state&quot;: 0, &quot;supportedFeatures&quot;: 1711 }, { &quot;advertisedFeatures&quot;: 1711, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 640, &quot;hardwareAddress&quot;: &quot;a0:36:9f:00:ed:05&quot;, &quot;name&quot;: &quot;eth2&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 2, &quot;state&quot;: 1, &quot;supportedFeatures&quot;: 1711 }, { &quot;advertisedFeatures&quot;: 1711, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 640, &quot;hardwareAddress&quot;: &quot;a0:36:9f:00:ed:07&quot;, &quot;name&quot;: &quot;eth4&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 4, &quot;state&quot;: 1, &quot;supportedFeatures&quot;: 1711 }, { &quot;advertisedFeatures&quot;: 0, &quot;config&quot;: 1, &quot;currentFeatures&quot;: 0, &quot;hardwareAddress&quot;: &quot;a0:36:9f:00:ed:04&quot;, &quot;name&quot;: &quot;br0&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 65534, &quot;state&quot;: 1, &quot;supportedFeatures&quot;: 0 }, { &quot;advertisedFeatures&quot;: 1711, &quot;config&quot;: 0, &quot;currentFeatures&quot;: 640, &quot;hardwareAddress&quot;: &quot;a0:36:9f:00:ed:04&quot;, &quot;name&quot;: &quot;eth1&quot;, &quot;peerFeatures&quot;: 0, &quot;portNumber&quot;: 1, &quot;state&quot;: 1, &quot;supportedFeatures&quot;: 1711 } ], &quot;tables&quot;: -1, &quot;type&quot;: &quot;FEATURES_REPLY&quot;, &quot;version&quot;: 1, &quot;xid&quot;: 4 } }  採用真正的網卡後，就會發現currentFeatures的值是有變化的，這代表OVS的確有實作這個功能，於是我就開始好奇，為什麼Mininet中得到的數值都是10G,tc link到底是什麼 Mininet​ 在仔細研究mininet的source code後，大致瞭解了整個運作流程 當mininet要在兩個switch間創造一條link的時候，是透過下列手段達成的 ip link add name s1-eth1 type veth peer name s2-eth1 這種系統指令創造一個特殊的裝置veth，這兩個裝置的封包會彼此互通，因此就達成了link的功用 此時透過ethtool s1-eth1可以觀察到其中 Speed: 10000Mb/s這樣的設定，他的速度就是設定成10G 在OVS中會使用 struct ethtool_cmd這種結構來獲取port的相關資訊，這時候他會根據speed這個欄位來設定currentFeautres的數值 因此mininet創造出來的link預設都是10G，所以OVS那邊都會抓到10G的資訊 traffic control的部分則是透過系統的tc指令來做到速度限制的功能，所以不會動到每個port的設定 ","version":"Next","tagName":"h3"},{"title":"OpenvSwitch - 2","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openvswitch-source-2","content":"OpenvSwitch - 2 ovs-vsctl add-port br eth1 (netlink) kernel side: Register a generic netlinkCall function when receive command by netlink from userspace (ovs-vsctl)Use the interface name to get the net_deviceRegister the send and receive event handler. Register generic netlink static struct genl_ops dp_vport_genl_ops[] = { { .cmd = OVS_VPORT_CMD_NEW, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = vport_policy, .doit = ovs_vport_cmd_new }, { .cmd = OVS_VPORT_CMD_DEL, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = vport_policy, .doit = ovs_vport_cmd_del }, { .cmd = OVS_VPORT_CMD_GET, .flags = 0, /* OK for unprivileged users. */ .policy = vport_policy, .doit = ovs_vport_cmd_get, .dumpit = ovs_vport_cmd_dump }, { .cmd = OVS_VPORT_CMD_SET, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = vport_policy, .doit = ovs_vport_cmd_set, }, }; 從netlink收到 OVS_VPORT_CMD_NEW的cmd時，就會執行 ovs_vport_cmd_new static int ovs_vport_cmd_new(struct sk_buff *skb, struct genl_info *info) { struct nlattr **a = info-&gt;attrs; struct ovs_header *ovs_header = info-&gt;userhdr; struct vport_parms parms; struct sk_buff *reply; struct vport *vport; struct datapath *dp; u32 port_no; int err; err = -EINVAL; if (!a[OVS_VPORT_ATTR_NAME] || !a[OVS_VPORT_ATTR_TYPE] || !a[OVS_VPORT_ATTR_UPCALL_PID]) goto exit; ovs_lock(); dp = get_dp(sock_net(skb-&gt;sk), ovs_header-&gt;dp_ifindex); err = -ENODEV; if (!dp) goto exit_unlock; if (a[OVS_VPORT_ATTR_PORT_NO]) { port_no = nla_get_u32(a[OVS_VPORT_ATTR_PORT_NO]); err = -EFBIG; if (port_no &gt;= DP_MAX_PORTS) goto exit_unlock; vport = ovs_vport_ovsl(dp, port_no); err = -EBUSY; if (vport) goto exit_unlock; } else { for (port_no = 1; ; port_no++) { if (port_no &gt;= DP_MAX_PORTS) { err = -EFBIG; goto exit_unlock; } vport = ovs_vport_ovsl(dp, port_no); if (!vport) break; } } parms.name = nla_data(a[OVS_VPORT_ATTR_NAME]); parms.type = nla_get_u32(a[OVS_VPORT_ATTR_TYPE]); parms.options = a[OVS_VPORT_ATTR_OPTIONS]; parms.dp = dp; parms.port_no = port_no; parms.upcall_portid = nla_get_u32(a[OVS_VPORT_ATTR_UPCALL_PID]); vport = new_vport(&amp;parms); err = PTR_ERR(vport); if (IS_ERR(vport)) goto exit_unlock; err = 0; if (a[OVS_VPORT_ATTR_STATS]) ovs_vport_set_stats(vport, nla_data(a[OVS_VPORT_ATTR_STATS])); reply = ovs_vport_cmd_build_info(vport, info-&gt;snd_portid, info-&gt;snd_seq, OVS_VPORT_CMD_NEW); if (IS_ERR(reply)) { err = PTR_ERR(reply); ovs_dp_detach_port(vport); goto exit_unlock; } ovs_notify(reply, info, &amp;ovs_dp_vport_multicast_group); exit_unlock: ovs_unlock(); exit: return err; } 這邊會呼叫 new_vport /* Called with ovs_mutex. */ static struct vport *new_vport(const struct vport_parms *parms) { struct vport *vport; vport = ovs_vport_add(parms); if (!IS_ERR(vport)) { struct datapath *dp = parms-&gt;dp; struct hlist_head *head = vport_hash_bucket(dp, vport-&gt;port_no); hlist_add_head_rcu(&amp;vport-&gt;dp_hash_node, head); } return vport; } 這邊會呼叫 ovs_vport_add struct vport *ovs_vport_add(const struct vport_parms *parms) { struct vport *vport; int err = 0; int i; for (i = 0; i &lt; ARRAY_SIZE(vport_ops_list); i++) { if (vport_ops_list[i]-&gt;type == parms-&gt;type) { struct hlist_head *bucket; vport = vport_ops_list[i]-&gt;create(parms); if (IS_ERR(vport)) { err = PTR_ERR(vport); goto out; } bucket = hash_bucket(ovs_dp_get_net(vport-&gt;dp), vport-&gt;ops-&gt;get_name(vport)); hlist_add_head_rcu(&amp;vport-&gt;hash_node, bucket); return vport; } } err = -EAFNOSUPPORT; out: return ERR_PTR(err); } 這邊會執行 vport_ops_list[i]-&gt;create(parms) /* List of statically compiled vport implementations. Don't forget to also * add yours to the list at the bottom of vport.h. */ static const struct vport_ops *vport_ops_list[] = { &amp;ovs_netdev_vport_ops, &amp;ovs_internal_vport_ops, #if IS_ENABLED(CONFIG_NET_IPGRE_DEMUX) &amp;ovs_gre_vport_ops, &amp;ovs_gre64_vport_ops, #endif &amp;ovs_vxlan_vport_ops, &amp;ovs_lisp_vport_ops, }; 假設是個netdev的port,就會執行 ovs_netdev_vport_ops裡面的create const struct vport_ops ovs_netdev_vport_ops = { .type = OVS_VPORT_TYPE_NETDEV, .create = netdev_create, .destroy = netdev_destroy, .get_name = ovs_netdev_get_name, .send = netdev_send, }; 這時候就會執行 netdev_create static struct vport *netdev_create(const struct vport_parms *parms) { struct vport *vport; struct netdev_vport *netdev_vport; int err; vport = ovs_vport_alloc(sizeof(struct netdev_vport), &amp;ovs_netdev_vport_ops, parms); if (IS_ERR(vport)) { err = PTR_ERR(vport); goto error; } netdev_vport = netdev_vport_priv(vport); netdev_vport-&gt;dev = dev_get_by_name(ovs_dp_get_net(vport-&gt;dp), parms-&gt;name); if (!netdev_vport-&gt;dev) { err = -ENODEV; goto error_free_vport; } if (netdev_vport-&gt;dev-&gt;flags &amp; IFF_LOOPBACK || netdev_vport-&gt;dev-&gt;type != ARPHRD_ETHER || ovs_is_internal_dev(netdev_vport-&gt;dev)) { err = -EINVAL; goto error_put; } rtnl_lock(); err = netdev_master_upper_dev_link(netdev_vport-&gt;dev, get_dpdev(vport-&gt;dp)); if (err) goto error_unlock; err = netdev_rx_handler_register(netdev_vport-&gt;dev, netdev_frame_hook, vport); if (err) goto error_master_upper_dev_unlink; dev_set_promiscuity(netdev_vport-&gt;dev, 1); netdev_vport-&gt;dev-&gt;priv_flags |= IFF_OVS_DATAPATH; rtnl_unlock(); netdev_init(); return vport; error_master_upper_dev_unlink: netdev_upper_dev_unlink(netdev_vport-&gt;dev, get_dpdev(vport-&gt;dp)); error_unlock: rtnl_unlock(); error_put: dev_put(netdev_vport-&gt;dev); error_free_vport: ovs_vport_free(vport); error: return ERR_PTR(err); } 這邊會調用 dev_get_by_name 用該name取得對應的device.調用 netdev_rx_handler_register 註冊 rx handler. userspace side:","keywords":"","version":"Next"},{"title":"TCP使用sendto","type":0,"sectionRef":"#","url":"/docs/techPost/2013/tcp-sendto","content":"TCP使用sendto OS:Linux 以前在寫Socket Programming的時候，對於TCP跟UDP在使用上會有一些區別 TCP要先建立連線，接者透過該連線把資料送出去，而UDP因為沒有連線，每次送出資料時都要指定對方的位置 寫TCP的時候，我習慣使用write跟send兩個function 來傳送資料 寫UDP的時候，我習慣使用sendto來傳送資料 ssize_t send(int s, const void *msg, size_t len, int flags); ssize_t sendto(int s, const void *msg, size_t len, int flags, const struct sockaddr *to, socklen_t tolen); 但是近期聽學長說，看到有程式碼以sendto來傳送資料，這是我第一次聽到，好奇之下，便去查詢了一下 這邊以linux-3.5.4版本為例 當使用者呼叫send來傳送資料時，會先呼叫 SYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len, unsigned int, flags) { return sys_sendto(fd, buff, len, flags, NULL, 0); } 這邊可以看到，send做的事情非常簡單，就是在去呼叫sendto，然後後面兩個位置的部分就給他填為NULL，所以對TCP連線來說，使用sendto並且後兩個參數也給NULL，也一樣可以work。 那這邊就好奇了，既然同樣都是使用sendto來傳送資料，那TCP沒有給定位置是因為本身已經有連線了， 那到底在sendto中是如何辦到這件事情了，所以又繼續往下看 SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len, unsigned int, flags, struct sockaddr __user *, addr, int, addr_len) { struct socket *sock; struct sockaddr_storage address; int err; struct msghdr msg; struct iovec iov; int fput_needed; if (len &gt; INT_MAX) len = INT_MAX; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (!sock) goto out; iov.iov_base = buff; iov.iov_len = len; msg.msg_name = NULL; msg.msg_iov = &amp;iov; msg.msg_iovlen = 1; msg.msg_control = NULL; msg.msg_controllen = 0; msg.msg_namelen = 0; if (addr) { err = move_addr_to_kernel(addr, addr_len, &amp;address); if (err &lt; 0) goto out_put; msg.msg_name = (struct sockaddr *)&amp;address; msg.msg_namelen = addr_len; } if (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK) flags |= MSG_DONTWAIT; msg.msg_flags = flags; err = sock_sendmsg(sock, &amp;msg, len); out_put: fput_light(sock-&gt;file, fput_needed); out: return err; } 這邊可以看到會利用sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK來檢查是否是個nonblock的傳送。 回歸正題，先執行move_addr_to_kernel這個function,把對方位置給轉移到kernel space中， 如果是TCP連線的話，傳進去的參數就會是NULL跟0，而 int move_addr_to_kernel(void __user *uaddr, int ulen, struct sockaddr_storage *kaddr) { if (ulen &lt; 0 || ulen &gt; sizeof(struct sockaddr_storage)) return -EINVAL; if (ulen == 0) return 0; if (copy_from_user(kaddr, uaddr, ulen)) return -EFAULT; return audit_sockaddr(ulen, kaddr); } 可以看到，當傳入的ulen是0的時候，就會回傳0，因此這邊對於TCP就不會回傳錯誤。 這邊可以看到 msg.msg_name = (struct sockaddr *)&amp;address; msg.msg_namelen = addr_len; 這邊可以看到會把對方位置的相關資訊給存到msg中，估計是之後UDP會用到 接下來透過sock_sendmsg傳送資料 sock_sendmsg -&gt;sock_sendmsg-&gt;sock_sendmsg_nosec static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size) { struct sock_iocb *si = kiocb_to_siocb(iocb); sock_update_classid(sock-&gt;sk); sock_update_netprioidx(sock-&gt;sk); si-&gt;sock = sock; si-&gt;scm = NULL; si-&gt;msg = msg; si-&gt;size = size; return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size); } 這邊可以看到 最後會透過sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size) 這行把資料送出去 根據socket的種類是TCP還是UDP，對應到不同的function pointer 分別是tcp_sendmsg,udp_sendmsg 而在udp_sendmsg中就會去使用到剛剛在sendto那邊設定的msg 節錄自udp_sendmsg 這邊可以明顯看到會把msg中關於對方位置的資訊給抓出來，然後設定到daddr以及dport if(msg-&gt;msg_name) { struct sockaddr_in *usin = (struct sockaddr_in *)msg-&gt;msg_name; if (msg-&gt;msg_namelen &lt; sizeof(*usin)) return -EINVAL; if (usin-&gt;sin_family != AF_INET) { if (usin-&gt;sin_family != AF_UNSPEC) return -EAFNOSUPPORT; } daddr = usin-&gt;sin_addr.s_addr; dport = usin-&gt;sin_port; if (dport == 0) return -EINVAL; } else { if (sk-&gt;sk_state != TCP_ESTABLISHED) return -EDESTADDRREQ; daddr = inet-&gt;inet_daddr; dport = inet-&gt;inet_dport; /* Open fast path for connected socket. Route will not be used, if at least one option is set. */ connected = 1; } 此外，recv以及recvfrom也是一樣的組合，與sent和sendto的關係差不多","keywords":"","version":"Next"},{"title":"MongoDB","type":0,"sectionRef":"#","url":"/docs/techPost/2014/mongodb","content":"","keywords":"","version":"Next"},{"title":"Install​","type":1,"pageTitle":"MongoDB","url":"/docs/techPost/2014/mongodb#install","content":"Refet to MongoDB Installation ","version":"Next","tagName":"h3"},{"title":"Manipulate​","type":1,"pageTitle":"MongoDB","url":"/docs/techPost/2014/mongodb#manipulate","content":"在 MongoDB中  db: 就是databasecollection: 就是以前看到的Tabledocument: 就是以前看到的record 在操作 collection的時候，不需要事先定義每個 document的欄位以及屬性，每個 documents的欄位數量可以不同，並且可以共存於同一張 collection之中。 Mongob Command Line​ DB show dbs : 顯示目前有哪些dbdb: 顯示目前使用哪個dbuse xxx: 切換到哪個db (若不存在，就會新增) Collection show collections : 顯示當前db下有哪些 collectionsdb.collection.command: collection的操作都是按照 db.${collection_name},${command} db.test.insert( { &quot;key1&quot;:&quot;value1&quot;, &quot;key2&quot;:&quot;value2&quot;}) : 增加一個新的 document,如果沒有該 collections,就會順便產生新的db.test.drop() : 刪除該 collectiondb.test.remove( { &quot;key1&quot;:&quot;value1&quot;, &quot;key2&quot;:&quot;value2&quot;}): 刪除特定的 document More refer to MongoDB Shell Use python​ 目前是用 PyMongo這個第三方套件 Install pip install pymongoRefer to Pymongo Installation Connection client = MongoClient('localhost', 27017) : 與mongodb server連線 Manipulate Refer to Pymongo Tutorial ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openvswitch-2","content":"","keywords":"","version":"Next"},{"title":"datapath.c​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#datapathc","content":"This file is the main part of the kernel module and it will be compiled to the datapath.ko. module_init(dp_init); module_exit(dp_cleanup); MODULE_DESCRIPTION(&quot;Open vSwitch switching datapath&quot;); MODULE_LICENSE(&quot;GPL&quot;); MODULE_VERSION(VERSION);  The kernel will call its init function dp_init after the kernel module has been loaded. The following is the work flow of the dp_init. ovs_workqueues_init()ovs_flow_init()ovs_vport_initregister_pernet_device(&amp;ovs_net_ops);register_netdevice_notifier(&amp;ovs_dp_device_notifier);dp_register_genl();schedule_delayed_work(&amp;rehash_flow_wq, REHASH_FLOW_INTERVAL); ","version":"Next","tagName":"h2"},{"title":"ovs_workqueues_init​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#ovs_workqueues_init","content":"workqueue.c static struct task_struct *workq_thread; spin_lock_init(&amp;wq_lock); INIT_LIST_HEAD(&amp;workq); init_waitqueue_head(&amp;more_work); workq_thread = kthread_create(worker_thread, NULL, &quot;ovs_workq&quot;); if (IS_ERR(workq_thread)) return PTR_ERR(workq_thread); wake_up_process(workq_thread);  Initail the worker queue.Create a kernel thread and the handler is worker_threadstart the kernel thread by calling wake_up_process static int worker_thread(void *dummy) { for (;;) { wait_event_interruptible(more_work, (kthread_should_stop() || !list_empty(&amp;workq))); if (kthread_should_stop()) break; run_workqueue(); } return 0; }  wait_event_interruptible make the thread hibernation and add into the queue more_work.The thread will wake up until the condition &quot;kthread_should_stop() || !list_empty(&amp;workq))&quot; is true.It will call the run_workqueue after it wake up. static void run_workqueue(void) { spin_lock_irq(&amp;wq_lock); while (!list_empty(&amp;workq)) { struct work_struct *work = list_entry(workq.next, struct work_struct, entry); work_func_t f = work-&gt;func; list_del_init(workq.next); current_work = work; spin_unlock_irq(&amp;wq_lock); work_clear_pending(work); f(work); BUG_ON(in_interrupt()); spin_lock_irq(&amp;wq_lock); current_work = NULL; } spin_unlock_irq(&amp;wq_lock); }  Get the work from the workq list and call the fucntion. ","version":"Next","tagName":"h3"},{"title":"ovs_flow_init​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#ovs_flow_init","content":"flow.c /* Initializes the flow module. * Returns zero if successful or a negative error code. */ int ovs_flow_init(void) { BUILD_BUG_ON(__alignof__(struct sw_flow_key) % __alignof__(long)); BUILD_BUG_ON(sizeof(struct sw_flow_key) % sizeof(long)); flow_cache = kmem_cache_create(&quot;sw_flow&quot;, sizeof(struct sw_flow), 0, 0, NULL); if (flow_cache == NULL) return -ENOMEM; return 0; }  use the kmem_cache_create to create a kernel cache with size sw_flow struct sw_flow { struct rcu_head rcu; struct hlist_node hash_node[2]; u32 hash; struct sw_flow_key key; struct sw_flow_key unmasked_key; struct sw_flow_mask *mask; struct sw_flow_actions __rcu *sf_acts; spinlock_t lock; /* Lock for values below. */ unsigned long used; /* Last used time (in jiffies). */ u64 packet_count; /* Number of packets matched. */ u64 byte_count; /* Number of bytes matched. */ u8 tcp_flags; /* Union of seen TCP flags. */ };  This struct store the info of each flow, including count, flow_key and flow_mask. ","version":"Next","tagName":"h3"},{"title":"ovs_vport_init​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#ovs_vport_init","content":"vport.c&quot; /** * ovs_vport_init - initialize vport subsystem * * Called at module load time to initialize the vport subsystem. */ int ovs_vport_init(void) { dev_table = kzalloc(VPORT_HASH_BUCKETS * sizeof(struct hlist_head), GFP_KERNEL); if (!dev_table) return -ENOMEM; return 0; }  Use kzalloc malloc the memory from kernel. ","version":"Next","tagName":"h3"},{"title":"register_pernet_device(&ovs_net_ops)​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#register_pernet_deviceovs_net_ops","content":" register_pernet_device(&amp;ovs_net_ops);  Register a network device ovs_net_ops static struct pernet_operations ovs_net_ops = { .init = ovs_init_net, .exit = ovs_exit_net, .id = &amp;ovs_net_id, .size = sizeof(struct ovs_net), };  ovs_net_ops inherent from pernet_operations, it should implement some function (init, exit) static int __net_init ovs_init_net(struct net *net) { struct ovs_net *ovs_net = net_generic(net, ovs_net_id); INIT_LIST_HEAD(&amp;ovs_net-&gt;dps); INIT_WORK(&amp;ovs_net-&gt;dp_notify_work, ovs_dp_notify_wq); return 0; }  Use net_generic get the pointer to ovs_net.Use INIT_WORK to create a worker and set the function (dp_notify_work) as its work. struct ovs_net { struct list_head dps; struct vport_net vport_net; struct work_struct dp_notify_work; };  need to study later. void ovs_dp_notify_wq(struct work_struct *work) { struct ovs_net *ovs_net = container_of(work, struct ovs_net, dp_notify_work); struct datapath *dp; ovs_lock(); list_for_each_entry(dp, &amp;ovs_net-&gt;dps, list_node) { int i; for (i = 0; i &lt; DP_VPORT_HASH_BUCKETS; i++) { struct vport *vport; struct hlist_node *n; hlist_for_each_entry_safe(vport, n, &amp;dp-&gt;ports[i], dp_hash_node) { struct netdev_vport *netdev_vport; if (vport-&gt;ops-&gt;type != OVS_VPORT_TYPE_NETDEV) continue; netdev_vport = netdev_vport_priv(vport); if (netdev_vport-&gt;dev-&gt;reg_state == NETREG_UNREGISTERED || netdev_vport-&gt;dev-&gt;reg_state == NETREG_UNREGISTERING) dp_detach_port_notify(vport); } } } ovs_unlock(); }  search datapathes and list all its vport.Delete the vport if its status is UNREGISTERED of UNREGISTERING. ","version":"Next","tagName":"h3"},{"title":"register_netdevice_notifier​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#register_netdevice_notifier","content":" register_netdevice_notifier(&amp;ovs_dp_device_notifier);  Register the network notification chain, it will call ovs_dp_device_notifier when event occur. struct notifier_block ovs_dp_device_notifier = { .notifier_call = dp_device_event };  ovs_dp_device_notifier contains a function pointer which point to dp_device_event.This function will be call when the notification has occur. static int dp_device_event(struct notifier_block *unused, unsigned long event, void *ptr) { struct ovs_net *ovs_net; struct net_device *dev = ptr; struct vport *vport = NULL; if (!ovs_is_internal_dev(dev)) vport = ovs_netdev_get_vport(dev); if (!vport) return NOTIFY_DONE; if (event == NETDEV_UNREGISTER) { ovs_net = net_generic(dev_net(dev), ovs_net_id); queue_work(&amp;ovs_net-&gt;dp_notify_work); } return NOTIFY_DONE; }  Need to study. ","version":"Next","tagName":"h3"},{"title":"dp_register_genl​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#dp_register_genl","content":"datapath.c static int dp_register_genl(void) { int n_registered; int err; int i; n_registered = 0; for (i = 0; i &lt; ARRAY_SIZE(dp_genl_families); i++) { const struct genl_family_and_ops *f = &amp;dp_genl_families[i]; err = genl_register_family_with_ops(f-&gt;family, f-&gt;ops, f-&gt;n_ops); if (err) goto error; n_registered++; if (f-&gt;group) { err = genl_register_mc_group(f-&gt;family, f-&gt;group); if (err) goto error; } } return 0; error: dp_unregister_genl(n_registered); return err; }  Register four types of gereric netlink (datapath, vport, flow, packet).You can see the detail info in dp_genl_familiesgenl_register_family_with_ops : register a generic netlink family with ops. static const struct genl_family_and_ops dp_genl_families[] = { { &amp;dp_datapath_genl_family, dp_datapath_genl_ops, ARRAY_SIZE(dp_datapath_genl_ops), &amp;ovs_dp_datapath_multicast_group }, { &amp;dp_vport_genl_family, dp_vport_genl_ops, ARRAY_SIZE(dp_vport_genl_ops), &amp;ovs_dp_vport_multicast_group }, { &amp;dp_flow_genl_family, dp_flow_genl_ops, ARRAY_SIZE(dp_flow_genl_ops), &amp;ovs_dp_flow_multicast_group }, { &amp;dp_packet_genl_family, dp_packet_genl_ops, ARRAY_SIZE(dp_packet_genl_ops), NULL }, };  struct genl_family_and_ops { struct genl_family *family; struct genl_ops *ops; int n_ops; struct genl_multicast_group *group; };  A genl_family_and_ops contains a pointer to its family and a pointer to its operations. static struct genl_family dp_datapath_genl_family = { .id = GENL_ID_GENERATE, .hdrsize = sizeof(struct ovs_header), .name = OVS_DATAPATH_FAMILY, .version = OVS_DATAPATH_VERSION, .maxattr = OVS_DP_ATTR_MAX, .netnsok = true, SET_PARALLEL_OPS }; static struct genl_ops dp_datapath_genl_ops[] = { { .cmd = OVS_DP_CMD_NEW, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = datapath_policy, .doit = ovs_dp_cmd_new }, { .cmd = OVS_DP_CMD_DEL, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = datapath_policy, .doit = ovs_dp_cmd_del }, { .cmd = OVS_DP_CMD_GET, .flags = 0, /* OK for unprivileged users. */ .policy = datapath_policy, .doit = ovs_dp_cmd_get, .dumpit = ovs_dp_cmd_dump }, { .cmd = OVS_DP_CMD_SET, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = datapath_policy, .doit = ovs_dp_cmd_set, }, };  Take dp_datapath_genl_ops for example. when the event is OVS_DP_CMD_NEW it will call it function handler ovs_dp_cmd_new. ","version":"Next","tagName":"h3"},{"title":"schedule_delayed_work​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#schedule_delayed_work","content":"Need to study ","version":"Next","tagName":"h3"},{"title":"other​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-2#other","content":"pr_info is printk(KERN_INFO,pr_fmt(fmt), ##VA_ARGS) ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2014/mpd5-on-freebsd-100","content":"","keywords":"","version":"Next"},{"title":"Pkg​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/mpd5-on-freebsd-100#pkg","content":"pkg install mpd5 ","version":"Next","tagName":"h2"},{"title":"Ports​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/mpd5-on-freebsd-100#ports","content":"portmaster net/mpd5 Config ","version":"Next","tagName":"h2"},{"title":"VPN Configuraion​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/mpd5-on-freebsd-100#vpn-configuraion","content":"cp /usr/local/etc/mpd5/mpd.conf.sample /usr/local/etc/mpd5/mpd.confset user hwchiu 123456 used to config the admin's accoutn and password of the web page.set web self 0.0.0.0 5006 is the listen ip address and port of the web page. startup: # configure mpd users set user hwchiu 123456 # configure the console set console self 127.0.0.1 5005 set console open # configure the web server set web self 0.0.0.0 5006 set web open  comment the dialup and add pptp_server, we will config the options of pptp_server later. default: #load dialup load pptp_server  set ippool add pool1 ip_start, ip_end is used to set the private ip range for vpn user. the name ippool and pool1 must be the same as set ipcp ranges 192.168.1.1/32 ippool pool1set ipcp ranges 192.168.1.1/32 ippool pool1 is the ip address of the server.set ipcp dns 172.31.0.2 is used to set the dns server. In my case, since my machine is behind the EC2, i used the same configuration in my FreeBSD.set ipcp nbns 172.31.0.2is used to for windows client.set pptp self 172.31.18.110. You should set your ip address which is shown on the network interface. # Define dynamic IP address pool. set ippool add pool1 192.168.1.50 192.168.1.99 # Create clonable bundle template named B create bundle template B set iface enable proxy-arp set iface idle 1800 set iface enable tcpmssfix set ipcp yes vjcomp # Specify IP address pool for dynamic assigment. set ipcp ranges 192.168.1.1/32 ippool pool1 set ipcp dns 172.31.0.2 set ipcp nbns 172.31.0.2 # The five lines below enable Microsoft Point-to-Point encryption # (MPPE) using the ng_mppc(8) netgraph node type. set bundle enable compression set ccp yes mppc set mppc yes e40 set mppc yes e128 set mppc yes stateless # Create clonable link template named L create link template L pptp # Set bundle template to use set link action bundle B # Multilink adds some overhead, but gives full 1500 MTU. set link enable multilink set link yes acfcomp protocomp set link no pap chap eap set link enable chap # We can use use RADIUS authentication/accounting by including # another config section with label 'radius'. # load radius set link keep-alive 10 60 # We reducing link mtu to avoid GRE packet fragmentation. set link mtu 1460 # Configure PPTP set pptp self 172.31.18.110 # Allow to accept calls set link enable incoming  ","version":"Next","tagName":"h2"},{"title":"Use configuration​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/mpd5-on-freebsd-100#use-configuration","content":"cp /usr/local/etc/mpd5/mpd.secret.sample /usr/local/etc/mpd5/mpd.secretThe format of mpd.secret is username password ip_address per line.Example fred &quot;fred-pw&quot; joe &quot;foobar&quot; 192.168.1.1  ","version":"Next","tagName":"h2"},{"title":"System configuration​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/mpd5-on-freebsd-100#system-configuration","content":"sysctl net.inet.ip.forwarding=1Pf configuraion use NAT for internal private network.skip the lo interface.block adll traffic adn log all packet by default.pass in tcp for port 1723 (PPTP)pass in protocol grepass in from any to internal private network and vice versa.Use the pfctl -f file to reload the pf instead of /etc/rc.d/pf restart, the latter will disconnect all exist connection. my_int = &quot;xn0&quot; internal_net = &quot;192.168.0.0/16&quot; external_addr = &quot;172.31.18.110&quot; nat on $my_int from $internal_net to any -&gt; $external_addr set skip on lo block in log all pass in on $my_int proto tcp from any to any port 1723 keep state pass in on $my_int proto tcp from any to any port 443 keep state pass in quick on $my_int proto icmp all keep state pass in proto gre all keep state pass in from any to $internal_net pass in from $internal_net to any pass out proto { gre, tcp, udp, icmp } all keep state  ","version":"Next","tagName":"h2"},{"title":"Log configuration.​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/mpd5-on-freebsd-100#log-configuration","content":"Edit /etc/syslog.conf !mpd *.* /var/log/mpd.log  Touch /var/log/mpd.logRestart syslog Usage /usr/local/etc/rc.d/mpd5 start ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openvswitch-3","content":"","keywords":"","version":"Next"},{"title":"User space​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-3#user-space","content":"這部分還有點卡住，對 ovsdb 有更瞭解後再補充 bridge_reconfigure bridge_refresh_ofp_port bridge_refresh_one_ofp_port iface_create iface_do_create ofproto_port_add VLOG_INFO static int port_add(struct ofproto *ofproto_, struct netdev *netdev) { ... error = dpif_port_add(ofproto-&gt;backer-&gt;dpif, netdev, &amp;port_no); ... }  int dpif_port_add(struct dpif *dpif, struct netdev *netdev, odp_port_t *port_nop) { const char *netdev_name = netdev_get_name(netdev); odp_port_t port_no = ODPP_NONE; int error; COVERAGE_INC(dpif_port_add); if (port_nop) { port_no = *port_nop; } error = dpif-&gt;dpif_class-&gt;port_add(dpif, netdev, &amp;port_no); if (!error) { VLOG_DBG_RL(&amp;dpmsg_rl, &quot;%s: added %s as port %&quot;PRIu32, dpif_name(dpif), netdev_name, port_no); } else { VLOG_WARN_RL(&amp;error_rl, &quot;%s: failed to add %s as port: %s&quot;, dpif_name(dpif), netdev_name, ovs_strerror(error)); port_no = ODPP_NONE; } if (port_nop) { *port_nop = port_no; } return error; }  static void dp_initialize(void) { static struct ovsthread_once once = OVSTHREAD_ONCE_INITIALIZER; if (ovsthread_once_start(&amp;once)) { int i; for (i = 0; i &lt; ARRAY_SIZE(base_dpif_classes); i++) { dp_register_provider(base_dpif_classes[i]); } ovsthread_once_done(&amp;once); } }  這邊會去註冊每個 base_dipf_classes. struct dpif_class { .... /* Adds 'netdev' as a new port in 'dpif'. If '*port_no' is not * UINT32_MAX, attempts to use that as the port's port number. * * If port is successfully added, sets '*port_no' to the new port's * port number. Returns EBUSY if caller attempted to choose a port * number, and it was in use. */ int (*port_add)(struct dpif *dpif, struct netdev *netdev, odp_port_t *port_no); ... }  dpif_class是一個base class,裡面存放的都是function pointer. static const struct dpif_class *base_dpif_classes[] = { #ifdef LINUX_DATAPATH &amp;dpif_linux_class, #endif &amp;dpif_netdev_class, };  這邊會根據type去實例化不同類型的 dpif_class,這邊我們關心的是 dpif_linux_class. const struct dpif_class dpif_linux_class = { &quot;system&quot;, dpif_linux_enumerate, NULL, dpif_linux_open, dpif_linux_close, dpif_linux_destroy, NULL, /* run */ NULL, /* wait */ dpif_linux_get_stats, dpif_linux_port_add, dpif_linux_port_del, dpif_linux_port_query_by_number, dpif_linux_port_query_by_name, dpif_linux_get_max_ports, dpif_linux_port_get_pid, dpif_linux_port_dump_start, dpif_linux_port_dump_next, dpif_linux_port_dump_done, dpif_linux_port_poll, dpif_linux_port_poll_wait, dpif_linux_flow_get, dpif_linux_flow_put, dpif_linux_flow_del, dpif_linux_flow_flush, dpif_linux_flow_dump_start, dpif_linux_flow_dump_next, dpif_linux_flow_dump_done, dpif_linux_execute, dpif_linux_operate, dpif_linux_recv_set, dpif_linux_queue_to_priority, dpif_linux_recv, dpif_linux_recv_wait, dpif_linux_recv_purge, };  這邊定義了 dpif_class的一些操作function.我們關心的 port_add 實際上對應的是 dpif_linux_port_add static int dpif_linux_port_add(struct dpif *dpif_, struct netdev *netdev, odp_port_t *port_nop) { struct dpif_linux *dpif = dpif_linux_cast(dpif_); int error; ovs_mutex_lock(&amp;dpif-&gt;upcall_lock); error = dpif_linux_port_add__(dpif_, netdev, port_nop); ovs_mutex_unlock(&amp;dpif-&gt;upcall_lock); return error; }  這邊會呼叫 dpif_linux_port_add__去創立vport. static int dpif_linux_port_add__(struct dpif *dpif_, struct netdev *netdev, odp_port_t *port_nop) { .... request.cmd = OVS_VPORT_CMD_NEW; request.dp_ifindex = dpif-&gt;dp_ifindex; request.type = netdev_to_ovs_vport_type(netdev); ... error = dpif_linux_vport_transact(&amp;request, &amp;reply, &amp;buf); ... }  這邊會設定request的cmd為 OVS_VPORT_CMD_NEW使用 dpif_linux_vport_transact 把這個request透過netlink的方式送到kernel端，kernel端會再根據這個request的cmd來執行特定的function. ","version":"Next","tagName":"h2"},{"title":"Gereric netlink​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-3#gereric-netlink","content":"static int dp_register_genl(void) { int n_registered; int err; int i; n_registered = 0; for (i = 0; i &lt; ARRAY_SIZE(dp_genl_families); i++) { const struct genl_family_and_ops *f = &amp;dp_genl_families[i]; err = genl_register_family_with_ops(f-&gt;family, f-&gt;ops, f-&gt;n_ops); if (err) goto error; n_registered++; if (f-&gt;group) { err = genl_register_mc_group(f-&gt;family, f-&gt;group); if (err) goto error; } } return 0; error: dp_unregister_genl(n_registered); return err; }  當 datapath kernel module被載入的時候，會執行對應的 init,裡面會執行 dp_register_genl.這邊會呼叫所有的 dp_genl_families 來註冊 generic netlink相關的function. static const struct genl_family_and_ops dp_genl_families[] = { { &amp;dp_datapath_genl_family, dp_datapath_genl_ops, ARRAY_SIZE(dp_datapath_genl_ops), &amp;ovs_dp_datapath_multicast_group }, { &amp;dp_vport_genl_family, dp_vport_genl_ops, ARRAY_SIZE(dp_vport_genl_ops), &amp;ovs_dp_vport_multicast_group }, { &amp;dp_flow_genl_family, dp_flow_genl_ops, ARRAY_SIZE(dp_flow_genl_ops), &amp;ovs_dp_flow_multicast_group }, { &amp;dp_packet_genl_family, dp_packet_genl_ops, ARRAY_SIZE(dp_packet_genl_ops), NULL }, };  這邊定義了所有 gerneric netlink相關type的結構成員 (dp,vport,flow,packet) static struct genl_ops dp_vport_genl_ops[] = { { .cmd = OVS_VPORT_CMD_NEW, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = vport_policy, .doit = ovs_vport_cmd_new }, { .cmd = OVS_VPORT_CMD_DEL, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = vport_policy, .doit = ovs_vport_cmd_del }, { .cmd = OVS_VPORT_CMD_GET, .flags = 0, /* OK for unprivileged users. */ .policy = vport_policy, .doit = ovs_vport_cmd_get, .dumpit = ovs_vport_cmd_dump }, { .cmd = OVS_VPORT_CMD_SET, .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */ .policy = vport_policy, .doit = ovs_vport_cmd_set, }, };  這邊定義了 gerneric netlink 相關的vport operation.當 cmd 是 OVS_VPORT_CMD_NEW的時候，就會執行對應的 function handler ovs_vport_cmd_new ","version":"Next","tagName":"h2"},{"title":"Kernel space​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-3#kernel-space","content":"static int ovs_vport_cmd_new(struct sk_buff *skb, struct genl_info *info) { ... parms.name = nla_data(a[OVS_VPORT_ATTR_NAME]); parms.type = nla_get_u32(a[OVS_VPORT_ATTR_TYPE]); parms.options = a[OVS_VPORT_ATTR_OPTIONS]; parms.dp = dp; parms.port_no = port_no; parms.upcall_portid = nla_get_u32(a[OVS_VPORT_ATTR_UPCALL_PID]); vport = new_vport(&amp;parms); ... }  struct vport_parms parms 用來記錄一些vport的資訊填寫相關訊息後，就呼叫 new_vport來創立 vport static struct vport *new_vport(const struct vport_parms *parms) { struct vport *vport; vport = ovs_vport_add(parms); if (!IS_ERR(vport)) { struct datapath *dp = parms-&gt;dp; struct hlist_head *head = vport_hash_bucket(dp, vport-&gt;port_no); hlist_add_head_rcu(&amp;vport-&gt;dp_hash_node, head); } return vport; }  這邊會呼叫 *ovs_vport_add 來創立該vport struct vport *ovs_vport_add(const struct vport_parms *parms) { struct vport *vport; int err = 0; int i; for (i = 0; i &lt; ARRAY_SIZE(vport_ops_list); i++) { if (vport_ops_list[i]-&gt;type == parms-&gt;type) { struct hlist_head *bucket; vport = vport_ops_list[i]-&gt;create(parms); if (IS_ERR(vport)) { err = PTR_ERR(vport); goto out; } bucket = hash_bucket(ovs_dp_get_net(vport-&gt;dp), vport-&gt;ops-&gt;get_name(vport)); hlist_add_head_rcu(&amp;vport-&gt;hash_node, bucket); return vport; } } err = -EAFNOSUPPORT; out: return ERR_PTR(err); }  掃過所有 vport_ops_list, 如果那個ops的 type跟傳進來的type是一樣的，那就呼叫對應的 create function. static const struct vport_ops *vport_ops_list[] = { &amp;ovs_netdev_vport_ops, &amp;ovs_internal_vport_ops, #if IS_ENABLED(CONFIG_NET_IPGRE_DEMUX) &amp;ovs_gre_vport_ops, &amp;ovs_gre64_vport_ops, #endif &amp;ovs_vxlan_vport_ops, &amp;ovs_lisp_vport_ops, };  vport_ops 有多種type.這邊我們專注於 ovs_netdev_vport_ops這種type. const struct vport_ops ovs_netdev_vport_ops = { .type = OVS_VPORT_TYPE_NETDEV, .create = netdev_create, .destroy = netdev_destroy, .get_name = ovs_netdev_get_name, .send = netdev_send, };  vport當type是 netdev的時候，成員的function pointer就會按此設定為各個function.可以看到他的 create 對應到 netdev_create static struct vport *netdev_create(const struct vport_parms *parms) ... err = netdev_rx_handler_register(netdev_vport-&gt;dev, netdev_frame_hook, vport); ...  這邊註冊netdevice的receive handler, handler為 netdev_frame_hook int netdev_rx_handler_register(struct net_device *dev, rx_handler_func_t *rx_handler, void *rx_handler_data) { ASSERT_RTNL(); if (dev-&gt;rx_handler) return -EBUSY; /* Note: rx_handler_data must be set before rx_handler */ rcu_assign_pointer(dev-&gt;rx_handler_data, rx_handler_data); rcu_assign_pointer(dev-&gt;rx_handler, rx_handler); return 0; }  這邊會把設定該 net_device中的兩個pointerrx_handler_data 這邊就設定成該vport.rx_handler 接收到封包的處理函式，這邊就是netdev_frame_hook。 ","version":"Next","tagName":"h2"},{"title":"Binomial Heap","type":0,"sectionRef":"#","url":"/docs/techPost/2014/binomial-heap","content":"Binomial Heap Binomial Heap是由一群 Binomail Tree所組成的Binomial Tree(BT)含有下列特性 高度為k的 BT共有2^k個node高度為k的 BT可以看成 BT0~BTk-1的組合 再加上一個root組成 Binomial Heap 是 mergable heap由一群 Binomial Tree組成，每個BT都滿足 min-heap的性質對於高度為k的BT只能存在最多一棵以二進位來看待的話，第K位就代表是否存在高度為K的BT 以下圖為例，就是11001 (右邊最小)因此任何數量的結點都可以用不同的BT給組合出來 ##Implement## 採用 Left-Child Right-sibling的方式來實現，左邊指向child,右邊指向同輩value: node的值degree: 以此node為root的BT的高度parent: 指向其parent class Node{ public: Node* parent; Node* child; Node* sibling; int value; int degree; Node(){ parent = NULL; child = NULL; sibling = NULL; value = 0; degree = 0; } }; ##Functions## getMinsizeTravese (postorder)mergeHeapInsertdeleteMin ##getMin## 由於每個BT本身都已經是min-heap的特性了，因此只要針對每個BT的root比較其值即可 int getMin(){ Node* x = head; int min = INT_MAX; while(x!=NULL){ if(x-&gt;value &lt; min) min = x-&gt;value; x = x-&gt;sibling; } return min; } ##size## 由於 Binomial Heap內都是由 Binomial Tree組成，所以可以由每個BT的degree得到其node數量 再把所有加總即可。 int size(){ Node* tmp = head; int count=0; while(tmp){ count+= (1&lt;&lt;tmp-&gt;degree); // 2^degree tmp = tmp-&gt;sibling; } return count; } ##Postorder## 這邊是每個BT都要獨立跑一次Postorder的結果，所以在遞迴的過程中要對root做一些控制 //對每一棵BT都跑一次postorder void postorder(){ Node* tmp = head; while(tmp){ _postorder(tmp); tmp = tmp-&gt;sibling; } printf(&quot;\\n&quot;); } //用parent判斷是不是root,避免root跑去呼叫到別的BT void _postorder(Node* node){ if(!node) return; _postorder(node-&gt;child); if(node-&gt;parent) _postorder(node-&gt;sibling); printf(&quot;%d &quot;,node-&gt;value); } ##MergeHeap## 要合併兩個 Binomial Heap 先把兩個 Binomail Heap的 BT list給重新串接起來，以degree為key做sorting.再根據這個新的BT list開始進行一系列的合併如果只有兩個高度相同的BT，就直接合併如果有三個高度相同的BT，就把後面兩棵合併(維持sorting) void MergeHeap(BinomialHeap &amp;bh){ mergeHeap(bh); //先把BT list給重新串接起來 Node* prev = NULL; Node* x = head; Node* next = x-&gt;sibling; while(next){ if( (x-&gt;degree != next-&gt;degree) || next-&gt;sibling &amp;&amp; next-&gt;sibling-&gt;degree == x-&gt;degree){ prev = x; //前後兩棵BT的高度不同 或是 後面三棵BT的高度都相同 x = next; //那就把指標往前移動，下次再合併 } else if( x-&gt;value &lt;= next-&gt;value){ //前面BT的值比較小，所以後面的合併進來 x-&gt;sibling = next-&gt;sibling; mergeTree(next,x); } else{ //前面那棵BT的值比較大，要往後合併，視情況也要更新 head指標 if(!prev){ head = next; //更新head 指標 } else{ prev-&gt;sibling = next; } mergeTree(x,next); //合併 x = next; } next = next-&gt;sibling; } } 要把兩個 Binomial Heap的BT list給重新串接起來，採用 merge sort的方法 使用 newHead紀錄合併後的頭使用 newCurr來紀錄每次合併後的尾 void mergeHeap(BinomialHeap &amp;bh){ Node* head2 = bh.head; Node* head1 = head; Node* newHead, *newCurr; if(!head1){ //如果本身是空的，就不需要合併，直接指向對方即可 head = head2; return ; } else if(!head2){ //對方是空的，也不需要合併 return ; } //先行尋找誰的開頭比較小，當做新串列的頭 if(head1-&gt;degree &gt; head2-&gt;degree){ newHead = newCurr = head2; head2 = head2-&gt;sibling; } else { newHead = newCurr = head1; head1 = head1-&gt;sibling; } while(head1 &amp;&amp; head2){ if(head1-&gt;degree &lt; head2-&gt;degree){ newCurr-&gt;sibling = head1; newCurr = head1; head1 = head1-&gt;sibling; } else { newCurr-&gt;sibling = head2; newCurr = head2; head2 = head2-&gt;sibling; } } while(head1){ newCurr-&gt;sibling = head1; newCurr = head1; head1 = head1-&gt;sibling; } while(head2){ newCurr-&gt;sibling = head2; newCurr = head2; head2 = head2-&gt;sibling; } head = newHead; } 合併兩個 Binomial Tree，由於我們是min-heap的特性，所以當兩棵高度相等的BT要合併時，根據root的值來決定誰是合併後的root. 假設已經知道BT(y)的值比BT(z)還要大，所以BT(z)會是合併後的root y的parent指到zy的sibling 指到 z本來的childz的child 指到yz的degree 加一 void mergeTree(Node* y,Node* z){ y-&gt;parent = z; y-&gt;sibling = z-&gt;child; z-&gt;child = y; z-&gt;degree++; } ##Insert## 要插入一個新的元素，就是創見一個新的 Binomial Heap，然後跟原本的Heap執行合併即可 void insert(int value){ BinomialHeap bh; bh.head = new Node(); bh.head-&gt;value = value; MergeHeap(bh); } ##Delete## 要從 BinomialHeap中刪除當前最小元素 先找到最小元素所在的那棵BT把該BT從list裡面拔除把該BT的children給反向排序(degree為key)在跟原本的BT list合併 void deleteMin(){ int min = head-&gt;value; Node* tmp = head; Node* minPre = NULL; Node* minCurr = head; // 找到最小的node位於何處，由於要將該BT給拔除，所以必須要記得該BT前面那棵BT // 如果最小棵的是第一棵，那也要一併更新 head 指標 while(tmp-&gt;sibling){ if(tmp-&gt;sibling-&gt;value &lt; min){ min = tmp-&gt;sibling-&gt;value; minPre = tmp; minCurr = tmp-&gt;sibling; } tmp = tmp-&gt;sibling; } if(!minPre &amp;&amp; minCurr) //最小棵是第一個 head = minCurr-&gt;sibling; else if(minPre &amp;&amp; minCurr) minPre-&gt;sibling = minCurr-&gt;sibling; //H' Make-BINOMIAL-HEAP() Node *pre,*curr; //用三個指標反轉一個 single link list pre = tmp = NULL; curr = minCurr-&gt;child; while(curr){ tmp = curr-&gt;sibling; curr-&gt;sibling = pre; curr-&gt;parent = NULL; pre = curr; curr = tmp; } //創建一棵新的binomial heap,並且讓他的head 指向反轉後的BT list BinomialHeap bh ; bh.head = pre; //再度合併 MergeHeap(bh); } 圖片來自 Binomial WikiIntroduction To Algorithms，Chapter 19 Binomial Heaps","keywords":"","version":"Next"},{"title":"Multipath routing with Group table at mininet","type":0,"sectionRef":"#","url":"/docs/techPost/2014/multipath-routing-with-group-table-at-mininet","content":"","keywords":"","version":"Next"},{"title":"Purpose​","type":1,"pageTitle":"Multipath routing with Group table at mininet","url":"/docs/techPost/2014/multipath-routing-with-group-table-at-mininet#purpose","content":"在Group table中，有一個類型為select，此類型的group會隨機執行底下的其中一個bucket。若我們將所有的output action都放進這個group中，則switch會將封包隨機導向不同的port，藉此達成multipath routing的功用。 ","version":"Next","tagName":"h2"},{"title":"Environment​","type":1,"pageTitle":"Multipath routing with Group table at mininet","url":"/docs/techPost/2014/multipath-routing-with-group-table-at-mininet#environment","content":"使用下列的圖作為我們的網路環境，在此圖中。S1~S5都是支援OpenFlow 1.3的OpenFlow switch，左邊的Host 1則是一個Sender，會對於右邊的九個Host發送資料 ","version":"Next","tagName":"h2"},{"title":"Step​","type":1,"pageTitle":"Multipath routing with Group table at mininet","url":"/docs/techPost/2014/multipath-routing-with-group-table-at-mininet#step","content":"使用mininet搭配其script來創造網路拓墣，該script可以在此找到 group.py mn --custom group.py --topo group  讓所有的創造的openvswitch都支持openflow 1.3 ovs-vsctl set bridge s1 protocols=OpenFlow13 ovs-vsctl set bridge s2 protocols=OpenFlow13 ovs-vsctl set bridge s3 protocols=OpenFlow13 ovs-vsctl set bridge s4 protocols=OpenFlow13 ovs-vsctl set bridge s5 protocols=OpenFlow13  在S1上面加入一個group table，此group table能夠把封包給隨機導向Port 1,2,3。 ovs-ofctl -O OpenFlow13 add-group s1 group_id=5566,type=select,bucket=output:1,bucket=output:2,bucket=output:3  在S1上面加入一個Flow entry，所有從Host1進來的封包，都去執行剛剛所創立的group table。 ovs-ofctl -O OpenFlow13 add-flow s1 in_port=4,actions=group:5566   由於本實驗沒有採用任何Controller，因此要手動的寫入Flow entry到其餘的Switch。在S1上面加入剩下的Flow entry，使得送回Host1的封包能夠順利抵達Host1 ovs-ofctl -O OpenFlow13 add-flow s1 eth_type=0x0800,ip_dst=10.0.0.1,actions=output:4 ovs-ofctl -O OpenFlow13 add-flow s1 eth_type=0x0806,ip_dst=10.0.0.1,actions=output:4  在S2、S3、S4上各加入兩條Flow entry，讓封包能夠通過 ovs-ofctl -O OpenFlow13 add-flow s2 in_port=1,actions=output:2 ovs-ofctl -O OpenFlow13 add-flow s2 in_port=2,actions=output:1 ovs-ofctl -O OpenFlow13 add-flow s3 in_port=1,actions=output:2 ovs-ofctl -O OpenFlow13 add-flow s3 in_port=2,actions=output:1 ovs-ofctl -O OpenFlow13 add-flow s4 in_port=1,actions=output:2 ovs-ofctl -O OpenFlow13 add-flow s4 in_port=2,actions=output:1  在S5上根據destination ip來把封包導向不同的host #IP ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.2,actions=output:4 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.3,actions=output:5 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.4,actions=output:6 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.5,actions=output:7 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.6,actions=output:8 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.7,actions=output:9 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.8,actions=output:10 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.9,actions=output:11 #ARP ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.2,actions=output:4 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.3,actions=output:5 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.4,actions=output:6 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.5,actions=output:7 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.6,actions=output:8 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.7,actions=output:9 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.8,actions=output:10 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.9,actions=output:11  由於本實驗要觀察的是Host1送過來的封包能否走不同路徑，對於送回給Host1的封包就固定於同一條路徑(S5 - S2 - S1) ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0800,ip_dst=10.0.0.1,actions=output:1 ovs-ofctl -O OpenFlow13 add-flow s5 eth_type=0x0806,ip_dst=10.0.0.1,actions=output:1  接下來依序執行下列指令來產生網路流量 mininet&gt; iperfudp 1G h1 h2 mininet&gt; iperfudp 1G h1 h3 mininet&gt; iperfudp 1G h1 h4 mininet&gt; iperfudp 1G h1 h5 mininet&gt; iperfudp 1G h1 h6 mininet&gt; iperfudp 1G h1 h7 mininet&gt; iperfudp 1G h1 h8 mininet&gt; iperfudp 1G h1 h9 mininet&gt; iperfudp 1G h1 h10  接下來觀察每個switch的flow table。結果如圖 mininet&gt; sh ovs-ofctl dump-flows s2 -O OpenFlow13 mininet&gt; sh ovs-ofctl dump-flows s3 -O OpenFlow13 mininet&gt; sh ovs-ofctl dump-flows s4 -O OpenFlow13   在圖中可以觀察到，S2、S3、S4上面都有流量經過，證實了S1使用了group table會將不同的flow給隨機執行不同的buckets，在此範例中則是會導向不同的port。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2014/n-queen-problem","content":"","keywords":"","version":"Next"},{"title":"第一回合​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第一回合","content":"Position: 10000 Left: 00000 (因為1的左對角線就超出邊界了，所以此時的Left就是空的） Right: 01000 ","version":"Next","tagName":"h2"},{"title":"第二回合​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第二回合","content":"Position: 10100 (因為第二個row的皇后是放在第三個col，因此就跟上一回合的Position給結合) Left: 01000 Right: 00110 (之前的Right到了這一個回合，要先往右邊移動一格，變成00100，然後第二回合的皇后所產生的右對角線則是00010，將這兩個集合起來就會變成00110) ","version":"Next","tagName":"h2"},{"title":"第三回合​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第三回合","content":"Position: 10101 Left: 10010 (上一回合的Left到此回合後，就要往左移動一格） Right: 00011 ","version":"Next","tagName":"h2"},{"title":"第四回合​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第四回合","content":"Position: 11101 Left: 10100 Right: 00101 ...以此類推 所以目前已經想好了要如何使用三個整數的bit來表示當前盤面的情況，如果要挑選一個合法的位置，首先將Position|Left|Right給得到一個新的值，這個值中為0的bit就代表是可以用的地方，因此從這些地方來挑選下一個合法位置即可。 以剛剛的範例來說，過的第一回合後， Position: 10000 Left: 00000 Right: 01000 將三個變數取ＯＲ得到新的值（101000），此值代表了第二回合要選取皇后時，第一跟第三個col都不能放皇后，只能放2,4,5,6四個col。 這些表達方式都釐清後，接下來最困難的就是如何自動的選出合法位置，以剛剛的範例來說(101000)，針對此範例，我們知道皇后只能放在2,4,5,6四個col，我希望能夠找出一種方法來依序取出這四個位置，並且在四個位置都去放下皇后試試看。 這邊最直覺的方法就是將該值(101000)不停地/2與%2，如此一來就可以知道每個bit是0還是1，但是這種方法的卻點就是要跑太多次迭代了，我希望能夠找出一個方法，該數值中有多少個0，就迭代多少次，不需要額外的次數來處理。 原本數值:101000 ","version":"Next","tagName":"h2"},{"title":"第一次迭代​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第一次迭代","content":"這次選出了第一個0，（000001) 因此當前的Position就會變成 (101000) | (000001) = 101001 ","version":"Next","tagName":"h2"},{"title":"第二次迭代​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第二次迭代","content":"這次選出了第二個0，（000010) 因此當前的Position就會變成 (101000) | (000010) = 101010 ","version":"Next","tagName":"h2"},{"title":"第三次迭代​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第三次迭代","content":"這次選出了第三個0，（000100) 因此當前的Position就會變成 (101000) | (000100) = 101100 ","version":"Next","tagName":"h2"},{"title":"第四次迭代​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2014/n-queen-problem#第四次迭代","content":"這次選出了第四個0，（010000) 因此當前的Position就會變成 (101000) | (010000) = 111000 除了更新Position，Left以及Right也都要一併更新。簡單來說，我希望依序取出bit中是0的位置。 這邊我使用了二補數的概念來處理，如下。 由於使用的是一個32bit的整數來處理，但是實際上對於8X8盤面只需要8個bit，因此在計算時，必須要注意沒有使用到超過8bit以後得值，這邊我用一個limit (11111111)來作為一個極限的判斷。  currentPos = position | left | right; int newPos; while( currentPos &lt; limit){ newPos = (currentPos+1) &amp; ~currentPos; newPosition = position | newPos; left = limit &amp; ((left | newPos) &lt;&lt;1); right = limit &amp; ((right | newPos) &gt;&gt;1); //dosomething currentPos = currentPos | newPos; }  這邊會將每次選到的位置都放到newPos，並用此變數來更新Position等變數， 最後使用currentPos來記錄還有哪些位置可以選，如此一來就能夠依序的把bit為0的位置取出來。 整個程式的完整程式碼如下，執行時輸入N，此Ｎ代表的是該局為NXN的皇后。 #include&lt;iostream&gt; using namespace std; int limit; int N; int counter; void DFS(int position,int left, int right,int depth){ if(depth == N){ counter++; return; } int currentPos = position | left | right; int newPos; while( currentPos &lt; limit ){ newPos = (currentPos+1) &amp; ~currentPos; DFS( position | newPos, limit &amp; ((left | newPos)&lt;&lt;1), limit&amp;((right | newPos)&gt;&gt;1),depth+1); currentPos = currentPos | newPos; } } int main(){ cin &gt;&gt; N; counter =0; limit = (1&lt;&lt;(N))-1; DFS(0,0,0,0); cout &lt;&lt; counter&lt;&lt;endl; }  ","version":"Next","tagName":"h2"},{"title":"Ports information in Floodlight","type":0,"sectionRef":"#","url":"/docs/techPost/2014/port-s-information-in-floodlight","content":"","keywords":"","version":"Next"},{"title":"Question​","type":1,"pageTitle":"Ports information in Floodlight","url":"/docs/techPost/2014/port-s-information-in-floodlight#question","content":"How can we get the port's information in Floodlight? ","version":"Next","tagName":"h2"},{"title":"Solution​","type":1,"pageTitle":"Ports information in Floodlight","url":"/docs/techPost/2014/port-s-information-in-floodlight#solution","content":"The Floodlight use a ImmutablePort class to represent a switch port and a IOFSwitch class has a Portmanager which will manager all ImmutablePort. The content of ImmutablePort is the same as what it described in Openflow specification 1.0. public class ImmutablePort { private final short portNumber; private final byte[] hardwareAddress; private final String name; private final EnumSet&lt;OFPortConfig&gt; config; private final boolean portStateLinkDown; private final OFPortState stpState; private final EnumSet&lt;OFPortFeatures&gt; currentFeatures; private final EnumSet&lt;OFPortFeatures&gt; advertisedFeatures; private final EnumSet&lt;OFPortFeatures&gt; supportedFeatures; private final EnumSet&lt;OFPortFeatures&gt; peerFeatures; ....  The Portmanger provide some API to allow other object to fetch the ImmutablePort. public ImmutablePort getPort(String name) { if (name == null) { throw new NullPointerException(&quot;Port name must not be null&quot;); } lock.readLock().lock(); try { return portsByName.get(name.toLowerCase()); } finally { lock.readLock().unlock(); } } public ImmutablePort getPort(Short portNumber) { lock.readLock().lock(); try { return portsByNumber.get(portNumber); } finally { lock.readLock().unlock(); } } public List&lt;ImmutablePort&gt; getPorts() { lock.readLock().lock(); try { return portList; } finally { lock.readLock().unlock(); } } public List&lt;ImmutablePort&gt; getEnabledPorts() { lock.readLock().lock(); try { return enabledPortList; } finally { lock.readLock().unlock(); } }  Since the Portmanager is a private member of IOFSwitch, you can't directly use it. You must use the API provied by IOFSwitch to interact with Portmanager.  @Override @JsonIgnore public Collection&lt;ImmutablePort&gt; getEnabledPorts() { return portManager.getEnabledPorts(); } @Override @JsonIgnore public Collection&lt;Short&gt; getEnabledPortNumbers() { return portManager.getEnabledPortNumbers(); } @Override public ImmutablePort getPort(short portNumber) { return portManager.getPort(portNumber); } @Override public ImmutablePort getPort(String portName) { return portManager.getPort(portName); } @Override @JsonProperty(&quot;ports&quot;) public Collection&lt;ImmutablePort&gt; getPorts() { return portManager.getPorts(); }  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Ports information in Floodlight","url":"/docs/techPost/2014/port-s-information-in-floodlight#example","content":"Assume the type of sw is IOFSwitch. Collection&lt;ImmutablePort&gt; swPorts = sw.getPorts(); Iterator&lt;ImmutablePort&gt; it = swPorts.iterator(); while(it.hasNext()){ ImmutablePort port = it.next(); //do something }  ","version":"Next","tagName":"h2"},{"title":"REST API services in Floodlight (Device)","type":0,"sectionRef":"#","url":"/docs/techPost/2014/rest-api-services-in-floodlight","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"REST API services in Floodlight (Device)","url":"/docs/techPost/2014/rest-api-services-in-floodlight#introduction","content":"Device的API主要分成兩類，回傳符合條件的Device或是EntityEntity是一個包含MAC、VLAN、IPV4型態，用來代表網路中最基本的一個元件Device可以包含很多個Entity，每個Device除了會有多個Entity外，還會有所謂的AttachmentPoint。AttachmentPoint會紀錄該Device是與哪個Switch的哪個Port相連一般情況下， Device與Entity是1:1的關係，但是某些情況下，可能一個Device會擁有多個AttachmentPoint、多個IP address或是相同的MAC address擁有不同的VLAN TAG，在此情況下就會有一個Device擁有多個Entity。 ","version":"Next","tagName":"h2"},{"title":"API​","type":1,"pageTitle":"REST API services in Floodlight (Device)","url":"/docs/techPost/2014/rest-api-services-in-floodlight#api","content":"/wm/device/ : 回傳Devices Method: GETParameter: 這邊都是過慮的條件，只有符合這些條件的device才會被選出來。如果什麼參數都沒有，預設就是所有device。 mac : device的mac addressvlan : device的vlan tagipv4 : device的ipv4 addressdpid : 這個device所連接到的switch的dpidport : 這個device是連接到該switch的哪個portmac_startwith : 以下的參數都如同上面的概念，只不過上面的是要完整符合，這邊的是開頭符合就好vlan_startwith :ipv4_startwith :dpid_startwith :port_startwith : Code: DeviceRoutable.javaAbstractDeviceResource.javaDeviceResource.javaDeviceSerializer.java Example: + curl -s http://localhost:8080/wm/device/?ipv4_startwith=10 | python -mjson.tool curl -s http://localhost:8080/wm/device/| python -mjson.toolcurl -s http://localhost:8080/wm/device/?ipv4=10.0.0.2 | python -mjson.tool [ { &quot;attachmentPoint&quot;: [ { &quot;errorStatus&quot;: null, &quot;port&quot;: 2, &quot;switchDPID&quot;: &quot;00:00:00:00:00:00:00:03&quot; } ], &quot;entityClass&quot;: &quot;DefaultEntityClass&quot;, &quot;ipv4&quot;: [ &quot;10.0.0.2&quot; ], &quot;lastSeen&quot;: 1408555008093, &quot;mac&quot;: [ &quot;00:00:00:00:00:02&quot; ], &quot;vlan&quot;: [] } ]  /wm/device/debug : 回傳Entities Method: GETParameter: 沒有參數，就回傳所有的Entity。Code: DeviceRoutable.javaAbstractDeviceResource.javeDeviceEntityResource.java Example: + curl -s http://localhost:8080/wm/device/debug | python -mjson.tool  [ [ { &quot;activeSince&quot;: 1408555008106, &quot;ipv4Address&quot;: null, &quot;lastSeenTimestamp&quot;: 1408555008106, &quot;macAddress&quot;: &quot;00:00:00:00:00:01&quot;, &quot;switchDPID&quot;: &quot;00:00:00:00:00:00:00:02&quot;, &quot;switchPort&quot;: 2, &quot;vlan&quot;: null }, { &quot;activeSince&quot;: 1408555007885, &quot;ipv4Address&quot;: &quot;10.0.0.1&quot;, &quot;lastSeenTimestamp&quot;: 1408555007885, &quot;macAddress&quot;: &quot;00:00:00:00:00:01&quot;, &quot;switchDPID&quot;: &quot;00:00:00:00:00:00:00:02&quot;, &quot;switchPort&quot;: 2, &quot;vlan&quot;: null } ], [ { &quot;activeSince&quot;: 1408555008093, &quot;ipv4Address&quot;: &quot;10.0.0.2&quot;, &quot;lastSeenTimestamp&quot;: 1408555008093, &quot;macAddress&quot;: &quot;00:00:00:00:00:02&quot;, &quot;switchDPID&quot;: &quot;00:00:00:00:00:00:00:03&quot;, &quot;switchPort&quot;: 2, &quot;vlan&quot;: null } ] ]  ","version":"Next","tagName":"h2"},{"title":"Environment","type":0,"sectionRef":"#","url":"/docs/techPost/2014/znc-irc-bouncer","content":"","keywords":"","version":"Next"},{"title":"pkgng​","type":1,"pageTitle":"Environment","url":"/docs/techPost/2014/znc-irc-bouncer#pkgng","content":"pkg install znc ","version":"Next","tagName":"h2"},{"title":"Ports​","type":1,"pageTitle":"Environment","url":"/docs/techPost/2014/znc-irc-bouncer#ports","content":"cd /usr/ports/irc/zncmake configmake install &amp; clean Config znc --makeconf add listen port.add useradd network. ex: freenode add irc server. ex: irc.freenode.netyou can add the default channel passowd by a key=? option in znc.conf Usage ","version":"Next","tagName":"h2"},{"title":"Find a irc client​","type":1,"pageTitle":"Environment","url":"/docs/techPost/2014/znc-irc-bouncer#find-a-irc-client","content":"AndChat on Androidkiwiirc on Web ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2015/openvswitch-bonding","content":"","keywords":"ovs bonding","version":"Next"},{"title":"Active-backup​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#active-backup","content":"這種 mode 的用途主要在於穩定，平常只會使用 bonding 中的其中一條 link 進行傳輸，當該 link down 時，會馬上切換到其他 link 繼續傳輸。本質上沒有辦法提升 throughput。 ","version":"Next","tagName":"h2"},{"title":"Balance-slb​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#balance-slb","content":"這種 mode 的 hash 方式是根據封包的 source MAC + vlan tag來處理，可以參考此篇文章有更詳細的說明 ","version":"Next","tagName":"h2"},{"title":"Balance-tcp​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#balance-tcp","content":"這種 mode 的 hash 是根據封包的 L2/L3/L4 header 來處理的，所以每條 connection 可能會走不同的 link 出去，但是相同 connection 則會一直固定以避免發生 out of order 之類的事情。 註: 如果使用 linux 本身的 round-robin bonding 則可以讓一條 connction 走不同的 link，兩條 1G 的link大概可以衝到 1.5G左右 對於 Balance-slb 以及 Balance-tcp 來說，這邊還能夠再增加是否要開啟 LACP (802.3ad) 的設定。 當開啟 LACP 後，會使用 balance-slb 或是 balance-tcp 的 hash method 當作其分配封包的方式。 唯一要注意的是 balance-tcp 一定要搭配 LACP 才可以使用。 ","version":"Next","tagName":"h2"},{"title":"Commands​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#commands","content":"","version":"Next","tagName":"h2"},{"title":"創造 bonding​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#創造-bonding","content":"ovs-vsctl add-br my_testovs-vsctl add-bond my_test bond0 eth0 eth1 eth2 此指令會在 my_test 此 bridge 上面創造一個 bonding interface bond0，此 bonding interface 會將 eth0, eth1, eth2 給綁起來 ","version":"Next","tagName":"h3"},{"title":"改變 bonding mode​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#改變-bonding-mode","content":"預設的 bonding mode 是 active-back，可以再創造的時候設定或是之後再改變 ovs-vsctl add-bond my_test bond0 eth0 eth1 eth2 bond_mode=balance-slbovs-vsctl set port my_test bond_mode=balance-slb ","version":"Next","tagName":"h3"},{"title":"看 bonding 相關資訊​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#看-bonding-相關資訊","content":"ovs-appctl bond/show bond0ovs-appctl bond/list bond0ovs-appctl bond/hash bond0 (可以看 hash 對應的 slave interface)ovs-appctl bond/migrate (能夠將某 hash 從某slave 搬移到別的slave) ","version":"Next","tagName":"h3"},{"title":"Testing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#testing","content":"","version":"Next","tagName":"h2"},{"title":"測試配備如下​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#測試配備如下","content":"HP ProCurve Switch 2824 (J4903A) 針對 LACP 的實驗，必須要在這邊開啟 LACP Linux PC *1Windows PC *2 ","version":"Next","tagName":"h3"},{"title":"測試拓樸一​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#測試拓樸一","content":" linux PC 上面安裝 OpenvSwitch，並且與 HP Switch 以兩個 1G 的 port 進行 bonding。兩台 Windows PC 都連接在 HP SwitchLinux PC 與 Windows PC 以 iperf 作為產生流量的工具TX 測試 linux PC 跑 iperf client (-P4) 分別打到兩台 windows PCwindows PC 分別跑 iperf server RX 測試 linux PC 跑 iperf serverwindows PC 分別跑 iperf client，分別用 iperf -P4 去連接 linux PC 實驗數據 (TX、RX是分開跑) 數據分析方式請看最後面 分析 Active-backup 就只有用一條link傳輸，沒有辦法達到 speed up 的效果Balance-slb without lacp 因為我們的 source mac 都是 linuxPC 本身，所以也只會用一條 link 來傳輸，本身沒有任何幫助Balance-slb with lacp 因為有打開 LACP 的功能，所以從 switch 回來的封包會分兩個 link 去送，所以 RX 可以看到有明顯的上升，大概1.9G左右Balance-tcp with lacp 因為是根據 L2/L3/L4 來進行 hash，所以同一個 Host 發出的不同 connection 可以分散在不同 link上，所以 TX 的速度也有明顯上升 ","version":"Next","tagName":"h3"},{"title":"測試拓樸二​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#測試拓樸二","content":" linux PC 上面安裝 OpenvSwitch，並且與 HP Switch 以兩個 1G 的 port 進行 bonding。linux PC 上面設定兩個獨立的 network namespace，並且把此兩個 NS 的給掛到 OpenvSwitch 上面兩台 Windows PC 都連接在 HP SwitchLinux PC 上的 NS 與 Windows PC 以 iperf 作為產生流量的工具TX 測試 NS 分別跑 iperf client (-P4)windows PC 分別跑 iperf server一個 NS 對應一個 Windows PC RX 測試 windows PC 分別跑 iperf client (-P4)NS 分別跑 iperf server一個 NS 對應一個 Windows PC 實驗數據 (TX、RX是分開跑)數據分析方式請看最後面分析 Balance-slb without lacp 因為我們的 source mac 是兩台不同的 NS ，所以有機會兩台 NS 的 MAC會被 hash 到不同的 link，所以 TX 的速度也有明顯上升Balance-slb with lacp TX 方面理由如上， RX是因為 LACP 而加速Balance-tcp with lacp 理由如同實驗一 ","version":"Next","tagName":"h3"},{"title":"個人心得​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2015/openvswitch-bonding#個人心得","content":"只有一台 host 本身的話，其實跑 OVS 的bonding沒有太大效果，除非你外面有支援 LACP的switch 可以用，不然就直接用 linux 原先的 XOR 之類的hash就好若是在多 VM 的環境下，這時候有 balance-slb 與 balance-tcp 可以考慮(假設想要 speed up)，這兩個主要考慮的點在於 使用 balance-slb 的話，會讓同一個 VM 的所有流量都走同一個 interface 出去，所以若當前其他 VM 都閒置的情況下，該 VM 還是只能用到一條 link 的資源。 若採用 balance-tcp 的話，則會依照 connction 來分，所以不論何種情況都能夠盡量使用每條 link 的資源 ","version":"Next","tagName":"h2"},{"title":"Build Mozilla NSS on windows","type":0,"sectionRef":"#","url":"/docs/techPost/2016/build_mozilla_nss_on_windows","content":"Build Mozilla NSS on windows 最近因為在處理 firefox 的憑證問題，所以要藉由 mozilla 自己的 certutil.exe來進行憑證的操作， 由於Mozilla現在已經不在官網提供執行檔，必須要自己手動下載來Build，因此就到官網來查詢安裝步驟了 #Fetch Source Code 由於NSS會需要使用到NSPR內的一些header files,所以在建置ＮＳＳ的時候也必須要將ＮＳＰＲ給一併抓下來 官方推薦可以使用下列的方式獲取最新的 source code hg clone https://hg.mozilla.org/projects/nspr hg clone https://hg.mozilla.org/projects/nss 若要抓取特定的 release 版本，可以到這裡進行下載。 #Build Environment 由於是在 Windows 上面建置，所以必須要先安裝好對應的安裝環境，可以參考此篇教學將整個 Mozilla Build給建置完畢。 #Build 基本上按照這篇文章的步驟就可以開始Build Code了，所有的變數都是環境變數，如OS_TARGET等 #Trouble Shooting 由於我的 Windows 是中文版的，在建置的過程中會因為踩到Warning C4819的問題，有嘗試使用過chcp的方式將 code page 給換掉也沒有用，後來是參考這篇文章，將Windows系統的 Code Page 切換到英文或是其他 SBCS Code Page即可","keywords":"","version":"Next"},{"title":"Mininet with different network subnet (v2)","type":0,"sectionRef":"#","url":"/docs/techPost/2014/mininet-with-different-network-subnet-v2","content":"","keywords":"","version":"Next"},{"title":"Solution​","type":1,"pageTitle":"Mininet with different network subnet (v2)","url":"/docs/techPost/2014/mininet-with-different-network-subnet-v2#solution","content":"在本篇中，我們直接撰寫mininet的python script來模擬網路，基本的撰寫教學請參考mininet官方文件就有了。本篇主要是針對不同subnet的host要如何創建。 首先，在我們創造hosts的時候，可以透過ip這個參數來控制此host的預設ip位置，這時候我們就可以設定10.0.0.0/24或是20.0.0.0/24等ip給予欲創建的host，這樣就可以省掉之前的ifconfig的步驟。 接下來，我們要處理Default gateway的問題，這邊也有defaultRoute的參數可以使用，這邊我們就可以輸入defaultRoute='h1-eth0'來處理，這樣就可以省掉之前所輸入的route add default gw的步驟。 這兩個參數都正確填寫完畢後，我們就創立好了不同subnet的網路，並且基本的設定已經完成了，接下來就按照上一篇的說明來將flow entry給寫入switch即可。 ###完整的mininet python script #!/usr/bin/python from mininet.net import Mininet from mininet.node import Controller, RemoteController, OVSController from mininet.node import CPULimitedHost, Host, Node from mininet.node import OVSKernelSwitch, UserSwitch from mininet.node import IVSSwitch from mininet.cli import CLI from mininet.log import setLogLevel, info from mininet.link import TCLink, Intf def myNetwork(): net = Mininet( topo=None, build=False, ipBase='10.0.0.0/8') info( '*** Adding controller\\n' ) c0=net.addController(name='c0', controller=RemoteController, ip='127.0.0.1') info( '*** Add switches\\n') s1 = net.addSwitch('s1', cls=OVSKernelSwitch) info( '*** Add hosts\\n') h1 = net.addHost('h1', cls=Host, mac='00:00:00:00:00:01', ip='10.0.0.1/24', defaultRoute='h1-eth0') h2 = net.addHost('h2', cls=Host, mac='00:00:00:00:00:02', ip='20.0.0.1/24', defaultRoute='h2-eth0') h3 = net.addHost('h3', cls=Host, mac='00:00:00:00:00:03', ip='30.0.0.1/24', defaultRoute='h3-eth0') info( '*** Add links\\n') linkBW = {'bw':100} net.addLink(h1, s1, cls=TCLink , **linkBW) net.addLink(h2, s1, cls=TCLink , **linkBW) net.addLink(h3, s1, cls=TCLink , **linkBW) info( '*** Starting network\\n') net.build() info( '*** Starting controllers\\n') for controller in net.controllers: controller.start() info( '*** Starting switches\\n') net.get('s1').start([c0]) info( '*** Configuring switches\\n') CLI(net) net.stop() if __name__ == '__main__': setLogLevel( 'info' ) myNetwork()  ###測試用的flow entries #Those two flow will handle the arp-request for the gateway, it will send the arp-request to s1 table=0,priority=65535,arp,arp_tpa=10.0.0.254 actions=LOCAL table=0,priority=65535,arp,arp_tpa=20.0.0.254 actions=LOCAL table=0,priority=65535,arp,arp_tpa=30.0.0.254 actions=LOCAL table=0,priority=1,arp,nw_dst=10.0.0.1,actions=output:1 table=0,priority=1,arp,nw_dst=20.0.0.1,actions=output:2 table=0,priority=1,arp,nw_dst=30.0.0.1,actions=output:3 table=0,priority=0,actions=resubmit(,1) #table1 - forward/route table=1,icmp,nw_dst=10.0.0.1,actions=mod_dl_dst=00:00:00:00:00:01,output:1 table=1,icmp,nw_dst=20.0.0.1,actions=mod_dl_dst=00:00:00:00:00:02,output:2 table=1,icmp,nw_dst=30.0.0.1,actions=mod_dl_dst=00:00:00:00:00:03,output:3  ","version":"Next","tagName":"h2"},{"title":"NFQUEUE drop UDP packets","type":0,"sectionRef":"#","url":"/docs/techPost/2016/netfilter-nfqueue","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#introduction","content":"此篇文章用來記錄最近遇到的一個問題，在一個執行 NFQUEUE 的系統上，當一個尚未被 conntrack 紀錄的連續 UDP 封包經過系統且都經過 NFQUEUE 處理後，第二個 UDP 封包都會遺失的問題。 ","version":"Next","tagName":"h2"},{"title":"NFQUEUE​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#nfqueue","content":"一種 Queue，由 netfilter (ipables) 所提供的一種 target，能夠將封包內容藉由 netlink/nmap 送到 user-space 去，大部分的 IPS/IDS 都會藉由此方式來分析封包，如 suricata。User space 有對應的 library 可以用來接收此封包，參考此link相關的 tutorial 可參考 ","version":"Next","tagName":"h2"},{"title":"系統資訊​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#系統資訊","content":"Linux kernel 3.6 ","version":"Next","tagName":"h2"},{"title":"Problem​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#problem","content":"系統在 filter chain 的 forward table 中加入一條 iptables 將封包導向 NFQUEUE。User space 可參考此project，使用一個最簡易的 sample，將封包收到後就送回 kernel系統運行 NAT當有連續 UDP 封包經由系統往外送出時，可觀察到第二個 UDP (可能更多)都會遺失 此連續 UDP 封包必須還沒有被 kernel 的 conntrack 給紀錄 若系統沒有運行 NFQUEU ,則此問題不存在 ","version":"Next","tagName":"h2"},{"title":"問題觀察​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#問題觀察","content":"","version":"Next","tagName":"h2"},{"title":"若沒有運行 NFQUEUE，為什麼封包正常​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#若沒有運行-nfqueue為什麼封包正常","content":"請參考此流程圖 第一個封包進入到系統時，於1處時，會判斷該封包是第一次建立 connection，所以是 unconfirm 的狀態，於3的狀態時，會去將該 connection 給 confirm，並且將該資訊給存入 kernel 的 hash 之中。 第二個封包進入時，於1處時去判斷，就會知道該 connection 已經建立了，所以就不會進入到3，後續封包都按照此流程傳送。 ","version":"Next","tagName":"h2"},{"title":"若運行 NFQUEUE，為什麼封包會丟棄​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#若運行-nfqueue為什麼封包會丟棄","content":"請參考此流程圖 第一個封包進入到系統時，於1處時，會判斷該封包是第一次建立 connection，所以是 unconfirm 的狀態，接下來就透過 netlink 要送到 user space 去。 第二個封包進入到系統時，由於第一個封包還沒有被 kernel 內的 nf_conntrack_confirm 處理完畢(可能封包還在 user space)，所以於1處時，也會判斷封包是第一次建立 connection，是 unconfirm 的狀態。 當 user space 將封包打回 kernel 後，會於3/4開始處理，會從先前的 queue 將 skb 所記錄 conntrack 的資訊給取出，所以這兩個封包都會認為自己是 unconfirm 的狀態 第一個封包接下來會走完全部的路途，並且送出去 第二個封包當走到 nf_conntrack_confirm 時，會因為覺得自己是 unconfirm 的，所以呼叫 __nf_conntrack_confirm 去處理。 code: ref 當第二個封包跑到 __nf_conntrack_confirm 時，會嘗試將自己的 conntrack 給加入到 kernel hash中，但是第一個封包已經加入過了，所以 kernel 會覺得你有病，就將該封包給丟棄了。 ","version":"Next","tagName":"h2"},{"title":"結論​","type":1,"pageTitle":"NFQUEUE drop UDP packets","url":"/docs/techPost/2016/netfilter-nfqueue#結論","content":"此問題發生的根本在於 conntrack 的衝突，當 conntrack 的結果已經被 kernel 紀錄的情況下，有第二個封包嘗試將 conntrack 再次存到 kernel 中，此封包就會被丟棄 code: ref看過 linux 4.4 的程式碼，在 netlink 接收也沒有針對收到的封包去重新處理 conntrack 的問題，我想是 UDP 掉封包是合理的，而 TCP 會自己重傳，所以上層的應用程式不會有感覺。 ","version":"Next","tagName":"h2"},{"title":"Sublime Text 3 + cscope (windows)","type":0,"sectionRef":"#","url":"/docs/techPost/2014/sublime-text-3-cscope","content":"","keywords":"","version":"Next"},{"title":"Install Package Control​","type":1,"pageTitle":"Sublime Text 3 + cscope (windows)","url":"/docs/techPost/2014/sublime-text-3-cscope#install-package-control","content":"按下 CTRL+`貼上 import urllib.request,os,hashlib; h = '7183a2d3e96f11eeadd761d777e62404e330c659d4bb41d3bdf022e94cab3cd0'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://sublime.wbond.net/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)  Refer to installation_Package_Control ","version":"Next","tagName":"h2"},{"title":"Install cscope​","type":1,"pageTitle":"Sublime Text 3 + cscope (windows)","url":"/docs/techPost/2014/sublime-text-3-cscope#install-cscope","content":"按下 CTRL+SHIFT+P先輸入 INSTALL PACKAGE ，之後再輸入 cscope，即可安裝完成產生 cscope.out F:\\ovs&gt;cscope.exe -Rbv  Refer to cscope plugin source ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"Sublime Text 3 + cscope (windows)","url":"/docs/techPost/2014/sublime-text-3-cscope#usage","content":"按下 **ctrl+**後，根據需求來使用直接使用 鍵盤操作Ctrl/Super + \\ - Show Cscope optionsCtrl/Super + LCtrl/Super + S - Look up symbol under cursorCtrl/Super + LCtrl/Super + D - Look up definition under cursorCtrl/Super + LCtrl/Super + E - Look up functions called by the function under the cursorCtrl/Super + LCtrl/Super + R - Look up functions calling the function under the cursorCtrl/Super + Shift + [ - Jump backCtrl/Super + Shift + ] - Jump forward ","version":"Next","tagName":"h2"},{"title":"mTCP 讀後筆記","type":0,"sectionRef":"#","url":"/docs/techPost/2016/mtcp-reading-note","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"mTCP 讀後筆記","url":"/docs/techPost/2016/mtcp-reading-note#introduction","content":"當前的 TCP connection 中，大部分的封包都是小size的，如何快速的處理這些封包是個提升效能的重點根據這篇論文，在特定的網路環境下(large cellular network)，超過 90% 的 TCP 封包都小於 32KB，超過 50% 的則是小於 4KB。當前的 Linux Kenrl 的架構使得處理小封包的速度沒有很好的表現。目前世界上有很多種方案嘗試解決此問題。 修改 kernel 的，如 MegaPipe, FlexSC在 user-space 提供高速的 Packet I/O，這部分通常是直接跟網卡操作，跳過 kernel。如 netmap, DPDK, PSIO。 上述的方案都難以應用到現有的系統 改 kernel code 對於已經應用的伺服器來說，不是那麼方便user-space library 的缺點 沒有實作 TCP stack，所以使用者都要自己想辦法去處理整個data (每個 Layer 自行處理）沒有提供統一的介面，現存的應用程式很難 porting 本篇 paper 提出的一個新的架構 mTCP，宗旨就是解決上述所有問題 不修改 kernel, 實作於 user-space讓現有的應用程式可以容易使用，快速轉換 提供良好的 wrapper 給當前的 BSD-socket API，同時也提供 event 相關的(epoll)實作 TCP stack架構上要解決當前 kernel 的架構問提，提升整體的處理速度 ","version":"Next","tagName":"h2"},{"title":"Current Limitation​","type":1,"pageTitle":"mTCP 讀後筆記","url":"/docs/techPost/2016/mtcp-reading-note#current-limitation","content":"作者在文章中提及當前 linux kernel 的四個問題，這些問題導致當前的 TCP stack 沒有辦法很有效率的處理封包。 ####Lack of connection locality 有不少應用程式會使用 Multi-Thread，這些 threads 會一起共享一個 listen socket's accept queue. 這種情況下這些 thread 彼此間要透過 lock 來搶奪該 socket 的使用權，這邊會使得thread的效率下降 Kernel 對於 connection locality的不 support 會因為 CPU 的cache miss產生額外的負擔 ####Shared file descriptor space 對於 POSIX-compliant 的 OS 來說， 對於每個 process 來說，其 fd 是共享的，舉例來說每次在創建新的 fd 時，都要去尋找當前最小可用的數字來使用。 對於一個處理大量連線的忙碌 server 來說，每個 thread 在建立 socket 的時候，就會因為 lock 間的爭奪而產生一個額外的負擔。 對 socket 也使用 fd 來進行操作，也會對 linux kernel 內的 VFS 造成額外的負擔。（這邊我看不太懂） ####Inefficient per-packet processing 先前的研究指出，龐大的封包結構(sk_buff),每個封包的記憶體處理以及ＤＭＡ這些行為是小封包處理效率不佳的主因。 ####System call overhead 對於短週期的連線來說，BSD socket API 需要頻繁地在 user/kernel space做切換，根據 FlexSC 和 VOS的研究指出，大量的system call會對 cpu的處理狀態造成混亂(top-level caches, branch prediction table, etc)，因此效能會下降。 ","version":"Next","tagName":"h2"},{"title":"Current Solution​","type":1,"pageTitle":"mTCP 讀後筆記","url":"/docs/techPost/2016/mtcp-reading-note#current-solution","content":"####Lack of connection locality Affinity-AcceptMegaPipeLinux kernel's socket option SO_REUSEPORT (after 3.9.4) ####Shared file descriptor space MegaPipe ####Inefficient per-packet processing User level packet I/O libray Intel DPDKLibzero for DNAnetmap ####System call overhead FlexSCVOS ","version":"Next","tagName":"h2"},{"title":"Why User-level TCP​","type":1,"pageTitle":"mTCP 讀後筆記","url":"/docs/techPost/2016/mtcp-reading-note#why-user-level-tcp","content":"可擺脫和 kernel 的糾纏 當前 kernel 中的架構，因為 fd 的共用，TCP stack 很難從 kernel 中獨立抽出來。 可直接套用當前一些高效率的 packet I/O library，如 netmap, DPDK,etc.在不修改 kernel 的情況下，可以批次的處理封包。能夠輕易的支援現存的 application。 mTCP 提供了類似 BSD-like 的 socket API. ","version":"Next","tagName":"h2"},{"title":"Design​","type":1,"pageTitle":"mTCP 讀後筆記","url":"/docs/techPost/2016/mtcp-reading-note#design","content":"####Introduction mTCP 希望在向下支援當前的 multi-threaded, event-driven 應用程式的前提下，提供在多核心系統下有高擴展性的系統mTCP 必須要提供 BSD-like 的 socket API 以及 event-driven API，能夠讓現存的應用程式簡單的轉換過去mTCP 由兩大物件組成，分別是 User-level TCP stack 以及 Packet I/O library. ####Implementation 對於每一個應用程式來說，mTCP於每個 CPU Core 上運行一個 thread. mTCP 會透過其 Packet I/O library直接對 NIC 處理封包的收送 由於 mTCP 這部分是依賴現存的解決方案，而當前所有的 Packet I/O library 都有一個限制，就是每個 NIC 上面只能運行一個 application。這限制作者相信未來會被解決的。 ","version":"Next","tagName":"h2"},{"title":"User-level Packet I/O Library​","type":1,"pageTitle":"mTCP 讀後筆記","url":"/docs/techPost/2016/mtcp-reading-note#user-level-packet-io-library","content":"Introduction​  當前有很多高速的 (100M packets/seconds) packet I/O system 都不適合來實作 Transport-layer stack (such TCP),因為這些 system 的底層都是採用 polliing 的方ㄕ取處理封包，採用 pollung 的方式會浪費 cpu cycle。 此外，mTCP 希望能夠提供在多網卡狀況下，能夠高效率處理 TX/RX queues 的多工能力。舉例來說，假如系統當前正在等待 control packet 的到來，若此時因為要去 polling RX 封包，就會導致 TX 沒有辦法順利的將封包送出，若 TX 想要送出的是如 ACK/SYN 之類的封包，可能就會觸發 TCP 的重傳機制導致整體速度下降。 為了解決這個問題， mTCP 這邊採用了 PacketShader I/O engine (PSIO) 來提供高效率的 event-driven packet I/O interface. PSIO 使用 RSS 這個技術來達到 flow-level 的封包分配技術，讓每條 connection 的封包都能夠維持在同一個 RX queue中。藉此降低不同 CPU競爭封包的負擔。  Implementation​ 提供了與 select 類似的 ps_select。 此 API 會去監聽有興趣網卡的TX/RX queue事件，與 netmap 提供的 selece/poll 類似 ","version":"Next","tagName":"h2"},{"title":"User-level TCP Stack​","type":1,"pageTitle":"mTCP 讀後筆記","url":"/docs/techPost/2016/mtcp-reading-note#user-level-tcp-stack","content":"Introduction​ 為了減少大量 system call 對於 kernel 造成的負擔，必須要將kernel內關於TCP的操作都搬移到 user-space來操作在 mTCP 中，採用一個名為 zero-thread TCP來提供此功能，主要的應用程式可以透過簡單的 function call而不是system call來達到一樣的功能，同時有更好的效能。上述設計的唯一限制就是內部 TCP 處理的正確性會依賴於該 application 是如何去呼叫 TCP 相關的 function( timely invocation)在 mTCP 中，採用不同的 thread 來處理上述的問題，應用程式的 thread 跟 mTCP 的 thread 中間是透過一個 share buffer 來交換資料，而 application 只能使用 mTCP 提供的 function 來操作 share buffer。 藉由這種方式，可以確保 TCP 的 data在共用上是安全且正確無誤的. 當應用程式想要修改 share buffer 時，會發送一個 write request 到所謂的 job queue內，接者 mTCP 內的 thread當搶到 CPU 後，會去把 job queue 內的工作取出，然後執行對應的指令。然而，上述的設計其實會因為共用的資料跟mTCP與application的切換而產生額外的負擔，這些負擔反而比傳統的system call還來得龐大接下來的章節，會講述 mTCP 最後如何實作並且克服上述的問題 Implementation​ Basic TCP Processing​   - mTCP Thtread 從網卡的 RX queue 讀取批次資料(batch)後，直接傳給內部的 TCP 邏輯處理。 - 對於每一個封包來說，首先會先搜尋(或創造)對應的 TCP control block(TCB)，此 TCB 會存放於 flow hash table。 以上圖為例，當 server 收到一個對應於其 SYN/ACK 的ACK時，新的 connection 就會被建立，此時會將對應的 TCB 給放到 **accept queue**，同時會在產生一個 **read event** 給對應的 **listen socket**。 在連線建立後，當資料封包到達 mTCP時，mTCP 會將封包的內容給複製一份到 socket 的 **read buffer**，同時也產生一份 **read event** 給對應的 socket，這樣 application 那邊就可以用 **read** 的函式讀取到封包的內容。 - 當 mTCP 處理好所有接收到的封包後，會將所有在 **queue **內的 **event **都推到 application 上的 **queue** 去，同時透過 **signal** 的方式叫起該 application 來處理封包。 - 對於 application 來說，接下來封包的 write 處理都不會產生 content switch，而是會透過 mTCP 的架構將所有要往外送的封包都寫入一個 send buffer 同時也將對應的 tcb 放到 **write queue**內。接下來 mTCP 會收集所有要往外送的 tcb，然後統一放入一個 **send list**中，最後批次的將這些封包直接送到網卡的 **TX queue** 處理。 Lock-free, Per-core Data Structures​ 為了減少mTCP threads之間的 CPU 競爭， mTCP 將所有資源(flow pool, socket buffers,etc.)每個 CPU core都放置一份，此外，還可以透過 RSS的技術來達到 flow-level 的CPU affinity。 此外，mTCP在 application 與 mTCP 之間使用了 lock-free 的資料結構，同時也提供了一種更有效率的方式來管理 TCP timer相關的操作。 Thread mapping and flow-level core affinityFlow-level core affinity 總共分成兩個階段執行 packet I/O 這層要確保在當前可用且搭配 RSS 的 CPU 上去分配 TCP connection，透過此機制可以處理每個 core 上面的 TCP 規模問題。mTCP 對於每個 application thread都會產生一個 thread，並且讓這兩個 thread 都處於同一個 physical CPU core上，這樣可以確保packet與flow的在處理上能夠享有 core affinity。 Multi-core and cache-friendly data structures 下列常用的資料結構都會存放在每個 TCP Thread 中保有獨立一份 Flow Hash Table Socket Id MangerPool of TCBsocket buffers. 藉由上述資料的安排，能夠大幅減少跨 threads/CPU cores 之間的資料存取，同時提供良好的平行性。假如今天有一個資料必須要跨 Thread 存取(譬如 mTCP thread 跟 application thread)，首先會先將所有的資料結構對每一個CPU都放一份，然後使用single-producer/single-consumer來達到lock-free data structure的存取 從 application 到 mTCP 來看， mTCP 維護 write, connect close 的 queues，反過來則是維護一個accept的 queue。為了能夠更加利用 CPU cacue 機制，mTCP也會記住目前比較常用的資料結構大小並使其符合CPU cache的機制，然後讓其大小對齊於CPU的 cache line 大小 舉例來說，TCB會被分成兩個部分，第一個部分是 64 bytes，存放了最常使用到的欄位以及兩個指標指向剩下比較少用到的部分，分別是128以及192 bytes 5.為了將記憶體要求與釋放造成的負擔最小化，mTCP會在每個core都去要求記憶體來存放TCB與socket buffers， 此外，由於TCB存取模式很隨機，為了降低TCB在 TLM miss 的機率，於是使用了大量的 page，並且將 tcb 與 hash table index 相關的資訊都放入pages中。 Efficient TCP timter management: 在 TCP 的運作過程中，有三個地方需要有 timrer 的處理 重傳的 timeout connection 在TIME_WAIT狀態時的等待connection keep-alive 的檢查 mTCP 提供了兩種模式的 timers，一種是以排序的list來管理，另外一種則是以hash table來管理。 對於coarse grain timers來說(如TIME_WAIT, keep-alive check)，mTCP使用一個 list來記住所有tcb，並且依據其timeout的值來進行排序。(要維護這個 sorting list是簡單的事情，因為每次要被加進來的新TCB，其timeout一定是比當前list內的還要大) mTCP每一秒都會進行確認，檢查該 list 內是否有 tcb 已經過時需要被處理了。 對於fine-grained retransmission timers來說，mTCP使用了 hash table 來找查 tcb，而使用的 key 則是當前剩下的時間(使用milliseconds為單位)。當一個 hash bucket的時間到達時，就會一口氣將bucket內所有的 tcbs一起進行處理。 Batched Event Handling​   - mTCP 藉由batch的方式一口氣處理多個 flow event，藉此可以降低大量 event 造成的 content switch。 - 當 mTCP 收到封包時，會自己產生一個 flow-level event，最後會統一將該 event 通知到 application。如上圖所示 若 application 要送封包時，會把所有的 write event 放到 write queue，之後 mTCP 會從 queue 內將 jobs取出，然後一口氣送到 NIC 的 TX queue去處理。 - 這部分的並不是獨創的想法，目前**MegaPipe**,**VOS**都有實作這功能 Optimizing for Short-lived Connections​ mTCP 採用了兩個方式來最佳化小封包的傳輸，分別是Priority-based packet queueing 以及Lightweight connection setup。 Priority-based packet queueing​ 對於TCP連線來說，控制封包(SYN/ACK)不但是個小封包的傳輸，也對整個傳輸速率扮演很重要的角色，因此SYN/ACK能夠愈早送到對方是愈好的。 然後當系統中有大量的資料封包要傳輸時，這些控制封包可能就會因為要競爭 TX queue 而提高了 queueing dealy。 mTCP為了解決這個問題，決定導入Priority的概念來處理封包，針對這些控制封包給予更高的優先權，能夠盡早的往外送， 外了達成這個概念，在TX部分實作了三種list，每種list分別存放不同種類的封包，分別是 Control, Ack, Data 三種。 當要把封包推向TX queue的時候，會依此順序將三個 list 的封包從TX queue 送出，藉此避免這些重要封包會有過大的queueing delay。 Lightweight connection setup​ 根據研究發現，在建立起整個 TCP connection 過程中，有很大一部份的負擔都是在於要配置記憶體給TCB以及Socket Buffer。 當同時有多個thread呼叫malloc,free時，kernel內的記憶體管理者會很忙碌地來服務每個thread的請求。 為了解決這個問題，mTCP會事先從kernel配置一個很大的記憶體池，當有任何Thread想要配置記憶體時，就可以直接從該記憶體池中去存取， ","version":"Next","tagName":"h2"},{"title":"USACO 1.2","type":0,"sectionRef":"#","url":"/docs/techPost/2014/usaco-12","content":"","keywords":"","version":"Next"},{"title":"Brief​","type":1,"pageTitle":"USACO 1.2","url":"/docs/techPost/2014/usaco-12#brief","content":"本章節的題目也是沒有什麼特定演算法，有些可以使用DP來處理，有些則是根據題目敘述來解即可 ","version":"Next","tagName":"h2"},{"title":"Milking Cows​","type":1,"pageTitle":"USACO 1.2","url":"/docs/techPost/2014/usaco-12#milking-cows","content":"此提使用DP來解，思考過程如下 將所有farmers的工作時間依據其起點由早到晚排序  for(int i=0;i&lt;n;i++){ farmers* f = new farmers(); fin &gt;&gt; f-&gt;low &gt;&gt; f-&gt;high; data.push_back(f); } sort(data.begin(),data.end(),compare);  掃過所有farmers的工作時間，判斷當前的farmers是否有跟上一個farmers的時間有重疊，如果有重疊，就更新當前紀錄的工作時間，如果沒有重疊，就代表此時需要閒置，因此就要更新當前閒置時間，每次更新的同時，也順便紀錄最大值  currLow = data[0]-&gt;low; currHigh= data[0]-&gt;high; maxMilked = currHigh - currLow; maxNotMilked = 0; for(int i=1;i&lt;data.size();++i){ if(data[i]-&gt;low &gt; currHigh){ //沒有重疊 maxNotMilked = max(maxNotMilked,data[i]-&gt;low - currHigh); currLow = data[i]-&gt;low; currHigh = data[i]-&gt;high; maxMilked = max(maxMilked,currHigh-currLow); } else if (data[i]-&gt;high &gt; currHigh){ //有重疊 currHigh = data[i]-&gt;high; maxMilked = max(maxMilked,(currHigh - currLow)); } }  全部掃過一遍及可找到答案 ","version":"Next","tagName":"h2"},{"title":"Transformations​","type":1,"pageTitle":"USACO 1.2","url":"/docs/techPost/2014/usaco-12#transformations","content":"此題不難，只要想好如何將一個矩陣給順時針旋轉90即可 void turn90(char* data,int n){ char *tmp = new char[n*n]; for(int i=0;i&lt;n;i++){ for(int j=0;j&lt;n;j++){ tmp[j*n+(n-i-1)] = data[i*n + j]; } } memcpy(data,tmp,n*n); delete tmp; return ; }  根據題目所規定的順序，每種都去嘗試，若符合就印出答案，不合就繼續嘗試下一種即可。  //#1 turn90(src,n); if( 0 == memcmp(src,dst,n*n)){ fout &lt;&lt;&quot;1&quot;&lt;&lt;endl; return 0; } //#2 turn90(src,n); if( 0 == memcmp(src,dst,n*n)){ fout &lt;&lt;&quot;2&quot;&lt;&lt;endl; return 0; } //#3 turn90(src,n); if( 0 == memcmp(src,dst,n*n)){ fout &lt;&lt;&quot;3&quot;&lt;&lt;endl; return 0; } //#4 turn90(src,n); reflect(src,n); if( 0 == memcmp(src,dst,n*n)){ fout &lt;&lt;&quot;4&quot;&lt;&lt;endl; return 0; } //#5 for(int i=0;i&lt;3;i++){ turn90(src,n); if( 0 == memcmp(src,dst,n*n)){ fout &lt;&lt;5&lt;&lt;endl; return 0; } } //#6 turn90(src,n); reflect(src,n); if( 0 == memcmp(src,dst,n*n)){ fout &lt;&lt;&quot;6&quot;&lt;&lt;endl; return 0; } //#7 fout&lt;&lt;&quot;7&quot;&lt;&lt;endl;  ","version":"Next","tagName":"h2"},{"title":"Name That Number​","type":1,"pageTitle":"USACO 1.2","url":"/docs/techPost/2014/usaco-12#name-that-number","content":"一開始先讀取dict.txt，只將長度符合題目要求的單字給存起來 while(din &gt;&gt; tmp){ if(tmp.size() == input.size()) data.push_back(tmp); }  接下來依據位數來一個一個檢查。先針對單字內所有的第一位進行檢查是否符合規則，若不合就將其從字典內刪除，以此往下即可找到所有符合的答案  for(int i=0;i&lt;input.size();++i){ for(list&lt;string&gt;::iterator itr = data.begin(); itr != data.end() ; ){ if(check(input[i]-'0',(*itr)[i])){ ++itr; } else{ itr = data.erase(itr); } } }  ","version":"Next","tagName":"h2"},{"title":"Palindromic Squares​","type":1,"pageTitle":"USACO 1.2","url":"/docs/techPost/2014/usaco-12#palindromic-squares","content":"對於1~300之間的每個數字都去進行驗證首先將先該數字給平方，接者去判斷是否迴文，若是就印出答案即可  void PalindromesSquare(int n,int base){ string baseString; string squareString; int tmp = n; n = n *n; while(n){ //計算平方後的字串 squareString.push_back(getBaseChar(n%base)); n/=base; } while(tmp){ //計算當前的字串 baseString.push_back(getBaseChar(tmp%base)); tmp/=base; } if ( checkPalin(squareString) ){ n = baseString.size(); for(int i=0;i&lt;baseString.size();++i) fout&lt;&lt;baseString[n-i-1]; fout&lt;&lt;&quot; &quot;&lt;&lt;squareString&lt;&lt;endl; } }  ","version":"Next","tagName":"h2"},{"title":"Dual Palindromes​","type":1,"pageTitle":"USACO 1.2","url":"/docs/techPost/2014/usaco-12#dual-palindromes","content":"這題跟 Dual Palindromes非常類似，先根據題目的需求對每個數字去做處理每次處理都以2~10進位去試試看有沒有迴文，若能夠產生迴文的base數量超過兩個就直接印出結果，直接測是下一個數字即可  int main() { ifstream fin (&quot;dualpal.in&quot;); int limit; int start; int count; fin &gt;&gt; limit &gt;&gt; start; while(limit){ ++start; count =0; for(int i=2;i&lt;=10;++i){ if( PalindromesSquare(start,i)){ ++count; if( 2 == count){ fout&lt;&lt;start&lt;&lt;endl; --limit; break; } } } } return 0; }  ","version":"Next","tagName":"h2"},{"title":"[Switchdev] Introduuction To Switchdev","type":0,"sectionRef":"#","url":"/docs/techPost/2016/switchdev-i","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"[Switchdev] Introduuction To Switchdev","url":"/docs/techPost/2016/switchdev-i#introduction","content":"Switchdev 在 linux kernel 3.19+ 以後才正式支援的，此專案希望能夠讓整合 hardware switches 與 Linux kernel。以前的 hardware switch 都有實作自己的 L2/L3 offloading，同時廠商會在 user space 提供自己的 tool 用來操控該 switchdev。在此架構下，很多常用的 user space tool，如 ethtool, ip, brctl ..等都沒有辦法針對 hardware swtich 去控制，這會使得上層的軟體都要針對不同的底層硬體去客製化處理為了解決這個問題，希望在 kernel 中加入一層 switchdev，各廠商在 kernel 內實現自己 driver 的 switchdev，然後 swtichdev 本身會與原本的 user space tool 整合，這樣的話 user space 就不用額外提供 tool 了。 以下使用 Hardware switches - the open-source approach 內的圖片來說明 ","version":"Next","tagName":"h2"},{"title":"Before Switchdev​","type":1,"pageTitle":"[Switchdev] Introduuction To Switchdev","url":"/docs/techPost/2016/switchdev-i#before-switchdev","content":" 此圖片顯示的就是目前的狀態，右邊顯示的是一般常見的 kernel 狀況，包含一些 tool 與底層 NIC 是如何操作的。左邊則是當前 hardware switch 的普遍設計，整個操作都跳過 linux kernel，一切都是廠商自己的程式在處理而已。當前架構下，沒有辦法於系統上觀察到實體 switchdev 到底有哪些 port，就像現在多數的家用 router 一樣，明明有四個 lan 孔，但是透過 ifconfig 看都只會有一個。 ","version":"Next","tagName":"h2"},{"title":"After Switchdev​","type":1,"pageTitle":"[Switchdev] Introduuction To Switchdev","url":"/docs/techPost/2016/switchdev-i#after-switchdev","content":" 此圖顯示的是 switchdev 此專案希望的架構此架構中，廠商根據 switchdev 定義好的架構去實現自己的 driver，這樣原生的 tools 都可以直接對真正的 hardware switch 進行操作，廠商也不需要自己在額外開發 user space tool 了。 ","version":"Next","tagName":"h2"},{"title":"Vendor Implementation​","type":1,"pageTitle":"[Switchdev] Introduuction To Switchdev","url":"/docs/techPost/2016/switchdev-i#vendor-implementation","content":"第一個實作完成的 switch driver 是 RockerRocker 的 code 可以在 kernel 內的 /drivers/net/ethernet/rocker/ 內看到Mellanox 的 code 可以在 kernel 內的 /drivers/net/ethernet/mellanox/mlxsw/ 內看到 ","version":"Next","tagName":"h2"},{"title":"Next Page​","type":1,"pageTitle":"[Switchdev] Introduuction To Switchdev","url":"/docs/techPost/2016/switchdev-i#next-page","content":"接下來的章節將會介紹 switchdev 在 kernel 內的架構實作以及與 Rocker 這個 switch driver 是如何互動的。 大抵架構如下 ","version":"Next","tagName":"h2"},{"title":"Reference​","type":1,"pageTitle":"[Switchdev] Introduuction To Switchdev","url":"/docs/techPost/2016/switchdev-i#reference","content":"kernel documentHardware switches - the open-source approachRocker switchdev prototyping vehicle ","version":"Next","tagName":"h2"},{"title":"[Switchdev] How Kernel Implement SwitchDev(i)","type":0,"sectionRef":"#","url":"/docs/techPost/2016/switchdev-ii","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#introduction","content":"此篇文章用來說明在當前 kernel 中, switchdev 相關的檔案有哪些，哪些是 switchdev 的核心，哪些是與原先的 linux kernel 整合，同時簡述一下各整合的用途為何。 ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#architecture","content":"switchdev 在 kernel 中的檔案架構如下 ","version":"Next","tagName":"h2"},{"title":"Header​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#header","content":"","version":"Next","tagName":"h2"},{"title":"Linux Netowrk Function Integration​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#linux-netowrk-function-integration","content":"include/net/dsa.h 這個是 Distributed Switch Architecture, 於 2015 年此 commit(b73adef) 將 switchdev 給整合進來 根據 2008 的第一筆 commit log 來看, DSA是用來控制 hardware switch chips 的協定，不過大部分的功能都是在 2014 年後才慢慢實作，目前還無法確認此協定能夠做什麼 Distributed Switch Architecture is a protocol for managing hardware switch chips. It consists of a set of MII management registers and commands to configure the switch, and an ethernet header format to signal which of the ports of the switch a packet was received from or is intended to be sent to. include/inux/netdevice.h net_device 用來代表整個 kernel 中所有的網路裝置，包含了常見的 network interface.主要針對 net_device 這個結構進行擴充，加上與 swticdev 的整合。 加上 switchdev_ops 來提供相關的 operation加上 offload_fwd_mark 來避免已經被 offload 的 packet 再次被 forward. const struct switchdev_ops *switchdev_ops; u32 offload_fwd_mark;  ","version":"Next","tagName":"h3"},{"title":"SwtichDev Implemnetation####​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#swtichdev-implemnetation","content":"include/net/switchdev.h switchdev.h 包含了所有的 struct, function, 要瞭解 switchdev 的核心就必須要看此檔案 ","version":"Next","tagName":"h3"},{"title":"implementation​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#implementation","content":"","version":"Next","tagName":"h2"},{"title":"Linux Netowrk Function Integration​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#linux-netowrk-function-integration-1","content":"net/8021q/vlan_dev.c 若底下的 bridge port 是個 vlan interface 的話，為了要能夠取得其 static FDB 以及 port 相關的狀態，在 net_device_ops 中把相關的 operation handler 都設定為 switchdev 的 function. net/bridge/br.cnet/bridge/br_fdb.cnet/bridge/br_if.cnet/bridge/br_mdb.cnet/bridge/br_stp.cnet/bridge/br_stp_if.cnet/bridge/br_vlan.c 以上所有 bridge 相關的改動都是要將 hardware switch 的 l2 offload 與 linux kernel 給整合，包含了 STP/FDB/vlan/MDB 的變動。當底下 hardware switch 有任何變動時，都必須要主動通知 kernel 內的 bridge function 來處理。 net/core/net-sysfs.c export 一個新的 interface /sys/class/net/$iface/phys_switch_id, 可用來知道 iface** 此 port 所屬的 hardware switch ID net/core/rtnetlink.c 新增一種 rtnl type IFLA_PHYS_SWITCH_ID, 可用來獲得特定 netdevice 所屬的 switch id。在 iproute2 也加入了此 type 的支援，意味者 user space 的 tool 也一併支援此功能了。 net/dsa/slave.c For DSA 使用，不熟所以忽略 net/ipv4/fib_trie.c 將 ipv4 的 FIB forwarding 與 hardware switch 整合，當 kernel 內關於 FIB 有任何更動時(ADD/DEL/MOD)時，要主動通知 hardware switch，將該 flow 加入到 ipv4 offload rules 中。並非所有的 FIB 都會通知底下，目前的規範是 /* Don't offload route if using custom ip rules or if * IPv4 FIB offloading has been disabled completely. */  ","version":"Next","tagName":"h3"},{"title":"SwtichDev Implementation​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#swtichdev-implementation","content":"net/switchdev/switchdev.c switchdev 的實作都在這邊，包含了與 hardware switch 以及 kernel 內相關 function 的互動。 ","version":"Next","tagName":"h3"},{"title":"Switch Driver Implementation​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(i)","url":"/docs/techPost/2016/switchdev-ii#switch-driver-implementation","content":"drivers/net/ethernet/rocker/drivers/net/ethernet/mellanox/mlxsw/ 目前 kernel 只有兩個真正的實作而已，而 rocker 算是作者開發 switchdev 中的共同產物，所以 mellanox 應該算是第一個進入的廠商。 ","version":"Next","tagName":"h3"},{"title":"USACO 1.1","type":0,"sectionRef":"#","url":"/docs/techPost/2014/usaco-11","content":"","keywords":"","version":"Next"},{"title":"Brief​","type":1,"pageTitle":"USACO 1.1","url":"/docs/techPost/2014/usaco-11#brief","content":"本章節的題目比較沒有什麼特定演算法，根據題目敘述去解即可 ","version":"Next","tagName":"h2"},{"title":"Your Ride Is Here​","type":1,"pageTitle":"USACO 1.1","url":"/docs/techPost/2014/usaco-11#your-ride-is-here","content":"將兩個input都分別轉換成數字，最後再用mod去比較看看是否相等即可 ","version":"Next","tagName":"h2"},{"title":"Greedy Gift Givers​","type":1,"pageTitle":"USACO 1.1","url":"/docs/techPost/2014/usaco-11#greedy-gift-givers","content":"將所有人的金錢按照規則分配下去，即可得到答案 ","version":"Next","tagName":"h2"},{"title":"Friday the Thirteenth​","type":1,"pageTitle":"USACO 1.1","url":"/docs/techPost/2014/usaco-11#friday-the-thirteenth","content":"題目規定輸出的時候，必須要按照&quot;Saturday, Sunday, Monday, Tuesday, ..., Friday,&quot;這邊是有涵義的， 因為 1900/01/13 第一個十三號就是星期六，以此為條件在計算上會更方便。 使用一個陣列，先記住每個月份的天數 int days[12]={31,28,31,30,31,30,31,31,30,31,30,31};  接者根據題目要求的年數來迴圈計算，每次計算時要先判斷該年是不是閏年 if( ( 0 == i%4 &amp;&amp; 0 !=i%100) || (i+1900)%1000==0) leap = 1;  接下來則是計算每個月份的天數，使用day這個變數紀錄十三號是星期幾，使用ans這個array來紀錄每個星期出現多少次十三號，如果是閏年的二月份，則記得要再多加一天， for(int j=0; j&lt;12;++j){ ans[day]++; addDays(day,days[j]); if(j==1 &amp;&amp; leap) addDays(day,1); } }  最後就按照順序把結果輸出即可 ","version":"Next","tagName":"h2"},{"title":"Broken Necklace​","type":1,"pageTitle":"USACO 1.1","url":"/docs/techPost/2014/usaco-11#broken-necklace","content":"這題可以使用暴力法，每個點都往左右兩邊去試試看，此方法就很直覺，不再多說。 若為了效率，我們可以採用DP的方法來計算答案， 首先，為了計算方便，先將input給複製一份並串起來 我們的DP的概念是紀錄第I點能夠往左與往右的最大可能性，紅珠跟藍珠要分開來計算 左邊部分，我們計算不包含I以前的往左最大值 r_left[i] = r_left[i-1]+1 if input[i-1] == 'red' or input[i-1] == 'white' 0 if input[i-1] == 'blue' b_left[i] = b_left[i-1]+1 if input[i-1] == 'blue' or input[i-1] == 'white' 0 if input[i-1] == 'red'  右邊部分則是計算包含I後往右的最大值。 r_right[i] = r_right[i+1]+1 if input[i] =='red' or input[i] =='white' 0 if input[i] =='blue' b_right[i] = b_right[i+1]+1 if input[i] =='blue' or input[i] =='white' 0 if input[i] =='red'  當全部都計算完畢後，就取最大值，每個點的最大值就是左邊最大加上右邊最大。 for(int i=0;i&lt;num*2;i++){ max_ans = max(max_ans,max(r_left[i],b_left[i])+max(r_right[i],b_right[i])); }  ","version":"Next","tagName":"h2"},{"title":"How to download http file in Android","type":0,"sectionRef":"#","url":"/docs/techPost/2017/android-http","content":"How to download http file in Android 本篇文章用來記錄如何在Android裡面透過 Http 抓取檔案 這邊主要會用到兩個物件，分別是 URL 以及 HttpURLConnection。 步驟如下 根據目標的URL去初始化對應的URL物件透過該 URL 得到對應的 HttpURLConnection從該 HttpURLConncetion 取得回應，譬如 Header或是Body 所以接下來看一下每個詳細步驟,這邊假設使用 http://127.0.0.1/test 作為檔案的測試 URL url = null; try { url = new URL(&quot;http://127.0.0.1/test&quot;); } exception (MalformedURLException e) { System.out.println(e.getMessage()); } 由於 URL 本身會有 MalformedURLException 要處理，所以記得用 Try/Catch 包起來處理一下錯誤 HttpURLConnection httpConn = (HttpURLConnction)url.openConnection(); 這樣就可以取得該 HTTP 的連線了，接下來就可以針對 ResponseCode 以及 Data 本身去做後續的處理 int responseCode = httpConn.getResponseCode(); if (HttpURLConnection.HTTP_OK == responseCode) { InputStream is = httpConn.getInputStream(); //Handle InputStream } ","keywords":"","version":"Next"},{"title":"Anki 使用感想 (tutorial)","type":0,"sectionRef":"#","url":"/docs/techPost/2017/anki-tutorial","content":"","keywords":"","version":"Next"},{"title":"Anki 使用感想 (後)​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#anki-使用感想-後","content":"有鑑於之前的文章 Anki 使用感想 所提到的自動爬網站並加入卡片的功能對於一般使用者不方便理解與使用。 但是目前也時間沒有什麼多的時間去將其改善成一個友善的one-click的程式來完成這件事情，所以決定先寫一篇文章來手把手教學如何在 Windows 上面設定這個程式。等之後有時間與想法可以再來將其整個改善。 畢竟此程式一開始就只是為了讓自己方便使用而已，所以在程式撰寫上各種醜陋XD ","version":"Next","tagName":"h2"},{"title":"Dict​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#dict","content":"本程式主要主要的字典查詢網站是透過 Yahoo Dict 為主，以 infest 當作範例。 目前抓取的邏輯非常簡單! 卡片的正面是該單字的英文與其英標，而其背面則是剩下的所有解釋。 如下圖所示 如果對於這邊卡片的正反面，字典抓取來源網站有任何想法的都可以提出來。 ","version":"Next","tagName":"h2"},{"title":"Environment​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#environment","content":"要執行這個程式前，必須有一些軟體要先安裝在電腦內，包含了 Anki ApplicationAnki Source Code + addtoAnki Source CodePython3 這邊接下來會針對這三個進行說明，並且解釋每個部分有哪邊需要注意。 ","version":"Next","tagName":"h2"},{"title":"Anki Application​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#anki-application","content":"基本上一般的使用者基本上都已經會將此程式給安裝完畢，不然系統上也沒有辦法運行 Anki 來使用。 這邊要注意的是，接下來的程式會需要用到兩個設定，分別是 個人檔案 以及 牌組。 因為本程式會自動將爬完字典網站的結果寫入到特定個人檔案的特定牌組內，所以必須要取得系統上關於這兩個資訊所對應的檔案路徑。 這兩個設定可以參考下列如圖來瞭解分別是什麼(請原諒我如此潦草的作畫..) 圖中紅色框起來的地方就是個人檔案的名稱，而藍色框框則代表的是牌組的名稱。 假設我想要透過此程式自動加入到個人檔案(hwchiu)的牌組(Test)，則首先到檔案總管中輸入 %userprofile%\\AppData\\Roaming\\Anki2這時候畫面應該會出現如下圖的訊息 則時候滑鼠點選進去到 hwchiu裡面，又會看到如下圖這時候該資料夾的路徑是C:/Users/hwchiu/AppData/Roaming/Anki2/hwchiu/collection.anki2這個路徑必須要記錄下來，之後我們的程式會需要這個路徑來對底下的 collection.anki2 這個 collection 來處理。 這樣我們就已經處理好了 個人檔案 所需要的檔案路徑了，至於 牌組的部分稍後在城市內處理即可。 ","version":"Next","tagName":"h3"},{"title":"Python3​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#python3","content":"由於本程式是透過 Python3 撰寫，所以請先在系統上安裝 Python3，目前測試過python-3.6.2.exe 是可以的，不過我相信只要是 Python3 應該都沒有問題。 Python3 可以從官網這邊下載。 安裝的時候請特別注意該 Python 的安裝路徑，如下圖本範例的安裝路徑是，請記住自己的安裝路徑，此外，如果你有客製化自己的安裝內容的話，請確保 pip 一定要安裝。 C:\\Users\\hwchiu\\AppData\\Local\\Programs\\Python\\Python36-32\\  ","version":"Next","tagName":"h3"},{"title":"Anki Source Code + addtoAnki Source Code​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#anki-source-code--addtoanki-source-code","content":"首先，以下程式放置的位置隨個人喜好，自己找得到就好。我則以 D:\\add_anki為範例。 首先下載上述兩個檔案，分別是AddToAnki Source CodeAnki Source Code然後將其放到 D:\\add_anki裡面，然後分別解壓縮。 這時候該資料夾看起來就像 這邊我們要記住的一個路徑是 D:\\add_anki\\anki-master，因為我們接下來要讓 python 知道 anki 的原始碼在哪裡，所以需要此路徑。 到這一步驟為止，相關的檔案都已經抓取下來了，接下來開始要針對一些部分進行調整，這部分會比較瑣碎一點，主要是我的程式什麼都沒有自動判斷XD，因此有一些東西要自己去處理。 ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#configuration","content":"接下來大概有三個步驟要完成 設定系統環境變數，主要會設定 PATH，PYTHONPATH 以及 PYTHONIOENCODING。透過 PIP 安裝第三方套件供 PYTHON 使用設定本程式要執行的相關設定，如前述提到的個人檔案路徑位置以及對應的牌組 ","version":"Next","tagName":"h2"},{"title":"Environment Variable​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#environment-variable","content":"不論是 windows7或是windows 10，設定環境變數的方法大同小異，不熟悉的人可以參考win 7 環境變數設定 這網站來 設定 首先，為了讓我們能夠在 CMD 的環境中能夠直接執行 python 以及 pip，我們要先針對 PATH 變數去設定。 PATH​ 基本上此變數 PATH 都已經存在於系統之中，所以我們只要擴充其數值就好。 這邊需要上述安裝 PYTHON3 時所提到的安裝路徑， C:\\Users\\hwchiu\\AppData\\Local\\Programs\\Python\\Python36-32\\  我們需要幫 PATH 加入額外兩個變數，分別是 C:\\Users\\hwchiu\\AppData\\Local\\Programs\\Python\\Python36-32\\ C:\\Users\\hwchiu\\AppData\\Local\\Programs\\Python\\Python36-32\\scripts  我這邊以我的 windows10為範例，大概會如下圖這樣 PYTHONIOENCODING​ 接下來就如同上面的步驟設定PYTHONIOENCODING其數值為 utf8 PATHONPATH​ PATHONPATH其數值為 %PYTHONPATH% 以及 D:\\add_anki\\anki-master。 按下儲存都完畢後，要如何檢查自己的環境變數是否有設定成功，請打開 CMD(命令提示字元)，要如何打開請自行 google 打開後執行 set 這個指令就會在畫面上顯示當前所有的環境變數，這時候再去檢查剛剛設定的是否都有存在。 ","version":"Next","tagName":"h3"},{"title":"pip​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#pip","content":"假設上面的設定都正常的話，在 CMD 下面執行 pip，應該會出現類似下面的文字 (版本號可能會些許不同) C:\\Users\\hwchiu&gt;pip --version pip 9.0.1 from c:\\users\\hwchiu\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (python 3.6)  這時候依序輸入下列四行指令 pip install six pip install packaging pip install appdirs pip install bs4 pip install lxml pip install decorator pip install pyaudio  ","version":"Next","tagName":"h3"},{"title":"Config​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#config","content":"接下來我們將資料夾切換到 D:\\add_anki(這邊的路徑是上述第三步驟所下載的位置)，然後透過滑鼠的點擊，直接進入到 D:\\add_anki\\addToAnki-master\\examples\\YahooDict 裡面。 可以看到裡面目前有四個檔案，分別是 config.jsoninputmain.pyREADME.md 這邊要注意的檔案只有兩個，一個是 config.json 以及 input。 用任何你喜歡的文字編輯器打開 config.json,其內容大概如下 { &quot;profiles&quot;: [ { &quot;file&quot;:&quot;input&quot;, &quot;deck&quot;:&quot;test&quot;, &quot;collection&quot;:&quot;C:/Users/hwchiu/AppData/Roaming/Anki2/hwchiu/collection.anki2&quot; } ] }  這邊有三個資訊需要你自己去修改，分別是 file, deck 以及 collection。 首先 collection 就是之前提過的個人檔案的位置，而 deck 則是你想要加入到的牌組名稱，而 file 則是指名你英文單字的來源檔案(該檔案的格式是每個單字一行，可以參考本來的 input) 所以按照我目前預設的設定，其意義就是將 input 檔案內的英文單字都去自動去 Yahoo Dict 抓取解釋並且產生卡片，這些卡片都加入到 hwchiu 這個個人檔案的 test 牌組中。 當這些設定都完畢後，就可以來準備測試了。 ","version":"Next","tagName":"h3"},{"title":"Run​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#run","content":"強烈建議一開始先創立一個測試用的牌組來測試，確認一切都沒有問題後，才將 deck 改成自己常用的 deck執行前請關掉正在運行的 Anki，因為同時只能有一隻程式去存取 Anki 的資料庫。否則可能會看到類似下面的訊息 sqlite3.OperationalError: database is locked.  直接點擊 main.py 即可。 運行的畫面大致如下運行完畢後就可以打開 Anki，確認卡片都有被加入進去。 ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#summary","content":"目前的程式建置上還頗麻煩的，不過只要建置完畢後，之後都只需要修改來源檔案的內容，然後執行 main.py 就可以發呆等程式跑完即可。 至於之後是否將這些東西更簡單地去處理，我想之後有時間&amp;有需求的時候再來考慮好了。 ","version":"Next","tagName":"h2"},{"title":"Reference​","type":1,"pageTitle":"Anki 使用感想 (tutorial)","url":"/docs/techPost/2017/anki-tutorial#reference","content":"HuangJung1216的部落格 ","version":"Next","tagName":"h3"},{"title":"Mininet 運作原理","type":0,"sectionRef":"#","url":"/docs/techPost/2014/mininet-parsing","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Mininet 運作原理","url":"/docs/techPost/2014/mininet-parsing#introduction","content":"此篇文章的目標是用來釐清mininet是如何emulate網路中的host、switch以及link主要以圖解、指令與mininet實際中的程式碼來描述所有的行為Mininet version: 2.1.0p1Mininet目前是採用network namespaces來達到network isolation的功能，可以參考這邊的說明底下會描述要如何在系統中仿真出一個如下圖般的拓樸  ","version":"Next","tagName":"h2"},{"title":"Steps​","type":1,"pageTitle":"Mininet 運作原理","url":"/docs/techPost/2014/mininet-parsing#steps","content":"一開始，我們的系統環境完全是空的，如下 首先，我們要先為系統中創立兩個Host，在mininet裡面，每個Host其實就是一個Node的物件，可以在node.py中看到此物件的定義，如下。 class Node( object ): &quot;&quot;&quot;A virtual network node is simply a shell in a network namespace. We communicate with it using pipes.&quot;&quot;&quot; portBase = 0 # Nodes always start with eth0/port0, even in OF 1.0 def __init__( self, name, inNamespace=True, **params ): &quot;&quot;&quot;name: name of node inNamespace: in network namespace? params: Node parameters (see config() for details)&quot;&quot;&quot; # Make sure class actually works self.checkSetup() self.name = name self.inNamespace = inNamespace # Stash configuration parameters for future reference self.params = params self.intfs = {} # dict of port numbers to interfaces self.ports = {} # dict of interfaces to port numbers # replace with Port objects, eventually ? self.nameToIntf = {} # dict of interface names to Intfs # Make pylint happy ( self.shell, self.execed, self.pid, self.stdin, self.stdout, self.lastPid, self.lastCmd, self.pollOut ) = ( None, None, None, None, None, None, None, None ) self.waiting = False self.readbuf = '' # Start command interpreter shell self.startShell()  這邊可以看到，這邊會有一個變數inNamespace用來決定此Host是否要透過network namespaces來達到network isolation的功能，當一切變數都初始化後，就會呼叫startShell()來執行此Host。  def startShell( self ): &quot;Start a shell process for running commands&quot; if self.shell: error( &quot;%s: shell is already running&quot; ) return # mnexec: (c)lose descriptors, (d)etach from tty, # (p)rint pid, and run in (n)amespace opts = '-cdp' if self.inNamespace: opts += 'n' # bash -m: enable job control # -s: pass $* to shell, and make process easy to find in ps cmd = [ 'mnexec', opts, 'bash', '-ms', 'mininet:' + self.name ] self.shell = Popen( cmd, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True ) self.stdin = self.shell.stdin self.stdout = self.shell.stdout ...  這邊可以觀察到，mininet是透過一隻叫做mnexec的程式來執行， ","version":"Next","tagName":"h2"},{"title":"mnexec​","type":1,"pageTitle":"Mininet 運作原理","url":"/docs/techPost/2014/mininet-parsing#mnexec","content":"透過參數-n來將此process給轉換到network namespaces中程式內會透過execvp去執行參數中的指令，在此範例中該指令就是&quot;-ms mininet:&quot;+self.name。 這邊可以透過執行mininet後，在執行ps auxww | grep mininet，應該會看到類似下面的結果 root 22071 0.0 0.0 12308 1384 ? Ss 23:10 0:00 bash -ms mininet:c0 root 22079 0.0 0.0 12308 1384 ? Ss 23:10 0:00 bash -ms mininet:h1 root 22080 0.0 0.0 12308 1380 ? Ss 23:10 0:00 bash -ms mininet:h2 root 22081 0.0 0.0 12308 1380 ? Ss 23:10 0:00 bash -ms mininet:h3 root 22082 0.0 0.0 12308 1380 ? Ss 23:10 0:00 bash -ms mininet:h4 root 22085 0.0 0.0 12312 1384 ? Ss 23:10 0:00 bash -ms mininet:s1 root 22090 0.0 0.0 12312 1388 ? Ss 23:10 0:00 bash -ms mininet:s2 root 22095 0.0 0.0 12312 1388 ? Ss 23:10 0:00 bash -ms mininet:s3 root 22100 0.0 0.0 12312 1384 ? Ss 23:10 0:00 bash -ms mininet:s4 root 22105 0.0 0.0 12312 1384 ? Ss 23:10 0:00 bash -ms mininet:s5  並且把該mnexec的stdout,stdin給接起來，未來會需要透過這兩個FD來與該host溝通。 當初始化兩個Host後，系統中就會出現了兩個Host，且這兩個host都會透過namespace來達到network isolation，理論上我們要可以透過ip netns show來看到這些namespaces，實際上卻看不到，原因如同此篇所說。 此時，我們的系統如下 創立好Host後，接下來要創立Switch，Switch有很多種選擇，包含了OVSLegacyKernelSwitch、UserSwitch、OVSSwitch，IVSSwitch此四種，一般常用的就是OVSSwitch這四種Switch都繼承自Switch物件，而Switch物件則繼承自Node Node SwitchOVSLegacyKernelSwitchUserSwitchOVSSwitchIVSSwitch 在switch創立後，會透過start此function來進行相關初始化的動作，以OVSSwitch為例，就會執行一系列我們所熟悉的ovs-*指令，如下。  def start( self, controllers ): &quot;Start up a new OVS OpenFlow switch using ovs-vsctl&quot; if self.inNamespace: raise Exception( 'OVS kernel switch does not work in a namespace' ) # We should probably call config instead, but this # requires some rethinking... self.cmd( 'ifconfig lo up' ) # Annoyingly, --if-exists option seems not to work self.cmd( 'ovs-vsctl del-br', self ) self.cmd( 'ovs-vsctl add-br', self ) if self.datapath == 'user': self.cmd( 'ovs-vsctl set bridge', self,'datapath_type=netdev' ) int( self.dpid, 16 ) # DPID must be a hex string self.cmd( 'ovs-vsctl -- set Bridge', self, 'other_config:datapath-id=' + self.dpid ) self.cmd( 'ovs-vsctl set-fail-mode', self, self.failMode ) for intf in self.intfList(): if not intf.IP(): self.attach( intf ) # Add controllers clist = ' '.join( [ 'tcp:%s:%d' % ( c.IP(), c.port ) for c in controllers ] ) if self.listenPort: clist += ' ptcp:%s' % self.listenPort self.cmd( 'ovs-vsctl set-controller', self, clist ) # Reconnect quickly to controllers (1s vs. 15s max_backoff) for uuid in self.controllerUUIDs(): if uuid.count( '-' ) != 4: # Doesn't look like a UUID continue uuid = uuid.strip() self.cmd( 'ovs-vsctl set Controller', uuid, 'max_backoff=1000' )  在此程式中會去進行 設定bridge設定datapath_type設定fail-mode設定controller 此時系統如下，系統中已經創立好了switch以及兩個host，這三個Node都分別透過namespace來達到了network isolation，只是彼此之間都尚未有任何Link存在。 接下來，會根據拓墣的Link情況去創建對應的Iterface。首先，這邊使用到Link這個物件來表示每一條Link，每個Link實際上對應到的是兩個Node上面的Interface。 class Link( object ): &quot;&quot;&quot;A basic link is just a veth pair. Other types of links could be tunnels, link emulators, etc..&quot;&quot;&quot; def __init__( self, node1, node2, port1=None, port2=None, intfName1=None, intfName2=None, intf=Intf, cls1=None, cls2=None, params1=None, params2=None ): .... if port1 is None: port1 = node1.newPort() if port2 is None: port2 = node2.newPort() if not intfName1: intfName1 = self.intfName( node1, port1 ) if not intfName2: intfName2 = self.intfName( node2, port2 ) self.makeIntfPair( intfName1, intfName2 )  這邊要觀察到的，Link物件會呼叫makeIntfPair此方法，此方法就可以將兩個Interface給串接起來 def makeIntfPair( intf1, intf2 ): &quot;&quot;&quot;Make a veth pair connecting intf1 and intf2. intf1: string, interface intf2: string, interface returns: success boolean&quot;&quot;&quot; # Delete any old interfaces with the same names quietRun( 'ip link del ' + intf1 ) quietRun( 'ip link del ' + intf2 ) # Create new pair cmd = 'ip link add name ' + intf1 + ' type veth peer name ' + intf2 return quietRun( cmd )  這邊可以看到，mininet實際上是透過系統中的ip link的方法將兩個interface創造一條veth的Link。 此時系統如下 接下來，我們要把這些interface給綁到特定的Node身上，在Link物件初始化後段，會去初始化兩個Interface真正的物件本體， class Intf( object ): &quot;Basic interface object that can configure itself.&quot; def __init__( self, name, node=None, port=None, link=None, **params ): &quot;&quot;&quot;name: interface name (e.g. h1-eth0) node: owning node (where this intf most likely lives) link: parent link if we're part of a link other arguments are passed to config()&quot;&quot;&quot; self.node = node self.name = name self.link = link self.mac, self.ip, self.prefixLen = None, None, None # Add to node (and move ourselves if necessary ) node.addIntf( self, port=port ) # Save params for future reference self.params = params self.config( **params )  這邊要觀察的重點是每個Interface都會去呼叫node.addIntf( self, port=port )來處理，  def addIntf( self, intf, port=None ): &quot;&quot;&quot;Add an interface. intf: interface port: port number (optional, typically OpenFlow port number)&quot;&quot;&quot; if port is None: port = self.newPort() self.intfs[ port ] = intf self.ports[ intf ] = port self.nameToIntf[ intf.name ] = intf debug( '\\n' ) debug( 'added intf %s:%d to node %s\\n' % ( intf, port, self.name ) ) if self.inNamespace: debug( 'moving', intf, 'into namespace for', self.name, '\\n' ) moveIntf( intf.name, self )  此方法最後會呼叫 moveIntf 來將該interface給處理，moveIntf則會呼叫moveIntfNoRetry將Interface給綁入到每個Node中。 def moveIntfNoRetry( intf, dstNode, srcNode=None, printError=False ): &quot;&quot;&quot;Move interface to node, without retrying. intf: string, interface dstNode: destination Node srcNode: source Node or None (default) for root ns printError: if true, print error&quot;&quot;&quot; intf = str( intf ) cmd = 'ip link set %s netns %s' % ( intf, dstNode.pid ) if srcNode: srcNode.cmd( cmd ) else: quietRun( cmd ) links = dstNode.cmd( 'ip link show' ) if not ( ' %s:' % intf ) in links: if printError: error( '*** Error: moveIntf: ' + intf + ' not successfully moved to ' + dstNode.name + '\\n' ) return False return True  這邊可以看到，透過的指令則是ip link set %s netns %s，會將特定的interface給塞入特定Node的namespace之中 此時，我們的系統如下 ","version":"Next","tagName":"h3"},{"title":"最後​","type":1,"pageTitle":"Mininet 運作原理","url":"/docs/techPost/2014/mininet-parsing#最後","content":"透過ovs-vsctl add-port將Switch上面的Interface都給OVS控管  def attach( self, intf ): &quot;Connect a data port&quot; self.cmd( 'ovs-vsctl add-port', self, intf ) self.cmd( 'ifconfig', intf, 'up' ) self.TCReapply( intf )  設定Host上面網卡的MAC、IP、Default Route，此步驟會在Mininet噴出Configuring hosts時處理  def config( self, mac=None, ip=None, defaultRoute=None, lo='up', **_params ): &quot;&quot;&quot;Configure Node according to (optional) parameters: mac: MAC address for default interface ip: IP address for default interface ifconfig: arbitrary interface configuration Subclasses should override this method and call the parent class's config(**params)&quot;&quot;&quot; # If we were overriding this method, we would call # the superclass config method here as follows: # r = Parent.config( **_params ) r = {} self.setParam( r, 'setMAC', mac=mac ) self.setParam( r, 'setIP', ip=ip ) self.setParam( r, 'setDefaultRoute', defaultRoute=defaultRoute ) # This should be examined self.cmd( 'ifconfig lo ' + lo ) return r  這三個指令最後都會呼叫到sendCmd來處理，此函式會利用先前執行mnexec得到的stdin,stout來與底下的host交換訊息。 以上就是一個mininet如何創造一個拓樸的簡單流程，有滿多細節都省略掉的，只挑重要的指令來看，來瞭解是如何透過系統指令來完成這些拓樸的。 ","version":"Next","tagName":"h2"},{"title":"Ceph Network Architecture 研究(一)","type":0,"sectionRef":"#","url":"/docs/techPost/2017/ceph-network-i","content":"","keywords":"","version":"Next"},{"title":"Environment​","type":1,"pageTitle":"Ceph Network Architecture 研究(一)","url":"/docs/techPost/2017/ceph-network-i#environment","content":"本文所觀看的架構以及程式碼都基於 ceph 12.0.0所有網路的程式碼都放在 /src/msg/ 內部資料夾內。可以透過修改 ceph.conf中的 ms_type 來改變當前 ceph 要使用何種網路架構來連線。 ","version":"Next","tagName":"h3"},{"title":"Introduction​","type":1,"pageTitle":"Ceph Network Architecture 研究(一)","url":"/docs/techPost/2017/ceph-network-i#introduction","content":"在此版本中， ceph 內部的網路架構大致上可以分成三種來使用 SimpleAsyncXio 若要更細微去分的話，則 Async 又可以細分成三成類型，分別是 PosixDPDKRDMA 接下來會介紹一下這些類型的概念。 Simple​ Simple顧名思義就是簡單，其網路架構也是非常的簡單，是 ceph 最早期的設計 每一對節點之間都會有一條 connection，而每一條 connection 又會產生兩個 thread，分別負責send/write的行為。 所以當 connection 數量一多的時候，會產生非常多的 thread，然後每個 thread 都各自在自己的迴圈內去處理自己的事情。這種網路程式設計的方法到現今幾乎都被 event-based 的方法給取代，當有事件發生時再來處理，而不是開一個 thread 在那邊癡癡的等待封包處理。 若要使用這種架構，則將 ms_type 修改為 simple 即可。 Xio​ Xio比較不算是原生的功能，其用到了第三方的library Accelio XIO。Accelio XIO是一套提供穩定，高速網路資料交換的函式庫，除了支援常見的 Ethernet Network 外，也有支援 RDMA, 所以現在網路上看到很多 ceph 關於RDMA的效能測量都是基於使用 Xio作為其網路傳輸的實現。 然而 ceph 後來也沒有在維護使用xio作為其網路傳輸的一部份，主要還是因為要引入第三方函式庫，不是直接使用 ceph本身的架構，同時若有任何bug出現，除非等待xio修復，否則開發人員還要再花時間理解一套第三方函式庫，並且嘗試解決問題，這在整理維護成本上是不容忽視的。 若要使用這種架構，則將 ms_type 修改為 xio 即可。 Async​ 相對於 simple 採用兩條 thread 專門負責收送動作的做法，Async則是採用 event-driven的方式來處理請求，整體設計上會有一個thread pool，會有一個固定數量（還是可透過API動態調整)的 thread。 然後將每一條 connection 分配到一個 thread 身上，然後透過監聽該connection的狀況，當底層觸發了收或是送的事件時，更精準的應該是講 read/write，就會呼叫對應的函式來處理相關的行為。 此種設計可大大減少系統上開啟的 thread 數量，減少系統的消耗。 在此設計下， ceph 又提供了三種模式，分別是 POSIX, DPDK 以及 RDMA。 這三者的切換方式分別是將ms_type修改為 asyncasync+dpdkasync+rmda POSIX​ POSIX代表的就是走 kernel 大家熟悉的event機制，譬如select, epoll 以及 kqueue. 同時此類型也是 async的預設類型，因為此類型完全不需要任何硬體的幫忙，完全是靠software的方式就可以完成的，主要是看 kernel 本身的支援程度來決定實際上會呼叫出哪一種實現方式來使用。 DPDK​ Data Plane Development Kit(DPDK)是一套 intel所主導的技術，基本上就是使用CPU換取效能的機制，藉由此機制，user-space的程式可以略過 kernel 直接跟硬體溝通，這部分採用的是polling的方式，藉由不停地詢問來加強整理的效能，然而也會因為一直不停的polling使得cpu使用率提升。 然而此技術只要是x86的 CPU 基本上都支援，所以在使用上其實可以說是非常的廣泛，很容易被支援，不太會有被特定硬體綁住限制的機會。 RDMA​ RDMA代表的是遠方記憶體存取，是一種擁有low latency, low cpu consumption, kernel by pass等特性的傳輸方法，藉由跳過kernel-space的方式，讓整體資料流量直接透過網卡去處理，同時也可以直接針對遠方的記憶體進行修改而不會讓遠方的CPU察覺這件事情。 一般來說支援的網路底層有 Infiniband 以及 Ethernet，這部分由於封包會忽略 kernel space，因此資料在 internet 上傳遞勢必要符合當前廣泛的格式，譬如 Ethernet，因此這邊會採用 ROCE 的方式來處理封包之間的標頭問題。 目前 ceph 上面已經可以運行 RDMA，不過根據開發者在 Mail 中的說法，目前還是在尋求穩定性為主，效能上還沒有達到最佳化的部分，因此使用起來與 POSIX w/ TCP/IP 在各方面都不會有太明顯的提升。 在看完這些網路底層的實現後，接下來要來探討Network是如何提供介面給其他元件，如OSD,Mon等使用的。 ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"Ceph Network Architecture 研究(一)","url":"/docs/techPost/2017/ceph-network-i#architecture","content":"整個網路最基礎的架構程式碼都放在 /src/msg/ 裡面，排除三種類型的資料夾外，大致上就是下列四種類型 MessageConnectionMessengerDispatcher 而剛剛上述提到的那些種類，其實就是繼承這些基本架構，並且實現了每個介面的功能而已。 因此接下來會比較偏向概念性的去分析這四種概念的用途。 Messege​ 此物件主要用來定義封包的格式，所有 ceph node之間的封包傳送都必須要參照此格式，不過對於應用層(osd,mon等)不需要擔心這邊，這邊是由網路層去負責包裝跟解析的 Connection​ 代表任意兩個 ceph node 之間的連線，彼此之間可以傳送/接收封包。 Messenger​ 此物件用來管理連線，一種Messenger可以管理多條連線，目前在 osd 的使用中，是採用一種類型的連線使用一個 Messenger，而底下可以有很多條connection。 舉例來說，今天有兩種連線類型分別代表 heartbeat public 以及 heartbeat cluster，且環境中有 三台osd，所以於 mesh 的架構中，總共會有六條連線，(每台 osd 彼此互連，且都有兩種類型的連線)。 在heartbeat public的 messenger 會採用 public network去建立 connection，而在 heartbeat cluster 的 messenger 則是會採用 cluster network 去建立 connection。 這邊的範例剛好是使用不同網路類型的 *messenger，實際上也可以是不同用途的，譬如用來傳遞 heartbeat，用來傳送 control message 或是用來傳送 data message**之類的。 這邊的架構如附圖  可參考 OSD 實際創造這些 messenger 的程式碼  Messenger *ms_public = Messenger::create(g_ceph_context, public_msgr_type, entity_name_t::OSD(whoami), &quot;client&quot;, getpid(), Messenger::HAS_HEAVY_TRAFFIC | Messenger::HAS_MANY_CONNECTIONS); Messenger *ms_cluster = Messenger::create(g_ceph_context, cluster_msgr_type, entity_name_t::OSD(whoami), &quot;cluster&quot;, getpid(), Messenger::HAS_HEAVY_TRAFFIC | Messenger::HAS_MANY_CONNECTIONS); Messenger *ms_hb_back_client = Messenger::create(g_ceph_context, cluster_msgr_type, entity_name_t::OSD(whoami), &quot;hb_back_client&quot;, getpid(), Messenger::HEARTBEAT); Messenger *ms_hb_front_client = Messenger::create(g_ceph_context, public_msgr_type, entity_name_t::OSD(whoami), &quot;hb_front_client&quot;, getpid(), Messenger::HEARTBEAT); Messenger *ms_hb_back_server = Messenger::create(g_ceph_context, cluster_msgr_type, entity_name_t::OSD(whoami), &quot;hb_back_server&quot;, getpid(), Messenger::HEARTBEAT); Messenger *ms_hb_front_server = Messenger::create(g_ceph_context, public_msgr_type, entity_name_t::OSD(whoami), &quot;hb_front_server&quot;, getpid(), Messenger::HEARTBEAT); Messenger *ms_objecter = Messenger::create(g_ceph_context, public_msgr_type, entity_name_t::OSD(whoami), &quot;ms_objecter&quot;, getpid(), 0);  Dispatcher​ Dispatcher 這邊的概念簡單來說就是當接收到封包後，要怎麼處理，每個應用層(OSD/MON.等)創建 Messenger 後，要向其註冊 dispatcher，這行為解讀成，當該 messenger 內的 connection 有從對方收到訊息後，所要執行的對應 function。 該 function 原型內可以判別是由哪一條 connection所觸發的。a若要註冊該 dispatcher，可參考 OSD 實際程式碼如下  client_messenger-&gt;add_dispatcher_head(this); cluster_messenger-&gt;add_dispatcher_head(this); hb_front_client_messenger-&gt;add_dispatcher_head(&amp;heartbeat_dispatcher); hb_back_client_messenger-&gt;add_dispatcher_head(&amp;heartbeat_dispatcher); hb_front_server_messenger-&gt;add_dispatcher_head(&amp;heartbeat_dispatcher); hb_back_server_messenger-&gt;add_dispatcher_head(&amp;heartbeat_dispatcher);  最後做一個簡單總結，每個應用層可以自由創建 messenger，並且向其註冊對應的 dispatcher，同時使用該 messenger去管理該類型的多條連線。 至於底層是如何連線，封包如何收送，訊息如何解析、封裝，都些都是 Networking 本身的事情，就是本文開頭提到的那幾種方式去實作。 因此接下來的文章，將探討 Async 這類型的傳輸方式是如何將上述的概念給實作的。 ","version":"Next","tagName":"h3"},{"title":"Ceph Network Architecture 研究(二)","type":0,"sectionRef":"#","url":"/docs/techPost/2017/ceph-network-ii","content":"","keywords":"","version":"Next"},{"title":"Event​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#event","content":"這邊先從最底層開始看起，首先是 Event。 整個 Event 代表了底層 I/O 的處理，目前支援三種實作方式，分別是 EpoolKqueueSelect 在這三種實作上面，又提供了三種 EventType 供上層使用，分別是 File_EventTime_EventExternal_Event ","version":"Next","tagName":"h2"},{"title":"File_Event​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#file_event","content":"File event 是最常見也是最普遍使用的，針對每一個 file descriptor 進行設定，該 file descriptor 關注的是 read event 還是 write event 以及當該 event 發生時，應該要進行什麼樣的處理。 ","version":"Next","tagName":"h3"},{"title":"Time_Event​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#time_event","content":"Time event 本身跟 file descriptor 無關，單純的是依照時間來驅動的事件，對於每一種 Time Event 則會有兩個設定，一個是多久(ms)之後要執行該事件，以及該執行的事件是什麼，這邊都是使用 function pointer 來指向該事件。 ","version":"Next","tagName":"h3"},{"title":"External_Event​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#external_event","content":"External event 實際上跟 Time event 是相同的，可以視為時間是 0ms 的 Time event，就是馬上執行該事件。 整體架構如下圖，系統中會有一個 Event Center，底層支援各種 I/O 的實作，此外，本身會提供介面供上層使用，可以接受上述三種事件的註冊。 待一切都準備就緒後，就會開始透過底層 I/O 的事件去處理這三種事件，譬 如有封包到來的時候呼叫對應的函式處理，亦或是時間到的時候執行對應的Time event。  ","version":"Next","tagName":"h3"},{"title":"Worker and Network Stack​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#worker-and-network-stack","content":"上述看完了 EventCenter的概念後，我們知道每個 EventCenter專門用來負責底層的 I/O 處理，那這邊為了提高整體的效率，採用了 ThreadPool的概念，事先根據系統上的能力創造一批固定數量的 Thread，這邊統稱為 Worker，這些 Worker 的數量可以動態的增加，減少。 然後每一個 Worker 都配上一個 EventCenter，盡可能的讓所有的 Worker去平均分擔所有的 Network I/O 負擔。 在這群 Worker 之上存在一層 Network Stack，此 Stack 會掌管所有的 Workers，包含其創建/增加/刪除等行為。 綜合以上概念，目前認知的架構圖如下。 ","version":"Next","tagName":"h3"},{"title":"AsyncConnection​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#asyncconnection","content":"接下來看到 AsyncConnection ，此物件代表者任意兩個 ceph node之間的連線，可以是 osd&lt;-&gt;osd，也可以是 mon&lt;-&gt;mon亦或是 osd&lt;-&gt;mon。 此物件本身除了代表連線外，跟網路傳輸相關的功能，如發送封包，接收封包等事情都會在這邊提供一個介面供更上層的應用(OSD)來使用。 不過這邊只是中介層而已，真正收送發包的還是上述提到 EventCenter在處理。 由於系統上可以同時存在非常多條 connections，為了讓這些 connections能夠同時運作且不會互相影響，這邊配給每一個connection一個worker來處理。 由於 connection的數量基本上都會比worker還要多，因此在配對上就是盡量平均分擔下去，盡量讓每一個worker負擔相同數量的connection。 所以看到這邊已經可以大概理解，系統上每一條 connection 都配上一個 thread 來處理，而每一個 thread 實際上可能會負責不少條 connection，這邊採用數量的方式來分散這些 connection，若能夠根據實際負擔作為分散的權重也許可以讓每個thread 的負擔更為平均。 因此看到這邊，目前的架構圖如下，圖中的 Async Connections只是一個抽象的概念，描述眾多的 Async Connection而已。  ","version":"Next","tagName":"h3"},{"title":"AsyncMessenger​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#asyncmessenger","content":"最後出場的就是 AsyncMessenger，其概念與之前講述過的 Messenger一樣，管理所有的 AsyncConnections，同時是最直接供上層應用程式的物件。 每個 AsyncMessenger都可以被註冊 Dispatcher，接者當底下的 connection 有收到來自遠方的封包(Messege)時，就呼叫對應的 Dispatcher 來處理。 所以整體架構如下圖。  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Ceph Network Architecture 研究(二)","url":"/docs/techPost/2017/ceph-network-ii#summary","content":"到這邊大致上已經對 Async 系列有一個基本的認識了，不過這基本上只局限於 async+poxis 類型而已，對於 RDMA 以及 DPDK 則因為這兩種運作方式的不同，其真正運作的方式又是截然不同的，這部分可以從原始碼中看到 RDMA/DPDK 都有額外的資料夾，且資料夾內又有為數不少的檔案可以大略猜測出來。 接下來的文章會比較偏技術性質，會直接看進這些程式碼內，透過這些程式碼能夠更瞭解整體的架構以及實作細節，順便學習增廣見聞。 ","version":"Next","tagName":"h2"},{"title":"REST API services in Floodlight (Topology)","type":0,"sectionRef":"#","url":"/docs/techPost/2014/rest-api-services-in-floodlight-topology","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"REST API services in Floodlight (Topology)","url":"/docs/techPost/2014/rest-api-services-in-floodlight-topology#introduction","content":"Topology這邊則是維護整個網路拓樸的資訊，包含Link、Cluster以及routing path。Topology的所有資訊都是建立於LinkDiscovery所發送的LLDP與BDDP，再搭配演算法去得到整個網路拓樸的情況。每個Cluster都會包含一個以上的Switch，同一個Cluster內的Switch可以組成一個Strongly Connected Components (SCC)。 ","version":"Next","tagName":"h2"},{"title":"API​","type":1,"pageTitle":"REST API services in Floodlight (Topology)","url":"/docs/techPost/2014/rest-api-services-in-floodlight-topology#api","content":"/wm/topology/links/json : 回傳所有Links，盡可能回傳BIDRECTONAL Method: GETParameter: 無，Code: TopologyWebRoutable.javaLinksResource.java Example: + curl -s http://127.0.0.1:8080/wm/topology/links/json | python -mjson.tool  [ { &quot;direction&quot;: &quot;bidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:04&quot;, &quot;src-port&quot;: 3, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;bidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:05&quot;, &quot;src-port&quot;: 4, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;bidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:02&quot;, &quot;src-port&quot;: 1, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;bidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:03&quot;, &quot;src-port&quot;: 2, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; } ]  /wm/topology/directed-links/json : 回傳所有Type是 DIRECT_LINK或是TUNNEL的Link，都以UNIDIRECTIONAL的方式呈現 Method: GETParameter: 沒有參數，就回傳所有符合條件的LinksCode: TopologyWebRoutable.javaDirectedLinksResource.java Example: + curl -s http://127.0.0.1:8080/wm/topology/directed-links/json | python -mjson.tool  [ { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:05&quot;, &quot;src-port&quot;: 4, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 3, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;src-port&quot;: 1, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:04&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;src-port&quot;: 1, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:02&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:03&quot;, &quot;src-port&quot;: 2, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 2, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;src-port&quot;: 1, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:03&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 4, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;src-port&quot;: 1, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:05&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:04&quot;, &quot;src-port&quot;: 3, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; }, { &quot;direction&quot;: &quot;unidirectional&quot;, &quot;dst-port&quot;: 1, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:02&quot;, &quot;src-port&quot;: 1, &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:01&quot;, &quot;type&quot;: &quot;internal&quot; } ]  /wm/topology/external-links/json : 回傳透過BDDP所發現的Links。External-Links可以用來找出在網路環境中，不受controoler控制的switch的links。使用情境請參考這篇 Method: GETParameter: 沒有參數，就回傳所有符合條件的LinksCode: TopologyWebRoutable.javaExternalLinksResource.java Example: + curl -s http://127.0.0.1:8080/wm/topology/external-links/json | python -mjson.tool  [ { &quot;src-switch&quot;: &quot;00:00:00:00:00:00:00:02&quot;, &quot;src-port&quot;: 3, &quot;dst-switch&quot;: &quot;00:00:00:00:00:00:00:03&quot;, &quot;dst-port&quot;: 3, &quot;type&quot;: &quot;external&quot;, &quot;direction&quot;: &quot;bidirectional&quot; } ]  /wm/topology/tunnellinks/json : 回傳所有的tunnelLink,我還沒有嘗試過有tunnel的網路拓墣，所以此API也不是很了解，只能從code去看。 Method: GETParameter: 沒有參數，就回傳所有tunnelLinks，每條link都會以NodePortTuple的形式呈現，該型態包含了switch的DPID以及對應的PortCode: TopologyWebRoutable.javaTunnelLinksResource.java Example: + curl -s http://127.0.0.1:8080/wm/topology/tunnellinks-links/json | python -mjson.tool /wm/topology/switchclusters/json : 回傳Controller底下的所有cluster，每個cluster是由一群switches所組成的SCC。 Method: GETParameter: 沒有參數，就回傳所有的clusterCode: SwitchClustersResource.javaTopologyWebRoutable.java Example: + curl -s http://127.0.0.1:8080/wm/topology/switchclusters/json | python -mjson.tool  { &quot;00:00:00:00:00:00:00:01&quot;: [ &quot;00:00:00:00:00:00:00:01&quot;, &quot;00:00:00:00:00:00:00:02&quot;, &quot;00:00:00:00:00:00:00:03&quot;, &quot;00:00:00:00:00:00:00:04&quot;, &quot;00:00:00:00:00:00:00:05&quot; ] }  由此範例可以看到目前網路拓墣中有一個cluster，包含了五個switches。cluster的識別碼就由最小的switch DPID來決定。由於是SCC的關係，這五個switches都有辦法傳送封包到彼此。 /wm/topology/broadcastdomainports/json : 回傳所有的broadcast domain ports，此port跟externel link有關係。返回值代表某switch上的某port所連接到的是一個不受controller所控制的switch。所以未來若是有收到廣播封包的話，這邊的要收起來。 Method: GETParameter: 沒有參數，就回傳所有符合條件的NodePortTupleCode: TopologyWebRoutable.javaBroadcastDomainPortsResource.java Example: + curl -s http://127.0.0.1:8080/wm/topology/broadcastdomainports/json | python -mjson.tool  [ { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:05&quot; }, { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:04&quot; }, { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:03&quot; }, { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:02&quot; } ]  /wm/topology/enabledports/json : 回傳所有Switch上面的所有Ports。 Method: GETParameter: 沒有參數Code: EnabledPortsResource.javaTopologyWebRoutable.java Example: + curl -s http://127.0.0.1:8080/wm/topology/enabledports/json | python -mjson.tool  [ { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:02&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:02&quot; }, { &quot;port&quot;: 65534, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:02&quot; }, { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:03&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:03&quot; }, { &quot;port&quot;: 65534, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:03&quot; }, { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:04&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:04&quot; }, { &quot;port&quot;: 65534, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:04&quot; }, { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:05&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:05&quot; }, { &quot;port&quot;: 65534, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:05&quot; } ]  /wm/topology/blockedports/json : 此API我還沒有測試成功，看CODE也沒有發現情況的port會被加入到blockedports，會再寄信詢問。 Method: GETParameter: 沒有參數，就回傳所有符合條件的LinksCode: TopologyWebRoutable.javaBlockedPortsResource.java Example: + curl -s http://127.0.0.1:8080/wm/topology/blockedports/json | python -mjson.tool /mw/topology/route/{src-dpid}/{src-port}/{dst-dpid}/{dst-port/json : 一個用來找出最短路徑的REST API Method: GETParameter: + src-dpid: source switch dpid src-port: source port of the source switchdst-dpid: destination switch dpiddst-port: destination port of the destination switch Code: TopologyWebRoutable.javaRouteResource.java Example: + curl -s http://127.0.0.1:8080/wm/topology/route/00:00:00:00:00:00:00:04/2/00:00:00:00:00:00:00:07/2/json | python -mjson.tool  [ { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:04&quot; }, { &quot;port&quot;: 3, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:04&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:02&quot; }, { &quot;port&quot;: 3, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:02&quot; }, { &quot;port&quot;: 1, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:01&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:01&quot; }, { &quot;port&quot;: 3, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:05&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:05&quot; }, { &quot;port&quot;: 3, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:07&quot; }, { &quot;port&quot;: 2, &quot;switch&quot;: &quot;00:00:00:00:00:00:00:07&quot; } ]  以一個 --topo tree,3的範例來說，若想要從switch4的port2到switch7的port2，可以得到上述結果的走法。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2013/openvswitch-source-code3","content":"","keywords":"","version":"Next"},{"title":"netdev_frame_hook​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#netdev_frame_hook","content":"static rx_handler_result_t netdev_frame_hook(struct sk_buff **pskb) { struct sk_buff *skb = *pskb; struct vport *vport; if (unlikely(skb-&gt;pkt_type == PACKET_LOOPBACK)) return RX_HANDLER_PASS; vport = ovs_netdev_get_vport(skb-&gt;dev); netdev_port_receive(vport, skb); return RX_HANDLER_CONSUMED; }  net_device收到封包後，變變呼叫這個fucntion call來處理。先判斷是不是LOOPBACK的，是的話就不需要處理了。透過 ovs_netdev_get_vport取得該dev對應的vport呼叫 netdev_port_receive來處理這個封包 ","version":"Next","tagName":"h2"},{"title":"netdev_port_receive​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#netdev_port_receive","content":"static void netdev_port_receive(struct vport *vport, struct sk_buff *skb) { if (unlikely(!vport)) goto error; if (unlikely(skb_warn_if_lro(skb))) goto error; /* Make our own copy of the packet. Otherwise we will mangle the * packet for anyone who came before us (e.g. tcpdump via AF_PACKET). * (No one comes after us, since we tell handle_bridge() that we took * the packet.) */ skb = skb_share_check(skb, GFP_ATOMIC); if (unlikely(!skb)) return; skb_push(skb, ETH_HLEN); ovs_skb_postpush_rcsum(skb, skb-&gt;data, ETH_HLEN); ovs_vport_receive(vport, skb, NULL); return; error: kfree_skb(skb); }  skb_warn_if_lro判斷其LRO的設定有沒有問題skb_push調整skb中的data指標ovs_skb_postpush_rcsum 處理ip checksum。呼叫 ovs_vport_receive繼續處理 ","version":"Next","tagName":"h2"},{"title":"ovs_vport_receive​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#ovs_vport_receive","content":"void ovs_vport_receive(struct vport *vport, struct sk_buff *skb, struct ovs_key_ipv4_tunnel *tun_key) { struct pcpu_tstats *stats; stats = this_cpu_ptr(vport-&gt;percpu_stats); u64_stats_update_begin(&amp;stats-&gt;syncp); stats-&gt;rx_packets++; stats-&gt;rx_bytes += skb-&gt;len; u64_stats_update_end(&amp;stats-&gt;syncp); OVS_CB(skb)-&gt;tun_key = tun_key; ovs_dp_process_received_packet(vport, skb); }  struct pcpu_tstats 紀錄當前cpu對於封包的一些計數 (rx,tx)由 this_cpu_ptr取得這個vport的(tx,rx) counter.Update counters (packets,bytes)透過 ovs_dp_process_received_packet繼續處理 ","version":"Next","tagName":"h2"},{"title":"ovs_dp_process_received_packet​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#ovs_dp_process_received_packet","content":"這個function的目的就是 由skb內取出封包各欄位的資訊(L2,L3,L4)去查詢該 datapath的flow table中否有符合的flow entry有找到，就執行對應的action.沒有找到，就執行 ovs_dp_upcall送到 user space void ovs_dp_process_received_packet(struct vport *p, struct sk_buff *skb) { struct datapath *dp = p-&gt;dp; struct sw_flow *flow; struct dp_stats_percpu *stats; struct sw_flow_key key; u64 *stats_counter; int error; stats = this_cpu_ptr(dp-&gt;stats_percpu); /* Extract flow from 'skb' into 'key'. */ error = ovs_flow_extract(skb, p-&gt;port_no, &amp;key); if (unlikely(error)) { kfree_skb(skb); return; } /* Look up flow. */ flow = ovs_flow_lookup(rcu_dereference(dp-&gt;table), &amp;key); if (unlikely(!flow)) { struct dp_upcall_info upcall; upcall.cmd = OVS_PACKET_CMD_MISS; upcall.key = &amp;key; upcall.userdata = NULL; upcall.portid = p-&gt;upcall_portid; ovs_dp_upcall(dp, skb, &amp;upcall); consume_skb(skb); stats_counter = &amp;stats-&gt;n_missed; goto out; } OVS_CB(skb)-&gt;flow = flow; OVS_CB(skb)-&gt;pkt_key = &amp;key; stats_counter = &amp;stats-&gt;n_hit; ovs_flow_used(OVS_CB(skb)-&gt;flow, skb); ovs_execute_actions(dp, skb); out: /* Update datapath statistics. */ u64_stats_update_begin(&amp;stats-&gt;sync); (*stats_counter)++; u64_stats_update_end(&amp;stats-&gt;sync); }  先由 vport 取得 對應的 datapath由 this_cpu_ptr取得這個datapath的(tx,rx) counter.透過 ovs_flow_extract 把 skb, vport 的資訊都寫入 sw_flow_key key之中呼叫 ovs_flow_lookup 去查詢這個packet在table之中有沒有match的flow entry.如果沒有找到，那就透過 upcall的方式，把這個flow_miss告訴 ovs-vswitchd去處理如果有找到，先透過 ovs_flow_used更新該flow的一些資訊(usedtime,packet,byte,tcp_flag),接者在呼叫 ovs_execute_actions 執行這個flow 對應的actions ","version":"Next","tagName":"h2"},{"title":"ovs_flow_extract​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#ovs_flow_extract","content":"int ovs_flow_extract(struct sk_buff *skb, u16 in_port, struct sw_flow_key *key) { int error; struct ethhdr *eth; memset(key, 0, sizeof(*key)); key-&gt;phy.priority = skb-&gt;priority; if (OVS_CB(skb)-&gt;tun_key) memcpy(&amp;key-&gt;tun_key, OVS_CB(skb)-&gt;tun_key, sizeof(key-&gt;tun_key)); key-&gt;phy.in_port = in_port; key-&gt;phy.skb_mark = skb-&gt;mark; skb_reset_mac_header(skb);  OVS_CB是一個marco,會把skbuff中的cb區域拿來使用，並且轉型成 ovs_skb_cb#define OVS_CB(skb) ((struct ovs_skb_cb *)(skb)-&gt;cb)如果該packet有使用 tunnel_key的話，就把該 tun_key給複製到 key。把收到封包的port number也記錄到key裡面( ingress port)使用 skb_reset_mac_header 得到 mac header (放在 skb-&gt;mac_header)  /* Link layer. We are guaranteed to have at least the 14 byte Ethernet * header in the linear data area. */ eth = eth_hdr(skb); memcpy(key-&gt;eth.src, eth-&gt;h_source, ETH_ALEN); memcpy(key-&gt;eth.dst, eth-&gt;h_dest, ETH_ALEN); __skb_pull(skb, 2 * ETH_ALEN); /* We are going to push all headers that we pull, so no need to * update skb-&gt;csum here. */ if (vlan_tx_tag_present(skb)) key-&gt;eth.tci = htons(vlan_get_tci(skb)); else if (eth-&gt;h_proto == htons(ETH_P_8021Q)) if (unlikely(parse_vlan(skb, key))) return -ENOMEM; key-&gt;eth.type = parse_ethertype(skb); if (unlikely(key-&gt;eth.type == htons(0))) return -ENOMEM; skb_reset_network_header(skb); __skb_push(skb, skb-&gt;data - skb_mac_header(skb)); ....  取得 ethernet header,並且把 sourcee/destinaion mac address給複製到key。透過 __skb_pull,把 skb-data給往下指 ETH_ALEN*2檢查有沒有用 vlan tag,有的話就把tci給抓近來使用 parse_ethertype 得到該封包的 ethernet type使用 skb_reset_network_header 得到 network header (放在 skb-&gt;network_header)透過 __skb_push 把skb中的data指標往上移(這樣可以取回mac header的一些資訊)，供未來使用後面就是針對 (IP,IPV6,ARP)等在做更細部的資料取得 struct sw_flow *ovs_flow_lookup(struct flow_table *tbl, const struct sw_flow_key *key) { struct sw_flow *flow = NULL; struct sw_flow_mask *mask; list_for_each_entry_rcu(mask, tbl-&gt;mask_list, list) { flow = ovs_masked_flow_lookup(tbl, key, mask); if (flow) /* Found */ break; } return flow; }  從 datapath的flow table中先取得所有的 mask_list使用 ovs_masked_flow_lookup去搜尋進來的封包是否有match最後回傳 flow. static struct sw_flow *ovs_masked_flow_lookup(struct flow_table *table, const struct sw_flow_key *unmasked, struct sw_flow_mask *mask) { struct sw_flow *flow; struct hlist_head *head; int key_start = mask-&gt;range.start; int key_end = mask-&gt;range.end; u32 hash; struct sw_flow_key masked_key; ovs_flow_key_mask(&amp;masked_key, unmasked, mask); hash = ovs_flow_hash(&amp;masked_key, key_start, key_end); head = find_bucket(table, hash); hlist_for_each_entry_rcu(flow, head, hash_node[table-&gt;node_ver]) { if (flow-&gt;mask == mask &amp;&amp; __flow_cmp_masked_key(flow, &amp;masked_key, key_start, key_end)) return flow; } return NULL; }  OVS 2.0後增加對megaflow的支持，所以在kernel端也可以支援wildcard的flow matching.sw_flow_key實際上就是個wildcard，每個進來的封包都先跟wildcard做 ovs_flow_key_mask,然後在用mask後的結果去table中尋找有沒有可以match的使用mask過後的結果來做hash,並且透過 find_bucket找到那個hash值所在的bucket針對那個bucket中所有的flow去做比對，如果 mask相同且 flow_cmpmasked結果為真，就代表match.key的start &amp; end 還不是很明瞭其目的以及用途，待釐清 void ovs_flow_key_mask(struct sw_flow_key *dst, const struct sw_flow_key *src, const struct sw_flow_mask *mask) { const long *m = (long *)((u8 *)&amp;mask-&gt;key + mask-&gt;range.start); const long *s = (long *)((u8 *)src + mask-&gt;range.start); long *d = (long *)((u8 *)dst + mask-&gt;range.start); int i; /* The memory outside of the 'mask-&gt;range' are not set since * further operations on 'dst' only uses contents within * 'mask-&gt;range'. */ for (i = 0; i &lt; range_n_bytes(&amp;mask-&gt;range); i += sizeof(long)) *d++ = *s++ &amp; *m++; }  把 src用 mask去處理，結果放到 dst上這邊可以看到做mask的方法就是不停地用 &amp;來取結果而已。 ","version":"Next","tagName":"h2"},{"title":"Found​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#found","content":"尋找到flow後 更新該flow的一些統計資訊 ( ovs_flow_used)執行該flow entry上的actions (ovs_execute_actions) ","version":"Next","tagName":"h2"},{"title":"ovs_flow_used​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#ovs_flow_used","content":"void ovs_flow_used(struct sw_flow *flow, struct sk_buff *skb) { u8 tcp_flags = 0; if ((flow-&gt;key.eth.type == htons(ETH_P_IP) || flow-&gt;key.eth.type == htons(ETH_P_IPV6)) &amp;&amp; flow-&gt;key.ip.proto == IPPROTO_TCP &amp;&amp; likely(skb-&gt;len &gt;= skb_transport_offset(skb) + sizeof(struct tcphdr))) { u8 *tcp = (u8 *)tcp_hdr(skb); tcp_flags = *(tcp + TCP_FLAGS_OFFSET) &amp; TCP_FLAG_MASK; } spin_lock(&amp;flow-&gt;lock); flow-&gt;used = jiffies; flow-&gt;packet_count++; flow-&gt;byte_count += skb-&gt;len; flow-&gt;tcp_flags |= tcp_flags; spin_unlock(&amp;flow-&gt;lock); }  如果該封包滿足(IP/IPV6,TCP)且TCP有額外的flag的話，就更新其TCP_FLAGS更新該flow的相關資訊used(上次使用時間),單位是 jiffiescounter. ","version":"Next","tagName":"h3"},{"title":"ovs_execute_actions​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#ovs_execute_actions","content":"/* Execute a list of actions against 'skb'. */ int ovs_execute_actions(struct datapath *dp, struct sk_buff *skb) { struct sw_flow_actions *acts = rcu_dereference(OVS_CB(skb)-&gt;flow-&gt;sf_acts); struct loop_counter *loop; int error; /* Check whether we've looped too much. */ loop = &amp;__get_cpu_var(loop_counters); if (unlikely(++loop-&gt;count &gt; MAX_LOOPS)) loop-&gt;looping = true; if (unlikely(loop-&gt;looping)) { error = loop_suppress(dp, acts); kfree_skb(skb); goto out_loop; } OVS_CB(skb)-&gt;tun_key = NULL; error = do_execute_actions(dp, skb, acts-&gt;actions, acts-&gt;actions_len, false); /* Check whether sub-actions looped too much. */ if (unlikely(loop-&gt;looping)) error = loop_suppress(dp, acts); out_loop: /* Decrement loop counter. */ if (!--loop-&gt;count) loop-&gt;looping = false; return error; }  先從flow_sf_acts中取出對應的actions(sw_flow_actions)這邊會限制執行 do_execute_actions的次數，設計理念尚未明瞭。呼叫 do_execute_actions來做後續的處理 ","version":"Next","tagName":"h3"},{"title":"do_execute_actions​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#do_execute_actions","content":"static int do_execute_actions(struct datapath *dp, struct sk_buff *skb, const struct nlattr *attr, int len, bool keep_skb) { /* Every output action needs a separate clone of 'skb', but the common * case is just a single output action, so that doing a clone and * then freeing the original skbuff is wasteful. So the following code * is slightly obscure just to avoid that. */ int prev_port = -1; const struct nlattr *a; int rem; for (a = attr, rem = len; rem &gt; 0; a = nla_next(a, &amp;rem)) { int err = 0; if (prev_port != -1) { do_output(dp, skb_clone(skb, GFP_ATOMIC), prev_port); prev_port = -1; } switch (nla_type(a)) { case OVS_ACTION_ATTR_OUTPUT: prev_port = nla_get_u32(a); break; case OVS_ACTION_ATTR_USERSPACE: output_userspace(dp, skb, a); break; case OVS_ACTION_ATTR_PUSH_VLAN: err = push_vlan(skb, nla_data(a)); if (unlikely(err)) /* skb already freed. */ return err; break; case OVS_ACTION_ATTR_POP_VLAN: err = pop_vlan(skb); break; case OVS_ACTION_ATTR_SET: err = execute_set_action(skb, nla_data(a)); break; case OVS_ACTION_ATTR_SAMPLE: err = sample(dp, skb, a); break; } if (unlikely(err)) { kfree_skb(skb); return err; } } if (prev_port != -1) { if (keep_skb) skb = skb_clone(skb, GFP_ATOMIC); do_output(dp, skb, prev_port); } else if (!keep_skb) consume_skb(skb); return 0;  flow的action都是透過nlattr來儲存，這是netlink相關的資料結構，因為 user space也會透過netlink的方式要求kernel直接處理封包，所以action都用 nlattr來處理因為可以 output到多個port去，每次都會需要複製 skb，所以這邊使用 prev_port來處理只有一次 output的情況(不用複製) ","version":"Next","tagName":"h3"},{"title":"Not found​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#not-found","content":"如果沒有找到該flow，kernel就會透過netlink的方式，把這個封包送到 ovs-vswitched去處理。  struct dp_upcall_info upcall; upcall.cmd = OVS_PACKET_CMD_MISS; upcall.key = &amp;key; upcall.userdata = NULL; upcall.portid = p-&gt;upcall_portid; ovs_dp_upcall(dp, skb, &amp;upcall);  struct dp_upcall_info { u8 cmd; const struct sw_flow_key *key; const struct nlattr *userdata; u32 portid; };  每個 dp_upcall_info都是透過 netlink的方式把資料送到 userspace,這邊要記錄資料設定完畢後， 呼叫 ovs_dp_upcall來處理 int ovs_dp_upcall(struct datapath *dp, struct sk_buff *skb, const struct dp_upcall_info *upcall_info) { struct dp_stats_percpu *stats; int dp_ifindex; int err; if (upcall_info-&gt;portid == 0) { err = -ENOTCONN; goto err; } dp_ifindex = get_dpifindex(dp); if (!dp_ifindex) { err = -ENODEV; goto err; } if (!skb_is_gso(skb)) err = queue_userspace_packet(ovs_dp_get_net(dp), dp_ifindex, skb, upcall_info); else err = queue_gso_packets(ovs_dp_get_net(dp), dp_ifindex, skb, upcall_info); if (err) goto err; return 0; err: stats = this_cpu_ptr(dp-&gt;stats_percpu); u64_stats_update_begin(&amp;stats-&gt;sync); stats-&gt;n_lost++; u64_stats_update_end(&amp;stats-&gt;sync); return err; }  檢查 porrid(port number)是否正常取得該 datapath的index根據有使用 gso的話，就會特別處理，因為封包的長度比較大，會透過多次的 queue_userspace_packet來處理支援 gso的封包。如果發生error，意味者這個封包就不會有人處理了，因此把lost的值增加 ","version":"Next","tagName":"h2"},{"title":"queue_userspace_packet​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2013/openvswitch-source-code3#queue_userspace_packet","content":" static int queue_userspace_packet(struct net *net, int dp_ifindex, struct sk_buff *skb, const struct dp_upcall_info *upcall_info) { struct ovs_header *upcall; struct sk_buff *nskb = NULL; struct sk_buff *user_skb; /* to be queued to userspace */ struct nlattr *nla; int err; if (vlan_tx_tag_present(skb)) { nskb = skb_clone(skb, GFP_ATOMIC); if (!nskb) return -ENOMEM; nskb = __vlan_put_tag(nskb, nskb-&gt;vlan_proto, vlan_tx_tag_get(nskb)); if (!nskb) return -ENOMEM; vlan_set_tci(nskb, 0); skb = nskb; } if (nla_attr_size(skb-&gt;len) &gt; USHRT_MAX) { err = -EFBIG; goto out; } user_skb = genlmsg_new(upcall_msg_size(skb, upcall_info-&gt;userdata), GFP_ATOMIC); if (!user_skb) { err = -ENOMEM; goto out; } upcall = genlmsg_put(user_skb, 0, 0, &amp;dp_packet_genl_family, 0, upcall_info-&gt;cmd); upcall-&gt;dp_ifindex = dp_ifindex; nla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY); ovs_flow_to_nlattrs(upcall_info-&gt;key, upcall_info-&gt;key, user_skb); nla_nest_end(user_skb, nla); if (upcall_info-&gt;userdata) __nla_put(user_skb, OVS_PACKET_ATTR_USERDATA, nla_len(upcall_info-&gt;userdata), nla_data(upcall_info-&gt;userdata)); nla = __nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, skb-&gt;len); skb_copy_and_csum_dev(skb, nla_data(nla)); err = genlmsg_unicast(net, user_skb, upcall_info-&gt;portid); out: kfree_skb(nskb); return err; }  這邊產生 generic netlink 然後把資料設定完畢後，就送出到 userspacegenlmsg_系列尚未完全瞭解，待補充 ","version":"Next","tagName":"h3"},{"title":"How to enable Ceph with RDMA","type":0,"sectionRef":"#","url":"/docs/techPost/2017/ceph-with-rdma","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#introduction","content":"RDMA (Remote Direct Memory Access) is a mechanism which allow the host to accessing(read, write) memory on a remote host without interrupting CPU.The advantage of RDMA Zero-copyKernel bypassNo CPU involvement` With RDMA, our data can transfer without the involvement of the linux kernel network stack and provide hight performance, low latency, low CPU consumption.This article focus on how to enable the ceph with RDMA, including how to install ceph and enable the RDMA function. ","version":"Next","tagName":"h2"},{"title":"Install​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#install","content":"I introduce two ways to install the ceph with RDMA, one is use widly used tool ceph-deploy and the other is manually build the ceph. ","version":"Next","tagName":"h2"},{"title":"ceph-deploy​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#ceph-deploy","content":"If you use the ceph-deploy to install the ceph, you must make sure the source package you installed is configure with -DWITH_RDMA=ON.You can use the argument --dev and -dev--commit to select the source packet form the official ceph build phase. you can find those avaliabe repos in the ceph sitechoose the one you want to install and clink it into the next page, you will see something like this Repos ceph &gt; wip-jd-testing &gt; da2c3dabdad80c01ec3d3258b51640cc0a93e842 &gt; defaultwip-jd-testing is for --dev and da2c3... is for --dev-commit.use the following command to install the ceph from above repos.  ceph-deploy install --dev=wip-jd-testing --dev-commit=da2c3dabdad80c01ec3d3258b51640cc0a93e842  ","version":"Next","tagName":"h3"},{"title":"Manually build​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#manually-build","content":"Refer to followings step to build the ceph with RDMA. cd ceph ./install-deps.sh ./do_cmake.sh -DWITH_RDMA=ON cd build time make -j54 sudo make install  You can add any other options in command do_cmake.shAnd than you should install the ceph to you environment and set up the monitor/osd by yourself. ","version":"Next","tagName":"h3"},{"title":"Enable RDMA​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#enable-rdma","content":"Before we enable the RDMA, there're something we need prepare for, including the RDMA environment, systemd config (if you need) and the ceph.conf Before we enable the RDMA, we must setup the RDMA environment, you should install the NIC driver and validate RDMA functionalities ","version":"Next","tagName":"h2"},{"title":"RDMA environment​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#rdma-environment","content":"I use the mellanox ConnectX-3 Pro in my environment and you can refer to HowTo Enable, Verify and Troubleshoot RDMAUse rdma tools to make sure your RDMA work well. ","version":"Next","tagName":"h3"},{"title":"Systemd config​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#systemd-config","content":"If you want to use the systemd to manage the ceph daemons, you should modify the systemd config to make it support RDMA because of the default config will fail for some access permission problem. You can wait the official PR and use the next version.Refer to this PR to modfiy the systemd config by yourself, and you can use systemctl reload the systemd config if you need. ","version":"Next","tagName":"h3"},{"title":"ceph.conf​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#cephconf","content":"Modify the ms_type to async+rdma, which tell the ceph use the AsyncMessenger + RDMA as your message type.You can use ms_cluster_type and ms_public_type to indicate the message type for your cluster network or public network.Use the command ibdev2netdev to get your device name and use it for ms_async_rdma_device_nameIf your want to use the port 2 in your NIC for RDMA, set the ms_async_rdma_port_num to 2.You can also use ms_async_rdma_buffer_size, ms_async_rdma_send_buffers and ms_async_rdma_receive_buffers to set the memory you want to allocate for RDMA. ms_async_rdma_send_buffers and ms_async_rdma_receive_buffers are how many work requestes for RDMA send/receive queue respectively.ms_async_rdma_buffer_size is the size os a single registered buffer.the total memory we allocate for each application is ms_async_rdma_buffer_size * (ms_async_rdma_send_buffers + ms_async_rdma_receive_buffers) and you can refer to here to know more about it. Example ceph.conf [global] ... ms_type=async+rdma ms_async_rdma_device_name=mlx4_0 ms_async_rdma_send_buffers=1024 ms_async_rdma_receive_buffers=1024 ...  Update the ceph.conf for each node and restart all daemons, after that, the ceph cluster will use the RDMA for all public/cluster network. If you want ot make sure the RDMA works, you can use the following method to dump the RDMA packet and use the wireshark to open it. 1. echo &quot;options mlx4_core log_num_mgm_entry_size=-1&quot; || sudo -a tee /etc/modprobe.d/mlx4.conf 2. sudo /etc/init.d/openibd restart 3. ibdump  ","version":"Next","tagName":"h3"},{"title":"Reference​","type":1,"pageTitle":"How to enable Ceph with RDMA","url":"/docs/techPost/2017/ceph-with-rdma#reference","content":"Mellanox-HowTo Configure Ceph RDMA ","version":"Next","tagName":"h2"},{"title":"Docker image for Hexo (一)","type":0,"sectionRef":"#","url":"/docs/techPost/2017/docker-build-image","content":"","keywords":"","version":"Next"},{"title":"Environment​","type":1,"pageTitle":"Docker image for Hexo (一)","url":"/docs/techPost/2017/docker-build-image#environment","content":"OS: Ubuntu 16.04Docker version 1.12.6, build 78d1802 ","version":"Next","tagName":"h2"},{"title":"Install​","type":1,"pageTitle":"Docker image for Hexo (一)","url":"/docs/techPost/2017/docker-build-image#install","content":"首先，我們要創建一個 Dockerfile，這個檔案除了用來描述該 Docker image的基本資訊外，也包含了當該Docker image被創建時，要執行些什麼指令，可以用來更新套件中心，安裝套件甚至是啟動特定服務都可以。 我們先從最基本的資訊開始。 透過 mkdir 創建一個資料夾，我們接下來要在此放 Dockerfile mkdir hexo_image  使用習慣的編輯器(vim/nano/ee) 開啟 Dockerfile cd hexo_image vim Dockerfile  接下來開始編輯內容，在該檔案內, #開頭的代表註解。 首先透過FROM來指示該 image 的環境，這邊採用的格式是 dis:version，舉例來說可以使用 ubuntu:16.04接下來透過 MAINTAINER 來說明該 image 是由誰維護的。 FROM ubuntu:16.04 MAINTAINER hwchiu(sppsorrg@gmail.com)  接下來我們可以執行一些指令，譬如將一些基本指令完畢，這樣每次創建該 image 時，就可以有一些基本的環境 RUN apt-get update RUN apt-get install -y net-tools RUN apt-get install -y git RUN apt-get install -y npm RUN apt-get install -y vim  ","version":"Next","tagName":"h2"},{"title":"Build​","type":1,"pageTitle":"Docker image for Hexo (一)","url":"/docs/techPost/2017/docker-build-image#build","content":"上述事情都完畢後，接下來我們要透過 docker build 此指令去建立該 image，在創建之前，我們可以先透過 docker images 觀看系統上目前擁有的 docker images 資訊 sppsorrg@ubuntu:~/hexo_image$ sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE  這邊總共有四行欄位,基本上其含義就如同其名稱一樣，我們在創建自己的 image 的時候，可以指定該 REPOSITORY 以及 TAG。 接下來就來使用 docker build 來使用剛剛的 Dockerfile docker build -t myhexo:latest .  這邊使用到了兩個參數，分別是 -t myhexo:latest 以及 .由指令說明可以知道 -t可以用來指示該 image 的名稱，分別對應到上述的 REPOSITORY 以及 TAG，而 . 則是說明想要使用的 Dockerfile 的位置在哪裡 -t, --tag value Name and optionally a tag in the 'name:tag' format (default [])  接下來就等待一段時間，當跑完後，就可以再度使用 docker images 觀看創建好的 images 資訊了。 sppsorrg@ubuntu:~/hexo_image$ sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE myhexo latest ef7f745a9126 43 minutes ago 481.7 MB  若想要刪除 images，可以透過 docker rmi $imput 來刪除，其中 imput 可以是 name:tag 或是 IMAGE ID 的形式。 ","version":"Next","tagName":"h2"},{"title":"Run​","type":1,"pageTitle":"Docker image for Hexo (一)","url":"/docs/techPost/2017/docker-build-image#run","content":"一切都創建完畢後，接下來就要透過 docker run 這個指令來執行剛剛創立的 docker image。 這邊我們會使用三個參數，分別是 --name,-i 以及 -t-name代表的是該創建出來的 container的名稱，之後 rm/stop等指令都可以使用此名稱操作，不然就要使用 containerID。-it則是將該 container的 stdin/stdout都導出到外面的 tty 上面，因為我這個 container 之後是想要可以進去進行 hexo 的編輯，所以這邊就將其導出來。 docker run --name hexo -it myhexo  如果想要觀察這些 container 的狀況，可以使用 docker ps -a的方式觀察。 ","version":"Next","tagName":"h2"},{"title":"Push​","type":1,"pageTitle":"Docker image for Hexo (一)","url":"/docs/techPost/2017/docker-build-image#push","content":"最後我們要將當前創好的 image 給上傳到 docker hub 上，請自行 往docker hub創建帳號並且設定好一個 repository。 這邊要注意的是，我們這邊是上傳剛剛創建好的 images 而並非後續運行的 container，所以若有任何步驟是在 container 內執行的，則上傳上去的 images 並不會有任何修改。 首先，先透過 docker login 登入到遠方的 docker hub。 由於遠方 docker hub 上還沒有任何 image, 所以我們這邊要主動上傳剛剛創造的 images接下來使用 docker tag 將剛剛創建的 images 指定到 docket hub上的帳號 sudo docker tag myhexo hwchiu/hexo  接下來就可以透過 docker images 觀察到系統上多了一個 images，其名稱是 hwchiu/hexo, 但是 image ID則會跟剛剛創立的一樣。 最後透過 docker push myhexo 則可以將該 images 給傳上 docker hub上的 repository 了。 所有步驟到這邊，已經可以創好一個簡單的 docker image，接下來就可以在 Dockerfile 內加入更多的指令，讓該 image 一創建時就有更完善的環境。 ","version":"Next","tagName":"h2"},{"title":"Install DRBD v9.0 on Ubuntu 16.04","type":0,"sectionRef":"#","url":"/docs/techPost/2017/drbd-9-0-install-on-ubuntu-16-04","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Install DRBD v9.0 on Ubuntu 16.04","url":"/docs/techPost/2017/drbd-9-0-install-on-ubuntu-16-04#introduction","content":"本篇文章主要講述如何再 Ubuntu 16.04 with kernel 4.4.3 的環境下安裝 drbd 9.0 並進行簡單的設定與操作。 ","version":"Next","tagName":"h3"},{"title":"Install​","type":1,"pageTitle":"Install DRBD v9.0 on Ubuntu 16.04","url":"/docs/techPost/2017/drbd-9-0-install-on-ubuntu-16-04#install","content":"這邊為了方便日後的研究，這邊安裝的方式是抓取 source code 下來，然後進行編譯安裝，由於 drbd v8.4.5 後將 module 以及相關的 utils 是分開在不同的 git repostory，所以我們會有兩個 project 來編譯及安裝。 首先到官網的 git 首頁可以看到滿滿的 projects，這邊我們會需要的兩個 project 分別是 drbd-9.0 以及 drbd-utils。 接下來就說明這兩個 project 要如何編譯及安裝 drbd-9.0​ 此 project 負責的是 kernel module部分，所以在編譯時會需要 kernel source 來編譯，如果你是正常安裝的 ubuntu 16.04，系統內應該都已經有 source 可以用了，這部分不太需要額外設定即可，若有特定的 kernel version 想要使用，則記得要先將該 kernel source 抓下來，然後編譯的時候指定特定的 kernel source 路徑即可。 這方面可以參考官方的文件說明 流程基本上就是 clone git projectbuildinstall 基本上此編譯此 project 的過程非常順利，再執行make完畢後，會顯示一段文字 我們可以知道若想要使用 drbd 9.0 的 kernel 版本，則我們的 drbd-utils 至少要 8.9.11 版本。  Module build was successful. ======================================================================= With DRBD module version 8.4.5, we split out the management tools into their own repository at http://git.linbit.com/drbd-utils.git (tarball at http://links.linbit.com/drbd-download) That started out as &quot;drbd-utils version 8.9.0&quot;, and provides compatible drbdadm, drbdsetup and drbdmeta tools for DRBD module versions 8.3, 8.4 and 9. Again: to manage DRBD 9 kernel modules and above, you want drbd-utils &gt;= 8.9.11 from above url. =======================================================================  最後執行 make install 將相關的 kernel module 給安裝到系統的路徑，然後透過檢視可以發現實際上安裝的 modules 有 drbd.ko 以及 drbd_transport_tcp.ko。 分別是整個 drbd 核心的部分，以及網路功能的部分，若是商業化版本還可以多看到 drbd_transport_rdma.ko 供 RDMA 使用。 整個步驟如下。 git clone http://git.drbd.org/drbd-9.0.git cd drbd-9.0 make make install  drbd-utils​ 此 project 提供 drbd user space 的所有工具，包含了 drbdadm, drbdsetup等常用工具。 基本上流程也是滿順利的 clone git projectautogenconfigurebuildinstall 透過 autogen.sh 產生好對應的 configure 檔案時，會有下列文字說明 suggested configure parameters: # prepare for rpmbuild, only generate spec files ./configure --enable-spec # or prepare for direct build ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc  這邊就建議依照他的說法去設定 configure，不然之後執行 drbdadm up resource 的時候會發現有些東西找不到，如果不想要建議舊版的 tools 的話，可以加上這兩個參數--without-83support 以及 --without-84support此外，如果最後再建置的時候發現 documentation/v9 一直建置不過，然後又不需要文件的話，可以加上下列參數 --without-manual 這邊要注意的就是在 make 的時候會需要 xsltproc 這個套件，所以若有發現錯誤顯示 xsltproc: command not found，則記得透過 apt-get install xsltproc 安裝該套件即可。 整個步驟如下。 git clone http://git.drbd.org/drbd-utils.git ./autogen.sh ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --without-83support --without-84support --without-manual make make install  ","version":"Next","tagName":"h3"},{"title":"Configure​","type":1,"pageTitle":"Install DRBD v9.0 on Ubuntu 16.04","url":"/docs/techPost/2017/drbd-9-0-install-on-ubuntu-16-04#configure","content":"drbd 使用 drbd.conf 來設定相關資訊，預設的存放位置是 /usr/local/etc/drbd.conf，若之前在 configure 時有透過 --sysconfdir=/etc，則該 configure 的預設位置是 /etc/drbd.conf。 這個 config 需要每一台要跑 drbd 的機器上都要有一份，所以當設定完畢後，請自行 copy 到另外一台。本文中假設有兩台機器，其 hostname 分別是 node-1 以及 node-2。 大致步驟如下 設定 /etc/hosts設定 config將 config 複製到所有機器 首先由於 drbd 設定 host的時候，會使用 hostname 去尋找對應的 host 欄位，所以建議先修改 /etc/hosts 將所有用到的 hostname 與其 ip 對應關係都寫上去。 加入下列資訊魚 /etc/hosts 10.0.0.15 node-1 10.0.0.16 node-2  接下來我們要設定 drbd.conf，假設我們要使用系統上的 /dev/nvme0n1 當作我們的 disk，提供出來的 block device 是 /dev/drbd0，則範例如下 global { usage-count no; } common { protocol C; } resource r0 { on node-1 { device /dev/drbd0; disk /dev/nvme0n1; address 10.0.0.15:7788; meta-disk internal; } on node-2 { device /dev/drbd0; disk /dev/nvme0n1; address 10.0.0.16:7788; meta-disk internal; } }  接下來可以透過 scp 之類的指令將該設定檔複製到另外一台 node-2，或是有任何方法都可以，只要確保兩台有一樣的資料即可。 ","version":"Next","tagName":"h3"},{"title":"Run​","type":1,"pageTitle":"Install DRBD v9.0 on Ubuntu 16.04","url":"/docs/techPost/2017/drbd-9-0-install-on-ubuntu-16-04#run","content":"設定檔都準備完成後，接下來要依賴 drbdadm 幫忙進行相關的設定 首先我們使用 drbdadm create-md 將該 resource 給建立起來，大概訊息如下 You want me to create a v09 style flexible-size internal meta data block. There appears to be a v09 flexible-size internal meta data block already in place on /dev/nvme0n1 at byte offset 400088453120 Do you really want to overwrite the existing meta-data? [need to type 'yes' to confirm] yes initializing activity log initializing bitmap (11924 KB) to all zero Writing meta data... New drbd meta data block successfully created.  接下來透過 drbdadm up r0 將整個 resource 運行起來，包含將 device bloack attach，建立網路連線等。 待node-1以及node-2執行好上述指令後，我們要將 node1 當作 primary，所以這時候再 node-1 上面執行 drbdadm primary r0 如此一來就會將 node-1上面的資料從給 mirror 到 node-2上了。 接下來應該可以透過下列指令觀察到一些狀態 drbdadm cstate r0 觀察網路連線狀態 drbdadm dstate r0 觀察 disk 的狀態 drbdadm status r0 觀察整體狀態，包含其他的node是 primary/secondary等 drbd-overview 顯示當前cluster內的狀態 drbdsetup status r0 --verbose --statistics 顯示當前 sync 統計資訊，譬如還有多少資料未sync r0 node-id:1 role:Primary suspended:no write-ordering:flush volume:0 minor:0 disk:UpToDate size:390699424 read:390700584 written:12390400 al-writes:2750 bm-writes:0 upper-pending:0 lower-pending:0 al-suspended:no blocked:no node-2 node-id:0 connection:Connected role:Secondary congested:no volume:0 replication:Established peer-disk:UpToDate resync-suspended:no received:0 sent:403089824 out-of-sync:0 pending:0 unacked:0  更多的指令用法可參考官方文件上的說明 ","version":"Next","tagName":"h3"},{"title":"Test​","type":1,"pageTitle":"Install DRBD v9.0 on Ubuntu 16.04","url":"/docs/techPost/2017/drbd-9-0-install-on-ubuntu-16-04#test","content":"為了確認是否真的有流量在兩個 node 之間運行，可以使用 dd 這個指令於 node-1上面去寫入資料，然後透過drbdsetup status r0 --verbose --statistics確認有產生大量的資料進行 sync dd if=/dev/zero of=/dev/drbd0 bs=1M count=10000  ","version":"Next","tagName":"h3"},{"title":"Trouble Shooting​","type":1,"pageTitle":"Install DRBD v9.0 on Ubuntu 16.04","url":"/docs/techPost/2017/drbd-9-0-install-on-ubuntu-16-04#trouble-shooting","content":"執行 drbdadm primary r0 出現錯誤 State change failed: (-2) Need access to UpToDate data 執行 drbdadm primary r0 --force 強迫蓋掉 ","version":"Next","tagName":"h3"},{"title":"Drbd Networking Structure Introduction","type":0,"sectionRef":"#","url":"/docs/techPost/2017/DRBD-networking-structure","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Drbd Networking Structure Introduction","url":"/docs/techPost/2017/DRBD-networking-structure#introduction","content":"本文主要分析 drbd 在 kernel space 中關於 networking 這一部分用到的所有資料結構，這些資料結構主要分成兩個部分，一部分是通用的，一部分則是 TCP 連線專用的 通用​ struct drbd_resourcestruct drbd_connectionstruct drbd_pathstruct drbd_listenerstruct drbd_transportstruct drbd_transport_opsstruct drbd_transport_class TCP專用​ struct drbd_tcp_transportstruct dtt_listenerstruct dtt_socket_containerstruct dtt_path ","version":"Next","tagName":"h3"},{"title":"Environment​","type":1,"pageTitle":"Drbd Networking Structure Introduction","url":"/docs/techPost/2017/DRBD-networking-structure#environment","content":"Drbd 9.0Using TCP as DRBD Transport 接下來為了比較好理解整個過程，會先介紹每個結構在做什麼事情，然後在闡述這些結構之間的關係。 ","version":"Next","tagName":"h3"},{"title":"分析​","type":1,"pageTitle":"Drbd Networking Structure Introduction","url":"/docs/techPost/2017/DRBD-networking-structure#分析","content":"架構解釋​ drbd_transport_class​ drbd_transport_class 用來紀錄 networking module 相關資訊，譬如該 module 的名稱， 該 kernel module的 init function，其中 instance_size 以及 path_instance_size 則是用來記錄該網路實作過程中，繼承自 drbd_tanasport_class以及drbd_path 那些物件真正的大小。 以 TCP 舉例來說，他設計了一個物件 drbd_tcp_transport，裡面包含了 drbd_transport_class 以及一些 TCP 會用到的變數，這些零零總總加起來的總大小就是此 TCP module 真正要用到的大小。 這邊會這樣設計的原因是因為在更上層要透過 kmalloc 去要空間的時候，需要計算真正用到的大小，如下列應用 3308 size = sizeof(*connection) - sizeof(connection-&gt;transport) + tc-&gt;instance_size;  0194 struct drbd_transport_class { 0195 const char *name; 0196 const int instance_size; 0197 const int path_instance_size; 0198 struct module *module; 0199 int (*init)(struct drbd_transport *); 0200 struct list_head list; 0201 };  drbd_transport_ops​ 這個結構用來定義所有跟網路相關的操作，如 connect, send 等。 每個要實作Networking Module的 kernel module都必須要實做這些功能，並且設定好對應的 function pointer。 在 drbd.ko中，就會透過 drbd_transport 的方式去存取到這些對應的操作來使用，譬如 0686 err = transport-&gt;ops-&gt;connect(transport);  0130 struct drbd_transport_ops { 0131 void (*free)(struct drbd_transport *, enum drbd_tr_free_op free_op); 0132 int (*connect)(struct drbd_transport *); 0133 ......... 0165 int (*recv)(struct drbd_transport *, enum drbd_stream, void **buf, size_t size, int flags); ........ 0179 int (*recv_pages)(struct drbd_transport *, struct drbd_page_chain_head *, size_t size); 0180 0181 void (*stats)(struct drbd_transport *, struct drbd_transport_stats *stats); 0182 void (*set_rcvtimeo)(struct drbd_transport *, enum drbd_stream, long timeout); 0183 long (*get_rcvtimeo)(struct drbd_transport *, enum drbd_stream); 0184 int (*send_page)(struct drbd_transport *, enum drbd_stream, struct page *, 0185 int offset, size_t size, unsigned msg_flags); 0186 int (*send_zc_bio)(struct drbd_transport *, struct bio *bio); 0187 bool (*stream_ok)(struct drbd_transport *, enum drbd_stream); 0188 bool (*hint)(struct drbd_transport *, enum drbd_stream, enum drbd_tr_hints hint); 0189 void (*debugfs_show)(struct drbd_transport *, struct seq_file *m); 0190 int (*add_path)(struct drbd_transport *, struct drbd_path *path); 0191 int (*remove_path)(struct drbd_transport *, struct drbd_path *path); 0192 };  drbd_transport​ 真正用來抽象整個 networking module 的結構，將上面提到的 drbd_transport_ops 以及 drbd_transport_class 收錄到此結構中，最外層的 drbd 透過此物件可以呼叫到當前 networking 的實作方法。 0103 struct drbd_transport { 0104 struct drbd_transport_ops *ops; 0105 struct drbd_transport_class *class; 0106 0107 struct list_head paths; 0108 0109 const char *log_prefix; /* resource name */ 0110 struct net_conf *net_conf; /* content protected by rcu */ 0111 0112 /* These members are intended to be updated by the transport: */ 0113 unsigned int ko_count; 0114 unsigned long flags; 0115 };  drbd_listener​ 接下來看到 drbd_listener，由於 DRBD 是分散式的架構，每個 host 同時是clinet也是 server，在扮演 server 的過程中，需要在本地上開啟一個 socket 並且透過 listen去處理該 socket 以接受之後的連線，這邊就用這個結構來儲存這相關的資訊。 就如同註解所說，這只是一個抽象概念而已，真正的實作則是依賴每個 networking model來處理，舉例來說， TCP module 則是包了一層 dtt_listener，裡面除了有最原始的 drbd_listener 之外，還放了 TCP server 使用的 listen socket。 0059 struct dtt_listener { 0060 struct drbd_listener listener; 0061 void (*original_sk_state_change)(struct sock *sk); 0062 struct socket *s_listen; 0063 0064 wait_queue_head_t wait; /* woken if a connection came in */ 0065 };  0204 /* An &quot;abstract base class&quot; for transport implementations. I.e. it 0205 should be embedded into a transport specific representation of a 0206 listening &quot;socket&quot; */ 0207 struct drbd_listener { 0208 struct kref kref; 0209 struct drbd_resource *resource; 0210 struct list_head list; /* link for resource-&gt;listeners */ 0211 struct list_head waiters; /* list head for paths */ 0212 spinlock_t waiters_lock; 0213 int pending_accepts; 0214 struct sockaddr_storage listen_addr; 0215 void (*destroy)(struct drbd_listener *); 0216 };  drbd_path​ drbd_path的概念與 drbd.conf 中設定檔的概念相同，用來描述兩個 host 之間的連線，主要內容是 ip(v4/v6):port，所以結構中會有 my_addr 以及 peer_addr，用來紀錄兩端點的位址。 由於每個 path 中，本地端不但要當 client 連過去，同時也要當 server 等待對方連線，因此 my_addr 就會拿來當作 listener 使用，所以可以看到該成員有一個指向 drbd_listener 的指標 listener。 0085 struct drbd_path { 0086 struct sockaddr_storage my_addr; 0087 struct sockaddr_storage peer_addr; 0088 0089 struct kref kref; 0090 0091 int my_addr_len; 0092 int peer_addr_len; 0093 bool established; /* updated by the transport */ 0094 0095 struct list_head list; /* paths of a connection */ 0096 struct list_head listener_link; /* paths waiting for an incomming connection, 0097 head is in a drbd_listener */ 0098 struct drbd_listener *listener; 0099 };  drbd_connection​ drbd_connection 的概念與 drbd.conf 中 connection的描述相同，相對於 path 來說是個更高一等的抽象概念，描述兩個 host 之間的連線， 這些連線是由很多個 path 所組成的所以一個 connection 可以有很多條 path 而connection 中間的傳輸會透過其中一條 path 來交換，對 connection 來說，同時只會使用一條 path 傳輸，並沒有辦法達到 link aggregation的功效。 下圖結構中可以觀察到有一個 transport，每條 connection 都會綁定一個 transport 的物件，該條 connection 的所有操作都依照該物件內容去執行，所以在此架構下，是可以做到每條 connection 使用不同的傳輸方法，譬如原生TCP或是自行實作的物件。 0904 struct drbd_connection { 0905 struct list_head connections; .................. 1047 1048 unsigned int peer_node_id; 1049 struct list_head twopc_parent_list; 1050 struct drbd_transport transport; /* The transport needs to be the last member. The acutal 1051 implementation might have more members than the 1052 abstract one. */ 1053 };  struct dtt_path​ 這邊除了將本來的 drbd_path 包起來外，還多了一個list來處理上述 dtt_socket_container 的物件，這邊目前沒有辦法理解為什麼需要用list來保存，我以為只需要用一個 socket 的物件就可以了。 0076 struct dtt_path { 0077 struct drbd_path path; 0078 0079 struct list_head sockets; /* sockets passed to me by other receiver threads */ 0080 };  struct dtt_socket_container​ 此物件還令人滿納悶的，目前還沒有想到什麼情況下會需要這個東西...，待之後突然領悟了再回來補足。 0071 struct dtt_socket_container { 0072 struct list_head list; 0073 struct socket *socket; 0074 };  struct dtt_listener​ 真正實作 drbd_listener 的物件，由於是走 TCP 的架構，所以需要一個 socket 來進行 listen 的動作。 其餘的成員 original_sk_state_change以及 wait 待詳細分析 tcp 程式碼時再說明。 0059 struct dtt_listener { 0060 struct drbd_listener listener; 0061 void (*original_sk_state_change)(struct sock *sk); 0062 struct socket *s_listen; 0063 0064 wait_queue_head_t wait; /* woken if a connection came in */ 0065 };  struct drbd_tcp_transport​ 這邊則是 TCP 方面對於 drbd_transport 的實現，由於要支援 DATA_STREAM 以及 CONTROL_STREAM，所以這邊每個 tcp_transport 則是會使用兩個 sockets 的物件來保存。 0051 struct drbd_tcp_transport { 0052 struct drbd_transport transport; /* Must be first! */ 0053 spinlock_t paths_lock; 0054 unsigned long flags; 0055 struct socket *stream[2]; 0056 struct buffer rbuf[2]; 0057 };  drbd_resource​ 在 kernel 層級這邊，最主要的管理者是 drbd_resource，他控管所有的資源，包含了 listener, connections等與網路相關的資訊。 可以看到 struct drbd_resource 中的成員有兩個 list_head 的成員，分別用來將 connection 以及 listener 給串起來。 connection(drbd_connection) 代表的就是每一對 host 的 connection，如同在 user-space 中 drbd.conf 中設定的那樣， 0820 struct drbd_resource { ... 0832 struct list_head connections; .... 0893 struct list_head listeners; ...  架構圖​ Connection and Path​ 抽象來說，每個 connection會擁有多條 path，但是實際上其實是每條 connection內的 drbd_transport 擁有多條 path。 為了容易理解，所以底下都使用 connection 取代 connection內部的 drbd_transport。 每一個 connection 會使用 double link list 的方式來維護多條 path，所以在 connetion 中會有一個 list head的物件來指向該 link list的第一個。 如下圖 Listener and Path​ 每一個 listener 都代表一個 listen socket，而不同的 path 則可以擁有相同的 listen socket，因為只會有其中一個真正被使用到，所以在架構上每個 listener 也會有一個 double link list 串起用到本身的 path。 如下圖 Resource and Conection/Listener​ Resource是最上層的物件，掌管所有的 connection，因此也會使用 double link list去掌管所有的 connections。 此外，為了在某些步驟能夠更快速的查找所有的 listener， resource 本身也用了一個 double link list 串起所有的 resource。 將上述這些所有結果都繪成圖片，並且將所有的 double link 都簡化成 single link 且透過不同的箭頭符號表現不同的 link type。則結果會如下圖。  ","version":"Next","tagName":"h3"},{"title":"結論​","type":1,"pageTitle":"Drbd Networking Structure Introduction","url":"/docs/techPost/2017/DRBD-networking-structure#結論","content":"drbd 9.0 之後為了支援 multiple path，在架構上有不小的改動，所以整個 path 相關的結構就變得複雜許多。本篇主要著重在抽象層的概念上，實作上還有許多小細節沒有提起，這些必須要到分析 TCP 模組實際上是怎麼運作時才有更深的理解與體悟。 ","version":"Next","tagName":"h3"},{"title":"docker image for lxr server","type":0,"sectionRef":"#","url":"/docs/techPost/2017/lxr-server-with-docker","content":"docker image for lxr server 之前曾經發過一篇文章LXR Server With Multiple Projects，主要介紹如何在 Ubuntu 上面安裝 lxr 並且支援多個 projects。由於整個 lxr 的安裝過程複雜，除了本體外還牽扯到不少第三方程式套件，如 perl, database, www server，且大部份都是安裝完畢後就再也不會更動，唯一的更動應該就是更換要被 indexing 的 project而已。 上述這種使用情境我覺得非常適合使用 docker 來建置一個 image， image 將所有相關的套件都全部安裝完畢，並且套用一個預設的設定檔當作基礎設定。 接下來每次運行的時候，只要將 source code 的位置以及相關的版本資訊都帶入到 docker run 的參數中，就可以跑起一個 www server 並且提供該 project 的網頁服務。 我將弄好的 docker image 放到 docker-lxr這邊，由於要支援多個 projects 上面，在 lxr.conf 中沒那麼好設定，同時又希望保留彈性，能夠在 docker run 的時候去處理，所以這邊先考慮 single project 的情況。 使用上只有幾個部分需要注意 source code 的位置，裡面每個版本都要有一個屬於自己的資料夾，且資料夾名稱就是版本號ip:port 對外使用的 ip:port 資訊。 假設我將 ceph code 分成三個版本，都放在 /tmp底下 &gt;ll /tmp total 12K drwxr-xr-x 23 root root 4.0K Jun 6 13:40 master drwxr-xr-x 23 root root 4.0K Jun 6 13:52 v12.0.3 drwxr-xr-x 23 root root 4.0K Jun 6 13:48 v9.2.1 且假設對外的 ip address 是 10.2.3.4 則使用方式很簡單 docker pull hwchiu/docker-lxr docker run --name lxr -it -v tmp:/source -p 800180 hwchiu/lxr:single 10.2.3.4 master v12.0.3 v9.2.1 更多的詳情可以參考 docker-lxr 或是 github/docker-lxr","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/techPost/2017/docker-hexo-ii","content":"Introduction 這篇延續上一篇Docker image for Hexo (一)，要使用上次的概念來打造一個屬於我自己的 hexo docker image，至於這邊為什麼是說屬於我自己的？ 目前網路上其實也有不少關於 hexo 相關的 docker image，在使用上大致上可以分成兩類 將 docker image 當成一次性的使用，可能提供了 setup local server 或者是 deploy to git 之類的服務 在這種架構下，通常都是把整個 blog 的 source 放在外面的 host 上，再透過 docker run 的時候，將這些檔案透過 volume的方式掛載到 container 內，然後 container 內就使用已經安裝好的 hexo 環境幫你產生一次性的 generate, deploy 之類的指令。這種 image 我覺得大概跟我差不多，都是為了練習而產生的，實際上使用沒有特別方便，原因在於 hexo 本身就是透過 npm 管理了，所有使用到的 modules 也都存放在自己本身的資料夾內，這種情況下根本沒有真的幫助使用者減少多少時間，畢竟 hexo 本身的安裝就一兩行就結束了。此外，這種模式最大的麻煩就是，因為你 hexo 的安裝都是在 image階段就結束了，你若有想要安裝額外的 npm modules 就會沒有辦法,所以其實使用上也不夠靈活 第二種就比較偏向將整個 source都放進去 image 內，然後提供不同的方法讓作者可以編輯文章並且 generate/deploy。 目前有看過的一種做法是讓他跟 github上面的 webhook 結合，然後當在 github編輯後，就會觸發 webhook，驅動 image內的 hexo 去進行 generate以及deploy。 在比較兩種類型後，我的使用情境比較會偏向第二種，不過第二種要弄得很完美其實苦工多非常多，否則其實就只是一個撰寫環境的打包 image。 不過也沒關係，就當作一個經驗練練 docker 也好。 由於我自己的這個專案只有打算給自己用，我主要想要省下的時間有 git/zsh 相關環境以及習慣的設定hexo 相關專案的抓取 所以我的 image 裡面做的事情大概如下 安裝相關套件(vim/zsh/node)clone blog-source安裝 hexo 這樣之後就可以進去直接該 container 並且在裡面進行 generate/deploy。 同時因為我的 hexo 是直接安裝我 blog-source 那一包檔案，因此我只要維護我自己 npm相關的檔案，image重新產生的時候裡面放的就會直接更新到最新的那一包 git repo了。 不過由於第二點的路徑是綁死在 image 內，所以這整個 docker image就當作一個自我對於 docker 的練習即可。 Steps 接下來就介紹一下我自己的 Dockerfile在這之前，我有把我常用的操作設定檔vim/git有開一個獨立的git repo，且我自己本身 blog 的 source 也有額外開一個 git repo，該repo內就是整個 hexo 的檔案，包含了 npm 安裝的資訊以及第三方 theme 的資訊。 首先，一開始先安裝會用到的套件，這邊包含了 vim/node/git。 RUN apt-get update &amp;&amp; \\ apt-get install -y git &amp;&amp;\\ apt-get install -y vim &amp;&amp;\\ apt-get install -y curl &amp;&amp;\\ curl -sL https://deb.nodesource.com/setup_6.x | bash - &amp;&amp; \\ apt-get install -y nodejs &amp;&amp;\\ npm install npm -g &amp;&amp;\\ apt-get clean &amp;&amp; \\ rm -rf /var/lib/apt/lists/* 接者就要先把自己常用的安裝檔案透過 git 的方式給抓下來 WORKDIR / RUN git clone https://xxxx/xxxxx/config.git &amp;&amp; \\ cp config/.gitconfig ~/ 再來我們就要處理 hexo，我希望將 hexo 給存放到 /hexo 這個路徑上，所以先透過 WORKDIR 切換當前位置。 接下來 透過 git clone 整個 hexo source repo透過 npm 安裝hexo-cli 這邊若安裝成全系統的-g會遇到一些錯誤，似乎跟當前執行者的身分有關，這邊就沒有花太多時間去研究之後為了處理這個問題，就把 hexo-cli/bin 加入倒當前 PATH 即可。 透過 npm 安裝相關套件 WORKDIR /Hexo RUN git clone --recursive https://xxxx/xxxx/xxx.git &amp;&amp; \\ cd blog-source &amp;&amp; \\ npm install hexo-cli &amp;&amp;\\ npm install &amp;&amp;\\ 然後處理 zsh/vim 的問題。 zsh 這邊想要使用 oh-my-zsh，安裝步驟參考這篇文章vim 的部分則是將之前上述步驟抓下來設定檔都複製到相關的位置 這邊需要 vim 原因是沒有調整過語系的 vim沒有辦法順利地開啟中文內容的檔案。 apt-get update &amp;&amp; \\ apt-get install -y zsh &amp;&amp;\\ apt-get install -y git-core &amp;&amp;\\ apt-get install -y wget &amp;&amp;\\ apt-get install -y tig &amp;&amp;\\ wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh || true &amp;&amp;\\ chsh -s `which zsh` &amp;&amp;\\ echo &quot;export PATH=$PATH:/Hexo/blog-source/node_modules/hexo-cli/bin/&quot; &gt;&gt; ~/.zshrc &amp;&amp;\\ mkdir ~/.vim &amp;&amp;\\ cp /config/vim/.vimrc ~/ &amp;&amp;\\ cp -r /config/vim/colors ~/.vim 最後，為了讓透過 docker run執行後，可以直接採用 zsh 當作進去的 shell，我們要透過 ENTRYPOINT去設定進入點，加入下列敘述於 Dockerfile 之中。 ENTRYPOINT [&quot;/usr/bin/zsh&quot;] 所以最後整個檔案的樣子大概就是類似 FROM ubuntu:16.04 MAINTAINER sppsorrg@gmail.com RUN apt-get update &amp;&amp; \\ apt-get install -y git &amp;&amp;\\ apt-get install -y vim &amp;&amp;\\ apt-get install -y curl &amp;&amp;\\ curl -sL https://deb.nodesource.com/setup_6.x | bash - &amp;&amp; \\ apt-get install -y nodejs &amp;&amp;\\ npm install npm -g &amp;&amp;\\ apt-get clean &amp;&amp; \\ rm -rf /var/lib/apt/lists/* WORKDIR / RUN git clone https://xxxx/xxxxx/config.git &amp;&amp; \\ cp config/.gitconfig ~/ WORKDIR /Hexo RUN git clone --recursive https://xxxx/xxxx/xxx.git &amp;&amp; \\ cd blog-source &amp;&amp; \\ npm install hexo-cli &amp;&amp;\\ npm install &amp;&amp;\\ apt-get update &amp;&amp; \\ apt-get install -y zsh &amp;&amp;\\ apt-get install -y git-core &amp;&amp;\\ apt-get install -y wget &amp;&amp;\\ apt-get install -y tig &amp;&amp;\\ wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh || true &amp;&amp;\\ chsh -s `which zsh` &amp;&amp;\\ echo &quot;export PATH=$PATH:/Hexo/blog-source/node_modules/hexo-cli/bin/&quot; &gt;&gt; ~/.zshrc &amp;&amp;\\ mkdir ~/.vim &amp;&amp;\\ cp /config/vim/.vimrc ~/ &amp;&amp;\\ cp -r /config/vim/colors ~/.vim #set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936 #set termencoding=utf-8 #set encoding=utf-8 ENTRYPOINT [&quot;/usr/bin/zsh&quot;] Usage 接下來要使用的話，先透過 docker build 產生對應的 image，最後使用 docker run運行對應的 container docker build -t hwchiu:test . docker run --name gg -it hwchiu:test 進去該 container 後，可以先到 /Hexo/blog-source 執行 hexo g 確定可以產生文章，且透過 vim觀看文章都沒問題後，就透過 ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; 產生對應的 key接者將上述產生的 public key送到 GITHUB 去，這樣就可以使用 hexo deploy 此功能了。 這邊是因為我的 git 是採用 https的方式去抓的，所以這邊要去特別設定。 Summary 經由本次一個簡單的練習，將常用的 hexo 的環境給打包起來，雖然不算完美，在編輯文章上還是要透過別的方法先撰寫好，在透過 vim 貼到 hexo 內來產生跟發布。也許哪天有機會再來嘗試改善，看看是否可以把hexo-editor整合進去，這樣該 container 也可以提供一個外部的網頁服務，直接編輯內部的 hexo project。 常用指令 docker imagesdocker rm i xxxdocker stop docker ps -q -ldocker rm docker ps -q -ldocker run -it -name xxx xxx:xxx","keywords":"","version":"Next"},{"title":"[Switchdev] How Kernel Implement SwitchDev(ii)","type":0,"sectionRef":"#","url":"/docs/techPost/2016/switchdev-iii","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#introduction","content":"本篇文章主要會專注於 switchdev 本身的實作上，包含了其結構以及提供的 API 等。 ","version":"Next","tagName":"h2"},{"title":"Structure​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#structure","content":"","version":"Next","tagName":"h2"},{"title":"Transaction​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#transaction","content":"struct switchdev_trans_item { struct list_head list; void *data; void (*destructor)(const void *data); }; struct switchdev_trans { struct list_head item_list; bool ph_prepare; }; static inline bool switchdev_trans_ph_prepare(struct switchdev_trans *trans) { return trans &amp;&amp; trans-&gt;ph_prepare; } static inline bool switchdev_trans_ph_commit(struct switchdev_trans *trans) { return trans &amp;&amp; !trans-&gt;ph_prepare; }  Switchdev 實作了一種 trans 的機制，對於 hardware switch 進行一些寫入的動作，如 set/add 時，該動作會被拆成兩部份，分別是 prepare/commit 兩部分。一開始會先將 ph_prepare 給設定為 true，然後寫入的資料傳給 driver，讓 driver 知道這次的寫入只是用來確認可行性而已，如果 driver 確定可以寫入後，會將 ph_prepare 變為 false 後，再次要求 driver 將真正的資料給寫入。 ","version":"Next","tagName":"h3"},{"title":"Attribute​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#attribute","content":"enum switchdev_attr_id { SWITCHDEV_ATTR_ID_UNDEFINED, SWITCHDEV_ATTR_ID_PORT_PARENT_ID, SWITCHDEV_ATTR_ID_PORT_STP_STATE, SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS, SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME, SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING, };  此 enum 用來定義 switch attribute 的種類，當 switch driver 收到一些如 get 的指令時，會根據該 attribute的種類回傳特定資料 struct switchdev_attr { struct net_device *orig_dev; enum switchdev_attr_id id; u32 flags; union { struct netdev_phys_item_id ppid; /* PORT_PARENT_ID */ u8 stp_state; /* PORT_STP_STATE */ unsigned long brport_flags; /* PORT_BRIDGE_FLAGS */ u32 ageing_time; /* BRIDGE_AGEING_TIME */ bool vlan_filtering; /* BRIDGE_VLAN_FILTERING */ } u; };  實際上用來紀錄 switch attribute 的結構 net_device 來記錄是哪個目標 deviceid 如前述所說的種類flags 目前有三種值 #define SWITCHDEV_F_NO_RECURSE BIT(0) #define SWITCHDEV_F_SKIP_EOPNOTSUPP BIT(1) #define SWITCHDEV_F_DEFER BIT(2)  u 則是用來存放該 id 所代表的值 ","version":"Next","tagName":"h3"},{"title":"Object​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#object","content":"enum switchdev_obj_id { SWITCHDEV_OBJ_ID_UNDEFINED, SWITCHDEV_OBJ_ID_PORT_VLAN, SWITCHDEV_OBJ_ID_IPV4_FIB, SWITCHDEV_OBJ_ID_PORT_FDB, SWITCHDEV_OBJ_ID_PORT_MDB, };  此 enum 用來記錄該 switch object 的種類， struct switchdev_obj { struct net_device *orig_dev; enum switchdev_obj_id id; u32 flags; };  此結構記錄 type， net_device 以及 flag，對應種類的數值因為太過於廣泛，所以此 structure 會再被其他的結構給包起來。 struct switchdev_obj_port_vlan { struct switchdev_obj obj; u16 flags; u16 vid_begin; u16 vid_end; }; struct switchdev_obj_ipv4_fib { struct switchdev_obj obj; u32 dst; int dst_len; struct fib_info fi; u8 tos; u8 type; u32 nlflags; u32 tb_id; }; struct switchdev_obj_port_fdb { struct switchdev_obj obj; unsigned char addr[ETH_ALEN]; u16 vid; u16 ndm_state; }; struct switchdev_obj_port_mdb { struct switchdev_obj obj; unsigned char addr[ETH_ALEN]; u16 vid; };  由上面可以觀察到，目前已經實作了四種的 switchdev obj，分別是 vlan 的設定， L2 的 FDB/MDB 以及 L3 的 FIB. ","version":"Next","tagName":"h3"},{"title":"Operation​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#operation","content":"/** * struct switchdev_ops - switchdev operations * * @switchdev_port_attr_get: Get a port attribute (see switchdev_attr). * * @switchdev_port_attr_set: Set a port attribute (see switchdev_attr). * * @switchdev_port_obj_add: Add an object to port (see switchdev_obj_*). * * @switchdev_port_obj_del: Delete an object from port (see switchdev_obj_*). * * @switchdev_port_obj_dump: Dump port objects (see switchdev_obj_*). */ struct switchdev_ops { int (*switchdev_port_attr_get)(struct net_device *dev, struct switchdev_attr *attr); int (*switchdev_port_attr_set)(struct net_device *dev, const struct switchdev_attr *attr, struct switchdev_trans *trans); int (*switchdev_port_obj_add)(struct net_device *dev, const struct switchdev_obj *obj, struct switchdev_trans *trans); int (*switchdev_port_obj_del)(struct net_device *dev, const struct switchdev_obj *obj); int (*switchdev_port_obj_dump)(struct net_device *dev, struct switchdev_obj *obj, switchdev_obj_dump_cb_t *cb); };  此結構被加入到 struct net_device內，所以 hardware switch driver 在創建 net_divce 時，要順便對該結構進行初始化，這樣對應的 function pointer 才有辦法在適當的時機被執行，這部分可以參考 rocker driver。 dev-&gt;switchdev_ops = &amp;rocker_port_switchdev_ops;  ","version":"Next","tagName":"h3"},{"title":"Notifier​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#notifier","content":"enum switchdev_notifier_type { SWITCHDEV_FDB_ADD = 1, SWITCHDEV_FDB_DEL, }; struct switchdev_notifier_info { struct net_device *dev; }; struct switchdev_notifier_fdb_info { struct switchdev_notifier_info info; /* must be first */ const unsigned char *addr; u16 vid; };  Notifier 是用來讓 hardware switch 通知 linux kernel 用的，目前只有實作 FDB 的部分。 當 hardware switch 的 FDB offload 有變化時(ADD/DEL)，要透過這個方式一路通知道 linux kernel 去，這樣的話使用 brctl show 指令去看的時候，就可以看到即時的狀態變化。 ","version":"Next","tagName":"h3"},{"title":"Implementation​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#implementation","content":"","version":"Next","tagName":"h2"},{"title":"SwitchDev Port Attribute​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#switchdev-port-attribute","content":"int switchdev_port_attr_get(struct net_device *dev, struct switchdev_attr *attr); int switchdev_port_attr_set(struct net_device *dev, const struct switchdev_attr *attr);  這兩個 function 是用來處理 attribute 的，其處理邏輯類似，基本上都按照下列走法 const struct switchdev_ops *ops = dev-&gt;switchdev_ops; if (ops &amp;&amp; ops-&gt;switchdev_port_attr_get) return ops-&gt;switchdev_port_attr_get(dev, attr); if (attr-&gt;flags &amp; SWITCHDEV_F_NO_RECURSE) return err; netdev_for_each_lower_dev(dev, lower_dev, iter) { /* do something */ }  先判斷該 device 是否有實作 switchdev_ops,若有的話則直接呼叫 fptr 來處理. 參考 rocker driver. static const struct switchdev_ops rocker_port_switchdev_ops = { .switchdev_port_attr_get = rocker_port_attr_get, .switchdev_port_attr_set = rocker_port_attr_set, } ... dev-&gt;switchdev_ops = &amp;rocker_port_switchdev_ops; ...  判斷該 device 是否有被設定不需要遞迴往下尋找，若有的話則直接結束因為 switch port 可能是屬於 bond/team/vlan 等 device 底下，所以若直接操作上層的 device 是沒有辦法碰到 switch port 的，這邊會使用 netdev_for_each_lower_dev 來嘗試抓取到底下所有的 device。 對於 get/set 來說，會針對底下每個 device 嘗試去 get/set 其 attribute. ","version":"Next","tagName":"h3"},{"title":"SwitchDev Port Object operation​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#switchdev-port-object-operation","content":"int switchdev_port_obj_add(struct net_device *dev, const struct switchdev_obj *obj); int switchdev_port_obj_del(struct net_device *dev, const struct switchdev_obj *obj); int switchdev_port_obj_dump(struct net_device *dev, struct switchdev_obj *obj, switchdev_obj_dump_cb_t *cb);  這三個 function 都是用來處理 object 的，其運作邏輯也類似 const struct switchdev_ops *ops = dev-&gt;switchdev_ops; if (ops &amp;&amp; ops-&gt;switchdev_port_obj_add) return ops-&gt;switchdev_port_obj_add(dev, obj, trans); netdev_for_each_lower_dev(dev, lower_dev, iter) { /* do something */ }  先檢查該 device 是否有實作 switchdev_ops,若有就呼叫對應的 function 來處理遞迴存取底下所有的 device (bond/team/vlan), 針對每個 device 都跑一次對應的結果。obj_dump 的部分還會傳入一個 call back function, 目前看到的只有兩個實作，分別是 switchdev_port_obj_dump 以及 switchdev_port_vlan_dump_cb。 兩者都要搭配另外一個 `switchdev_port_xxx_dump** 的結構來使用，目前感覺用途不是很 general. fdb 的 dump 與 rfnetlink 有關係，要搭配 ndo_fdb_dump 使用。user space tool 透過 netlink 來問 fdb 的資料時，會透過此 cb 將對應的內容填入到 netlink header 中，最後再一路送回 user space 去檢查。 vlan 的部分則是 rfnetlink 在使用的，會先呼叫到 ndo_bridge_getlink, 最後跑到 ndo_dflt_bridge_getlink 才會使用，ndo (network device operation) 的 netlink 操作有必要再多花一些時間去瞭解了。 ","version":"Next","tagName":"h3"},{"title":"Port Bridge​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#port-bridge","content":"int switchdev_port_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq, struct net_device *dev, u32 filter_mask, int nlflags); int switchdev_port_bridge_setlink(struct net_device *dev, struct nlmsghdr *nlh, u16 flags); int switchdev_port_bridge_dellink(struct net_device *dev, struct nlmsghdr *nlh, u16 flags);  這三個 function 是用來操作 bridge port attribute 的，基本上都是被設定成 ndo_bridge_xxx 的 handler。 目前可以參考的範例應該是使用 br 這個與 ip 類似的 user-space tool. 詳細說明可以參考此 link ","version":"Next","tagName":"h3"},{"title":"FDB Operations​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#fdb-operations","content":"int switchdev_port_fdb_add(struct ndmsg *ndm, struct nlattr *tb[], struct net_device *dev, const unsigned char *addr, u16 vid, u16 nlm_flags); int switchdev_port_fdb_del(struct ndmsg *ndm, struct nlattr *tb[], struct net_device *dev, const unsigned char *addr, u16 vid); int switchdev_port_fdb_dump(struct sk_buff *skb, struct netlink_callback *cb, struct net_device *dev, struct net_device *filter_dev, int idx);  這三個 function 是用來操作 fdb 的，當上層走 rtnetlink 中的 ndo_fdb_xxx type 時，就會觸發對應的 function handler，這些 function 最後都會呼叫到對應的 switchdev_port_obj_xxx。 關於整個 FDB 的操作，可以用下列這張圖來總結 藍線代表的是 Notifer，當 Rocket 在 FDB 有任何變更時，會一路通知到 Linux Kernel 去，以確保 FDB 資料一致。圖中紅線代表的是走 ndo 系列的 netlink event，會直接跟 Rocker 溝通，因此透過 br 此指令去修改 FDB entry時，會先走紅線到 Rocker 去，接者走藍線去通知 Kernel 同步 FDB。當透過brctl指令去操作時，這邊目前能做的只有部分 attribute/obj 的修改，如 STP 的狀態，此時則會一路從 switchdev 的核心傳到 Rocker 去處理。基本上 MDB 的操作則簡單很多，與 FIB 比較類似。 ","version":"Next","tagName":"h3"},{"title":"FIB Operations​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#fib-operations","content":"int switchdev_fib_ipv4_add(u32 dst, int dst_len, struct fib_info *fi, u8 tos, u8 type, u32 nlflags, u32 tb_id); int switchdev_fib_ipv4_del(u32 dst, int dst_len, struct fib_info *fi, u8 tos, u8 type, u32 tb_id); void switchdev_fib_ipv4_abort(struct fib_info *fi);  這三個 function 是用來操作 IPv4 FIB offload 的，不同於 FDB，此 offload rule 本身的學習只能靠 linux kernl 來管理，當 kernel 決定要針對特定 FIB route 處理時，會呼叫上述的 add/del 將相關的 FIB router 給加入到 hardware switch 中。 err = switchdev_fib_ipv4_add(key, plen, fi, new_fa-&gt;fa_tos, cfg-&gt;fc_type, cfg-&gt;fc_nlflags, tb-&gt;tb_id); if (err) { switchdev_fib_ipv4_abort(fi); kmem_cache_free(fn_alias_kmem, new_fa); goto out; }  當執行失敗的時候，會呼叫 abort 將 rules 給全部清空，並且將 IPv4 offload 給關閉 這部分還有待加強，由註解也可以看出來  /* There was a problem installing this route to the offload * device. For now, until we come up with more refined * policy handling, abruptly end IPv4 fib offloading for * for entire net by flushing offload device(s) of all * IPv4 routes, and mark IPv4 fib offloading broken from * this point forward. */  而目前在加入 rules 的部分，也有針對條件去篩選，並非所有的 FIB 都會被加入 #ifdef CONFIG_IP_MULTIPLE_TABLES if (fi-&gt;fib_net-&gt;ipv4.fib_has_custom_rules) return 0; #endif if (fi-&gt;fib_net-&gt;ipv4.fib_offload_disabled) return 0;  關於整個 FIB 的操作，可以用下列這張圖來總結 相對於 FDB，非常的簡單，只有 kernel 主動去加入 Rocker 而已目前 ndo_xxx_ooo 系列的操作中，還沒有看到 FIB 相關的，大部分都是 bridge/vlan/macvlan 等。 ","version":"Next","tagName":"h3"},{"title":"Notifier​","type":1,"pageTitle":"[Switchdev] How Kernel Implement SwitchDev(ii)","url":"/docs/techPost/2016/switchdev-iii#notifier-1","content":"int register_switchdev_notifier(struct notifier_block *nb); int unregister_switchdev_notifier(struct notifier_block *nb); int call_switchdev_notifiers(unsigned long val, struct net_device *dev, struct switchdev_notifier_info *info);  基本上就是用讓 hardware switch driver 呼叫的，當 switchdevb 有任何更動需要讓上層知道時就會呼叫 call_switchdev_notifiers，此時所有透過 register_switchdev_notifier 註冊的 handler 都會去處理目前有透過的 register_switchdev_notifier 註冊的只有 bridge(br.c), 目的是用來同步 FIB 資訊。 ","version":"Next","tagName":"h3"},{"title":"NAT Lookback Introduction","type":0,"sectionRef":"#","url":"/docs/techPost/2017/nat-loopback","content":"","keywords":"nat loopback","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"NAT Lookback Introduction","url":"/docs/techPost/2017/nat-loopback#introduction","content":"首先，假設有一個以下的網路環境，我們在 Router 後面設置了兩台 機器，一台是 Web Server，另外一台則是一般的 PC。 由於該 PC 跟該 Web Server 都屬於同一個網域且都在 Router 底下，因此兩台機器之間若要透過 IP addresss 來傳輸基本上沒有太多問題。  但是外網的機器想要存取該 Web Server 的話，由於 Web Server 本身的 IP address 屬於 Private Network，譬如192.168.0.0/16這個範圍內。 因此外網的機器本身並沒有辦法直接存取到該 Web Server，但是若我們能夠將封包送到前面的 Router，再透過某種方式告訴 Router 說這個封包不是給你的，請幫我往下轉發給底下的 Web Server，則封包就可以很順利的到達 Web Server 去，一切的連線就順利完成。 上述行為裡面最重要的部分就是如何讓 Router 知道什麼樣的封包要送給底下的 Web Server，一般來說都會採用 DNAT (Destination NAT)的做法。Router 本身指定一個 Port Number，當看到封包是這個 Port 的時候，就會將封包轉送到底下的 Web Server，並且將封包內容修改讓 Web Server 能夠處理該封包。 舉例來說，假設我們在 Router 上面放一條 DNAT 的規則 1.2.3.4:8001 ---&gt; 192.168.1.5:80  對於 Router 來說，當看到封包的 ip:port 是 1.2.3.4:8001，則會將封包標頭改成 192.168.1.5:80，然後依照本機端內的 route rules 將其轉發到底下的 Web Server 去。 所以假設今天外網的機器(9.8.7.6)發送了一個封包，其流向是9.8.7.6:1234 ---&gt; 1.2.3.4:8001當 Router 收到此封包後，就會將其轉換成9.8.7.6:1234 ---&gt; 192.168.1.5:80 當 Web Server 收到此封包後，會有一個回應的封包，此封包的流向是192.168.1.5:80 --&gt; 9.8.7.6:1234當此封包到達 Router 後， Router 會先查詢看看這個封包是不是經過上述規則轉換的，若是的話就將封包內容重新轉成（進來的封包轉換其 Destination, 回去的封包轉換其 Source)1.2.3.4:8001 --&gt; 9.8.7.6:1234 這樣外網的機器 (9.8.7.6) 就可以很順利跟內網內的 Web Server 溝通了。 上述的這個行為有些會稱 Port Forwarding，有些會稱 Virtual Server，不論怎麼稱呼，其背後的意義都相同。 然而在真實的環境中，我們通常不會去死記這些 IP address，我們會使用 DNS 的服務來幫這些 IP address 設定一組好記的名稱，舉例來說可以設定 webserver.com 指向 1.2.3.4。 在這種情況下，外面機器想要存取該 webserver 的流程就會是 外網機器(9.8.7.6)想要存取 webserver.com，因此向 DNS server 詢問其對應的 IP addressDNS server 回應 webserver.com 就是 1.2.3.4，因此外網機器接下來會發送封包到 1.2.3.4封包到達 1.2.3.4 後，根據 DNAT 的規則轉送到底下真正的 web server。底下的 web server 回送封包，透過 1.2.3.4 送回到外網機器(9.8.7.6) 其流程可以用下列兩張圖來說明 ","version":"Next","tagName":"h3"},{"title":"NAT Loopback​","type":1,"pageTitle":"NAT Lookback Introduction","url":"/docs/techPost/2017/nat-loopback#nat-loopback","content":"假設我們都已經瞭解上述的概念後，接下來我們將該外網電腦()的角色給放到同樣區網內(192.168.1.6)來看，基本上 NAT Loopback 代表的涵意就是讓內網的機器能夠遵循原本的流程去存取內網的機器。 在這種情況下，若內網的機器想要依循上述的流程運行 首先內網機器 (192.168.1.6) 透過 DNS 的服務，得到 webserver.com 指向 1.2.3.4接下來將封包送往到 1.2.3.4，遇到 DNAT 後將封包轉換 所以假設今天內部機器(192.168.1.6)發送了一個封包，其流向是192.168.1.6:1234 ---&gt; 1.2.3.4:8001當 Router 收到此封包後，就會將其轉換成192.168.1.6:1234 ---&gt; 192.168.1.5:80當 web server 收到封包後就會回應一個封包，該封包透過 Router 就會依循上述的模式回到內網的機器(192.168.1.6)。 上述的流程看起來是順利也沒有問題的，但是有時候實體網路環境中，可能這些機器(PC,Server)是接在同一台 switch 底下，譬如下列這種情況， 或是 Router 內含 Hardware L2 switch。 在這種環境下，上述的流程會變成下列情況，並且產生一個問題 DNS 的部分沒有問題，可以正常運作內網的機器封包可以順利到達 web server當 web server 收到請求並且將封包送回去時 這時候的封包標頭檔可能是192.168.1.5:80 ---&gt; 192.168.1.6:12344.當封包到達switch時，就會發現這是個同網段的封包，所以就直接幫他回傳給內網機器 192.168.1.6了 5.當內網機器收到這個封包時，就會感受到一臉困惑。 一開始送出去的封包是192.168.1.6:1234---&gt; 1.2.3.4:8001所以期望收回到的封包應該是1.2.3.4:8001 ---&gt; 192.168.1.6:1234所以當他看到不符合期望的封包標頭時，就會將其丟棄192.168.1.5:80 ---&gt; 192.168.1.6:1234 整個流程如下圖所示 這邊最大的問題就是 web server 送回去的封包必須要先給 Router 將其根據 DNAT 的規則給重新反轉一次。 但是在此環境下，因為中間有一台 switch 存在，所以封包就沒有送回到 router 那邊去處理而是直接送回去給內網機器了。 若要能夠處理上述的情況，我們就必須要想辦法將封包也送回到 router 端去處理，為了達到這個目的我們可以在 router 也採用 SNAT (Source NAT) 規則大概如下，只要是從某個 interface 近來的，就將此封包標頭內的 Source IP Address 變成 192.168.1.1。 in_interface = xxxx, source ip = 192.168.1.1:xxxx  至於實際上要採用 Masquerade 或是 SNAT 來決定怎麼轉換 Source IP 都可以。 因此，目前的設定中，Router同時會進行 SNAT 以及 DNAT，因此假設內網機器(192.168.1.6)要對 1.2.3.4:80進行存取。 接下來以下圖來解釋每個步驟中封包的變化。藍色區域 1,2: 192.168.1.6:1234 -&gt; 1.2.3.4:8001接下來封包會進入 router，執行 SNAT/DNAT3,4: 192.168.1.1:5678 -&gt; 192.168.1.5:80 當封包到達 web server後，接下來 web server 會回傳一個封包回去 1,2: 192.168.1.5:80 --&gt; 192.168.1.1:5678當封包到達 switch 時，查了一下目的地是 192.168.1.1,因此就會幾該封包送回到 router 去處理。 當封包到達 router 時，會根據之前的記錄瞭解該封包有使用過 SNAT 以及 DNAT，因此會將封包標頭給重新修改 3,4: 1.2.3.4:8001 --&gt; 192.168.1.6:1234 當內網機器(192.168.1.6)收到此封包後因為與預期的相同，所以就可以正確地建立起連線並且開始傳輸。 到這邊我們已經完成了最基本的 NAT Loopback，基本上大部分的情況都可以依照這種思路來完成。 當然若是你網路中間有遇到一些 Hardware 會幫你偷偷做事情的，那你的封包可能就會被影響導致整個傳輸都出問題，這邊要特別小心。 ","version":"Next","tagName":"h3"},{"title":"Linux Kernel trobule shooting​","type":1,"pageTitle":"NAT Lookback Introduction","url":"/docs/techPost/2017/nat-loopback#linux-kernel-trobule-shooting","content":"前面講了這麼多話之後，我們來看看實際操作上可能會遇到的問題。 以下列這張圖為範例  為了簡化問題，我們假設 router 含有八個實體連接埠，其中第一個連接埠跟底下的switch有連結。 假設這一台 Router 我們系統中有透過 Linux bridge 創建了一個 bridge br0，然後我們幫八個連接埠都接到該 br0底下，其中第一個連接埠對應到系統上的 interface 是 eth0所以這時候大概可以看到如下面的架構 br0: eth0 eth1 ... eth8  在這種情況下，剛剛上述 NAT Loopback 的封包會遇到一問題。 當內網機器的封包送到 router時，會先透過 eth0進入到系統後到達 br0，接下來進行 SNAT 以及 DNAT 的處理。 然後最後封包又要從 br0 往 eth0 出去，一切的料想都是如此美好。 然而實際上就會發現封包不見了!! 根據 Linux kernel 3.6 source code，當系統底下的 bridge 再轉發封包的時候，會呼叫到 br_forward 去處理。 /* called with rcu_read_lock */ void br_forward(const struct net_bridge_port *to, struct sk_buff *skb, struct sk_buff *skb0) { if (should_deliver(to, skb)) { if (skb0) deliver_clone(to, skb, __br_forward); else __br_forward(to, skb); return; } if (!skb0) kfree_skb(skb); }  /* Don't forward packets to originating port or forwarding diasabled */ static inline int should_deliver(const struct net_bridge_port *p, const struct sk_buff *skb) { return (((p-&gt;flags &amp; BR_HAIRPIN_MODE) || skb-&gt;dev != p-&gt;dev) &amp;&amp; p-&gt;state == BR_STATE_FORWARDING); }  上面程式碼有一個最重要的地方skb-&gt;dev != p-&gt;dev，如果當前封包進入的 bridge port 跟出去的 bridge port 是一樣的話，那就不會轉發，導致這個封包被丟棄了... 可是在當前的網路拓墣中你就是要這個封包去轉發，所以可以觀察到上述程式碼還有一個關鍵點(p-&gt;flags &amp; BR_HAIRPIN_MODE)， 根據這篇 patch, 只要針對 interface 去啟用 hairpin_mode 就可以讓封包順利從同個點進出來回了。 但是事情依然沒有這樣簡單，這樣完畢後封包的 IP 的確都有正確的修改了，但是在 MAC Address 的部分有點問題，Source MAC沒有如預期的被修改，所以這邊又要依賴另外一個工具 ebtables 來進行 MAC 的修改，再者種情況下，封包就可以順利通過了。 因此我們的 Router 就有三種設定 1.打開 hairpin mode2.執行 iptables 的 SNAT/DNAT(改 IP) 3.透過 ebtables 的 SNAT (改 MAC) 後來發現網路上也有其他人遇到一樣的問題，該使用者因為沒有辦法針對 user-space 去進行修改，所以只能從 kernel 內進行一些小部分的修改，希望可以處理這個問題 這邊可以參考這個 patch在這個 patch 中，該程式碼會先針對有進行 DNAT 的封包進行標記，然後在 bridge forward 的過程中，將該封包的 Source MAC 進行修改，最後再讓該封包通過往下轉發。 ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"NAT Lookback Introduction","url":"/docs/techPost/2017/nat-loopback#summary","content":"其實上述的問題一些家用 router 不會遇到的一個原因是 kernel 太舊了，就如同該 patch 所說, 於 2.6.35 後的系統就會有這樣的問題存在，有些家用 router 的 kernel 還在 2.6.x 然後沒有追上新的，因此剛好逃過此問題。 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2017/ovs-dpdk-docker","content":"Preface 最近在操作 OpenvSwitch 時遇到了一些問題，由於此問題實在過於有趣，所以決定寫下這篇文章來記錄此問題。 此問題會牽扯到三個元件，分別是 OVS, DPDK 以及 Docker，首先來看一下網路拓墣，如下。 Topology 首先，準備好兩台機器，其中一台機器為 Linux Based 機器，在其環境中安裝了 OpenvSwitch 2.8Docker + ubuntu imageNIC (Support DPDK)DPDK 17.05.2 此環境中，我們透過 ovs-vsctl 創造了一個 userspace mode 的 ovs switch(datapath), 其命名為 ovs_br。 然後我們透過 docker 與 pipework 在系統上開啟了兩個基本的 ubuntu container, 並且將這兩個 container 都掛到 ovs_br 上。 這兩個 container 裡面都有一張網卡 eth0、ip 地址分別是 10.55.66.8 以及 10.55.66.7。 然後將 ovs_br 設定為 10.55.66.1 最後再將系統上的網卡以DPDK的形式連接到 ovs_br 上。 接下來我們透過一般的 switch 將該 Linux Based 的機器與一般的 server相連，該 server 的IP 設定為 10.55.66.10。 整個網路環境就上圖所呈現。 所以目前網路中有四個元件可以用來進行操作，分別是兩台 Container， Linux Host 本體，以及外面的那台 server。 Scripts 首先我們使用ping 指令來測試網路狀況，沒意外的任意兩台機器都能夠順利的連接到對方。然而接下來使用TCP作為測試時，卻發現了一些詭異的情況。 在上述四種狀況中，總共有六種組合可以測試，然而其中卻只有一種組合能夠讓 TCP/UDP 順利連接成功。此組合就是 Linux Host 配上 外面那台 server。 其他組合完全沒有辦法建立起一條 TCP Connection。 為了解決此問題，我們使用了下列方式釐清問題所在 tcpdump 擷取封包，以 TCP 為例，發現只要是 Container 內部傳送回來的封包，雖然透過 tcpdump 可以看到該封包回來，但是另外一端的應用程式都不會將該封包收上去處理。因此對於 TCP 來說沒有辦法建立起一個正常的連線。懷疑是 container 的問題，所以手動用 ip netns 創建了兩個 namespace, 結果有一樣的問題。懷疑是 OVS 的問題，因此將 OVS 切換成 Linux bridge(DPDK也取消使用)，結果問題就順利解決了，一切網路都通了。懷疑是 Userspace OVS 的問題，因此切換成 kernel module mode(DPDK取消)，結果問題也是順利解決了。 最後經過網路上不停的搜查，我們找到了一篇文章在講類似的問題connection-issue-between-docker-container-and-other-machine 最後嘗試透過該文章所提到的方法，我們到 Container 裡面透過ethtool將其對外網卡eth0相關的 TX/RX offloading 功能關閉，果然功能就一切正常了。 為了釐清這個這個功能為什麼會對網路造成這些影響，我們重新利用tcpdump擷取封包，並且使用 wireshark 來觀察，發現了一個有趣的事情。 以下好所有圖示中的ＩＰ位置可能與上述拓墣不同，不過重點不在那邊，所以忽略即可。 首先，我們可以看到當網路不通時 wireshark 的解析，如下圖可以看到對於 TCP 連線來說，其實只有 SYN 以及 SYN/ACK，主要是 Client 端沒有將該 SYN/ACK 給收起來，最後導致一連串的重送。 所以接下來針對 SYN 以及 SYN/ACK 兩個封包去看看。 這兩個封包最有趣的地方就是，其 TCP 標頭檔怎麼看都不一樣，結果 Checksum 卻完全一樣，這邊看起來就是有問題。馬上懷疑是這邊造成收端沒有辦法收起對應的 SYN/ACK。 於是接下來馬上去試試看正常的 TCP 連線。 結果如同預期般，不同封包因為 TCP 標頭檔不同，其 Checksum 也不同。 所以整個問題就是 Container 內的 TX/RX offload 造成了其 Checksum 出現問題導致無法讓封包正確被處理。 而且此問題只會出現在 OVS+DPDK 上。 所以我又跑回去翻了一下最初 OVS+DPDK 相關的程式碼，在最初版本的 netdev-dpdk.c 中有提到 +Restrictions: +------------- + + - This Support is for Physical NIC. I have tested with Intel NIC only. + - vswitchd userspace datapath does affine polling thread but it is + assumed that devices are on numa node 0. Therefore if device is + attached to non zero numa node switching performance would be + suboptimal. + - There are fixed number of polling thread and fixed number of per + device queues configured. + - Work with 1500 MTU, needs few changes in DPDK lib to fix this issue. + - Currently DPDK port does not make use any offload functionality. 其中的 Currently DPDK port does not make use any offload 其中的段話讓我滿好奇的，但是在最新 OVS 2.8 中該敘述也已經不見了, 可能此限制也已經排除。所以我們為什麼會遇到這個問題，暫時還沒有頭緒，等有時間時再來細追看看，不然就先去 ovs-dicuss 那邊發問一下好了。 Reference First release of netdev-dpdkconnection-issue-between-docker-container-and-other-machine","keywords":"","version":"Next"},{"title":"CORD-Trellis Example","type":0,"sectionRef":"#","url":"/docs/techPost/2017/onos-trllis-testing","content":"CORD-Trellis Example Trellis 是 ONF 於 2017 年推出的 Network Architecture Solution，整個架構就於 ONOS SDN Controller 以及 Openflow-Enable switch. 本篇文章主要是在於如何透過 ONF 提供個環境快速搭建一個 Trellis 的測試環境。 Introduction 關於 Trellis 相關 script 的專案真正的位置是放在 ONF 內部的 Project，不過你也可以在 Github看到 Mirror 的版本。 在該 README 有提到該如何架設一個測試環境，因此本篇文章的內容就會基於該 README 去架設一個測試環境。 Environment 基本上該測試環境是基於 mininet 與 ONOS 來部署的，所以在機器的數量上面，最少需要一台機器，最多沒有限制,因為 ONOS 本身可以是可以同時跑起多台的 SDN Controller，可以架設一個 Cluster 的環境然後與 Mininet 互連。 不過為了方便測試，並沒有需要架設到這麼多的 ONOS SDN Controller。 因此在本環境中，決定採用兩台機器即可。 所需環境 Two Ubuntu Machine One for ONOS Controller (VM1)One for Mininet emulator (VM2) 於 VM1 上面請先安裝好 ONOS Controller, 詳細的安裝步驟可以參考 ONF 本身的 wiki, 這邊可以分成使用者跟開發者兩種運行方法，若是開發者本身會要你抓取 ONOS 的 source code，並且透過 buck 來進行建置與運行。此外，也可以直接使用 ONOS docker image 來運行 ONOS。於 VM2 上面安裝 Ubuntu 16.04 的環境，接下來就可以參考 README 的步驟來設置。 Steps VM1 (ONOS)​ 為了更放便控制於 ONOS 上面運行的 application，可以透過環境變數 ONOS_APPS 直接設定要運行的 apps。透過下列指令控制要運行的 app. export ONOS_APPS=drivers,openflow,segmentrouting,fpm,dhcprelay,netcfghostprovider,routeradvertisement 運行起 ONOS Controller另外開視窗，運行 ONOS Cli 工具，透過 apps -a -s 檢查運行的 apps 是否與上述吻合下載 trellis 相關設定檔案，並且透過下列工具將該設定檔寫入到 ONOS 中，其中 onos-ip 則是本機端的 IP address (此 IP 要讓 VM2 能夠存取得到) onos-netcfg &lt;onos-ip&gt; routing/trellis/trellis.json VM2 (Mininet)​ 安裝相關軟體 sudo apt-get update sudo apt-get install -y gawk texinfo python-pip build-essential iptables automake autoconf libtool sudo pip install -U pip sudo pip install ipaddress sudo apt-get install isc-dhcp-server sudo apt-get install mininet 安裝完畢後，魷魚 Trellis 架構內支援使用 Quagga 來當外部 BPG 溝通的橋樑，因此我們需要在本機上安裝 Quagga。這邊要特別注意，在預設情況下, quagga 本身會期望運行的使用者名稱為 quagga，同時你也要幫--sysconfdir以及--localstatedir 這兩個位置的資料夾設定全縣，讓 quagga 此使用者有權限可以寫入。若是單純測試的話，可以在 configure 的時候加入兩個選項 --enable-user=root --enable-group=root, 這樣 Quagga 相關應用程式就會採用 root 的身份去運行了。 git clone -b onos-1.11 https://gerrit.opencord.org/quagga cd quagga ./bootstrap.sh ./configure --enable-fpm --sbindir=/usr/lib/quagga make sudo make install cd .. 接下來要修改本地端的檔案，讓我們的 mininet/Zebra 相關的應用程式能夠跟 ONOS 連接得到，所以請修改下列兩個檔案 routing/trellis/trellis.pyrouting/trellis/zebradbgp1.conf 於 routing/trellis/trellis.py，請將下列三行指令中的後面兩行，並且將第一行指令中的 IP 換成 VM1 的 IP net.addController(RemoteController('c0', ip='&lt;onos-ip&gt;')) #net.addController(RemoteController('c1', ip='192.168.56.12')) #net.addController(RemoteController('c2', ip='192.168.56.13')) 於 routing/trellis/zebradbgp1.conf 裡面將 IP 的部分也換成 VM1 的 IP fpm connection ip &lt;onos-ip&gt; port 2620 接下來要將系統上 kernel 的保護機制 app armor 給關閉，執行下列指令 ln -s /etc/apparmor.d/usr.sbin.dhcpd /etc/apparmor.d/disable/ apparmor_parser -R /etc/apparmor.d/usr.sbin.dhcpd 最後要運行時，切換到 trellis 資料夾，執行 sudo ./trellis.py 即可以運行起 mininet 腳本，並且透過 ONOS GUI 可以觀察到大量的 Switch/Host 存在。透過下列指令三個指令都可以成功運行並且有回應 (須等待 dhcp 拿到 IP) mininet$: h1 ping h4 mininet$: h1 ping 10.0.99.2 mininet$: h1v6 ping6 2000::9902 如果要結束整個模擬環境，可以透過下列指令將所有相關的 Process/Daemon 給移除 sudo killall -9 dhclient dhcpd zebra bgpd sudo mn -c ","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/techPost/2017/ceph-network-iii","content":"","keywords":"","version":"Next"},{"title":"EventCallBack​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#eventcallback","content":"當事件發生時，要怎處理的介面雛形，所有的 CallBack 都必須要繼承此介面同時實作 do_request， 此 function 會得到一個 input，告知哪一個 fd 有事件發生了，然後實作對應的事情即可該 fd 到底是 read 還是 write 被呼叫，這個 EventCallBack 本身不處理，此邏輯交給 EventCenter 去處理，所以若你的 CallBack 要依據 read/write 有不同處理，請註冊兩種不同的 callBack 來使用 0054 class EventCallback { 0055 0056 public: 0057 virtual void do_request(int fd_or_id) = 0; 0058 virtual ~EventCallback() {} // we want a virtual destructor!!! 0059 };  ","version":"Next","tagName":"h2"},{"title":"EventDriver​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#eventdriver","content":"此物件是底層每個方法都需要實現的介面，基本上跟Event有關的操作都在這邊完成，譬如哪些fd要監聽，哪些不用，然後監聽結果為何等。 0068 /* 0069 * EventDriver is a wrap of event mechanisms depends on different OS. 0070 * For example, Linux will use epoll(2), BSD will use kqueue(2) and select will 0071 * be used for worst condition. 0072 */ 0073 class EventDriver { 0074 public: 0075 virtual ~EventDriver() {} // we want a virtual destructor!!! 0076 virtual int init(EventCenter *center, int nevent) = 0; 0077 virtual int add_event(int fd, int cur_mask, int mask) = 0; 0078 virtual int del_event(int fd, int cur_mask, int del_mask) = 0; 0079 virtual int event_wait(vector&lt;FiredFileEvent&gt; &amp;fired_events, struct timeval *tp) = 0; 0080 virtual int resize_events(int newsize) = 0; 0081 virtual bool need_wakeup() { return true; } 0082 };  ","version":"Next","tagName":"h2"},{"title":"init​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#init","content":"進行一些初始化的動作，以 select 為範例，就會將 read/write 的 FD 都初始化 ","version":"Next","tagName":"h3"},{"title":"add_event​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#add_event","content":"加入一個新的 fd 到需要觀察的清單中，而 mask 則有三種類型，分別是初始化/可讀/可寫。 0048 #define EVENT_NONE 0 0049 #define EVENT_READABLE 1 0050 #define EVENT_WRITABLE 2  以 select 為範例，此 function 就是將該 fd 加入到 fd set 中，同時更新當前最大 fd 數值 ","version":"Next","tagName":"h3"},{"title":"del_event​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#del_event","content":"將一個目標 fd 從觀察清單中移除，譬如當連線斷線後，就不需要在監聽此事件。以 select 為範例，就是使用 FD_CLR 將該 fd 清除 ","version":"Next","tagName":"h3"},{"title":"event_wait​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#event_wait","content":"這邊是真正重要的地方，此 function 會必須要真正去得到有哪些fd有對應的 event 產生，然後將這些 event收集起來。呼叫此 function 時，必須要傳入一個含有 FiredFileEvent 的 vector以及 timeout 的時間若這次等待中，有任何 event 被觸發，就將該 fd 放到該 vector 中即可。 FiredFileEvent 包含兩個成員，一個是發生事件的 fd，以及其對應的 mask (read/write) 0063 struct FiredFileEvent { 0064 int fd; 0065 int mask; 0066 };  ","version":"Next","tagName":"h3"},{"title":"resize_event​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#resize_event","content":"目前只有 kqueue 在使用，當加入新的 fd 超過當前容納數量，會透過此 function 去更新 ","version":"Next","tagName":"h3"},{"title":"need_wakup​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#need_wakup","content":"預設是回傳 True，目前只有 DPDK 有重新定義，原因是因為 DPDK 跟其他三者 event-based 的模式不相同，主要是一直依賴 polling 的方式去問。 -這邊比較有趣的是，DPDK 這種 polling-based 的也一併放到event的架構中實作，不過因為還沒有看完DPDK的實作，還沒看清他是如何轉換這兩邊概念的。 ","version":"Next","tagName":"h3"},{"title":"EventCenter​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#eventcenter","content":"此介面是用來給上層使用的，在這邊將Event分成三大類，分別是 read/write call-back eventtime event (多少ms後執行）external event (馬上執行，可以想成時間為 0 的 time event) 0087 class EventCenter { 0088 public: ...... 0102 struct FileEvent { 0103 int mask; 0104 EventCallbackRef read_cb; 0105 EventCallbackRef write_cb; 0106 FileEvent(): mask(0), read_cb(NULL), write_cb(NULL) {} 0107 }; 0108 0109 struct TimeEvent { 0110 uint64_t id; 0111 EventCallbackRef time_cb; 0112 0113 TimeEvent(): id(0), time_cb(NULL) {} 0114 }; 0115 ..... 0178 int process_time_events(); ..... 0201 // Used by internal thread 0202 int create_file_event(int fd, int mask, EventCallbackRef ctxt); 0203 uint64_t create_time_event(uint64_t milliseconds, EventCallbackRef ctxt); 0204 void delete_file_event(int fd, int mask); 0205 void delete_time_event(uint64_t id); 0206 int process_events(int timeout_microseconds); 0207 void wakeup();  此物件內部還有在包含一個 Poller 的物件，主要是給DPDK使用此外，針對非DPDK需要 wakeup 類型的 driver，如 EPOLL/KQUEUE/SELECT，實作了一個 read/write 的通知事件，為了避免 driver 卡在 wait 事件中，會透過 pipe 於本地創立一個專用的 FD，針對其 read fd 創造一個簡單的 read handler，單純讀取而已。之後就透過一個只會寫入特定字元的 write event 使得該 driver 能夠從 wait 事件中出來。 0038 class C_handle_notify : public EventCallback { 0039 EventCenter *center; 0040 CephContext *cct; 0041 0042 public: 0043 C_handle_notify(EventCenter *c, CephContext *cc): center(c), cct(cc) {} 0044 void do_request(int fd_or_id) override { 0045 char c[256]; 0046 int r = 0; 0047 do { 0048 r = read(fd_or_id, c, sizeof(c)); 0049 if (r &lt; 0) { 0050 if (errno != EAGAIN) 0051 ldout(cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read notify pipe failed: &quot; &lt;&lt; cpp_strerror(errno) &lt;&lt; dendl; 0052 } 0053 } while (r &gt; 0); 0054 } 0055 };  0315 void EventCenter::wakeup() 0316 { 0317 // No need to wake up since we never sleep 0318 if (!pollers.empty() || !driver-&gt;need_wakeup()) 0319 return ; 0320 0321 ldout(cct, 2) &lt;&lt; __func__ &lt;&lt; dendl; 0322 char buf = 'c'; 0323 // wake up &quot;event_wait&quot; 0324 int n = write(notify_send_fd, &amp;buf, sizeof(buf)); 0325 if (n &lt; 0) { 0326 if (errno != EAGAIN) { 0327 ldout(cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; write notify pipe failed: &quot; &lt;&lt; cpp_strerror(errno) &lt;&lt; dendl; 0328 ceph_abort(); 0329 } 0330 } 0331 }  ","version":"Next","tagName":"h2"},{"title":"init​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#init-1","content":"此 function 會決定底層要跑哪種driver，主要會基於參數type以及當前系統的平台，Linux優先走Epoll, FreeBSD則是Kqueue,兩種都不符合的話就走select。 接下來呼叫該 driver的 init。 如果該 dirver 需要 wakeup (目前是除了DPDK以外) 透過 pipe 創建一對 local fd並且設定為 non-blocking之後的 read/write notifier 會透過這對 fd 來傳輸。 ","version":"Next","tagName":"h3"},{"title":"destructor​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#destructor","content":"執行所有的 external events 並且從 queue 中移除。若之前有透過 pipe 創立 local fd，將其關閉移除 driver 以及供 wakup 使用的 notify_handler ","version":"Next","tagName":"h3"},{"title":"set_owner​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#set_owner","content":"讓該 eventCenter 記住擁有的 thread 是誰 此變數主要會給 in_thread 這隻 function 來比較當前呼叫 event 的人是否是其owner。 創立一個(或是回傳已經存在的)全域的 event center 此 global_cenets 是 AssociatedCenters 的物件裡面會存放 id 與 thread 的 mapping 關係主要是搭配 submit_to 使用，可以讓任意的人使用 submit_to 搭配特定的 id 馬上塞入一個 event 到對應的 thread 中 0095 struct AssociatedCenters { 0096 EventCenter *centers[MAX_EVENTCENTER]; 0097 AssociatedCenters(CephContext *c) { 0098 memset(centers, 0, MAX_EVENTCENTER * sizeof(EventCenter*)); 0099 } 0100 };  若當前 driver 需要 wakeup，則創立一個 read evnet handler 0196 if (driver-&gt;need_wakeup()) { 0197 notify_handler = new C_handle_notify(this, cct); 0198 int r = create_file_event(notify_receive_fd, EVENT_READABLE, notify_handler); 0199 assert(r == 0); 0200 }  ","version":"Next","tagName":"h3"},{"title":"create_file_event​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#create_file_event","content":"必須是同個 thread 才能夠透過此 function 來加入 event。若傳入的 fd 大小超過當前的 fd上限，則透過 driver-&gt;resize_events 來調整透過 _get_file_event(fd) 取得當前 FD 對應的 FileEvent 此物件主要記錄當前 FD 對應的 mask 0063 struct FiredFileEvent { 0064 int fd; 0065 int mask; 0066 };  若該 fd 以前就有 event，且其 mask 跟這次要加入的 mask 相同，那代表沒有任何改變，沒有必要繼續往下執行，故直接跳掉接下來透過 driver-&gt;add_event 去創立該 event 也有可能是修改已經存在 event 的 mask (read/write) 最後透過 mask 的數值設定其 read_cb/write_cb 對應的 event handler 0237 event-&gt;mask |= mask; 0238 if (mask &amp; EVENT_READABLE) { 0239 event-&gt;read_cb = ctxt; 0240 } 0241 if (mask &amp; EVENT_WRITABLE) { 0242 event-&gt;write_cb = ctxt; 0243 }  ","version":"Next","tagName":"h3"},{"title":"delete_file_event​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#delete_file_event","content":"跟 create_file_event 類似若該 fd 超過目前已知FD的上限，代表有問題，輸出 log 並離開透過 _get_file_event(fd) 取得該 fd 對應的 FileEvent若對應的 mask 是0，代表此 fd 還沒有設定過任何的 event handler，所以不需要刪除，可以直接離開呼叫 driver-&gt;del_event 刪除該 evnet移除對應的 call back，並且修改該 event 的 mask 0269 if (mask &amp; EVENT_READABLE &amp;&amp; event-&gt;read_cb) { 0270 event-&gt;read_cb = nullptr; 0271 } 0272 if (mask &amp; EVENT_WRITABLE &amp;&amp; event-&gt;write_cb) { 0273 event-&gt;write_cb = nullptr; 0274 } 0275 0276 event-&gt;mask = event-&gt;mask &amp; (~mask);  ","version":"Next","tagName":"h3"},{"title":"create_time_event​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#create_time_event","content":"透過 clock_type::now() 加上傳入的 microseconds 計算出 expire 的時間點 使用 clock_type::time_point 當作該 event 要 expire 的時間點 透過 multimap 記住每個 time_point 對應的 event Handler std::multimap&lt;clock_type::time_point, TimeEvent&gt; time_events 最後用一個 map 記住當前 id　對應上述 multimap 紀錄 id 則是用一個 global 的 time_event_next 來記住 0288 clock_type::time_point expire = clock_type::now() + std::chrono::microseconds(microseconds); 0289 event.id = id; 0290 event.time_cb = ctxt; 0291 std::multimap&lt;clock_type::time_point, TimeEvent&gt;::value_type s_val(expire, event); 0292 auto it = time_events.insert(std::move(s_val)); 0293 event_map[id] = it;  ","version":"Next","tagName":"h3"},{"title":"delete_time_event​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#delete_time_event","content":"這邊是根據 id 來刪除對應的 time event 0305 auto it = event_map.find(id); 0306 if (it == event_map.end()) { 0307 ldout(cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; id=&quot; &lt;&lt; id &lt;&lt; &quot; not found&quot; &lt;&lt; dendl; 0308 return ; 0309 } 0310 0311 time_events.erase(it-&gt;second); 0312 event_map.erase(it);  ","version":"Next","tagName":"h3"},{"title":"wakeup​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#wakeup","content":"判斷需不需要 wakeup 若走DPDK，則需要看 pollers 是否空的，若非空則往下走其餘總類都必須要 wakeup 藉由寫入之前的 notify_send_fd 來叫起整 event_waitDPDK 的部分必須要再研究，私以為DPDK不應該下來，因為其 notify_send_fd 應該不會被初始化。 ","version":"Next","tagName":"h3"},{"title":"process_time_events​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#process_time_events","content":"處理所有的 time events對於所有的 time event，如果當前的時間超過該 time event 的 expire time 將該 event 從結構中移除呼叫該 event 的 call back function這邊傳入的是 ID，跟 FD 無關 回傳這次總共處理了多少個 event ","version":"Next","tagName":"h3"},{"title":"process_events​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#process_events","content":"該 function 會傳入一個變數timeout_microseconds，這是給 driver 用的 timeout符合下列情況，將 timeout 設定為0，這會讓 driver 變成 non-blocking 存在外部 event存在 poller 否則，計算一下 timeout 的時候，讓 driver 會 block 一段時間透過 driver-&gt;evnet_wait 去詢問當前有哪些 event ready 了 0394 vector&lt;FiredFileEvent&gt; fired_events; 0395 numevents = driver-&gt;event_wait(fired_events, &amp;tv); 0396 for (int j = 0; j &lt; numevents; j++) { .... 0419 }  對於所有被觸發的 event, 呼叫對應的 read/write handler 若某個 FD同時可以進行 read/write 且對應的 call back handler 是相同的，則執行完 read 後，就不執行 write了 (不確定原因)&quot;可能&quot;是避免重複執行相同內容，因為 function 沒有辦法知道當前被叫起是 read or write event ready。 0405 if (event-&gt;mask &amp; fired_events[j].mask &amp; EVENT_READABLE) { 0406 rfired = 1; 0407 cb = event-&gt;read_cb; 0408 cb-&gt;do_request(fired_events[j].fd); 0409 } 0410 0411 if (event-&gt;mask &amp; fired_events[j].mask &amp; EVENT_WRITABLE) { 0412 if (!rfired || event-&gt;read_cb != event-&gt;write_cb) { 0413 cb = event-&gt;write_cb; 0414 cb-&gt;do_request(fired_events[j].fd); 0415 } 0416 }  嘗試執行 time_events，並且繼續記錄總共處理的 event 數量若當前 external queue 內有東西，則將全部都執行完畢若到現在都還沒有執行任何 event 且也不是 blocking mode，則呼叫 pollers去 polling 對應的 event，並且記錄下來總數最後回傳總數量 0421 if (trigger_time) 0422 numevents += process_time_events(); 0423 0424 if (external_num_events.load()) { 0425 external_lock.lock(); 0426 deque&lt;EventCallbackRef&gt; cur_process; 0427 cur_process.swap(external_events); 0428 external_num_events.store(0); 0429 external_lock.unlock(); 0430 while (!cur_process.empty()) { 0431 EventCallbackRef e = cur_process.front(); 0432 ldout(cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; do &quot; &lt;&lt; e &lt;&lt; dendl; 0433 e-&gt;do_request(0); 0434 cur_process.pop_front(); 0435 numevents++; 0436 } 0437 } 0438 0439 if (!numevents &amp;&amp; !blocking) { 0440 for (uint32_t i = 0; i &lt; pollers.size(); i++) 0441 numevents += pollers[i]-&gt;poll(); 0442 } 0443 0444 return numevents;  ","version":"Next","tagName":"h3"},{"title":"dispatch_event_external​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#dispatch_event_external","content":"將 event 存放到 external_events 內看看 external_num_events 這個變數是不是0，若是0則代表可以 wake external_num_events 是個 atomic 類型的變數用此變數來控管當前是否正在清除 external_queue 內的 event 若符合下列條件，則呼叫 wake 將 event 叫起來 caller 的 thread 跟真正擁有此 eventCenter 的 thread 是相同前述的 external_num_events 決定當前需要 0450 external_events.push_back(e); 0451 bool wake = !external_num_events.load(); 0452 uint64_t num = ++external_num_events; 0453 external_lock.unlock(); 0454 if (!in_thread() &amp;&amp; wake) 0455 wakeup();  ","version":"Next","tagName":"h3"},{"title":"submit_to​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/ceph-network-iii#submit_to","content":"此 function 可以將特定的 function 傳入給特定的 eventCenter，走 external event 的方式去執行這邊的 event 會被包裝成 C_submit_event 這邊 do_eqeust 會透過 condition_variable 與 wait來溝通 0217 class C_submit_event : public EventCallback { 0218 std::mutex lock; 0219 std::condition_variable cond; 0220 bool done = false; 0221 func f; 0222 bool nonwait; 0223 public: 0224 C_submit_event(func &amp;&amp;_f, bool nw) 0225 : f(std::move(_f)), nonwait(nw) {} 0226 void do_request(int id) override { 0227 f(); 0228 lock.lock(); 0229 cond.notify_all(); 0230 done = true; 0231 bool del = nonwait; 0232 lock.unlock(); 0233 if (del) 0234 delete this; 0235 } 0236 void wait() { 0237 assert(!nonwait); 0238 std::unique_lock&lt;std::mutex&gt; l(lock); 0239 while (!done) 0240 cond.wait(l); 0241 } 0242 };  若 caller 跟該 EventCenter 屬於同個 thread，就直接執行了，不再塞到 exterbnl_queue接下來根據變數中的 no_wait來決定要不要等待該 function 執行完畢假如是 no_wait，則丟到 external_queue 後就直接離開假如是 wait,則丟到 external_queue 後，馬上呼叫 wait()，等待 do_request()執行完畢後，會使得 wait 結束，然後離開此 function。 Summary 整個 Event 系列的檔案其實不會太困難，除了 DPDK 本身還有額外的實作外，其餘 POSIX 系列的三種只要熟悉本身的用法，來看這些程式碼就不會覺得太陌生，所有的運作都可以想像的到，同時也在預料之中。 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2017/ovs-dpdk-docker-2","content":"Preface 此文章主要接續前篇文章 OVS + DPDK + Docker 共同玩耍 進行後續探討。 根據 Jalen Lin 提出的一篇文章 Linux kernel bug delivers corrupt TCP/IP data to Mesos, Kubernetes, Docker containers，其中的原因似乎可以用來解釋此問題。 因此本篇文章會分成兩個部分，第一部份是先針對上述文章進行探討，第二部分則是將第一部分的結果與先前經驗去結合，來追出更深層的問題所在。 Problems Linux kernel bug delivers corrupt TCP/IP data to Mesos, Kubernetes, Docker containers 該篇文章中提到他們使用 docker 配上 veth 一起使用時，會發現 TCP 的連線有機率會不通。 veth 是用來將 Docker/Container 與 Host 本身串接的一種方法，每個 veth 都有兩個端點，從一端點進去的封包，就會從另外一個端點出來。因此透過這個技術就能夠讓封包在 Host 與 Docker/Container 之間傳遞。 詳細檢查分析後，發現當出現問題時，該 TCP 封包的 Checksum 是錯誤的，所以才會導致另外一端的應用程式沒有辦法收起該封包。 與我前述文章相同的是，只要將 Docker/Container 內的網卡相關的 TX/RX offloading 的功能關閉，則上述的問題就不會再出現了。 經過努力，他們最後將問題給縮小到 Veth 並且找出了 Root Cause。 原來是因為若網卡本身有設定 TX/RX Offloadiong 的設定下，封包經過 veth 時，當初為了速度的最佳化，這邊就會省略 CHECKSUM 的檢查。 這就意味者當若當時封包的 Checksum 有錯誤時，該封包的 CehckSum 不會被重新計算而就以這個錯誤的型態往外發送，導致收端看到的就是錯誤的 CehckSum。 一旦將 TX/RX Offloading 給關閉後，則 Veth 那邊就不會去處理 CheckSum 相關的邏輯，所以後續處理的部份就有機會將該 CheckSum 重新計算來校正該封包。 所以這邊條列一下整個問題發生的過程 封包本身因為不知名原因損毀（文章提到這不是不可能的，畢竟每個封包都會經過大量的 hardware/software 來處理)封包到達 Docker/Container 內處理，回覆的封包透過 veth 時 CheckSum 相關的設定被設定為 CHECKSUM_UNNECESSARY搭配者 CHECKSUM_UNNECESSARY 且本身就有損毀的封包就這樣從 Host 送出去到達目的地，卻因為 CheckSum 不正確，所以沒有被收起來。 該團隊最後送了一條 Patch 到 Linux Kernel 去修正這個問題，修正的方法則是 veth 那邊不要去對 Checksum 的設定有任何更動 詳細的 patch 內容如下。 diff — git a/drivers/net/veth.c b/drivers/net/veth.c index 0ef4a5a..ba21d07 100644 — — a/drivers/net/veth.c +++ b/drivers/net/veth.c @@ -117,12 +117,6 @@ static netdev_tx_t veth_xmit(struct sk_buff *skb, struct net_device *dev) kfree_skb(skb); goto drop; } - /* don’t change ip_summed == CHECKSUM_PARTIAL, as that - * will cause bad checksum on forwarded packets - */ - if (skb-&gt;ip_summed == CHECKSUM_NONE &amp;&amp; - rcv-&gt;features &amp; NETIF_F_RXCSUM) - skb-&gt;ip_summed = CHECKSUM_UNNECESSARY; if (likely(dev_forward_skb(rcv, skb) == NET_RX_SUCCESS)) { struct pcpu_vstats *stats = this_cpu_ptr(dev-&gt;vstats); Study 再探討完畢 veth 的問題後，要如何與我之前的問題給整合？ 首先, 根據上述文章的說明，該 Patch 只有 backport 回到 Linux 3.14, 而我的測試環境是 Linux Kernel 3.10，所以這意味者我的系統上並沒有上述的 Patch, 因此 veth 是有問題的。 確認 veth 有問題後，接下來就要確認為什麼 TCP 封包本身會有損毀，因為若沒有損毀的話，其實 veth 這邊的邏輯是不會造成封包有任何問題的。 根據我的測試結果，Linux Bridge/OVS Kernl Datapath/OVS Userspace Datapath 三種 software switch 中只有 OVS Userspace 會造成問題，因此我猜測是 OVS Userspace 會造成 TCP 封包的 CheckSum 出錯。 為了驗證我的想法，我繼續使用下圖的拓樸來進行驗證。 首先於圖中 Container2 架設一個簡易的 www server，然後使用右圖的機器作為一個 www client，當 www client 嘗試與 www server 建立TCP連線時，我於圖中標示1,2,3 三處分別使用 Tcpdump 去擷取封包來觀察 TCP封包的 CheckSum 是在哪邊出問題。 根據 Tcpdump 的結果，三處所看到的TCP回應封包(SYN/ACK)資訊如下 SYN/ACK 的 CheckSum 正確SYN/ACK 的 CheckSum 正確SYN/ACK 的 CheckSum 不正確，其數值與 SYN 封包相同 這實驗結果印證了我的猜測，封包在經過 OVS userspace datapath 後封包就出錯了，這就意味者 OVS 這邊的處理過程可能有問題。此外，值得注意的是 SYN/ACK 封包的 CheckSum 與 SYN 封包是一樣的，這邊更令人覺得應該是OVS處理上有錯誤。 總結來看，我遇到的問題有兩個 veth 的問題使得系統不會重新校正 CheckSumOVS 的問題使得 TCP 封包出現錯誤 基本上到這邊已經大致上找出問題點了，最後一步驟就是翻進 OVS 的程式碼內，找出對應的錯誤，若沒有時間找出來，就發一個 issue 到官方去詢問好了。","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2017/paper-maglve","content":"","keywords":"","version":"Next"},{"title":"System Overview​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-maglve#system-overview","content":"","version":"Next","tagName":"h2"},{"title":"Frontend Serving Architecture​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-maglve#frontend-serving-architecture","content":"接下來使用下圖來說明一下整體的系統架構 Google本身的服務，譬如 Gmail, Google Search本身都含有一組或是多組以上的 IP address,而這些 IP address 被稱為 Virtual IP(VIP)。原因是這些 IP 本身是不存在於任何實體網卡上，只是讓網際網路中的路由器能夠根據這些 VIP 將這些封包給導向到 Google 的服務器之中。接下來這些封包就會傳送到 Maglev這群服務器中去處理，再根據VIP找到對應的服務，然後把封包傳送給真正的服務器去處理。 假設以 Gmail 為範例，當使用者要連結到 gmail.com的時候，會先到 DNS 本身去詢問 gmail.com所對應的 IP address。 而這些 DNS 回應的 IP 對於 google 來說其實是 VIP，然後使用者的電腦都會嘗試將請求的封包送到 VIP 所對應的路由器去處理。 當 Router 收到這個 VIP 的封包後，接下來他要把這個封包送到底下的 Maglev 服務器群去處理，在 Router -&gt; Maglev 的過程中採用了 Equal Cost Multi Path(ECMP)的方式去傳送封包，盡可能的讓這些 Maglev服務器能夠平均的收到請求封包，這邊在我看來也是一種簡單的負載平衡的功能，不過著重的對象是Maglev而不是背後真正服務的服務器。 當 Maglev 收到這封包的時候，他就會根據目的地的 VIP 去反查，就可以知道當前這個封包應該要往哪個 service傳送，但是我們知道因為 VIP 本身是不存在的，所以這時候 Maglev 會幫當前整個封包再多包上一層 Generic Routing Encapsulation(GRE) 的標頭檔，該標頭內的資訊則是後端服務器真正的 IP address，因此封包就能夠順利的到達後端服務器，這也是圖中所標示Encapped inbound traffic的流向。 當後端服務器處理完畢請求時，這時候會回傳封包到發送端(也就是使用者電腦)，這邊有個要注意的事情是通常情況下, 使用端發出的請求封包會比服務器發出的回應封包還要來得小很多，因此 Maglev 並不想要讓這些回應封包還要回歸到 Maglev 去處理。 所以當服務器收到封包後，要先解讀GRE，接下來讀取到本來的 VIP,然後將此 VIP 當作封包的來源IP後讓該封包直接送回給使用者端。 所以就如同該圖示中三號紅色的 Unencapped outbound traffic。 至於要如何讓這些封包能夠從服務器本身不經過 Maglev 而直接到達上層的 Router，這邊論文內本身並沒有說明，只有提到透過 Direct Server Return (DSR)的技術來達到此功能。 最後要提到的是這些 VIP 為什麼可以被網際網路中的路由器給導向過來，原因是上圖中的咖啡色 BGP announcements，當後端服務跟對應的 VIP 有任何更動時，都會通知到 Maglev然後透過 BGP 的方式一路傳遞到網際網路去更新動態路由表，讓流向VIP的封包都能夠順利的導向內部服務器。 ","version":"Next","tagName":"h3"},{"title":"Maglev Configuration​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-maglve#maglev-configuration","content":"接下來使用下圖來說明整個 Maglev內部的設定。如同前面所述， Maglev 本身會透過 BGP 的方式向路由器去通知路由表的更新，因此在 Maglev 內部會有兩個元件，分別是 Forwarder 以及 Controller。 這兩個元件會透過 Config Objects來學習 VIP的資訊，可以是透過讀取檔案的方式，或是透過 RPC 的方式來更新。 Controller 會定期檢查 Forwarder 的資訊，只要當前有任何 VIP 變動，不論是新增或是減少，都會透過透過 BGP 將 VIP的更動一路往外宣傳，根據這種舉動可以確保 Router 要轉發封包時，一定會轉發到能夠正常運作的 Maglev 伺服器。 而當 VIP 的封包到達 Maglev 時，都會交由 Forwarder 來處理，對於 Forwarder 來說，每個 VIP 可以對應到一個或多個的 Backend Pool，而每個 Backend Pool 可能包含一組或多組的實體 IP，而這些IP則會對應到後方真正的服務器 IP。 對於每個 Backend Pool來說，背後都會對應到一組或多組的 Health Checker，這些 Health Checker會去檢查該 Backend Pool內所有的服務是否當前都運作正常，只有都運作的正常的Backend Pool才會被 Forwarder 視為一個封包轉送目的地的候選人。 根據此架構圖，其實可以看到有些後方服務器(IP)是對應到多組的 Backend Pool，所以在 Health Checker的時候會特別去進行這邊的去重複化，避免相同的事情重複多次來減少額外的開銷。 Forwarder Implementation 由於 Forwarder 要負責接收封包並轉發，所以必須要有極高的效能且穩定，所以接下來就會介紹 Maglev 內部的架構以及其實作原理。 ","version":"Next","tagName":"h2"},{"title":"Overrall Structure​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-maglve#overrall-structure","content":"接下來會使用下圖來介紹整個 Forwarder 處理封包的過程。整個過程簡單來說就是 從網卡(NIC)收到封包經過查詢計算後，透過 GRE 重新封裝該封包從網卡(NIC)送出封包 複雜來看的話，整個 Forwarder 分為兩個大模組來處理，分別是 Steering 以及 Muxing。 首先, Steering 模組會針對封包的 5-tuples 進行 Hash，根據 Hash 的值找到一個對應的 RX Queue來處理，而每一個 RX Queue 都會對應到一個 Packet Rewriter Thread 來處理。 而 Packer Rewriter Thread 則是會進行下列事情 先排除根本機無關的封包，譬如 VIP 不屬於本機上面的封包重新根據 5-tuples 進行一次 Hash 計算，然後根據此計算其果去查詢一個 Connection Track Table 表格。Connection Track Table 會記住每條 Connection 與後端服務器的對應關係。 這邊要重新計算 Hash的原因是因為不想要跟 Steering 共用一樣的數值，因為這樣就會有跨Thread之間的同步問題，這樣就會導致效率降低。當查詢 Connection Track Table 時 - 若 Hash 存在且對應的後端服務器依然正常服務，那就直接使用查詢出來的結果當作當前 **VIP**封包要轉送的對象 - 若 Hash 不存在或是後端服務器目前服務有問題，則會透過 **consistent hash**的方式算出對應的後端服務器位置，並且將其加入到該表格之中。 Consistent Hash 後面章節會再介紹。 - 若目前查詢的結果是沒有半個後端服務器可以使用，則就丟棄當前封包。 當知道當前 VIP 所要對應的服務器資訊後，就會透過 GRE/IP 的方式重新改寫當前封包內容，並且將封包送到 TX Queue 接下來 Muxing Module 則會定期 Polling 所有的 TX Queue，然後將封包從 TX Queue中取出來並且透過 NIC 將封包給轉送出去。 最後有提到為什麼 Steering Module 這邊要特別採用 Hash 的方式來選擇 RX Queue 而不是透過 Round-Robin 的方式。 有兩個原因 讓相同 Connection 的封包能夠由同一個 Queue去處理，避免因為不同 Queue 之間處理速率不同，導致相同 Connection 的封包以不同順序的結果從NIC出去，這對於 TCP 來說有可能會導致 Out of Order 的現象進而導致降速。對於 Connection Track Module 來說，每個 Connection 只需要在一個 Packet Rewrite Thread 去計算就好，這樣可以避免同樣結果多次計算藉此降低 CPU 使用率，同時也可以避免同一條 Connection 最後算出不同後端服務器導致該 Connection 出問題。 不過最後也有提到，若當前選擇的 RX Queue 沒有空間的情況下， Steering Module 則會自動切換成 Round-Robin 的模式來處理封包。 ","version":"Next","tagName":"h2"},{"title":"Fast Packet Processing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-maglve#fast-packet-processing","content":"在google內部普遍存在使用於 10Gbps 的網卡來說，假設每個 IP 封包的大小是 1500 byte, 則最高的情況下每秒處理封包數量就是 813 Kpps。但是由於使用者發送來的請求封包都會更小，所以假設每個請求的 IP 封包只有 100 byte, 則每秒收到的封包數量可高達 9.06M pps,這個數量非常可觀。 同時，Maglev 是個運行在一般 Linux Server 上的 User-space 應用程式，其實本身的功能並沒有需要 Linux Kernel Network 內這麼龐大的功能，所以這邊促使了 Google 提出了採用 Kernel Bypass 的架構來設計 Maglev，透過適當的技術(譬如 DPDK, Netmap)等技術，讓網卡(NIC)收到的封包不再需要經過 Linux Kernel Network Stack, 對於每次的收送來說，都可以減少至少兩次以上的 kernel-space 與 user-space 的資料複製操作。 在 kernel bypass 的架構下, 整個 Maglev 內 Forwarder 的架構就如下圖所示。當 Maglev 一旦啟動後，會預先產生一塊很大的空間 Packer Pool 供 Forwarer 與 網路卡(NIC) 共同使用。Steering 與 Muxing 都各自維護一個 Ring Queue，其內容則是各種指標，這些指標會直接指向 Packer Pool內的每一塊空間。 在 Ring Queue 內則維護了三個指標，分別是 Processed, Received 以及 Reserved。 對於接收端來說，當網卡收到封包後，就會將 Received 指標給移動，然後記住哪些封包目前已經收到。 接者 Steering 就會開始處理這些封包，只要 Sterring Module一旦將該封包送給 Packet Rewrite Thread 去處理後，該封包就會被標示為 Processed，並且繼續往下移動指標，直到遇到 Received指標。Reserved則是會從 Packer Pool 中預先拿一些還沒被使用的空間出來，供之後的 Received 使用。 相對於接收端，發送端其實概念也很類似。 當網卡把封包發送出去後，就會修改 Sent 的指標，而 Ready 則是 Muxing Module 將封包從 TX Queue取出後就會被更動。 這邊 google有特別強調的是在這些 Forwarder 的操作中，沒有任何一個封包是會被複製的，這意味者每個封包能夠減少大量的複製操作，藉此減少每個封包處理所需要的時間。 此外，在軟體架構方面，讓每個 Packet Rewrite Thread 擁有下列特性 彼此沒有任何資料要互相同步，避免同步產生的時間消耗每個 Thread 都運行在不同的 CPU 上 最後，想來探討在這種架構下，每個封包處理所花費的時間。 一般來說，每個封包大致上花費 350ns 來處理，不過有兩種特殊的情況可能會導致該時間變大。 首先要先瞭解到的是 Maglev 處理封包的方式是採用 Batch 的方式去處理，每次都會批量的處理直到一個固定數量或是該處理花費的時間已經達到了定時器的數值。 第一種情況 假設當前 Maglev 服務器處於一種沒啥流量的情況下，同時上述定時器的數值假設是 50us, 則最壞的情況下，每個封包的處理都至少要花到 50us 才可以處理完畢。 這種情況的解決方法可以是動態的去調整該該定時器的數值 (50us) 第二種情況 假設當前 Maglev 服務器處於一種高負載的情況下，當前存放封包的 Ring Queue 已經滿了，這時候多出來的封包都會被丟棄。 假設 Ring Queue 能夠存放 3000 個封包，同時 Maglev 處理封包的能力是 10Mpps, 則處理完這 3000 個封包則要額外花費 300us，所以每個封包處理所花費的時間不但是本身處理的時間，還要再加上在queue中等待的時間。 這種情況的解決方法可以透過增加更多的 Maglev 來平衡每台服務器所承受的壓力。 ","version":"Next","tagName":"h2"},{"title":"Backend Selection​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-maglve#backend-selection","content":"當 Maglev 收到一個 VIP 的封包時，要如何決定該 VIP 封包對應到後端的服務器是誰。對於 TCP 這種有連線概念的協定來說，若相同連線的封包沒有到達同一個收端，會對效能產生不好的影響，譬如 Out of Order。 因此要如何讓每條 Connection 內的所有封包都能夠選到同一個後端的服務器，就是這章節強調的重點。 就如同先前所述，每一個 Maglev 內的每個 Packet Rewrite Thread 都會自己維護一個對應的 Connection Track Table。 對該 VIP 封包根據 5-tuple進行 Hash 運算後， 若該 Hash值已經存在，則使用先前紀錄過的服務器位置來使用，否則就會根據 Consistent Hashing 的方式算出對應的後端並且將此結果存到對應的 Connection Track Table 中。 然後上述的這種架構，對於分散式的 Maglev 是不夠的，這邊舉了兩個範例。 第一個範例 因為 Maglev 是能夠動態的加入/移除/升級的一種分散式架構，所以搭配前述的 ECMP 路由演算法，同樣一條 Connection 可能會被導向不同的 Maglev 服務器，在此情況下，因為該台機器本身對於該 Connection 沒有任何記錄，所以必須要重新計算其對應的後端服務器是哪些。 第二個範例 因為 Connection Track Table 本身的大小有限制，所以假如該 Table 因為遇到大量的流量或是 SYN Flood 攻擊之類的導致 Table 滿載，這時候新加入的 Connection 就沒有辦法記錄下來，導致每個封包都要重新計算一次。 Google特別設計了一套 Consistent Hashing 來處理這上述這兩種情況，讓上述情況內的封包還是依然可以算出相同的後端伺服器以避免 Connection 出問題。 ","version":"Next","tagName":"h2"},{"title":"Consistent Hashing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-maglve#consistent-hashing","content":"這邊來說就是在說明假設當 Maglev 有任何機器壞掉的話，我要如何確保其他的 Maglev 服務器在進行 Connection Track Table建立時，能夠將相同的 Connection 給算到相同的後端服務器。 因為不想要服務器彼此之間有任何同步的行為，這些都會額外的支出都會減少每秒封包處理的速度。 這部分網路上已經有一篇文章在解釋這邊的行為，並且解釋得淺顯易懂，建議能的話一定要看一下這篇由 EvanLin 所撰寫的[論文中文導讀] Maglev : A Fast and Reliable Software Network Load Balancer (using Consistent Hashing) Operational Experience 這邊大致上就是一個更細節的探討，包含 Meaglve 的演化史，VIP 怎麼設計，遇到 IP Fragement如何處理 以及一些 Monitor的設計。 這邊有興趣的可以自行閱讀該篇文章，這邊就不多加敘述。 Evaluation 這邊效能評估的部分，我個人偏好 Kernel Bypass的部分，所以這邊只針對這邊去進行閱讀。 在此實驗中，變數總共有兩個，分別是 Linux Kernel Stack/Kernel BypassPacket Rewrite Thread 的數量 希望觀察到的是 Maglve 每秒能夠處理的封包數量。 實驗環境中，發送端(Sender)會產生不同 Source Port 的 UDP 封包，讓這些封包都會被分配到不同的 Packet Rewrite Thread。 每個 UDP 封包的大小都是 52 Byte，然後 Maglev 上的 每個 Thread 都會綁上一個專屬的 CPU 來處理。 實驗結果如下圖，基本上總結是 Thread 數量超過四以後就不會再上去了，這時候整個效能的瓶頸就從 Meaglve 轉移到 NIC 上Kernel Bypass 相對於 Linux Kernel Network Stack 有明顯大服務的差距 Reference [論文中文導讀] Maglev : A Fast and Reliable Software Network Load Balancer (using Consistent Hashing) ","version":"Next","tagName":"h2"},{"title":"RDMA Introduction (一)","type":0,"sectionRef":"#","url":"/docs/techPost/2017/rdma-introduction-i","content":"","keywords":"","version":"Next"},{"title":"Introduction RDMA​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#introduction-rdma","content":"RDMA，全名 Remote Direct Memory Access，顧名思義就是遠端直接記憶體存取，此技術其實是源自於 DMA (Direct Memory Access)，能夠在不牽扯到 CPU 的前提下直接存取本機上面的記憶體。因此 RDMA 則是在此情提上增加了 Remote 的功能，亦即可以遠端存取其他機器上面的記憶體。  所以 RDMA 到底有什麼優點？ 從 rdmamojo 引用來看，至少有五個優點值得我們去探究 ","version":"Next","tagName":"h2"},{"title":"Advantages​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#advantages","content":"","version":"Next","tagName":"h2"},{"title":"Zero-Copy​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#zero-copy","content":"此特性在 DMA 中就已經有了，我們用一個簡單的範例來說明其用途。 在傳統的網路應用程式中，一個封包的轉送會經過下列步驟 封包透過媒介送到網路卡，隨後透過 driver 層的處理到達了 kernel spacekernel space 內檢視該封包與相關資訊，找到該封包所屬的 user space application，並且準備將此封包轉送上去user space application 收到封包內容，進行處理後，準備將封包送出去user space application 想要送出的封包會先到達 kernel space，在 kernel 一層一層處理，最後透過網路卡將此封包透過媒介送出去。 在上述的過程中，當封包在 kernel space 與 user space 之間轉換時，由於其記憶體空間的規範不同，因此所有的資料都必須要複製一份新後才能夠往上/下處理，所以這樣至少就會有兩次資料複製的行為會出現。 而 Zero-Copy 強調的是能夠減少這些資料複製的行為，甚至將該次數給降低到完全不需要，這部分若有硬體的幫助，則甚至封包都不會在 kernel 內進行 copy. 詳細地內容可以參考 史丹利部落格 - 什麼是Zero-Copy? ","version":"Next","tagName":"h3"},{"title":"Kernel bypass​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#kernel-bypass","content":"此特性強調 user space application 能夠不牽扯到 kernel 的前提下直接處理資料，以 RDMA 來說，其應用程式可以直接跟網卡溝通直接取得資料，而該資料則不需要再經過 kernel 內一層又一層的 network stack，這樣也可以減少不必要的 CPU 運算。 其他技術如 DPDK, mmap 也是有一樣的特性。 ","version":"Next","tagName":"h3"},{"title":"No CPU involvement​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#no-cpu-involvement","content":"application 可以在不消耗遠方機器 CPU 的前提下直接對遠方機器上的記憶體進行操作。此外，由於遠方機器根本不會知道某些記憶體已經被修改了，所以 CPU read cache 也不會有任何的修改。 當然前提是兩方要事先有溝通過，彼此掌握一把鑰匙後，才可以這樣進行修改。 ","version":"Next","tagName":"h3"},{"title":"Message based transactions​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#message-based-transactions","content":"這部分則是比較偏向程式設計者要注意的部分， RDMA 的封包會以 message 的方式去傳遞，而不是 stream 的方式，意味者應用程式不需要自己去解析整個資料串流來取得每一次傳送的內容。 ","version":"Next","tagName":"h3"},{"title":"Scatter/Gather entries support​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#scattergather-entries-support","content":"此特性也是從 DMA 就已經有的，應用程式可以透過此透性一口氣連續的從多個緩衝區讀取資料，然後透過資料串流的方式寫出去。或是從資料串流讀取資料，並且將讀進來的資料給寫入多個緩衝區中。詳細的介紹可以參考 Vectored_I/O 所以有了上述的特色， RDMA 到底能帶來什麼樣的優勢？ low-latency:high-throughput:low-CPU usage: 透過 RDMA 減少CPU對於網路功能的消耗，讓 CPU 能夠處理更多其他的事情。 不過這些 RDMA 並沒有保證上述的三個特性是能夠同時擁有的， 可以從 Tips and tricks to optimize your RDMA code 這邊看到，針對不同的特性， RDMA 的使用方式都會有所不同，然而這些方法是互相抵觸的。 至於這些細節在下面文章會再次提到。 ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#architecture","content":"","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#architecture-1","content":"這邊使用一張從網路上找到的圖來說明 RDMA 的架構。 該圖分成上下兩個部分來看，上部分由 Application/User/Kernel 來組成，而下部分就是剩下部分。  BSD Socket​ 上半部分這邊採用對比的方式去呈現，左邊的部分是當 Application 要採用 BSD Socket API 時要走的路線。首先，在 Application 會有本地自己的 Buffers，接下來想要將該Buffer內的資料送出去，所以這時候通常會使用 Send/Write..etc 之類的 API 將封包送出去。而這些封包透過 System_Call 的方式到達 kernel 之後，必須要先跑過整個 Kernel Space內的 Network Stack，包含了 Layer4/Later3/Layer2 以及 Socket 這一層的處理，在一層一層的過程中，封包的雛形會逐漸產生，對應的 Header 會被逐漸加上去。 RDMA Side​ 最後當此封包到達 Driver 這層之後，就會順利的送出去了。 右邊的部分則是在描述 RDMA 的情況，可以看到有兩個不一樣 User-Space 採用的 API 不同，這邊採用的是 RDMA Verbs API透過上述的 API, 封包內的東西會直接送到 Device Driver 去處理，直接略過本來的 Network Stack，最後到達 Host CHannel Adapter 後再根據不同的方式走不同的協議出去。 Protocol​ 下半部分出現了三個名詞，分別是 InfiniBand, iWARP 以及 RoCE。 RDMA此技術要建立在這三種傳輸協議上，這三種協議不同點在於 架設環境不同，所需要的硬體支援能力也不同用來傳送RDMA的封包格式也不同，有的會多包TCP，有的包UDP。 所以從圖中可以觀察到左邊 InfiniBand 有強調需要搭配 InfiniBand Switch 才可以正常運作，而右邊兩種協定則只需要 Ethernet Switch 即可運作。 這邊稍微提一下 RoCE (RDMA over Converged Ethernet)這個協定，此協定本身也有所謂的版本的演進，最早期的 v1 版本中只會添加Ethernet Header，而沒有更上層的 IP, TCP/UDP，所以只能用來 Layer2 的轉發。到了 v2 的版本，則是疊加了 IP 以及 UDP 兩種 header。詳細的介紹可以參考两种以太网 RDMA 协议： iWARP 和 RoCE 針對 RoCE 有個有趣的議題就是 lossless network, 在整個封包的傳輸過程中，當發送端無止盡的將封包送往對面時，若對面的沒有足夠的能力去承受這些封包，就有可能發生封包被丟棄的狀態，這就會導致封包遺失。 若封包愈少遺失，則整體效能愈加。所以這部分從上層的 TCP Congestion Control 到 Ethernet IEEE 802.3x 規範的 Ethernet Flow Control都在致力於減少上述情況。 因此在 RoCE 的環境中，由於沒有 TCP 的幫助，甚至在 V1 的環境下只有 Ethernet 的存在，要如何達到 lossless network 就是一個有趣的議題。 在這篇 Mellanox 的文章 resilient-roce-relaxes-rdma-requirements 以及 HowTo Enable, Verify and Troubleshoot RDMA 中介紹了三種方法來達成 lossless network。分別是 Ethernet Flow Control (802.3x)PFC (Priority Flow Control)ECN (Explicit Congestion Notification) 三種詳細的介紹可以在上述連結看到，有興趣的可以自行參閱學習。 當 RoCE 發展到 2016 年時，已經發展成為了 Resilient RoCE，在此規格下，RoCE對於 PFC/ENC 可以同時開啟，也可以只開啟一個，並沒有規定一定要有 PFC 才能夠運行。 就如同文章內所述。 RoCE can be deployed with ECN only, PFC only, or both, if you want to ensure your pants (or network flows) won’t fall down. ","version":"Next","tagName":"h3"},{"title":"Use-Case​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#use-case","content":"看了上述這麼多 RDMA 的優點，我們來看看一些為什麼採用 RDMA 的實際案例。 ","version":"Next","tagName":"h2"},{"title":"Ceph​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#ceph","content":"這邊借用了 Haomai Wang於 OPENFABRICS ALLIANCE 2017 上的投影片 CEPH RDMA UPDATE 第一張圖說明了目前 TCP 的效能瓶頸問題，不過這也不能怪 TCP，畢竟 TCP 當初設計時所考慮的情況本來就沒有辦法符合所有的使用要求，不然 google 怎麼會在今年公開 BBR 演算法進一步提升 TCP 在某些情況下的效能呢。 此外這邊也有特別提到 TCP 的 latency，由於 Ceph 分散式架構上每個節點之間都透過網路來傳送訊息，不論是控制封包或是資料封包，其 latency能夠愈低當然是愈好。 第二張圖就比較有意思，透過 perf 進一段時間 smaple 後的視覺化的結果，可以觀察到當透過 network 去跟其他 node 讀取資料時，目測至少有20%的時間都花費在 TCP 本身傳送封包的處理上。 這也意味者Networking這邊至少有20%的部分可以投資去加強，可以嘗試減少其 CPU Usage。 CEPH 為了解決目前 TCP 造成的效能瓶頸，在 Networking 則提供了 DPDK 的使用，(最早之前其實 Ceph 就有支援 RDMA 了，不過這邊是牽扯到第三方函式庫 Accelio，所以不歸類在 RDMA 原生架構下的支持) 後來經由大陸公司 XSKY 的幫忙，推出了 SPDK + DPDK 的組合，也幫 Ceph 提供了另外一種選擇。 不過目前 SPDK 會用到 DPDK 且 DPDK 本身後來版本的 API 改動，使得 SPDK 這邊在連結 DPDK 的部分有點小問題，要使用上更需要花一些時間去修正。 XSKY 完成 DPDK + SDPK 後，就如同其投影片裡面述說的問題  於是乎 RDMA 的開發就成了另外一條新闢的路徑，而這邊目前也還在持續開發中，從 2016~2017 ceph mailling list 上面的討論可以觀察到目前 Ceph With RDMA 還在成長中，目前先追求穩定性，穩定後再求效能。 使用 2017/04 年左右 master 版本進行 RDMA 效能測試得到的數據顯示，目前的 RDMA 還沒有辦法帶來太多的優勢，不論是降低 latency 或是減少 CPU 使用量，這部分都還要依靠社群內繼續開發使其成熟。 ","version":"Next","tagName":"h3"},{"title":"DRBD​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#drbd","content":"DRBD(distributed replicated block device) 也是一個知名的分散式 block 層級的複製技術（軟體?)。由於是分散式的架構，其中間資料的傳輸也是依賴網路來處理。 DRBD 一開始只有提供 TCP 作為其傳輸協定，直到了 2015 年才開始提供了 RDMA 的方式來處理。 不過相對於 CEPH 的開放， DRBD with RDMA 則是一個商業版本才有的支援（就是要付錢$$$)，所以這邊沒有辦法針對其效能進行測量來評測看看其效果。 不過官方網站上倒是有不少篇文章在講 RDMA，包括了 what-is-rdmadrbd-9-over-rdma-with-micron-ssdsrdma-performance 這兩個分散式軟體在其網路傳輸中都有提供 RDMA 的方式，除了其開放程度的不同外，兩者在實作的層級也不太一樣。CEPH 將 RDMA 實作在 User-space，而 DRBD 則實作在 Kernel-space，並且以 kernel module 的方式提供此功能，因此若有跟 linbit 談好相關的商業行為，就可以獲得擁有 RDMA 功能的 kernel module來使用。 ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#summary","content":"本篇文章主要是以概念的方式來介紹 RDMA，比較屬於科普方面的介紹。 接下來則會有一篇文章會以 Programmer 的角度去看待 RDMA，包含其三種 Operation(SEND/RECEIVE, RDMA Write 以及 RDMA Read) 以及透過 ceph 此 Project 為範例來學習要如何撰寫一個 RDMA 的應用程式。 ","version":"Next","tagName":"h2"},{"title":"Reference​","type":1,"pageTitle":"RDMA Introduction (一)","url":"/docs/techPost/2017/rdma-introduction-i#reference","content":"Vectored_I/O史丹利部落格rdmamojoOpenfabrics-Ceph RDMA Updatetoward-a-paravirtual-vrdma-device-for-vmware-esxi-guestsresilient-roce-relaxes-rdma-requirements两种以太网 RDMA 协议： iWARP 和 RoCELinux Cluster ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2017/jupyter-converter","content":"Preface 在這邊文章前，必須要先知道什麼是 Jupyter Notebook，這方面網路上已經有太多的文章在介紹了，所以這邊就簡單介紹就好。 Jupyter Notebook 是一個介於 IDE 與編輯器之間的工具，可以讓使用者一行一行的寫程式並且直接運行觀察其結果，除了大家都熟知的 Python/R 等直譯式語言外，現在連 C++ 這種編譯式語言都可以執行了，非常的令人驚艷，想要看更多關於 C++ 支援的可以參考這篇文章 這篇文章的主軸在於如何透過程式化的方式將已知的 Jupyter Notebook 給轉換成一般常用的 Python 腳本。 基本上去網路上搜尋如何將 Jupyter Notebook 轉換成 Python 腳本，你都會得到使用下列指令的答案 ipython nbconvert --to script notebook.ipynb 該指令會將該 Jupyter notebook檔案 notebook.ipynb 轉換成 notebook.py這樣就可以透過 python 順利執行了。 但是事情通常都沒有這樣單純，事實上 Jupyter notebook 背後走的是 IPython 而非 Python，所以你可以在 Jupyter notebook 內採用 ! 前綴的方式去執行系統指令，譬如所以你轉換出來的 notebook.py 本身也需要透過 ipython 的程式去執行，這在某些情況會造成不方便。 所以這時候就有一個想法出現了，因為 ipython nbconvert 可以用來轉換 ipynb 到 py，如果我們能夠在轉換的過程中，將 ipython 的程式碼都遮蔽掉，是否就可以達到轉譯出來的腳本就是 Python 腳本了，而不需要使用 IPython 來執行。 此外，這整個方向要能夠正確的前提是原本的 Jupyter Notebook 內沒有使用 IPython 的結果來進行關鍵性的操作，這意味者即使去除所有 IPython 的程式碼也不能影響整體程式的運行結果。 為了研究這個方向，我決定採取下列步驟來研究是否有辦法完成 研究 IPython 檔案內，IPython 程式碼跟 Python 是否有差異研究是否有好的處理邏輯能夠將 IPython 程式碼與 Python 分離研究 ipython nbconvert 是如何透過程式的方式去轉換這個檔案 因此下列就針對這三個步驟進行一些分析與過程的介紹 Study Jupyter Notebook file 隨便撰寫一個簡單的 Jupyter Notebook，其內容如下 這時候透過 Jupyter Notebook 的下載工具將其轉換到不同格式，這邊我們嘗試了兩種格式，分別是 ipynb，另外一種則是 py。 所以上述的格式對於 .py 這種來看，則是 # coding: utf-8 # In[1]: a=123 # In[2]: print(a) # In[3]: get_ipython().system('ls') 假設若要針對 python 程式來轉換的話，我們可以大膽假設只要是get_ipython() 都屬於 IPython 的程式碼，所以我們可以寫一個程式碼讀取整個 Python 檔案，並且符合此特徵的程式碼給忽略，就可以將此 Python 變成純 Python 而不依賴於 IPYthon 了。 這種做法的話非常容易，就不提供相關的程式碼了，只要可以讀取檔案，過濾符合特徵的程式碼然後在存擋就可以完成。 那如果是另外一種 .ipynb 的格式的話，其格式長這樣 { &quot;cells&quot;: [ { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 1, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;a=123&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 2, &quot;metadata&quot;: {}, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stdout&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;123\\n&quot; ] } ], &quot;source&quot;: [ &quot;print(a)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 3, &quot;metadata&quot;: {}, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stdout&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;communities\\t\\t\\t Welcome to Haskell.ipynb\\r\\n&quot;, &quot;datasets\\t\\t\\t Welcome to Python.ipynb\\r\\n&quot;, &quot;featured\\t\\t\\t Welcome to Ruby.ipynb\\r\\n&quot;, &quot;Untitled.ipynb\\t\\t\\t Welcome to Spark with Python.ipynb\\r\\n&quot;, &quot;Welcome Julia - Intro to Gadfly.ipynb Welcome to Spark with Scala.ipynb\\r\\n&quot;, &quot;Welcome R - demo.ipynb\\t\\t work\\r\\n&quot; ] } ], &quot;source&quot;: [ &quot;!ls&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [] } ], &quot;metadata&quot;: { &quot;kernelspec&quot;: { &quot;display_name&quot;: &quot;Python 3&quot;, &quot;language&quot;: &quot;python&quot;, &quot;name&quot;: &quot;python3&quot; }, &quot;language_info&quot;: { &quot;codemirror_mode&quot;: { &quot;name&quot;: &quot;ipython&quot;, &quot;version&quot;: 3 }, &quot;file_extension&quot;: &quot;.py&quot;, &quot;mimetype&quot;: &quot;text/x-python&quot;, &quot;name&quot;: &quot;python&quot;, &quot;nbconvert_exporter&quot;: &quot;python&quot;, &quot;pygments_lexer&quot;: &quot;ipython3&quot;, &quot;version&quot;: &quot;3.6.3&quot; } }, &quot;nbformat&quot;: 4, &quot;nbformat_minor&quot;: 2 } 這個格式是依照 Json 的格式來輸出，可以觀察到物件中若有 source 欄位時，其值就是該步驟的指令。 而根據前述的規範，若指令是依照 ! 的前綴來撰寫都屬於 IPython 的，則我們可以判斷若該 Source 欄位的數值其開頭是 ! 的，就屬於 IPython 的程式碼，可以將該 Json Obejct 給移除。 剛好在網路上看到也有類似的問題，提出的解法也很類似，參考這篇gist &quot;source&quot;: [ &quot;!ls&quot; ] &quot;source&quot;: [ &quot;print(a)&quot; ] 看到這邊，不禁想到開頭所述的轉換指令，該指令可以將 .ipynb 轉換到 .py 的檔案 ipython nbconvert --to script notebook.ipynb 這意味者 ipython nbconvert 本身可以讀取這種 json 物件，並且將其轉換成 .py 的格式。 這樣我們就可以在中間讀取 Json 物件時，將檔案內的關於 IPython 的物件給移除，剩下就繼續處理即可。 所以接下來我們就來研究到底 ipython nbconvert 這個程式到底怎麼運作的。 首先，找到其 GitHub 的檔案，再仔細看了一下後，我們找到了一個名為 NbConvertApp 的物件，然後觀察到該物件有一個 Start 的函式 class NbConvertApp(JupyterApp): &quot;&quot;&quot;Application used to convert from notebook file type (``*.ipynb``)&quot;&quot;&quot; version = __version__ name = 'jupyter-nbconvert' aliases = nbconvert_aliases flags = nbconvert_flags .... def start(self): &quot;&quot;&quot;Run start after initialization process has completed&quot;&quot;&quot; super(NbConvertApp, self).start() self.convert_notebooks() 接下來我們仔細看一下 convert_notebooks 這個函式, 這個函式的內容頗長，大概看一下包含了 def convert_notebooks(self): &quot;&quot;&quot;Convert the notebooks in the self.notebook traitlet &quot;&quot;&quot; # check that the output base isn't specified if there is more than # one notebook to convert if self.output_base != '' and len(self.notebooks) &gt; 1: self.log.error( &quot;&quot;&quot; UsageError: --output flag or `NbConvertApp.output_base` config option cannot be used when converting multiple notebooks. &quot;&quot;&quot; ) self.exit(1) # initialize the exporter cls = get_exporter(self.export_format) self.exporter = cls(config=self.config) # no notebooks to convert! if len(self.notebooks) == 0 and not self.from_stdin: self.print_help() sys.exit(-1) # convert each notebook if not self.from_stdin: for notebook_filename in self.notebooks: self.convert_single_notebook(notebook_filename) else: input_buffer = unicode_stdin_stream() # default name when conversion from stdin self.convert_single_notebook(&quot;notebook.ipynb&quot;, input_buffer=input_buffer) 我們首先會觀察到，必須要先初始化一個 self.exporter，這個 exporter 意味者當前的轉換要轉換到什麼樣的格式，譬如 script 就是轉換到 python script。 接下來會根據你當前的轉換來源是哪邊，若你是透過 stdin 的方式傳輸內容近來，則會透過 input_buffer = unicode_stdin_stream() 來處理，否則就會根據輸入的檔案名稱來選擇 self.convert_single_notebook(notebook_filename) 所以在仔細檢查該函式，就會觀察到底下的事情分成四個部分去執行 def convert_single_notebook(self, notebook_filename, input_buffer=None): &quot;&quot;&quot;Convert a single notebook. Performs the following steps: 1. Initialize notebook resources 2. Export the notebook to a particular format 3. Write the exported notebook to file 4. (Maybe) postprocess the written file Parameters ---------- notebook_filename : str input_buffer : If input_buffer is not None, conversion is done and the buffer is used as source into a file basenamed by the notebook_filename argument. &quot;&quot;&quot; if input_buffer is None: self.log.info(&quot;Converting notebook %s to %s&quot;, notebook_filename, self.export_format) else: self.log.info(&quot;Converting notebook into %s&quot;, self.export_format) resources = self.init_single_notebook_resources(notebook_filename) output, resources = self.export_single_notebook(notebook_filename, resources, input_buffer=input_buffer) write_results = self.write_single_notebook(output, resources) self.postprocess_single_notebook(write_results) 分別是程式碼內註解所描述的行為 Initialize notebook resourcesExport the notebook to a particular formatWrite the exported notebook to file(Maybe) postprocess the written file 我們可以忽略第四個行為，專注於前面三個步驟，所以我們就要仔細研究上述三個行為對應的函式。 Initialize notebook resources 首先可以看到在 init_single_notebook_resources 的函式內，我們要想辦法產生一個 map 的物件，該物件按照說明，有三個 key 需要填寫，不過因為我們只是單純要進行轉移，所以其實 config_dir 這個 key 並不需要使用。 def init_single_notebook_resources(self, notebook_filename): &quot;&quot;&quot;Step 1: Initialize resources This initializes the resources dictionary for a single notebook. Returns ------- dict resources dictionary for a single notebook that MUST include the following keys: - config_dir: the location of the Jupyter config directory - unique_key: the notebook name - output_files_dir: a directory where output files (not including the notebook itself) should be saved &quot;&quot;&quot; basename = os.path.basename(notebook_filename) notebook_name = basename[:basename.rfind('.')] if self.output_base: # strip duplicate extension from output_base, to avoid Basename.ext.ext if getattr(self.exporter, 'file_extension', False): base, ext = os.path.splitext(self.output_base) if ext == self.exporter.file_extension: self.output_base = base notebook_name = self.output_base self.log.debug(&quot;Notebook name is '%s'&quot;, notebook_name) # first initialize the resources we want to use resources = {} resources['config_dir'] = self.config_dir resources['unique_key'] = notebook_name output_files_dir = (self.output_files_dir .format(notebook_name=notebook_name)) resources['output_files_dir'] = output_files_dir return resources Export the notebook to a particular format 接下來就是要把來源的檔案給放到 Exporter 去處理格式轉換的問題，但是這邊如果我們直接傳入的是一個本來的 .ipynb 的檔案的話，我們會沒有辦法能夠針對 IPython 部份去進行修改。 因此這邊我們必須要考慮 input_buffer 這個參數。我們要先自己讀取該檔案，將該檔案的內容解析完畢後，排除 IPython 相關的內容後當成 input_buffer 參數給丟進去處理。 def export_single_notebook(self, notebook_filename, resources, input_buffer=None): &quot;&quot;&quot;Step 2: Export the notebook Exports the notebook to a particular format according to the specified exporter. This function returns the output and (possibly modified) resources from the exporter. Parameters ---------- notebook_filename : str name of notebook file. resources : dict input_buffer : readable file-like object returning unicode. if not None, notebook_filename is ignored Returns ------- output dict resources (possibly modified) &quot;&quot;&quot; try: if input_buffer is not None: output, resources = self.exporter.from_file(input_buffer, resources=resources) else: output, resources = self.exporter.from_filename(notebook_filename, resources=resources) except ConversionException: self.log.error(&quot;Error while converting '%s'&quot;, notebook_filename, exc_info=True) self.exit(1) return output, resources Write the exported notebook to file 在上述已經將內容轉換完畢後，就可以透過最後的 writer 將該內容(放在 output)內給寫入到特定的地方。 def write_single_notebook(self, output, resources): &quot;&quot;&quot;Step 3: Write the notebook to file This writes output from the exporter to file using the specified writer. It returns the results from the writer. Parameters ---------- output : resources : dict resources for a single notebook including name, config directory and directory to save output Returns ------- file results from the specified writer output of exporter &quot;&quot;&quot; if 'unique_key' not in resources: raise KeyError(&quot;unique_key MUST be specified in the resources, but it is not&quot;) notebook_name = resources['unique_key'] if self.use_output_suffix and not self.output_base: notebook_name += resources.get('output_suffix', '') write_results = self.writer.write( output, resources, notebook_name=notebook_name) return write_results 所以看完上述的整理流程後，我們可以整理一下整體的 psuedo code 以及相關的安裝環境。 pip install ipython data=read_file(&quot;xxxx.ipynb&quot;) parse_json(data) filter_ipython(data) #get the exporter for script format cls = get_exporter(&quot;script&quot;) #initial the resources init_single_notebook_resources(...) #export the data to python script export_single_notebook(...,data) #writhe the output to file write_single_notebook(...) 由於這部分不會太麻煩，所以這邊就列了一個大概的 pseudo code，經過實際測試也是真的可以運作的。 整篇文章到這邊也就要結束了，主要是練習直接透過程式碼的方式進行 Jupyter Notebook 的轉換，同時也能夠有中間資料過濾的能力。","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2017/paper-redesign-data-center","content":"","keywords":"","version":"Next"},{"title":"End-to-end Service Demans​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#end-to-end-service-demans","content":"本文列出了四個 data center 內應用程式要追求的特性。 ","version":"Next","tagName":"h2"},{"title":"Location Independence​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#location-independence","content":"一個分散式的應用程式其運行於data center內的任一機器上都不應該要影響應用程式本身的功能。由於目前都採用 clos network 的方式來設計網路拓樸，所以整個網路拓樸方面應該是能夠提供足夠的流量來使用，而不會變成一個理由或是瓶頸使得應用程式必須要選擇特定的機器才可以運行。 ","version":"Next","tagName":"h3"},{"title":"Low Latency​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#low-latency","content":"雖然 Clos Network 的架構能夠提供足夠的頻寬給拓墣中內的服務器，對於流量的負載平衡能提供良好的效果，但是對於短暫連線來說，並沒有辦法提供低延遲的能力。 作者認為雖然能夠提供高流量輸出是一個很重要的議題，但是能夠提供低延遲的能力則是更重要且優先度更高。 ","version":"Next","tagName":"h3"},{"title":"Incast​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#incast","content":"Incast 這個專有名詞的介紹可以參考Data Centers and TCP Incast - Georgia Tech - Network Congestion，簡單來說就是同時間有大量的request進到data center內，這些request都會產生對應的reply然後這些reply連線同時間一起進入到 switch內，這會使得 switch 的緩衝區立刻滿載沒辦法繼續承受封包，導致部分的 TCP 連線需要降速重送。 作者認為一個好的網路架構遇到這種問題時，應該要能夠保護背後的應用程式連線，讓這些連線應該要盡可能地維持低延遲性。 ","version":"Next","tagName":"h3"},{"title":"Priority​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#priority","content":"這邊特性老實說有點難以想像，我盡力的就我所瞭解的去解釋。 假設今天有一個應用程式，同時會發送不同的 request 到後端去處理，而這些 request 所產生的 reply 本身是有依賴的關係。 所以假如這多個 reply 沒有依照當初發送 request 的順序回來的話，在應用程式端這邊就必須要特別去處理，譬如說不要同時送多個request，不過這樣就沒有更好的效能表現。 因此對於應用程式來說，若本身能夠有一個優先度的機制，能夠決定那些封包要先處理，就可以解決上述的問題，而讓應用程式本身就可以更自由的去處理。 ","version":"Next","tagName":"h3"},{"title":"Transport Protocol​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#transport-protocol","content":"目前 data center 內部採用的 Transport Protocol 雖然可以處理上述應用程式的問題，但是為了處理那些問題，反而會失去下列特性。 而 NDP 本身在設計時，希望能夠在滿足上述應用程式的需求，同時又保有下列的特性。 ","version":"Next","tagName":"h2"},{"title":"Zero-RTT connection setup​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#zero-rtt-connection-setup","content":"目前主流的 TCP 協定再傳送資料前，必須要先進行一次三項交握，這意味者當 data 送出去前，至少要先花費 RTT*3 的等待時間。 對於低延遲的應用程式來說，希望能夠達到 RTT*0，至少 RTT*1 的等待時間就能夠將資料送出，這意味者對 NDP 來說，在資料發送前，整個連線不需要有三方交握類似的行為，才可以一開始就直接送出資料。 ","version":"Next","tagName":"h3"},{"title":"Fast start​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#fast-start","content":"Data Center 不同於網際網路的地方在於網路拓樸中的每個 Switch/Link 都是自己掌握的。 所以 TCP 採用的 Slow Start 其實對於 Data Center 來說是沒有效率的，畢竟一開始就可以知道可用頻寬多少，不需要如同面對網際網路般的悲觀，慢慢地調整 Window Size，而是可以一開始就樂觀的傳送最大單位，在根據狀況進行微調。 若採取這種機制，則應用程式可以使用更快的速度去傳送封包。 ","version":"Next","tagName":"h3"},{"title":"Per-packet ECMP​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#per-packet-ecmp","content":"在 Clos Network 中，錯綜複雜的連結狀態提供了 Per-flow ECMP 一個很好發揮的場所，可以讓不同的連線走不同的路徑，藉此提高整體網路使用率。 然而如果今天 Hash 的結果相同的話，其實是有機會讓不同連線走相同的路徑，即使其他路徑當前都是閒置的。 若採取 MPTCP 這種變化型的 TCP 來處理的話，該協定本身的設計可以解決上述的問題，但是對於短暫流量或是延遲性來說，卻沒有很好的效果。 為了解決這個問題，如果可以將 Per-flow ECMP 轉換成 Per-Packet ECMP。因此 NDP 本身協定的設計就是以 Per-Packet ECMP 為主。 ","version":"Next","tagName":"h3"},{"title":"Reorder-tolerant handshake​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#reorder-tolerant-handshake","content":"假設我們已經擁有了 Zero RTT 以及 Per-packet ECMP 兩種特性，擇一條新連線的封包可能就會發生 Out of order 的情況，收送順序不同的狀況下，若對於 TCP 來說，就會觸發壅塞控制的機制進而導致降速。 因此 NDP 在設計時，必須要能夠處理這種狀況，可以在不依賴封包到達先後順序下去處理。 ","version":"Next","tagName":"h3"},{"title":"Optimized for Incast​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#optimized-for-incast","content":"即使整個 data center 的網路環境，如頻寬等資訊一開始就已經可以掌握， incast 的問題還是難以處理，畢竟網路中變化太多，也許有某些應用程式就突然同時大量產生封包，這些封包同時間到達 switch 就可能導致封包被丟棄。 因此 NDP 本身在設計時，也希望能夠解決這個問題。 ","version":"Next","tagName":"h3"},{"title":"Switch Service Model​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#switch-service-model","content":"在一個 data center內，除了應用程式特定，傳輸層協議(Transport Protocol)之外，還有一個性質也是很重要的，這個性質與上述兩個性質關係緊密，一起影響整個網路的運作，這個性質就是 Switch Service Model。 作者認為這性質中，最重要的就是當 switch port 發生阻塞時會怎麼處理，這個性質會完全影響到傳輸層協定以及壅塞控制演算法(Congestion control algorithms)的設計，甚至是傳輸相關的概念，如 per-packet ECMP 都會被影響到。 作者提到，目前有很多種預防壅塞機制，譬如 Loss as a congestion，Duplicate or selective Acks等，其中 Duplicate or selective Acks 會主動去觸發重送，這種技巧對於長時間連線來說是好的，但是對於需要低延遲的短暫連線來說是不好的，主要是這些重送都會經過一個 RTO(Retransmission timeouts)，這個時間的長短都會產生一些負面的影響，因此也不是一個萬用的解法。 ECN(Explicit Congestion Notification) 的出現幫助壅塞狀況提供了不少改進，譬如 DCTCP (data center TCP 就採用了這個技術來輔助。這個技術對於長時間的流量來說，能夠有效的減少封包遺失的數量，然而對短暫流量來說卻沒有太大的幫助，因為短暫流量太短暫了，根本來不及去接收 ECN的封包並處理。在現實情況中，switch本身會採用一個較大的緩衝區來處理資料封包以及ECN封包，這種設計對於 incast 的現象能夠有效地減緩封包被丟棄的情形。但是也因為緩衝區太大，每個封包在 Switch 內待的時間就會更長，因此重送機制的時間就會被拉長，導致低延遲性就會破功。 除此之外，對於 Lossless Ethernet 的傳輸來說，則是會透過 802.3X 的 Pause Frame 亦或是 802.1 Qbb 的 PFC (Priority Flow Control)。這類技術都是透過控制封包阻止送端停送封包，降低封包遺失的數量。然後這些就有技術也不是十全十美，以 PFC 來說，在一個高度乘載的網路拓樸中，有可能會有兩條不同的連線根據Hash導向走向相同，同時這兩個連線最初的設定是相同的優先度，則有可能會因為一條連線的壅塞進而導致其他連線也被影響到。 上述這些壅塞機制的設計除了 ECN 之外，其餘的對於 Per-packet ECMP 都沒有能夠提供良好的效果。 最後，提到了一種稱為 CP (Cut Payload) 的做法，這種機制就是當壅塞發生時，不丟棄整個封包，而只是丟棄封包的資料，而保留封包的標頭檔，對於一個常見的TCP封包來說，大概就是 (XX:14XX)的感覺。當應用程式端收到只有標頭檔，沒有資料的封包，就可以知道這個當前線路有壅塞的情況，可以進行對應的處理。 這種情況下，就可以避免減少 RTO 對於重送造成的時間延遲。 然而這種設計也有兩個問題(畢竟CP是1994提出的論文) 整個 Switch 採取FIFO的機制，所以這些被砍掉資料的封包，也是要慢慢等待switch將封包處理完畢才可以往外轉送，這邊就會大幅增加這些封包的RTT時間這個設計是基於 single-path 傳輸，沒有考慮到多條路徑傳輸，不論是 per-packet 或是 per-flow 的ECMP。 在看完了以上這麼多段落後，大概可以歸納出作者心目中的完美協定，簡單複習一下 對於短暫流量能夠提供低延遲，低延遲，低延遲對於長時間流量盡量能夠提供高流量輸出per-packet ECMP遇到 incast 等情況也能夠繼續保持低延遲 因此接下來的章節，就會講述 NDP 到底如何實作並且盡可能地有提供上述性質。 Design 為了滿足上述的條件，NDP 在設計時就決定整個設計要包含完整架構，包含了 1.switch的行為 2.routing3.全新的 Transport協定 基本上可以使用下圖做一個概括性的統整，透過全新設計的 NDP 架構，能夠滿足該圖中的每個特性。 接下來會簡單介紹每個區塊大致上要做的事情。後面的章節才會詳細敘述其設計方法。 對於 Clos Network 拓樸來說，因為網路內擁有多條錯綜複雜的連結，因此整體網路的頻寬是足夠的。這對於想要執行 Load-Balanced 的應用來說是綽綽有餘的，然而為了避免流量都流經相同的路徑導致某些路徑上面有大量的流量進而導致封包掉落並且對於延遲性與流量輸出都有影響。因此必須要採取必要的措施將連線給打散到不同的路徑之中。 此外，對於短暫連線來說，per-packet的ECMP相對於 per-flow來說更有意義，但是per-packet的ECMP必定會遇到封包順序不同產生的 out of order情況。 為了滿足短流量低延遲的效果，發送端不能利去探測當前網路的頻寬來決定傳送速度，必須要在第一個封包就將資料送出，也就是所謂的 Zero RTT。 對於 switch 來說，要達到低延遲，則緩衝區不能夠太大，否則封包待在緩衝區內的時間過長，就會導致該封包要花更長的時間才能夠被送過去。 在小緩衝去的前提下，封包數量一多則緩衝區就會馬上爆滿。當封包掉落同時加上因為 per-packet ECMP 所產生的封包順序錯亂問題 (out of order)，這會使得收送端(receiver)會非常難去處理這些問題，無法辨別當前沒有收到封包到底是什麼原因。 若想要完整的避免封包遺失，則 switch 的緩衝區就要夠大，譬如採取 pause frame，亦或是 PFC這類型 lossless network。 然而這類技術都會因為當前連線產生的壅塞判斷去影響網路中其他連線，這就會使得某些連線沒有辦法達到低延遲的效果。 所以NDP設計的理念是一種介於 loss 與 lossless 的概念。 最後，採取的 Packet Trimming，其設計理念與 Cuy Payload 極為相似，此種情況下， switch 採用小緩衝區，當接收端發現封包只有標頭沒有內容時，就可以判斷當前網路有壅塞發生。為了可以讓重送時間盡可能的快，對於這些被截斷內容的封包， switch 應該要盡可能的快讓該類型的封包送出去。 這類型的封包到達接收端後，會讓接收端有資訊可以知道當前那些應用的封包到達，藉由這些資訊加上一個完整的封包池(receiver-pulled pool)，接收端就可以精準地控制那些封包要優先處理。 ","version":"Next","tagName":"h2"},{"title":"NDP Switch Service Model​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#ndp-switch-service-model","content":"前述有提到過 Cut Payload 的技術，透過把資料丟掉，單純送標頭檔給對方。 這邊不丟掉封包的原因是希望能夠通知 接收端 端關於封包的訊息，讓 接收端 端可以要求 送端 重送。這整體所消耗的時間會比接收端依賴　Retransmission Timeout 來的還要短。 然而。本文作者發現到，若採用最原生的 Cut Payload 實作其實會引發下列問題，因此要將其改良以解決這些問題。 直接以下圖來解釋，在一個 9KB jumbograms 的情況下，去探討應用程式之間的 goodput， goodput 與 throughput 的差異是 goodput 代表的是應用程式真正接收到的封包量，不包含協定標頭檔以及重送封包所造成的流量。 圖中紅線的部分代表的是 原生 CP 實作，而藍線的部分代表則是 NDP 的實作。 X 軸代表的是網路中有多少條連結上面被大量被切斷的封包(被砍掉內容，只剩下標頭檔)。 Y 軸代表的是在當前網路下，應用程式真正收到的 goodput 佔理想值多少百分比。(最高100%)。 圖中實線代表的是所有應用程式的平均goodput，而虛線代表的是10%最差應用程式們的平均goodput。 這邊先來看原生CP的效果，可以明顯看到實線的部分，會因為流量愈多，而使得整體的 goodput 就大幾乎呈線性下線。 而對於虛線來說，整個幾乎慘不忍睹，代表後半部分的應用程式根本沒有辦法擁有好的傳輸量。 除了上述模擬結果外，原生CP還有一個問題是採取 FIFO 的機制來收送一般封包與被切斷後的封包，這意味者 CP 想要讓 收端盡快地知道有封包要重送，但是這些被截斷的封包又要等待 switch 內的緩衝區都被消耗掉才有機會被送出，其實這一來一回對於整體的是會造成一些損傷的。 為了解決上述原生CP 帶來的問題， NDP 提出三大改革 NDP Switch 維護兩個佇列緩衝區，低優先度的用於資料傳輸，高優先度的用於被截斷封包的傳輸，ACKS 以及 NACKS。 2. 作者認為這個設計聽起來有點詭異，看起來好像反而會讓資料封包更晚送出去，但是經過實驗證明，高優先度的封包到達接收端並且通知送端要重送封包時，通常 switch 低優先度緩衝去的封包都還沒有全部處理完畢。 高優先度跟低優先的佇列緩衝區彼此之間採取的是 10:1 的輪流機制，每送十個高優先度的封包，就送一個低優先度的封包，這可以讓要重送的封包盡早通知到接收端 為了打亂網路中平衡狀態(強者恆強，弱者恆弱)，每當低優先度的佇列滿載且遇到新封包時，這時候 Switch 有兩個行為會採用(機率分別是 50%)。 將新到的封包截斷，送到高優先度佇列將低優先度最後面的封包截斷，送到高優先度佇立，而剛進來的封包就送到低優先度佇列 藉由上述兩種行為，作者說可以打亂網路的平衡，這點可以由圖2的虛線看到，在 NDP 的環境中，即使是效能最差 10% 應用程式的 goodput 跟平均也是差不多的。 ","version":"Next","tagName":"h2"},{"title":"Routing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#routing","content":"前述已經提到， NDP 想要完成 per-packet ECMP而非 per-flow ECMP，於是作者在這邊提出了兩大類做法 讓 switch 本身隨機當前封包該怎傳送讓 送端 決定當前封包該怎傳送 根據作者自己的實驗證明，讓送端去選擇封包該怎轉送會比讓 switch 去隨機轉送封包來得更有效率，同時因為 switch 本身不用去思考怎該怎轉送封包，每個封包處理的速度可以更快速，因此 switch 的緩衝區可以設置的更小，對於低延遲的特性有更好的支持。 NDP 認為，由於本文的環境是在 data center 內，網路拓樸的狀況都是可以事先知道的，譬如從送端到接收端共有那些路徑可以走。 因此 NDP 的實作方式是 送端事先要先瞭解到送端之間有哪些路徑可以走，然後每遇到一個封包，就挑一個路徑出來發送，下一個封包就送剩下的路徑們中間隨機挑一條去發送，當所有的路徑都已經走過一次後，就全部重新來過。 這樣的做法達到兩個優點 1) 藉由分散封包到所有路徑去傳送，能夠提升網路整體的使用率，並且減少某條網路成為瓶頸的可能性。 2) 每次挑選時都透過類似隨機的方式，可避免多個應用程式會走到相同的走法導致網路出現太規律的走法，進而導致使用率不佳或是某些路徑過於壅塞。 至於，關於 讓送端決定當前封包該怎傳送 怎麼實作，作者提出了三種方法。 source-route: 作者沒解釋，不確定其實作方式。 label-switch: 事先規劃好那些 label 走那些路徑，然後送端幫封包上 label destination addresses: 根據目的端的位置來選擇，假如目的端有多個 IP 地址，則每個地址都可能有不同的走法。 ","version":"Next","tagName":"h3"},{"title":"Transport Protocol​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#transport-protocol-1","content":"接下來作者要詳細介紹整個 NDP Transport protocol 的設計。 NDP 在設計 Transport 協定時是基於 receiver-driven 的理念去設計的。希望藉由這個設計能夠跟之前提過的各式各樣技術結合，如 multipath forwarding、 packet trimming 以及 short switch queues。藉由與這些技術結合，NDP 想要提供1)低延遲2)高產出的效能。 作者這邊提到，TCP因為早期設計給網際網路使用，所以設計理念是悲觀的，因為不知道整個網路上每個節點的狀況，盡可能的小心去使用。譬如所謂的 slow start、三方交握等原則。 此外，對於 TCP 來說，當今天若同時發生網路壅塞掉封包或是多重路徑路由所導致封包的順序不一致，這些情況會讓 TCP 無法分辨到底發生什麼事情，於是最後只好要求送端重傳，這行為無形間就降低了整體的傳送速率。 然而對 NDP 來說就不一樣，由於是在 data center 內，能夠事先知道整個網路節點的各情況，因此 NDP 希望採取的是樂觀的設計理念，包含了 第一個 RTT 就送資料，不等三方交握一開始就根據當前線路的最大乘載量去設定傳送速率，假設都沒有其他流量使用。不依賴順序的協定內容 NDP 採用了截斷封包的方式，透過標頭檔的內容告訴接收端到底哪些封包需要重送，這使得接收端有更明確的知識知道網路發生什麼情況，因此封包到達的先後順序就顯得不重要。 接下來使用下圖來解釋截斷封包的運作模式。 假設今天從 SRC 要傳送九個封包到達 DST，當封包到達 DST 上層的 switch(TOR) 時，因為 switch 的資料緩衝區(低優先度佇列)只能承受八個封包。所以第九個封包就被截斷了(時間點為 T(trim)) 接下來透過不同優先度的佇列的設計，被截斷的封包可以在 T(header)的時間就馬上到達 DST， DST 在看到標頭檔後就馬上學習到第九個封包需要重送，因此馬上送個資訊回到送端。 此資訊到達送端的時間點為 T(rtx)，同時間 switch 還在處理剛剛的八個封包，大概處理到第二個左右。 當送端將第九個封包送到 switch 時，這時候 switch 處理到第六個左右，因此資料緩衝區不但有空間可以容納地九個封包，同時這過程中也沒有任何處於閒置的況狀，可以說是將緩衝區的使用率盡可能的提升。 所以到這邊為止，我們已經可以知道截斷封包能夠告訴收端哪些封包需要重送。 作者認為藉由上述的特性與 Zero RTT 的結合，認為讓送端去決定當收到一筆資料(Zero RTT)後送端要傳送多少封包是最佳的選擇。 接下來分成兩部份去介紹，第一部分介紹其設計理念及原則，第二部分則是詳細介紹步驟 當送端一開始用全速去傳送封包後，接下來就會停止傳送封包等待 接收端 發送請求要求 送端 繼續送封包接收端可以根據當前本身收送封包的速率(譬如對外連結上有多條連線導致每條連線都沒有辦法用到最高速度)，因此會由 接收端 去決定 送端 當前要送多 快。不論是重送的封包，抑或是全新的資料封包，接收端都可以要求送端去傳送。接收端 採用 Pull 的概念來達到控制 送端 的傳送速率，接收端 會維護一個 Pull 佇列，供所有連線一起使用。Pull 佇列內放置的是 Pull 封包，該封包要標示屬於哪個連線，以及一個計數，而該計數則是 per-sender的，用來記錄當前發送過多少個 Pull 封包。Pll 佇列預設會公平的去處理每個連線所對應的 Pull 封包，不過可以根據連線優先度調整其傳送的 Pull 封包數量，藉此提高對應送端的傳送速率。 詳細描述每個步驟行為如下 連線開始時，送端一開始就用全速將封包送往 接收端，這些封包都會包含類似 TCP的序號。若接收端收到的是被截斷的封包，會立刻傳送 NACK 給送端，要求其將準備重送封包(還不需要重送)若接收端收到的是資料封包，會立刻傳送 ACK 給 送端，通知其該封包已接收到(清除舊有資料，準備新封包，此時也還不會傳送)當 接收端 收到封包(資料或是截斷封包)，都會馬上加入一個 Pull 封包到其 Pull 佇列中。當 送端 收到由 接收端所發送的 Pull 封包時，會根據 Pull 封包內的計數來決定當下要傳送多少封包(可以是重送封包，也可以是新封包)當 送端 將所有封包都傳送完畢後，就會將最後一個封包打上標誌，當 接收端 看到此標誌時就知道對應的 送端 已經沒有封包要送了，就將其對應的 Pull 風包給移除，避免下次產生不必要的請求傳輸。若送端 之後又有資料想要傳輸，則必須主動傳輸給 接收端，而不會透過 接收端的 Pull 封包來取得。 根據作者本身的實驗數據，其相關設定如下 switch 的資料緩衝區只有八個封包MTU 設定為 9K (jumbograms)，switch 能力為 10Gb/s環境為 FatTree 在此環境下，每個封包完整處理的時間為 7.2µs，若考慮到優先佇列的使用，最差情況下的處理時間是 400µs，此數據相對於一般來說其實是非常小的。 因此在 NDP 的環境中，RTO (Retransmission timeout) 可以設計得更小。 回過來看先前提過的問題， NDP 要如何在 incast 的狀況下有良好表現? 假設一開始有很多個 送端，每個都用全速傳輸。可以想像得到的是會有很多封包都被截斷，接者每個對應的 接收端 都可以採用 Pull 的概念控制對應 送端的傳送速率，就可以讓每個送端的傳送速率總和能夠符合 switch 的處理速度，藉此能夠讓被截斷的封包數量降到最小，甚至沒有。 ","version":"Next","tagName":"h2"},{"title":"Coping with Reordering​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#coping-with-reordering","content":"根據 Per-packet 多重路徑路由，對於送端/接收端來說，資料封包，Pull 封包， ACK 以及 NACK 在收到的時候是非常有機率是順序錯亂的。雖然 NDP 一開始的設計是不用擔心這個情況的。 不過作者提下有提了一個情境是關於 Pull 的問題，這邊實在講得太難讓人理解，我看了好久，思索許久，還是無法理解到底在說什麼。 所以決定暫時放棄這個段落。 ","version":"Next","tagName":"h2"},{"title":"The First RTT​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#the-first-rtt","content":"NDP 為了達到能夠在第一個封包就直接送資料而不採用 TCP 的三項交握後送資料，必須要處理三個問題 避免遇到大量相通 Source IP 的垃圾封包影響 (因為 TCP有連線認證，沒有連線的 TCP 封包不會被處理，避免被大量垃圾影響) 避免同樣一條連線在 連線建立方面不小心處理多次處理在 第一個 RTT 之間經由多重路徑路由造成順序錯亂的資料封包 目前已知採用 First RTT 的實作，如 T/TCP 以及 TFO (TCP Fast Open)都有一些相關機制，譬如採用 Token 或是 Connection ID 來處理上述問題，但是沒有一個實作能夠同時處理三個問題。 這個問題 NDP 不想處理，認為可以透過 Hypervisor/NIC 之類的方式預防 逃避問題? 在 送端/接收端 兩邊都設計了 time-wait 的狀態。 這邊也看不太懂，到底要怎麼透過這個狀態處理此問題。估計不是重點，敘述很少。 由於 per-packet ECMP 的關係，第一個到達接收端的封包往往不是第一個送出的封包，為了處理這個問題，送端第一次送出的封包都會帶有一個SYN的標誌以及該封包是第一次送出封包中的第幾個。藉由這個資訊就可以讓任何一個第一次送出中的任一個封包來建立連線。 ","version":"Next","tagName":"h2"},{"title":"Robustness Optimizations​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#robustness-optimizations","content":"假如網路一切都順順利利的運行，上述 NDP 的實作其實運作得非常良好，然而好景不常，網路常常會出問題，譬如 switch 或是 link 會損毀。 這種情況下， NDP 要怎麼處理這些問題來繼續保持其提供的低延遲與高輸出的特性。 常見的問題有 switch, link 損毀導致封包不通link 出問題，突然降速，譬如從 10Gbs 降速成 1Gbs 作者表示，傳統的 single-path TCP 對於這些問題都沒有很好的處理方式，會讓整體的效能大幅度下降。 NDP 處理的方式就是採用大量的 Counter，去記住每條連線上面關於 ACK 以及 NACK 的數量，同時透過 per-packet 多重路由的方式來傳送封包。 因此 送端 會定期檢查，若當下所有可傳送路徑中，有某些路由上面的 counter 數值異常，代表可能出現問題，不論是降速抑或是網路不通。 此時就會將有問題的路由先從候選清單中排除，避免封包走到有問題的路由去。 作者表示，傳統的作法大部分都依賴路由協定去偵測問題並且動態修改路由表，這些雖然有效，但是要花費太多時間去處理，因此效果不彰。 ","version":"Next","tagName":"h2"},{"title":"Rteurn-to-Sender​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#rteurn-to-sender","content":"前述提過，採用 Cut Payload 的機制能夠有效的解決 incast 的網路問題，然而當 incast 的封包過多時， switch 上面的兩個佇列(高/低優先度)都可能會被塞滿，在這種情況下封包就會被丟棄了。 舉例來說，高優先度佇列可以存放1125 * 64-byte的封包量，而低優先度佇列只能存放 8 * 9K-bye 的封包量。假設 送端發送了一個封包過去，卻遲遲沒有等到回應，在經過 RTO 的時間後，就會重送封包。 根據前述的實驗， 最大的 RTT 時間是 400µs，因此作者認為最大的 RTO 設定為 1ms 即可。 然而根據 10Gbs　的傳輸能力來開，要處理1125 * 64B 的封包大概需要 8ms，這意味者重送的封包是可以在佇列被消耗完畢前送達到 switch，可以保障不會有 switch 處於閒置的狀態進而提高整體使用率。 為了追求極致的作者認為，在某些情況下，譬如高優先度的封包或是資料量極小的封包 當所有的佇列都滿的情況下，這時候 switch 就會採取神秘的作法，將封包的 source IP 以及 destination IP 給反轉，然後將該 header 迅速送回給 送端，告訴送端說你有封包要掉了，快重送。 然後這種機制只有在某些特殊情況下，可能網路當下有些問題之類的，作者想要加快更快的處理速度避免等待 RTO來處理 下圖是根據模擬環境得到的數據圖，該模擬環境如下 432 機器FatTree 該Y軸採用的是 CDF(cumulative distribution function)，X 軸紀錄的是當送端送出第一個封包到達其收到第一個ACK所花的時間。 此數值包含所有的 delay，也包含重送所導致的花費。 Permutation 顯示的是 432-node 中每個node都會往下一個 node 來進行連線，故會有 431 條連線Random 顯示的是 432-node 中每個node都會隨機找一個 node 來進行連線，故會有 431 條連線 上述這兩種 case 都表現不錯，能夠完整的用滿整個 FatTree 的網路頻寬，同時平均的 delay 大概落在 100ms。 而接下來 Incast 的情況則是挑選100 node 同時間對同一個 node 進行發送的實驗，變異數則是每個連線的傳輸大小。 可以觀察到的，當傳送數量只有 135KB 時，整體的效能是比較差的。 原因是每個連線都能透過第一次的RTT就將所有的封包傳送出去，這情況導致大量的封包被截斷甚至超過緩衝區的數量。同時大概只有 25%% 左右的標頭檔被最佳化給送回給 送端去加速處理。 以 delay 來看，最後一個封包花了 11,055µs 去傳送。 假設今天整體傳送量更大，到達了 1350KB，雖然第一個封包傳輸的行為會跟 135KB 的實驗一致，但是後續 NDP 的設計能夠讓整體後續的處理更為平順以及更穩，所以其平均的處理時間大概只有 95µs。 ","version":"Next","tagName":"h2"},{"title":"Congestion Control​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#congestion-control","content":"作者表示， NDP 本身沒有任何 Congestion Control，因為 Congestion Control 本身是為了下列兩個目的而誕生的。 Avoiding congestion collapsefairness 而 NDP 本身實作的特性已經完全避免掉上述的問題，所以根本沒有必要去實作沒必要的 Congestion Control。 NDP 的設計下，每個連線一開始都樂觀地採用全速發送，同時因為協議設計的關係，接收端擁有大量關於當前連線的資訊，透過 pull 佇列的設計能夠讓每條連線平均的使用當前網路流量。 不過作者也有提到，由於接收端可以決定送端傳送封包的速率，因此若有些連線被標註是高優先度的，則可以使用比一般連線更多的流量。這種情況下就是故意造成不公平的，因此也不是大問題。 ","version":"Next","tagName":"h2"},{"title":"Limitation of NDP​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2017/paper-redesign-data-center#limitation-of-ndp","content":"接下來的章節中，作者透過實驗證明了在各種流量模式中，NDP 提供的效果非常接近於 Clos 網路架構理論上的效能。 在非對證的網路架構中，例如 BCube、 Jellyfish， NDP 的表現會稍微差一點，主要是當前網路壅塞時，送端會將封包透過不同距離的路徑來傳輸封包(就不是ECMP了)。於這種情況下，若採用 sender-based per-path 的多重路由就會有良好的效果。 有一篇paper在講述上述的問題 C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and M. Handley. Improving datacenter performance and robustness with Multipath TCP. In Proc. ACM SIGCOMM, Aug. 2011. 對於一個超級高度負載的網路中心來說，當封包數量過多的時候，會發生送端發生的封包一直處於被截斷的情況，然而雖然表現不好，但是跟目前已知的協定，譬如 DCTCP 比起來， NDP 的效能還是勝出。 最後的問題就是 環境部屬問題， 只要當哪一天 P4 交換機能夠廣泛的部屬在 data center 中的時候，要部屬 NDP 就是很簡單的事情。此外，如何跟現有的 TCP 連線共存也是一個問題，因為 NDP 目前會吃掉該網路的流量導致 TCP 幾乎沒有辦法使用，因此在 P4 交換機端應該要有辦法去處理相關的問題。 Summary 到這邊為止，已經可以大概知道 NDP 的設計思維，接下來就要探討其如何實作以及其實驗效能。 不過必須說，只有Paper而缺少投影片或是影片的情況下，很多敘述都要靠想像力去思考到底怎實作，花了不少時間在思考，甚至有些情境還想不出來到底是什麼，只能憑感覺去想像。當然，這個會有這些問題其實是因為自己知道的東西還不夠多，所以變成很多作者認為是基本概念的東西，對我來說都要重新思考學習，才會導致自己思考不夠完善。 只好繼續多念書多加強自己了 ","version":"Next","tagName":"h2"},{"title":"Install LXR on Ubuntu 16.04","type":0,"sectionRef":"#","url":"/docs/techPost/2017/lxr-setup-with-multiple-projects","content":"","keywords":"","version":"Next"},{"title":"Environment​","type":1,"pageTitle":"Install LXR on Ubuntu 16.04","url":"/docs/techPost/2017/lxr-setup-with-multiple-projects#environment","content":"Ubuntu 16.04Lxr 2.2.1 You can download the tarball from here Ceph Github Page DRBD-9.0 Git Page ","version":"Next","tagName":"h2"},{"title":"Prepare LXN Environment​","type":1,"pageTitle":"Install LXR on Ubuntu 16.04","url":"/docs/techPost/2017/lxr-setup-with-multiple-projects#prepare-lxn-environment","content":"Install the required softwared perl (5.10 or above version)ctags (5.8 or above version) apt-get install perl apt-get install exuberant-ctags  Install the Datbase, I choosed the MySQL for my lxr env. apt-get install mysql-server  Install Perl DBI Install the CPAN for Perl.Type the following command to install DBI cpan DBI  Install the Web Server, I choose the Apacher for my lxr env. Also install perl module. apt-get install apache2 apt-get install libapache2-mod-perl2  Install free-text searching engine, I choose Glimpse. Download from the github page and follow the README.install to install. git clone https://github.com/gvelez17/glimpse cd glimpse ./configure make make install  Install Perl Module File::MMagicDBD::mysql apt-get install libmysqlclient-dev cpan File::MMagic cpan DBD::mysql  ","version":"Next","tagName":"h2"},{"title":"Install LXR​","type":1,"pageTitle":"Install LXR on Ubuntu 16.04","url":"/docs/techPost/2017/lxr-setup-with-multiple-projects#install-lxr","content":"Download the LXR-2.2.1 and expand the tarball Download from lxr-2.2.1.tgzDecompress into the /opt direcotory (choose the path you like) and rename to lxr Check the environment cd /opt/lxr ./genxref --checkonly  The result will like below. ERROR: could not open configuration file lxr.conf [ OK ] Perl version ... 5.22.1 Parameter 'ectagsbin' not defined - trying to find ctags ctags found at /usr/bin/ctags [ OK ] ctags version ... 5.9 Parameter 'glimpsebin' not defined - trying to find glimpse glimpse found at /usr/local/bin/glimpse Checked: glimpse version ... 4.18.7 Parameter 'glimpseindex' not defined - trying to find glimpseindex glimpseindex found at /usr/local/bin/glimpseindex Checked: glimpseindex version ... 4.18.7 Parameter 'swishbin' not defined - trying to find swish-e swish-e not found, `command -v swish-e` returned a null string genxref stopped without indexing by --checkonly option  Since we have not config the LXR, we won't have the lxr.conf. We choose the glimpse as our search engine and we can ignore the warning of swish-e. ","version":"Next","tagName":"h2"},{"title":"Configure LXR​","type":1,"pageTitle":"Install LXR on Ubuntu 16.04","url":"/docs/techPost/2017/lxr-setup-with-multiple-projects#configure-lxr","content":"Since the GIT type of source project doesn't support the submodule reference in LXR, we use FILE instead of. Before we generate the code reference, we should update code by ourself.Prepare the source fo DRBD and CEPH. I put them in /opt/lxr/source_code We refer to the master branch of ceph, for rdbe, is version 9.0. mkdir -p /opt/lxr/source_code/drbd mkdir -p /opt/lxr/source_code/ceph git clone --recursive http://git.drbd.org/drbd-9.0.git /opt/lxr/source_code/drbd/9.0 git clone --recursive http://git.drbd.org/drbd-9.0.git/tags /opt/lxr/source_code/ceph/master  Create both LXR and database configuration via tool configure-lxr.pl cd /opt/lxr ./scripts/configure-lxr.pl -vv  *** LXR configurator (version: 2.2) *** LXR root directory is /opt/lxr Configuration will be stored in custom.d/ Configure for single/multiple trees? [S/m] &gt; m *** LXR web server configuration *** Many different configurations are possible, they are related to the way LXR service is accessed, i.e. to the structure of the URL. Refer to the User's Manual for a description of the variants. LXR can be located at the server-root (so called dedicated) or lower in the server hierarchy (shared because there are usually other pages or sections). Server type? [dedicated/SHARED] &gt; SHARED Selecting which tree to display can be done in various ways: 1. from the host name (all names are different), 2. from a prefix to a common host name (similar to previous) 3. from the site section name (all different) 4. from interpretation of a section name part (similar to previous) 5. from the head of script arguments Method 5 is highly recommended because it has no impact on webserver configuration. Method 3 is second choice but involves manually setting up many symbolic links (one per source-tree). Method 1 &amp; 2 do not involve symbolic links but need populating webserver configuration with virtual hosts. Note that method 2 does not work well on //localhost. Method 4 is deprecated because it has proved not easily portable under alternate webservers (other than Apache). Tree designation?: ARGUMENT section name prefix in hos hostname embedded in section &gt; ARGUMENT The computer hosting the server is described by an URL. The form is scheme://host_name:port where: - scheme is either http or https (http: can be omitted), - host_name can be given as an IP address such as 123.45.67.89 or a domain name like localhost or lxr.url.example, - port may be omitted if standard for the scheme. --- Host name or IP? [//localhost] &gt; //127.0.0.1 --- Alias name or IP? &gt; URL section name for LXR in your server? [/lxr] &gt; /lxr Will it be shared by all trees? [YES/no] &gt; *** LXR database configuration *** The choice of the database engine can make a difference in indexing performance, but resource consumption is also an important factor. * For a small personal project, try SQLite which do not need a server and is free from configuration burden. * For medium to large projects, choice is between MySQL, PostgreSQL and Oracle. Oracle is not a free software, its interface has not been tested for a long time. * PostgreSQL databases are smaller than MySQL's and performance is roughly equivalent. * MySQL is at its best with large-sized projects (such as kernel cross-referencing) where it is fastest at the cost of bigger databases. * Take also in consideration the number of connected users. Database engine? [MYSQL/oracle/postgres/sqlite] &gt; The safest option is to create one database per tree. You can however create a single database for all your trees with a specific set of tables for each tree (though this is not recommended). How do you setup the databases? [PER TREE/global] &gt; All databases can be accessed with the same username and can also be described under the same names. Will you share database characteristics? [YES/no] &gt; Will you use the same username and password for all DBs? [YES/no] &gt; --- DB user name? [lxr] &gt; lxr --- DB password ? [lxrpw] &gt; lxrpw Will you give the same prefix to all tables? [YES/no] &gt; --- Common table prefix? [lxr_] &gt; --- Directory for glimpse databases? &gt; /opt/lxr/glimpse_db file .htaccess written into LXR root directory file apache2-require.pl written into configuration directory file apache-lxrserver.conf written into configuration directory file lighttpd-lxrserver.conf written into configuration directory file nginx-lxrserver.conf written into configuration directory file thttpd-lxrserver.conf written into configuration directory Mercurial support files written into configuration directory *** LXR master configuration file setup *** Global section part *** Configuring auxiliary tool paths *** Host name previously defined as http://104.154.246.9 *** Configuring HTML parameters *** 'Buttons-and-menus' interface is recommended for the kernel *** to avoid screen cluttering. --- Use 'buttons-and-menus' instead of 'link' interface? [YES/no] &gt; *** Configuring file subsection *** Configuring &quot;common factors&quot; *** Marking tree section *** LXR master configuration file setup *** Tree section part SQL script for database initialisation *** Configuring LXR server parameters *** The virtual root is the fixed URL part after the hostname. *** You previously defined the virtual root as /lxr --- Caption in page header? (e.g. Project XYZZY displayed by LXR) &gt; drbd Do you want a speed switch button for this tree ? [YES/no] &gt; --- Short title for button? (e.g. XYZZY) &gt; drbd --- Tree identification in URL? (e.g. the-tree) &gt; drbd Do you need a specific encoding for this tree ? [yes/NO] &gt; *** Describing tree location How is your tree stored? [FILES/cvs/git/svn/hg/bk] &gt; *** A source directory contains one sub-directory for every version. --- Source directory? (e.g. /home/myself/project-tree) &gt; /opt/lxr/source_code/drbd Name to display for the path root? (e.g. Project or $v for version) [$v] &gt; *** Enumerating versions Label for version selection menu? [Version] &gt; *** Versions can be explicitly enumerated, be read from a file or computed *** by a function. The latter case is recommended for VCS-stored trees. Version enumeration method? [LIST/file/function] &gt; --- Version name? &gt; No default choice, try again... --- Version name? &gt; 0.9 --- Version name? (hit return to stop) &gt; *** By default, first version in list is displayed. You may also indicate *** a prefered version. --- Default displayed version is first in 'range'? [YES/no] &gt; *** Setting directory lists *** Some directories may contain non-public project data (binaries, *** compilers caches, SCM control data, ...). They can be hidden from LXR. --- Directory to ignore, e.g. CVSROOT or CVS? (hit return to stop) &gt; *** If your source code uses &quot;include&quot; statements (#include, require, ...) *** LXR needs hints to resolve the destination file. --- Include directory, e.g. /include? (hit return to stop) &gt; *** Configuring data storage --- Database name? &gt; drbd Do you want to override the global 'lxr' user name? [yes/NO] &gt; Do you want to override the global 'lxr_' table prefix? [yes/NO] &gt; *** Configure another tree? [YES/no] &gt; , 'shortcaption' =&gt; 'drbd' *** Configuring LXR server parameters *** The virtual root is the fixed URL part after the hostname. *** You previously defined the virtual root as /lxr --- Caption in page header? (e.g. Project XYZZY displayed by LXR) &gt; Ceph Do you want a speed switch button for this tree ? [YES/no] &gt; --- Short title for button? (e.g. XYZZY) &gt; Ceph --- Tree identification in URL? (e.g. the-tree) &gt; Ceph Do you need a specific encoding for this tree ? [yes/NO] &gt; *** Describing tree location How is your tree stored? [FILES/cvs/git/svn/hg/bk] &gt; *** A source directory contains one sub-directory for every version. --- Source directory? (e.g. /home/myself/project-tree) &gt; /opt/lxr/source_code/ceph/ Name to display for the path root? (e.g. Project or $v for version) [$v] &gt; *** Enumerating versions Label for version selection menu? [Version] &gt; *** Versions can be explicitly enumerated, be read from a file or computed *** by a function. The latter case is recommended for VCS-stored trees. Version enumeration method? [LIST/file/function] &gt; --- Version name? &gt; master --- Version name? (hit return to stop) &gt; *** By default, first version in list is displayed. You may also indicate *** a prefered version. --- Default displayed version is first in 'range'? [YES/no] &gt; *** Setting directory lists *** Some directories may contain non-public project data (binaries, *** compilers caches, SCM control data, ...). They can be hidden from LXR. --- Directory to ignore, e.g. CVSROOT or CVS? (hit return to stop) &gt; *** If your source code uses &quot;include&quot; statements (#include, require, ...) *** LXR needs hints to resolve the destination file. --- Include directory, e.g. /include? (hit return to stop) &gt; *** Configuring data storage --- Database name? &gt; ceph Do you want to override the global 'lxr' user name? [yes/NO] &gt; Do you want to override the global 'lxr_' table prefix? [yes/NO] &gt; *** Configure another tree? [YES/no] &gt; no  Initail DB via tool initdb.sh ./custom.d/initdb.sh  Copy the lxr.conf from custom.d dir to root dir. cp ./custom.d/lxr.conf .  Generate Reference Generate the reference of project ceph ./genxref --url=http://localhost/lxr --tree=Ceph --version=master  Generate the reference of project drbd ./genxref --url=http://localhost/lxr --tree=drbd --version=9.0  Setup WEB Server Copy the server config to apache2 configuration dir. cp apache-lxrserver.conf /etc/apache2/conf-available a2enconf apache-lxrserver.conf  Start the apache2 service apaceh2 start  Test Go to http://localhost/lxr and your see there're two options there, ceph and drbd.choose any one of them and you can use that to help you trace the code now. Reference [LXR] (http://lxr.sourceforge.net/en/1-0-InstallSteps/1-0-install1tools.php) ","version":"Next","tagName":"h2"},{"title":"Ceph Network - AsyncConnection","type":0,"sectionRef":"#","url":"/docs/techPost/2017/ceph-async-connection","content":"","keywords":"","version":"Next"},{"title":"STATE_ACCEPTING​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_accepting","content":"因為先前切換到此狀態時，是透過 external event 呼叫的(只會執行一次)，所以這邊要將該 readable event handler (process) 正式的丟給 event center 一次接下來要發送一些訊息到對面去，這邊需要下列資訊 CEPH_BANNERAddr + Port 呼叫 try_send 去發送訊息 若成功 (r == 0), 狀態切換到 STATE_ACCEPTING_WAIT_BANNER_ADDR若失敗 (r &gt; 0),狀態切換到 STATE_WAIT_SEND，並且使用一個 strate_after_send 的變數來記住當成功送出後要切換成什麼狀態若失敗 (r &lt; 0),真的失敗了，就當作失敗處理。 1215 case STATE_ACCEPTING: 1216 { 1217 bufferlist bl; 1218 center-&gt;create_file_event(cs.fd(), EVENT_READABLE, read_handler); 1219 1220 bl.append(CEPH_BANNER, strlen(CEPH_BANNER)); 1221 1222 ::encode(async_msgr-&gt;get_myaddr(), bl, 0); // legacy 1223 port = async_msgr-&gt;get_myaddr().get_port(); 1224 ::encode(socket_addr, bl, 0); // legacy 1225 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; sd=&quot; &lt;&lt; cs.fd() &lt;&lt; &quot; &quot; &lt;&lt; socket_addr &lt;&lt; dendl; 1226 1227 r = try_send(bl); 1228 if (r == 0) { 1229 state = STATE_ACCEPTING_WAIT_BANNER_ADDR; 1230 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; write banner and addr done: &quot; 1231 &lt;&lt; get_peer_addr() &lt;&lt; dendl; 1232 } else if (r &gt; 0) { 1233 state = STATE_WAIT_SEND; 1234 state_after_send = STATE_ACCEPTING_WAIT_BANNER_ADDR; 1235 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; wait for write banner and addr: &quot; 1236 &lt;&lt; get_peer_addr() &lt;&lt; dendl; 1237 } else { 1238 goto fail; 1239 } 1240 1241 break; 1242 }  ","version":"Next","tagName":"h2"},{"title":"STATE_ACCEPTING_WAIT_BANNER_ADDR​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_accepting_wait_banner_addr","content":"讀取對方的封包的資訊(代表 Client Side 也必須要發送相關訊息) CEPH_BANNERAddr + Port 比較 banner 資訊若對方不知道自己的 addr，則透過 socket 的資訊取得並且記錄下來狀態改成 STATE_ACCEPTING_WAIT_CONNECT_MSG 1243 case STATE_ACCEPTING_WAIT_BANNER_ADDR: 1244 { 1245 bufferlist addr_bl; 1246 entity_addr_t peer_addr; 1247 1248 r = read_until(strlen(CEPH_BANNER) + sizeof(ceph_entity_addr), state_buffer); 1249 if (r &lt; 0) { 1250 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read peer banner and addr failed&quot; &lt;&lt; dendl; 1251 goto fail; 1252 } else if (r &gt; 0) { 1253 break; 1254 } 1255 1256 if (memcmp(state_buffer, CEPH_BANNER, strlen(CEPH_BANNER))) { 1257 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; accept peer sent bad banner '&quot; &lt;&lt; state_buffer 1258 &lt;&lt; &quot;' (should be '&quot; &lt;&lt; CEPH_BANNER &lt;&lt; &quot;')&quot; &lt;&lt; dendl; 1259 goto fail; 1260 } 1261 1262 addr_bl.append(state_buffer+strlen(CEPH_BANNER), sizeof(ceph_entity_addr)); 1263 { 1264 bufferlist::iterator ti = addr_bl.begin(); 1265 ::decode(peer_addr, ti); 1266 } 1267 1268 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; accept peer addr is &quot; &lt;&lt; peer_addr &lt;&lt; dendl; 1269 if (peer_addr.is_blank_ip()) { 1270 // peer apparently doesn't know what ip they have; figure it out for them. 1271 int port = peer_addr.get_port(); 1272 peer_addr.u = socket_addr.u; 1273 peer_addr.set_port(port); 1274 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; accept peer addr is really &quot; &lt;&lt; peer_addr 1275 &lt;&lt; &quot; (socket is &quot; &lt;&lt; socket_addr &lt;&lt; &quot;)&quot; &lt;&lt; dendl; 1276 } 1277 set_peer_addr(peer_addr); // so that connection_state gets set up 1278 state = STATE_ACCEPTING_WAIT_CONNECT_MSG; 1279 break; 1280 }  ","version":"Next","tagName":"h2"},{"title":"STATE_ACCEPTING_WAIT_CONNECT_MSG​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_accepting_wait_connect_msg","content":"讀取 connect_msg 大小的資料,並且存放到 AsyncConnection 的成員 connect_msg中，該結構如下，紀錄了如 feature, type 等資訊。最後將狀態轉換成 ** ** 0099 struct ceph_msg_connect { 0100 __le64 features; /* supported feature bits */ 0101 __le32 host_type; /* CEPH_ENTITY_TYPE_* */ 0102 __le32 global_seq; /* count connections initiated by this host */ 0103 __le32 connect_seq; /* count connections initiated in this session */ 0104 __le32 protocol_version; 0105 __le32 authorizer_protocol; 0106 __le32 authorizer_len; 0107 __u8 flags; /* CEPH_MSG_CONNECT_* */ 0108 } __attribute__ ((packed));  1282 case STATE_ACCEPTING_WAIT_CONNECT_MSG: 1283 { 1284 r = read_until(sizeof(connect_msg), state_buffer); 1285 if (r &lt; 0) { 1286 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read connect msg failed&quot; &lt;&lt; dendl; 1287 goto fail; 1288 } else if (r &gt; 0) { 1289 break; 1290 } 1291 1292 connect_msg = *((ceph_msg_connect*)state_buffer); 1293 state = STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH; 1294 break; 1295 }  ","version":"Next","tagName":"h2"},{"title":"STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_accepting_wait_connect_msg_auth","content":"根據之前讀取到的 connect_msg 來操作如果對方有設定 authorizer_len 的話，則在額外讀取 authorizer 相關的資訊設定 peer 的 host type根據 host type，取得對應的 policy呼叫 handle_connect_msg 處理該 connection_msg最後確認狀態已經不是本來的 STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH 狀態理論上要因為呼叫了 handle_connect_msg 而變換，正常來說要變成 STATE_ACCEPTING_WAIT_SEQ 1297 case STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH: 1298 { 1299 bufferlist authorizer_reply; 1300 1301 if (connect_msg.authorizer_len) { 1302 if (!authorizer_buf.length()) 1303 authorizer_buf.push_back(buffer::create(connect_msg.authorizer_len)); 1304 1305 r = read_until(connect_msg.authorizer_len, authorizer_buf.c_str()); 1306 if (r &lt; 0) { 1307 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read connect authorizer failed&quot; &lt;&lt; dendl; 1308 goto fail; 1309 } else if (r &gt; 0) { 1310 break; 1311 } 1312 } 1313 1314 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; accept got peer connect_seq &quot; 1315 &lt;&lt; connect_msg.connect_seq &lt;&lt; &quot; global_seq &quot; 1316 &lt;&lt; connect_msg.global_seq &lt;&lt; dendl; 1317 set_peer_type(connect_msg.host_type); 1318 policy = async_msgr-&gt;get_policy(connect_msg.host_type); 1319 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; accept of host_type &quot; &lt;&lt; connect_msg.host_type 1320 &lt;&lt; &quot;, policy.lossy=&quot; &lt;&lt; policy.lossy &lt;&lt; &quot; policy.server=&quot; 1321 &lt;&lt; policy.server &lt;&lt; &quot; policy.standby=&quot; &lt;&lt; policy.standby 1322 &lt;&lt; &quot; policy.resetcheck=&quot; &lt;&lt; policy.resetcheck &lt;&lt; dendl; 1323 1324 r = handle_connect_msg(connect_msg, authorizer_buf, authorizer_reply); 1325 if (r &lt; 0) 1326 goto fail; 1327 1328 // state is changed by &quot;handle_connect_msg&quot; 1329 assert(state != STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH); 1330 break; 1331 }  ","version":"Next","tagName":"h2"},{"title":"STATE_ACCEPTING_WAIT_SEQ​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_accepting_wait_seq","content":"從對面讀取其使用的 seq呼叫 discard_requeued_up_to 來處理，根據當前收到的 seq 來做條件 將 out_q 一些不符合條件的成員都移除 狀態改成 STATE_ACCEPTING_READY 1333 case STATE_ACCEPTING_WAIT_SEQ: 1334 { 1335 uint64_t newly_acked_seq; 1336 r = read_until(sizeof(newly_acked_seq), state_buffer); 1337 if (r &lt; 0) { 1338 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read ack seq failed&quot; &lt;&lt; dendl; 1339 goto fail_registered; 1340 } else if (r &gt; 0) { 1341 break; 1342 } 1343 1344 newly_acked_seq = *((uint64_t*)state_buffer); 1345 ldout(async_msgr-&gt;cct, 2) &lt;&lt; __func__ &lt;&lt; &quot; accept get newly_acked_seq &quot; &lt;&lt; newly_acked_seq &lt;&lt; dendl; 1346 discard_requeued_up_to(newly_acked_seq); 1347 state = STATE_ACCEPTING_READY; 1348 break; 1349 }  ","version":"Next","tagName":"h2"},{"title":"STATE_ACCEPTING_READY​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_accepting_ready","content":"清空 connect_msg狀態改成 STATE_OPEN如果當前 queue 內有資料(也許是先前 existing connection產生的?)，馬上送一個 write_handler 將其處理完畢 1351 case STATE_ACCEPTING_READY: 1352 { 1353 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; accept done&quot; &lt;&lt; dendl; 1354 state = STATE_OPEN; 1355 memset(&amp;connect_msg, 0, sizeof(connect_msg)); 1356 1357 if (delay_state) 1358 assert(delay_state-&gt;ready()); 1359 // make sure no pending tick timer 1360 if (last_tick_id) 1361 center-&gt;delete_time_event(last_tick_id); 1362 last_tick_id = center-&gt;create_time_event(inactive_timeout_us, tick_handler); 1363 1364 write_lock.lock(); 1365 can_write = WriteStatus::CANWRITE; 1366 if (is_queued()) 1367 center-&gt;dispatch_event_external(write_handler); 1368 write_lock.unlock(); 1369 maybe_start_delay_thread(); 1370 break; 1371 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open","content":"讀取 TAG，根據 TAG 不同的數值執行不同的事情 CEPH_MSGR_TAG_MSG: 代表有訊息近來，故將狀態切換成 STATE_OPEN_MESSAGE_HEADER 0343 case STATE_OPEN: 0344 { 0345 char tag = -1; 0346 r = read_until(sizeof(tag), &amp;tag); 0347 if (r &lt; 0) { 0348 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read tag failed&quot; &lt;&lt; dendl; 0349 goto fail; 0350 } else if (r &gt; 0) { 0351 break; 0352 } 0353 0354 if (tag == CEPH_MSGR_TAG_KEEPALIVE) { 0355 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; got KEEPALIVE&quot; &lt;&lt; dendl; 0356 set_last_keepalive(ceph_clock_now()); 0357 } else if (tag == CEPH_MSGR_TAG_KEEPALIVE2) { 0358 state = STATE_OPEN_KEEPALIVE2; 0359 } else if (tag == CEPH_MSGR_TAG_KEEPALIVE2_ACK) { 0360 state = STATE_OPEN_KEEPALIVE2_ACK; 0361 } else if (tag == CEPH_MSGR_TAG_ACK) { 0362 state = STATE_OPEN_TAG_ACK; 0363 } else if (tag == CEPH_MSGR_TAG_MSG) { 0364 state = STATE_OPEN_MESSAGE_HEADER; 0365 } else if (tag == CEPH_MSGR_TAG_CLOSE) { 0366 state = STATE_OPEN_TAG_CLOSE; 0367 } else { 0368 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; bad tag &quot; &lt;&lt; (int)tag &lt;&lt; dendl; 0369 goto fail; 0370 } 0371 0372 break; 0373 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_HEADER​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_header","content":"根據 feature 的值，決定要走新版還是舊版的 header讀取 header 大小的資料，然後將需要的資料都抓出來記錄下來。驗證對方送來資料的 CRC 是否正確將相關資料給 reset (這些結構都跟 header 有關) data_buffrontmiddledata 記錄收到時間的時間戳將狀態改變成 STATE_OPEN_MESSAGE_THROTTLE_MESSAGE 0435 case STATE_OPEN_MESSAGE_HEADER: 0436 { 0437 #if defined(WITH_LTTNG) &amp;&amp; defined(WITH_EVENTTRACE) 0438 ltt_recv_stamp = ceph_clock_now(); 0439 #endif 0440 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; begin MSG&quot; &lt;&lt; dendl; 0441 ceph_msg_header header; 0442 ceph_msg_header_old oldheader; 0443 __u32 header_crc = 0; 0444 unsigned len; 0445 if (has_feature(CEPH_FEATURE_NOSRCADDR)) 0446 len = sizeof(header); 0447 else 0448 len = sizeof(oldheader); 0449 0450 r = read_until(len, state_buffer); 0451 if (r &lt; 0) { 0452 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read message header failed&quot; &lt;&lt; dendl; 0453 goto fail; 0454 } else if (r &gt; 0) { 0455 break; 0456 } 0457 0458 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; got MSG header&quot; &lt;&lt; dendl; 0459 0460 if (has_feature(CEPH_FEATURE_NOSRCADDR)) { 0461 header = *((ceph_msg_header*)state_buffer); 0462 if (msgr-&gt;crcflags &amp; MSG_CRC_HEADER) 0463 header_crc = ceph_crc32c(0, (unsigned char *)&amp;header, 0464 sizeof(header) - sizeof(header.crc)); 0465 } else { 0466 oldheader = *((ceph_msg_header_old*)state_buffer); 0467 // this is fugly 0468 memcpy(&amp;header, &amp;oldheader, sizeof(header)); 0469 header.src = oldheader.src.name; 0470 header.reserved = oldheader.reserved; 0471 if (msgr-&gt;crcflags &amp; MSG_CRC_HEADER) { 0472 header.crc = oldheader.crc; 0473 header_crc = ceph_crc32c(0, (unsigned char *)&amp;oldheader, sizeof(oldheader) - sizeof(oldheader.crc)); 0474 } 0475 } 0476 0477 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; got envelope type=&quot; &lt;&lt; header.type 0478 &lt;&lt; &quot; src &quot; &lt;&lt; entity_name_t(header.src) 0479 &lt;&lt; &quot; front=&quot; &lt;&lt; header.front_len 0480 &lt;&lt; &quot; data=&quot; &lt;&lt; header.data_len 0481 &lt;&lt; &quot; off &quot; &lt;&lt; header.data_off &lt;&lt; dendl; 0482 0483 // verify header crc 0484 if (msgr-&gt;crcflags &amp; MSG_CRC_HEADER &amp;&amp; header_crc != header.crc) { 0485 ldout(async_msgr-&gt;cct,0) &lt;&lt; __func__ &lt;&lt; &quot; got bad header crc &quot; 0486 &lt;&lt; header_crc &lt;&lt; &quot; != &quot; &lt;&lt; header.crc &lt;&lt; dendl; 0487 goto fail; 0488 } 0489 0490 // Reset state 0491 data_buf.clear(); 0492 front.clear(); 0493 middle.clear(); 0494 data.clear(); 0495 recv_stamp = ceph_clock_now(); 0496 current_header = header; 0497 state = STATE_OPEN_MESSAGE_THROTTLE_MESSAGE; 0498 break; 0499 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_THROTTLE_MESSAGE​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_throttle_message","content":"在 Policy 中有兩個關於 Throttle 的變數 0085 /** 0086 * The throttler is used to limit how much data is held by Messages from 0087 * the associated Connection(s). When reading in a new Message, the Messenger 0088 * will call throttler-&gt;throttle() for the size of the new Message. 0089 */ 0090 Throttle *throttler_bytes; 0091 Throttle *throttler_messages;  這個 function 檢查是否有 throttle 訊息的數量限制，若有限制且超過上限，則建立一個 time event，並儲存下來。最後將狀態切換到 STATE_OPEN_MESSAGE_THROTTLE_BYTES 0501 case STATE_OPEN_MESSAGE_THROTTLE_MESSAGE: 0502 { 0503 if (policy.throttler_messages) { 0504 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; wants &quot; &lt;&lt; 1 &lt;&lt; &quot; message from policy throttler &quot; 0505 &lt;&lt; policy.throttler_messages-&gt;get_current() &lt;&lt; &quot;/&quot; 0506 &lt;&lt; policy.throttler_messages-&gt;get_max() &lt;&lt; dendl; 0507 if (!policy.throttler_messages-&gt;get_or_fail()) { 0508 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; wants 1 message from policy throttle &quot; 0509 &lt;&lt; policy.throttler_messages-&gt;get_current() &lt;&lt; &quot;/&quot; 0510 &lt;&lt; policy.throttler_messages-&gt;get_max() &lt;&lt; &quot; failed, just wait.&quot; &lt;&lt; dendl; 0511 // following thread pool deal with th full message queue isn't a 0512 // short time, so we can wait a ms. 0513 if (register_time_events.empty()) 0514 register_time_events.insert(center-&gt;create_time_event(1000, wakeup_handler)); 0515 break; 0516 } 0517 } 0518 0519 state = STATE_OPEN_MESSAGE_THROTTLE_BYTES; 0520 break; 0521 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_THROTTLE_BYTES;​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_throttle_bytes","content":"從 connection 讀取資料，分別對應到 header 中的三個成員 frontmiddledata 此 function 則是檢查 throttle 訊息的 bytes 數量，若數量超過上限，也是建議一個 time event並存下來，待之後處理最後將狀態切換到 STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE 0523 case STATE_OPEN_MESSAGE_THROTTLE_BYTES: 0524 { 0525 cur_msg_size = current_header.front_len + current_header.middle_len + current_header.data_len; 0526 if (cur_msg_size) { 0527 if (policy.throttler_bytes) { 0528 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; wants &quot; &lt;&lt; cur_msg_size &lt;&lt; &quot; bytes from policy throttler &quot; 0529 &lt;&lt; policy.throttler_bytes-&gt;get_current() &lt;&lt; &quot;/&quot; 0530 &lt;&lt; policy.throttler_bytes-&gt;get_max() &lt;&lt; dendl; 0531 if (!policy.throttler_bytes-&gt;get_or_fail(cur_msg_size)) { 0532 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; wants &quot; &lt;&lt; cur_msg_size &lt;&lt; &quot; bytes from policy throttler &quot; 0533 &lt;&lt; policy.throttler_bytes-&gt;get_current() &lt;&lt; &quot;/&quot; 0534 &lt;&lt; policy.throttler_bytes-&gt;get_max() &lt;&lt; &quot; failed, just wait.&quot; &lt;&lt; dendl; 0535 // following thread pool deal with th full message queue isn't a 0536 // short time, so we can wait a ms. 0537 if (register_time_events.empty()) 0538 register_time_events.insert(center-&gt;create_time_event(1000, wakeup_handler)); 0539 break; 0540 } 0541 } 0542 } 0543 0544 state = STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE; 0545 break; 0546 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_throttle_dispatch_queue","content":"如果剛剛有在 STATE_OPEN_MESSAGE_THROTTLE_BYTES 讀取到 front/middle/data 的資料的話，則這邊要確認 disaptch 本身的 throttle 有沒有超過，若超過也是送一個 time event 待稍後再來重新試試看紀錄 throttle 的時間戳狀態切換成 STATE_OPEN_MESSAGE_READ_FRONT 0548 case STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE: 0549 { 0550 if (cur_msg_size) { 0551 if (!dispatch_queue-&gt;dispatch_throttler.get_or_fail(cur_msg_size)) { 0552 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; wants &quot; &lt;&lt; cur_msg_size &lt;&lt; &quot; bytes from dispatch throttle &quot; 0553 &lt;&lt; dispatch_queue-&gt;dispatch_throttler.get_current() &lt;&lt; &quot;/&quot; 0554 &lt;&lt; dispatch_queue-&gt;dispatch_throttler.get_max() &lt;&lt; &quot; failed, just wait.&quot; &lt;&lt; dendl; 0555 // following thread pool deal with th full message queue isn't a 0556 // short time, so we can wait a ms. 0557 if (register_time_events.empty()) 0558 register_time_events.insert(center-&gt;create_time_event(1000, wakeup_handler)); 0559 break; 0560 } 0561 } 0562 0563 throttle_stamp = ceph_clock_now(); 0564 state = STATE_OPEN_MESSAGE_READ_FRONT; 0565 break; 0566 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_READ_FRONT​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_read_front","content":"接下來就是開始讀取真正的封包資料了，主要是分成三個部分 frontmiddledata 讀取前段資料，將內容先放到本身的 front 變數中將狀態切成 STATE_OPEN_MESSAGE_READ_MIDDLE 0568 case STATE_OPEN_MESSAGE_READ_FRONT: 0569 { 0570 // read front 0571 unsigned front_len = current_header.front_len; 0572 if (front_len) { 0573 if (!front.length()) 0574 front.push_back(buffer::create(front_len)); 0575 0576 r = read_until(front_len, front.c_str()); 0577 if (r &lt; 0) { 0578 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read message front failed&quot; &lt;&lt; dendl; 0579 goto fail; 0580 } else if (r &gt; 0) { 0581 break; 0582 } 0583 0584 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; got front &quot; &lt;&lt; front.length() &lt;&lt; dendl; 0585 } 0586 state = STATE_OPEN_MESSAGE_READ_MIDDLE; 0587 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_READ_MIDDLE​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_read_middle","content":"讀取中段資料，將內容先放到本身的 middle 變數中將狀態切成 STATE_OPEN_MESSAGE_READ_DATA_PREPARE 0589 case STATE_OPEN_MESSAGE_READ_MIDDLE: 0590 { 0591 // read middle 0592 unsigned middle_len = current_header.middle_len; 0593 if (middle_len) { 0594 if (!middle.length()) 0595 middle.push_back(buffer::create(middle_len)); 0596 0597 r = read_until(middle_len, middle.c_str()); 0598 if (r &lt; 0) { 0599 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read message middle failed&quot; &lt;&lt; dendl; 0600 goto fail; 0601 } else if (r &gt; 0) { 0602 break; 0603 } 0604 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; got middle &quot; &lt;&lt; middle.length() &lt;&lt; dendl; 0605 } 0606 0607 state = STATE_OPEN_MESSAGE_READ_DATA_PREPARE; 0608 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_READ_DATA_PREPARE​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_read_data_prepare","content":"準備好 buffer 供之後讀取 data 用，其中 data 部分除了長度外，還有 offset 也要處理這邊還不會讀取資料，只是會根據讀出來的空間長度預先配置一個空間供之後讀取資料使用將狀態切成 STATE_OPEN_MESSAGE_READ_DATA 0610 case STATE_OPEN_MESSAGE_READ_DATA_PREPARE: 0611 { 0612 // read data 0613 unsigned data_len = le32_to_cpu(current_header.data_len); 0614 unsigned data_off = le32_to_cpu(current_header.data_off); 0615 if (data_len) { 0616 // get a buffer 0617 map&lt;ceph_tid_t,pair&lt;bufferlist,int&gt; &gt;::iterator p = rx_buffers.find(current_header.tid); 0618 if (p != rx_buffers.end()) { 0619 ldout(async_msgr-&gt;cct,10) &lt;&lt; __func__ &lt;&lt; &quot; seleting rx buffer v &quot; &lt;&lt; p-&gt;second.second 0620 &lt;&lt; &quot; at offset &quot; &lt;&lt; data_off 0621 &lt;&lt; &quot; len &quot; &lt;&lt; p-&gt;second.first.length() &lt;&lt; dendl; 0622 data_buf = p-&gt;second.first; 0623 // make sure it's big enough 0624 if (data_buf.length() &lt; data_len) 0625 data_buf.push_back(buffer::create(data_len - data_buf.length())); 0626 data_blp = data_buf.begin(); 0627 } else { 0628 ldout(async_msgr-&gt;cct,20) &lt;&lt; __func__ &lt;&lt; &quot; allocating new rx buffer at offset &quot; &lt;&lt; data_off &lt;&lt; dendl; 0629 alloc_aligned_buffer(data_buf, data_len, data_off); 0630 data_blp = data_buf.begin(); 0631 } 0632 } 0633 0634 msg_left = data_len; 0635 state = STATE_OPEN_MESSAGE_READ_DATA; 0636 }  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_READ_DATA​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_read_data","content":"透過一個迴圈嘗試將資料讀取出來並且放到之前所預先配置的空間 DATA最後狀態切換到 STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH 0638 case STATE_OPEN_MESSAGE_READ_DATA: 0639 { 0640 while (msg_left &gt; 0) { 0641 bufferptr bp = data_blp.get_current_ptr(); 0642 unsigned read = MIN(bp.length(), msg_left); 0643 r = read_until(read, bp.c_str()); 0644 if (r &lt; 0) { 0645 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read data error &quot; &lt;&lt; dendl; 0646 goto fail; 0647 } else if (r &gt; 0) { 0648 break; 0649 } 0650 0651 data_blp.advance(read); 0652 data.append(bp, 0, read); 0653 msg_left -= read; 0654 } 0655 0656 if (msg_left &gt; 0) 0657 break; 0658 0659 state = STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH; 0660 } 0661  ","version":"Next","tagName":"h2"},{"title":"STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_open_message_read_footer_and_dispatch","content":"這個 function 比較長，不過可以說是最後一步驟了。跟 header 一樣，根據 feature 決定使用新舊版本的 footer 格式讀取 footer 的資料，如各區段的CRC等透過 decode_message 此 function，將收集到的 (front, middle, data, footer.etc) 組合成一個完整的 message 格式的封包 0270 Message *decode_message(CephContext *cct, int crcflags, 0271 ceph_msg_header&amp; header, 0272 ceph_msg_footer&amp; footer, 0273 bufferlist&amp; front, bufferlist&amp; middle, 0274 bufferlist&amp; data) .... 0315 // make message 0316 Message *m = 0; 0317 int type = header.type; 0318 switch (type) { 0319 0320 // -- with payload -- 0321 0322 case MSG_PGSTATS: 0323 m = new MPGStats; 0324 break; 0325 case MSG_PGSTATSACK: 0326 m = new MPGStatsAck; 0327 break; 0328 0329 case CEPH_MSG_STATFS: 0330 m = new MStatfs; 0331 break; 0332 case CEPH_MSG_STATFS_REPLY: 0333 m = new MStatfsReply; 0334 break; 0335 case MSG_GETPOOLSTATS: 0336 m = new MGetPoolStats; 0337 break; 0338 case MSG_GETPOOLSTATSREPLY: 0339 m = new MGetPoolStatsReply; ...  針對該 message 設定一些相關屬性 byte_throttlermessage_throttlerdispatch_throttle_sizerecv_stampthrottle_stamprecv_complete_stamp 針對 sequence 進行一些判斷，當前的 message 可能中間有遺漏，或是很久以前的 message將該 messaged 的 sequence 當作目前最後一個收到 sequence 0765 // note last received message. 0766 in_seq.set(message-&gt;get_seq());  將狀態改成 STATE_OPEN將本訊息塞入到 dispatch_queue 內，供應用層去處理到這邊就結束了，接下來 AsyncManager 去定期去檢查 dispatch_queue，當有封包進來後，就會將該封包送給所有註冊的 Dispatcher 去處理。 0662 case STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH: 0663 { 0664 ceph_msg_footer footer; 0665 ceph_msg_footer_old old_footer; 0666 unsigned len; 0667 // footer 0668 if (has_feature(CEPH_FEATURE_MSG_AUTH)) 0669 len = sizeof(footer); 0670 else 0671 len = sizeof(old_footer); 0672 0673 r = read_until(len, state_buffer); 0674 if (r &lt; 0) { 0675 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read footer data error &quot; &lt;&lt; dendl; 0676 goto fail; 0677 } else if (r &gt; 0) { 0678 break; 0679 } 0680 0681 if (has_feature(CEPH_FEATURE_MSG_AUTH)) { 0682 footer = *((ceph_msg_footer*)state_buffer); 0683 } else { 0684 old_footer = *((ceph_msg_footer_old*)state_buffer); 0685 footer.front_crc = old_footer.front_crc; 0686 footer.middle_crc = old_footer.middle_crc; 0687 footer.data_crc = old_footer.data_crc; 0688 footer.sig = 0; 0689 footer.flags = old_footer.flags; 0690 } 0691 int aborted = (footer.flags &amp; CEPH_MSG_FOOTER_COMPLETE) == 0; 0692 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; aborted = &quot; &lt;&lt; aborted &lt;&lt; dendl; 0693 if (aborted) { 0694 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; got &quot; &lt;&lt; front.length() &lt;&lt; &quot; + &quot; &lt;&lt; middle.length() &lt;&lt; &quot; + &quot; &lt;&lt; data.length() 0695 &lt;&lt; &quot; byte message.. ABORTED&quot; &lt;&lt; dendl; 0696 goto fail; 0697 } 0698 0699 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; got &quot; &lt;&lt; front.length() &lt;&lt; &quot; + &quot; &lt;&lt; middle.length() 0700 &lt;&lt; &quot; + &quot; &lt;&lt; data.length() &lt;&lt; &quot; byte message&quot; &lt;&lt; dendl; 0701 Message *message = decode_message(async_msgr-&gt;cct, async_msgr-&gt;crcflags, current_header, footer, 0702 front, middle, data, this); 0703 if (!message) { 0704 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; decode message failed &quot; &lt;&lt; dendl; 0705 goto fail; 0706 } 0707 0708 // 0709 // Check the signature if one should be present. A zero return indicates success. PLR 0710 // 0711 0712 if (session_security.get() == NULL) { 0713 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; no session security set&quot; &lt;&lt; dendl; 0714 } else { 0715 if (session_security-&gt;check_message_signature(message)) { 0716 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; Signature check failed&quot; &lt;&lt; dendl; 0717 message-&gt;put(); 0718 goto fail; 0719 } 0720 } 0721 message-&gt;set_byte_throttler(policy.throttler_bytes); 0722 message-&gt;set_message_throttler(policy.throttler_messages); 0723 0724 // store reservation size in message, so we don't get confused 0725 // by messages entering the dispatch queue through other paths. 0726 message-&gt;set_dispatch_throttle_size(cur_msg_size); 0727 0728 message-&gt;set_recv_stamp(recv_stamp); 0729 message-&gt;set_throttle_stamp(throttle_stamp); 0730 message-&gt;set_recv_complete_stamp(ceph_clock_now()); 0731 0732 // check received seq#. if it is old, drop the message. 0733 // note that incoming messages may skip ahead. this is convenient for the client 0734 // side queueing because messages can't be renumbered, but the (kernel) client will 0735 // occasionally pull a message out of the sent queue to send elsewhere. in that case 0736 // it doesn't matter if we &quot;got&quot; it or not. 0737 uint64_t cur_seq = in_seq.read(); 0738 if (message-&gt;get_seq() &lt;= cur_seq) { 0739 ldout(async_msgr-&gt;cct,0) &lt;&lt; __func__ &lt;&lt; &quot; got old message &quot; 0740 &lt;&lt; message-&gt;get_seq() &lt;&lt; &quot; &lt;= &quot; &lt;&lt; cur_seq &lt;&lt; &quot; &quot; &lt;&lt; message &lt;&lt; &quot; &quot; &lt;&lt; *message 0741 &lt;&lt; &quot;, discarding&quot; &lt;&lt; dendl; 0742 message-&gt;put(); 0743 if (has_feature(CEPH_FEATURE_RECONNECT_SEQ) &amp;&amp; async_msgr-&gt;cct-&gt;_conf-&gt;ms_die_on_old_message) 0744 assert(0 == &quot;old msgs despite reconnect_seq feature&quot;); 0745 break; 0746 } 0747 if (message-&gt;get_seq() &gt; cur_seq + 1) { 0748 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; missed message? skipped from seq &quot; 0749 &lt;&lt; cur_seq &lt;&lt; &quot; to &quot; &lt;&lt; message-&gt;get_seq() &lt;&lt; dendl; 0750 if (async_msgr-&gt;cct-&gt;_conf-&gt;ms_die_on_skipped_message) 0751 assert(0 == &quot;skipped incoming seq&quot;); 0752 } 0753 0754 message-&gt;set_connection(this); 0755 0756 #if defined(WITH_LTTNG) &amp;&amp; defined(WITH_EVENTTRACE) 0757 if (message-&gt;get_type() == CEPH_MSG_OSD_OP || message-&gt;get_type() == CEPH_MSG_OSD_OPREPLY) { 0758 utime_t ltt_processed_stamp = ceph_clock_now(); 0759 double usecs_elapsed = (ltt_processed_stamp.to_nsec()-ltt_recv_stamp.to_nsec())/1000; 0760 ostringstream buf; 0761 if (message-&gt;get_type() == CEPH_MSG_OSD_OP) 0762 OID_ELAPSED_WITH_MSG(message, usecs_elapsed, &quot;TIME_TO_DECODE_OSD_OP&quot;, false); 0763 else 0764 OID_ELAPSED_WITH_MSG(message, usecs_elapsed, &quot;TIME_TO_DECODE_OSD_OPREPLY&quot;, false); 0765 } 0766 #endif 0767 0768 // note last received message. 0769 in_seq.set(message-&gt;get_seq()); 0770 ldout(async_msgr-&gt;cct, 5) &lt;&lt; &quot; rx &quot; &lt;&lt; message-&gt;get_source() &lt;&lt; &quot; seq &quot; 0771 &lt;&lt; message-&gt;get_seq() &lt;&lt; &quot; &quot; &lt;&lt; message 0772 &lt;&lt; &quot; &quot; &lt;&lt; *message &lt;&lt; dendl; 0773 0774 if (!policy.lossy) { 0775 ack_left.inc(); 0776 need_dispatch_writer = true; 0777 } 0778 state = STATE_OPEN; 0779 0780 logger-&gt;inc(l_msgr_recv_messages); 0781 logger-&gt;inc(l_msgr_recv_bytes, cur_msg_size + sizeof(ceph_msg_header) + sizeof(ceph_msg_footer)); 0782 0783 async_msgr-&gt;ms_fast_preprocess(message); 0784 if (delay_state) { 0785 utime_t release = message-&gt;get_recv_stamp(); 0786 double delay_period = 0; 0787 if (rand() % 10000 &lt; async_msgr-&gt;cct-&gt;_conf-&gt;ms_inject_delay_probability * 10000.0) { 0788 delay_period = async_msgr-&gt;cct-&gt;_conf-&gt;ms_inject_delay_max * (double)(rand() % 10000) / 10000.0; 0789 release += delay_period; 0790 ldout(async_msgr-&gt;cct, 1) &lt;&lt; &quot;queue_received will delay until &quot; &lt;&lt; release &lt;&lt; &quot; on &quot; 0791 &lt;&lt; message &lt;&lt; &quot; &quot; &lt;&lt; *message &lt;&lt; dendl; 0792 } 0793 delay_state-&gt;queue(delay_period, release, message); 0794 } else if (async_msgr-&gt;ms_can_fast_dispatch(message)) { 0795 lock.unlock(); 0796 dispatch_queue-&gt;fast_dispatch(message); 0797 lock.lock(); 0798 } else { 0799 dispatch_queue-&gt;enqueue(message, message-&gt;get_priority(), conn_id); 0800 } 0801 0802 break; 0803 } 0804  上述已經大概跑完了整個 Accept 的流程，當然此流程中是確保沒有任何錯誤，一切都是順利往下進行的。 接下來來探討若 Client 想要連線，則會怎麼處理。 Connect 當 AsyncMessager 創立 AsyncConnection時，就會先呼叫此 function 進行連線了，後續若有訊息發送時，會透過 _connect重新連線。設定 peer 的 addr以及 policy呼叫 _connect 完成最後的連線步驟 0200 void connect(const entity_addr_t&amp; addr, int type) { 0201 set_peer_type(type); 0202 set_peer_addr(addr); 0203 policy = msgr-&gt;get_policy(type); 0204 _connect(); 0205 }  _connect 將狀態設定為 STATE_CONNECTING，接下來就將 read_handler 送給 Event Engine去處理，由於是透過 external event，所以則會馬上執行 read_handler，也就是 process。 1856 void AsyncConnection::_connect() 1857 { 1858 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; csq=&quot; &lt;&lt; connect_seq &lt;&lt; dendl; 1859 1860 state = STATE_CONNECTING; 1861 // rescheduler connection in order to avoid lock dep 1862 // may called by external thread(send_message) 1863 center-&gt;dispatch_event_external(read_handler); 1864 }  ","version":"Next","tagName":"h2"},{"title":"STATE_CONNECTING​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_connecting","content":"檢查 cs 此變數，如果之前有連線過，則關閉先前的連線 同時也先刪除之前的 event 呼叫 worker 跟對方的 socket 去連線創造一個 read_handler 的 event，來處理接下來收到封包的行為 read_handler 就是 process 狀態切換成 STATE_CONNECTING_RE 0870 case STATE_CONNECTING: 0871 { 0872 assert(!policy.server); 0873 0874 // reset connect state variables 0875 got_bad_auth = false; 0876 delete authorizer; 0877 authorizer = NULL; 0878 authorizer_buf.clear(); 0879 memset(&amp;connect_msg, 0, sizeof(connect_msg)); 0880 memset(&amp;connect_reply, 0, sizeof(connect_reply)); 0881 0882 global_seq = async_msgr-&gt;get_global_seq(); 0883 // close old socket. this is safe because we stopped the reader thread above. 0884 if (cs) { 0885 center-&gt;delete_file_event(cs.fd(), EVENT_READABLE|EVENT_WRITABLE); 0886 cs.close(); 0887 } 0888 0889 SocketOptions opts; 0890 opts.priority = async_msgr-&gt;get_socket_priority(); 0891 opts.connect_bind_addr = msgr-&gt;get_myaddr(); 0892 r = worker-&gt;connect(get_peer_addr(), opts, &amp;cs); 0893 if (r &lt; 0) 0894 goto fail; 0895 0896 center-&gt;create_file_event(cs.fd(), EVENT_READABLE, read_handler); 0897 state = STATE_CONNECTING_RE; 0898 break; 0899 }  ","version":"Next","tagName":"h2"},{"title":"STATE_CONNECTING_RE​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_connecting_re","content":"檢查當前 connectionSocket 連線狀態，本身若發現沒有連線則會自己重新連線 r &lt; 0, 連線依然失敗，則判定有問題， goto 離開r == 1, 成功，不做事情r == 0, 重連過程中有出現錯誤，可能是 EINPROGRESS 或是 EALREADY。 嘗試送出 CEPH_BANNER若成功，狀態切換成 STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY若失敗，狀態切換成 STATE_WAIT_SEND，待之後重送後再處理。 0055 int is_connected() override { 0056 if (connected) 0057 return 1; 0058 0059 int r = handler.reconnect(sa, _fd); 0060 if (r == 0) { 0061 connected = true; 0062 return 1; 0063 } else if (r &lt; 0) { 0064 return r; 0065 } else { 0066 return 0; 0067 } 0068 }  0901 case STATE_CONNECTING_RE: 0902 { 0903 r = cs.is_connected(); 0904 if (r &lt; 0) { 0905 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; reconnect failed &quot; &lt;&lt; dendl; 0906 if (r == -ECONNREFUSED) { 0907 ldout(async_msgr-&gt;cct, 2) &lt;&lt; __func__ &lt;&lt; &quot; connection refused!&quot; &lt;&lt; dendl; 0908 dispatch_queue-&gt;queue_refused(this); 0909 } 0910 goto fail; 0911 } else if (r == 0) { 0912 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; nonblock connect inprogress&quot; &lt;&lt; dendl; 0913 if (async_msgr-&gt;get_stack()-&gt;nonblock_connect_need_writable_event()) 0914 center-&gt;create_file_event(cs.fd(), EVENT_WRITABLE, read_handler); 0915 break; 0916 } 0917 0918 center-&gt;delete_file_event(cs.fd(), EVENT_WRITABLE); 0919 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; connect successfully, ready to send banner&quot; &lt;&lt; dendl; 0920 0921 bufferlist bl; 0922 bl.append(CEPH_BANNER, strlen(CEPH_BANNER)); 0923 r = try_send(bl); 0924 if (r == 0) { 0925 state = STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY; 0926 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; connect write banner done: &quot; 0927 &lt;&lt; get_peer_addr() &lt;&lt; dendl; 0928 } else if (r &gt; 0) { 0929 state = STATE_WAIT_SEND; 0930 state_after_send = STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY; 0931 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; connect wait for write banner: &quot; 0932 &lt;&lt; get_peer_addr() &lt;&lt; dendl; 0933 } else { 0934 goto fail; 0935 } 0936 0937 break; 0938 } 0223 }  ","version":"Next","tagName":"h2"},{"title":"STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_connecting_wait_banner_and_identify","content":"讀取 SERVER 端送來的資訊 CEPH_BANNERAddress (Server本身，以及Server看到的 Client)，所以有兩份。 比較兩邊的 CEPH_BANNER比較 peer addr (server address) 我自己 socket 看到的對方送過來的 將自己的 address 送給 server若成功，將狀態切換成 STATE_CONNECTING_SEND_CONNECT_MSG若失敗，將狀態切換成 STATE_WAIT_SEND，之後再處理。 0940 case STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY: 0941 { 0942 entity_addr_t paddr, peer_addr_for_me; 0943 bufferlist myaddrbl; 0944 unsigned banner_len = strlen(CEPH_BANNER); 0945 unsigned need_len = banner_len + sizeof(ceph_entity_addr)*2; 0946 r = read_until(need_len, state_buffer); 0947 if (r &lt; 0) { 0948 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read banner and identify addresses failed&quot; &lt;&lt; dendl; 0949 goto fail; 0950 } else if (r &gt; 0) { 0951 break; 0952 } 0953 0954 if (memcmp(state_buffer, CEPH_BANNER, banner_len)) { 0955 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; connect protocol error (bad banner) on peer &quot; 0956 &lt;&lt; get_peer_addr() &lt;&lt; dendl; 0957 goto fail; 0958 } 0959 0960 bufferlist bl; 0961 bl.append(state_buffer+banner_len, sizeof(ceph_entity_addr)*2); 0962 bufferlist::iterator p = bl.begin(); 0963 try { 0964 ::decode(paddr, p); 0965 ::decode(peer_addr_for_me, p); 0966 } catch (const buffer::error&amp; e) { 0967 lderr(async_msgr-&gt;cct) &lt;&lt; __func__ &lt;&lt; &quot; decode peer addr failed &quot; &lt;&lt; dendl; 0968 goto fail; 0969 } 0970 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; connect read peer addr &quot; 0971 &lt;&lt; paddr &lt;&lt; &quot; on socket &quot; &lt;&lt; cs.fd() &lt;&lt; dendl; 0972 if (peer_addr != paddr) { 0973 if (paddr.is_blank_ip() &amp;&amp; peer_addr.get_port() == paddr.get_port() &amp;&amp; 0974 peer_addr.get_nonce() == paddr.get_nonce()) { 0975 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; connect claims to be &quot; &lt;&lt; paddr 0976 &lt;&lt; &quot; not &quot; &lt;&lt; peer_addr 0977 &lt;&lt; &quot; - presumably this is the same node!&quot; &lt;&lt; dendl; 0978 } else { 0979 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; connect claims to be &quot; 0980 &lt;&lt; paddr &lt;&lt; &quot; not &quot; &lt;&lt; peer_addr &lt;&lt; &quot; - wrong node!&quot; &lt;&lt; dendl; 0981 goto fail; 0982 } 0983 } 0984 0985 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; connect peer addr for me is &quot; &lt;&lt; peer_addr_for_me &lt;&lt; dendl; 0986 lock.unlock(); 0987 async_msgr-&gt;learned_addr(peer_addr_for_me); 0988 if (async_msgr-&gt;cct-&gt;_conf-&gt;ms_inject_internal_delays) { 0989 if (rand() % async_msgr-&gt;cct-&gt;_conf-&gt;ms_inject_socket_failures == 0) { 0990 ldout(msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; sleep for &quot; 0991 &lt;&lt; async_msgr-&gt;cct-&gt;_conf-&gt;ms_inject_internal_delays &lt;&lt; dendl; 0992 utime_t t; 0993 t.set_from_double(async_msgr-&gt;cct-&gt;_conf-&gt;ms_inject_internal_delays); 0994 t.sleep(); 0995 } 0996 } 0997 0998 lock.lock(); 0999 if (state != STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY) { 1000 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; state changed while learned_addr, mark_down or &quot; 1001 &lt;&lt; &quot; replacing must be happened just now&quot; &lt;&lt; dendl; 1002 return 0; 1003 } 1004 1005 ::encode(async_msgr-&gt;get_myaddr(), myaddrbl, 0); // legacy 1006 r = try_send(myaddrbl); 1007 if (r == 0) { 1008 state = STATE_CONNECTING_SEND_CONNECT_MSG; 1009 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; connect sent my addr &quot; 1010 &lt;&lt; async_msgr-&gt;get_myaddr() &lt;&lt; dendl; 1011 } else if (r &gt; 0) { 1012 state = STATE_WAIT_SEND; 1013 state_after_send = STATE_CONNECTING_SEND_CONNECT_MSG; 1014 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; connect send my addr done: &quot; 1015 &lt;&lt; async_msgr-&gt;get_myaddr() &lt;&lt; dendl; 1016 } else { 1017 ldout(async_msgr-&gt;cct, 2) &lt;&lt; __func__ &lt;&lt; &quot; connect couldn't write my addr, &quot; 1018 &lt;&lt; cpp_strerror(r) &lt;&lt; dendl; 1019 goto fail; 1020 } 1021 1022 break; 1023 }  ","version":"Next","tagName":"h2"},{"title":"STATE_CONNECTING_SEND_CONNECT_MSG​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_connecting_send_connect_msg","content":"設定 connect_msg 的資訊 如同之前所述，包含 featrue, type等。 將該 connect_msg 的資訊封裝起來到 bl變數中，透過 try_send送出若成功則將狀態切換到 STATE_CONNECTING_WAIT_CONNECT_REPLY 1025 case STATE_CONNECTING_SEND_CONNECT_MSG: 1026 { 1027 if (!got_bad_auth) { 1028 delete authorizer; 1029 authorizer = async_msgr-&gt;get_authorizer(peer_type, false); 1030 } 1031 bufferlist bl; 1032 1033 connect_msg.features = policy.features_supported; 1034 connect_msg.host_type = async_msgr-&gt;get_myinst().name.type(); 1035 connect_msg.global_seq = global_seq; 1036 connect_msg.connect_seq = connect_seq; 1037 connect_msg.protocol_version = async_msgr-&gt;get_proto_version(peer_type, true); 1038 connect_msg.authorizer_protocol = authorizer ? authorizer-&gt;protocol : 0; 1039 connect_msg.authorizer_len = authorizer ? authorizer-&gt;bl.length() : 0; 1040 if (authorizer) 1041 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; connect_msg.authorizer_len=&quot; 1042 &lt;&lt; connect_msg.authorizer_len &lt;&lt; &quot; protocol=&quot; 1043 &lt;&lt; connect_msg.authorizer_protocol &lt;&lt; dendl; 1044 connect_msg.flags = 0; 1045 if (policy.lossy) 1046 connect_msg.flags |= CEPH_MSG_CONNECT_LOSSY; // this is fyi, actually, server decides! 1047 bl.append((char*)&amp;connect_msg, sizeof(connect_msg)); 1048 if (authorizer) { 1049 bl.append(authorizer-&gt;bl.c_str(), authorizer-&gt;bl.length()); 1050 } 1051 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; connect sending gseq=&quot; &lt;&lt; global_seq &lt;&lt; &quot; cseq=&quot; 1052 &lt;&lt; connect_seq &lt;&lt; &quot; proto=&quot; &lt;&lt; connect_msg.protocol_version &lt;&lt; dendl; 1053 1054 r = try_send(bl); 1055 if (r == 0) { 1056 state = STATE_CONNECTING_WAIT_CONNECT_REPLY; 1057 ldout(async_msgr-&gt;cct,20) &lt;&lt; __func__ &lt;&lt; &quot; connect wrote (self +) cseq, waiting for reply&quot; &lt;&lt; dendl; 1058 } else if (r &gt; 0) { 1059 state = STATE_WAIT_SEND; 1060 state_after_send = STATE_CONNECTING_WAIT_CONNECT_REPLY; 1061 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; continue send reply &quot; &lt;&lt; dendl; 1062 } else { 1063 ldout(async_msgr-&gt;cct, 2) &lt;&lt; __func__ &lt;&lt; &quot; connect couldn't send reply &quot; 1064 &lt;&lt; cpp_strerror(r) &lt;&lt; dendl; 1065 goto fail; 1066 } 1067 1068 break; 1069 }  ","version":"Next","tagName":"h2"},{"title":"STATE_CONNECTING_WAIT_CONNECT_REPLY​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_connecting_wait_connect_reply","content":"從 Server 端讀取資料，將資料放到 state_buffer，此資料的格式為 ceph_msg_connect_reply 0110 struct ceph_msg_connect_reply { 0111 __u8 tag; 0112 __le64 features; /* feature bits for this session */ 0113 __le32 global_seq; 0114 __le32 connect_seq; 0115 __le32 protocol_version; 0116 __le32 authorizer_len; 0117 __u8 flags; 0118 } __attribute__ ((packed));  將此資訊記錄下來後，狀態切換成 STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH 1071 case STATE_CONNECTING_WAIT_CONNECT_REPLY: 1072 { 1073 r = read_until(sizeof(connect_reply), state_buffer); 1074 if (r &lt; 0) { 1075 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read connect reply failed&quot; &lt;&lt; dendl; 1076 goto fail; 1077 } else if (r &gt; 0) { 1078 break; 1079 } 1080 1081 connect_reply = *((ceph_msg_connect_reply*)state_buffer); 1082 1083 ldout(async_msgr-&gt;cct, 20) &lt;&lt; __func__ &lt;&lt; &quot; connect got reply tag &quot; &lt;&lt; (int)connect_reply.tag 1084 &lt;&lt; &quot; connect_seq &quot; &lt;&lt; connect_reply.connect_seq &lt;&lt; &quot; global_seq &quot; 1085 &lt;&lt; connect_reply.global_seq &lt;&lt; &quot; proto &quot; &lt;&lt; connect_reply.protocol_version 1086 &lt;&lt; &quot; flags &quot; &lt;&lt; (int)connect_reply.flags &lt;&lt; &quot; features &quot; 1087 &lt;&lt; connect_reply.features &lt;&lt; dendl; 1088 state = STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH; 1089 1090 break; 1091 }  ","version":"Next","tagName":"h2"},{"title":"STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_connecting_wait_connect_reply_auth","content":"若前述回傳的 ceph_msg_connect_reply 有 authorizer 的資訊，則進行額外處理。最後呼叫 handle_connect_reply 進行處理 理論上 handle_connect_reply 會改變當前狀態，最後變成 STATE_CONNECTING_READY 上述處理完畢後，狀態必須要改變，若沒有代表有問題，直接 Assert 1093 case STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH: 1094 { 1095 bufferlist authorizer_reply; 1096 if (connect_reply.authorizer_len) { 1097 ldout(async_msgr-&gt;cct, 10) &lt;&lt; __func__ &lt;&lt; &quot; reply.authorizer_len=&quot; &lt;&lt; connect_reply.authorizer_len &lt;&lt; dendl; 1098 assert(connect_reply.authorizer_len &lt; 4096); 1099 r = read_until(connect_reply.authorizer_len, state_buffer); 1100 if (r &lt; 0) { 1101 ldout(async_msgr-&gt;cct, 1) &lt;&lt; __func__ &lt;&lt; &quot; read connect reply authorizer failed&quot; &lt;&lt; dendl; 1102 goto fail; 1103 } else if (r &gt; 0) { 1104 break; 1105 } 1106 1107 authorizer_reply.append(state_buffer, connect_reply.authorizer_len); 1108 bufferlist::iterator iter = authorizer_reply.begin(); 1109 if (authorizer &amp;&amp; !authorizer-&gt;verify_reply(iter)) { 1110 ldout(async_msgr-&gt;cct, 0) &lt;&lt; __func__ &lt;&lt; &quot; failed verifying authorize reply&quot; &lt;&lt; dendl; 1111 goto fail; 1112 } 1113 } 1114 r = handle_connect_reply(connect_msg, connect_reply); 1115 if (r &lt; 0) 1116 goto fail; 1117 1118 // state must be changed! 1119 assert(state != STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH); 1120 break; 1121 }  ","version":"Next","tagName":"h2"},{"title":"STATE_CONNECTING_READY​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#state_connecting_ready","content":"這邊有個有趣的註解 // hooray!，代表到這一步連線基本上已經完成了，剩下最後一步驟就結束了。對 dispatch_queue 設定當前 connection dispatch_queue 這邊有兩種類型，一種是存放 message，一種則是 Type + Connection，這邊屬於第二種在 dispatch_queue 的 loop 中，會針對這兩種去處理，若是 messag 的，則直接將此訊息丟給所有註冊的 dispatcher，反之則根據 type 執行不同的任務這邊放入的是 D_CONNECT 的 event，所以之後會執行 ms_deliver_handle_connect 這支 function。接者這支 function 則是會通知所有 dispathcer 目前有新的連線到來，呼叫對應的 ms_handle_connect來處理 呼叫 AsyncMessegner 內的 ms_deliver_handle_fast_connect fast_connect 相對於 connect 是更早會處理的函式，底層可確保此 function 一定會在有任何 message 被處理前先呼叫。 如果當前 queue 內有訊息，這時候再發送一個外部的 write_handler把queue給清空。 可能是由於先前的 try_send 沒有成功 到這邊後，連線就完成了，可以開始供應用層各種發送訊息了。 1163 case STATE_CONNECTING_READY: 1164 { 1165 // hooray! 1166 peer_global_seq = connect_reply.global_seq;  0155 while (!mqueue.empty()) { 0156 QueueItem qitem = mqueue.dequeue(); 0157 if (!qitem.is_code()) 0158 remove_arrival(qitem.get_message()); 0159 lock.Unlock(); 0160 0161 if (qitem.is_code()) { 0162 if (cct-&gt;_conf-&gt;ms_inject_internal_delays &amp;&amp; 0163 cct-&gt;_conf-&gt;ms_inject_delay_probability &amp;&amp; 0164 (rand() % 10000)/10000.0 &lt; cct-&gt;_conf-&gt;ms_inject_delay_probability) { 0165 utime_t t; 0166 t.set_from_double(cct-&gt;_conf-&gt;ms_inject_internal_delays); 0167 ldout(cct, 1) &lt;&lt; &quot;DispatchQueue::entry inject delay of &quot; &lt;&lt; t 0168 &lt;&lt; dendl; 0169 t.sleep(); 0170 } 0171 switch (qitem.get_code()) { 0172 case D_BAD_REMOTE_RESET: 0173 msgr-&gt;ms_deliver_handle_remote_reset(qitem.get_connection()); 0174 break; 0175 case D_CONNECT: 0176 msgr-&gt;ms_deliver_handle_connect(qitem.get_connection()); 0177 break; 0178 case D_ACCEPT: 0179 msgr-&gt;ms_deliver_handle_accept(qitem.get_connection()); 0180 break; 0181 case D_BAD_RESET: 0182 msgr-&gt;ms_deliver_handle_reset(qitem.get_connection()); 0183 break; 0184 case D_CONN_REFUSED: 0185 msgr-&gt;ms_deliver_handle_refused(qitem.get_connection()); 0186 break; 0187 default: 0188 ceph_abort(); 0189 } 0190 } else { 0191 Message *m = qitem.get_message(); 0192 if (stop) { 0193 ldout(cct,10) &lt;&lt; &quot; stop flag set, discarding &quot; &lt;&lt; m &lt;&lt; &quot; &quot; &lt;&lt; *m &lt;&lt; dendl; 0194 m-&gt;put(); 0195 } else { 0196 uint64_t msize = pre_dispatch(m); 0197 msgr-&gt;ms_deliver_dispatch(m); 0198 post_dispatch(m, msize); 0199 }  0610 /** 0611 * Notify each Dispatcher of a new Connection. Call 0612 * this function whenever a new Connection is initiated or 0613 * reconnects. 0614 * 0615 * @param con Pointer to the new Connection. 0616 */ 0617 void ms_deliver_handle_connect(Connection *con) { 0618 for (list&lt;Dispatcher*&gt;::iterator p = dispatchers.begin(); 0619 p != dispatchers.end(); 0620 ++p) 0621 (*p)-&gt;ms_handle_connect(con); 0622 }  0110 /** 0111 * This function will be called synchronously whenever a Connection is 0112 * newly-created or reconnects in the Messenger, if you support fast 0113 * dispatch. It is guaranteed to be called before any messages are 0114 * dispatched. 0115 * 0116 * @param con The new Connection which has been established. You are not 0117 * granted a reference to it -- take one if you need one! 0118 */ 0119 virtual void ms_handle_fast_connect(Connection *con) {}  上述在 Accpeting 或者是 Connecting 的過程中，會透過下列兩個方式做一些深層的處理，這些處理同時會改變當前狀態，這邊就簡單大致上看過而已。 ","version":"Next","tagName":"h2"},{"title":"handle_connect_reply​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#handle_connect_reply","content":"根據 reply 內的 tag 類型來執行各種不同事情, 大部分都是錯誤相關的處理，若一切都正常的話，則會是CEPH_MSGR_TAG_READY，此時會將狀態切換成 STATE_CONNECTING_READY CEPH_MSGR_TAG_FEATURESCEPH_MSGR_TAG_BADPROTOVERCEPH_MSGR_TAG_BADAUTHORIZERCEPH_MSGR_TAG_RESETSESSIONCEPH_MSGR_TAG_RETRY_GLOBALCEPH_MSGR_TAG_RETRY_SESSIONCEPH_MSGR_TAG_WAITCEPH_MSGR_TAG_SEQCEPH_MSGR_TAG_READY ","version":"Next","tagName":"h2"},{"title":"handle_connect_msg​","type":1,"pageTitle":"Ceph Network - AsyncConnection","url":"/docs/techPost/2017/ceph-async-connection#handle_connect_msg","content":"根據 peer type 取得對應的 proto_version，放到 ceph_msg_connect_reply 的變數中 0671 case CEPH_ENTITY_TYPE_OSD: return CEPH_OSDC_PROTOCOL; 0672 case CEPH_ENTITY_TYPE_MDS: return CEPH_MDSC_PROTOCOL; 0673 case CEPH_ENTITY_TYPE_MON: return CEPH_MONC_PROTOCOL;  若兩邊的 proto_version 不一致，則呼叫 _reply_aceept 去處理。若對方有要使用 cephX 根據不同的 protocol type (OSD/MDS/MOM) 進行不同的處理 檢查兩邊的 feature set 是否滿足彼此，若有問題則呼叫 _reply_accept 去處理進行用戶驗證，失敗則呼叫 _reply_accept若以前 peer addr 曾經有 connection 存在過，這時候就要進行一些處理，主要的處理都是基於兩個變數來決定，global_seq 以及 connect_seq global_seq 代表的是這個host已經建立過多少條 connectionconnect_seq 代表的是這個 session建立過多少條 connection 某些情況下，會嘗試捨棄舊有的 connection 並建立新的 connection 來使用某些情況則是會繼續使用舊有的 connection，然後把一些新的資訊賦予到舊有 connection 的成員中呼叫 accept_conn 將連線給記錄下來放到 conns 中，並且從 accepting_conns 中移除，最後則將狀態改成 STATE_ACCEPTING_WAIT_SEQ Summary 此 AsyncConnection 內容眾多，目前先主要觀察到整個建立連線的步驟，包含了 Accept 以及 connect 。 有機會再來把 read/write 相關的介面也都看一次，到時候可以更瞭解整體收送的行為。 ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow-ii","content":"","keywords":"","version":"Next"},{"title":"adm_new_connection​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow-ii#adm_new_connection","content":"首先，先確認當前還沒有 connection 存在，接下來我們要開始取得一些跟 network 相關的設定，所以這邊會先透過 kzalloc 在 kernel 內產生一個空間，接下來透過 net_conf_from_attrs 從 netlink 的 attribute中讀取相關的資料，然後 new_net_conf 結構中，由於 net_conf_from_attrs 是支由 MACRO 展開的 function，內容不好閱讀，只要知道能夠從 netlink 內讀取到想要的數據，並且拿出來即可。 3326 *ret_conn = NULL; 3327 if (adm_ctx-&gt;connection) { 3328 drbd_err(adm_ctx-&gt;resource, &quot;Connection for peer node id %d already exists\\n&quot;, 3329 adm_ctx-&gt;peer_node_id); 3330 return ERR_INVALID_REQUEST; 3331 } 3332 3333 /* allocation not in the IO path, drbdsetup / netlink process context */ 3334 new_net_conf = kzalloc(sizeof(*new_net_conf), GFP_KERNEL); 3335 if (!new_net_conf) 3336 return ERR_NOMEM; 3337 3338 set_net_conf_defaults(new_net_conf); 3339 3340 err = net_conf_from_attrs(new_net_conf, info); 3341 if (err) { 3342 retcode = ERR_MANDATORY_TAG; 3343 drbd_msg_put_info(adm_ctx-&gt;reply_skb, from_attrs_err_to_txt(err)); 3344 goto fail; 3345 }  接下來會從設定檔中判斷當前的網路連線是走什麼協定，一般免費社群使用的版本只有 tcp 可以，接洽購買後可以獲得 RDMA 相關的 kernel module 來使用。 所以這邊最後會透過 drbd_get_transport_class 根據對應的名稱來找到對應的 netowrk module 實作。 3347 transport_name = new_net_conf-&gt;transport_name[0] ? new_net_conf-&gt;transport_name : &quot;tcp&quot;; 3348 tr_class = drbd_get_transport_class(transport_name); 3349 if (!tr_class) { 3350 retcode = ERR_CREATE_TRANSPORT; 3351 goto fail; 3352 }  接下來則是透過 drbd_create_connection 來創建 connection，這邊會將剛剛得到的 transport_class 一併傳入，因為最後會需要該 transport_class 去執行底層的 init。 3354 connection = drbd_create_connection(adm_ctx-&gt;resource, tr_class); 3355 if (!connection) { 3356 retcode = ERR_NOMEM; 3357 goto fail_put_transport; 3358 }  ","version":"Next","tagName":"h2"},{"title":"drbd_create_connection​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow-ii#drbd_create_connection","content":"一開始，就先透過 kzalloc 去創建一個空間供 connection使用，這邊可以注意到 size 的算法非常特別，除了直接用 sizeof 算出該物件外，最後會有一個大小的微調- sizeof(connection-&gt;transport) + tc-&gt;instance_size這邊原因要牽扯到 drbd_connection 的實作內容，在其架構中有這樣一段註解 1050 struct drbd_transport transport; /* The transport needs to be the last member. The acutal 1051 implementation might have more members than the 1052 abstract one. */ 1053 };  可以看到其實最後一個欄位算是一個比較抽象的概念，實際上底層的實作可以有更多的變化，所以這邊在計算真正整體大小時，要先扣掉 sizeof drbd_transport，然後加上該實作真正用到的大小 tc-&gt;instance_size。 3302 struct drbd_connection *drbd_create_connection(struct drbd_resource *resource, 3303 struct drbd_transport_class *tc) 3304 { 3305 struct drbd_connection *connection; 3306 int size; 3307 3308 size = sizeof(*connection) - sizeof(connection-&gt;transport) + tc-&gt;instance_size; 3309 connection = kzalloc(size, GFP_KERNEL);  接下來就要開始初始化 drbd__connection 內部的各種結構，包含各種 link list 相關的結構。 在一切初始化完畢後，最後呼叫 transport class 自己本身的 init。 3374 if (tc-&gt;init(&amp;connection-&gt;transport)) 3375 goto fail;  在創建完畢 connection 後，接下來針對 net_option，crtpyo， peer device 去進行初始化的動作， 中間有一段則是將該 connetion 給加到 resource 此物件中，用 link list 的方式把所有的 connection 都綁起來，未來有其他指令要找到 connection 要使用時，就可以透過此方式找到之前創建的 connection 3401 spin_lock_irq(&amp;adm_ctx-&gt;resource-&gt;req_lock); 3402 list_add_tail_rcu(&amp;connection-&gt;connections, &amp;adm_ctx-&gt;resource-&gt;connections); 3403 spin_unlock_irq(&amp;adm_ctx-&gt;resource-&gt;req_lock);  最後呼叫drbd_thread_start 去創建一個 kernel thread來運行 drbd_sender　此 thread。 3467 drbd_thread_start(&amp;connection-&gt;sender);  大致上此 function 就結束了。 整個drbd_adm_new_peer執行完畢後， kernel 內的 resource 底下就會有一個 drbd_connection的物件在運行，接下來的指令都會嘗試透過 drbd_get_connection_by_node_id 的方式得到該 connection 來進行後續操作。 drbd_adm_new_path 在透過 drbd_adm_new_peer 創立一個 connection (peer) 後，接下來我們要在這條 connection 上創立一個新的 path， path 代表的就是實際上連線會對應的 ip address 以及對應的 port。 一開始會先透過 drbd_adm_prepare 進行一些資源的獲取，包含 connection 也會在裡面取得，然後放到 adm_ctx.connection 變數上。 接下來就透過 adm_add_path 進行細部的處理。 3680 int drbd_adm_new_path(struct sk_buff *skb, struct genl_info *info) 3681 { 3682 struct drbd_config_context adm_ctx; 3683 enum drbd_ret_code retcode; 3684 3685 retcode = drbd_adm_prepare(&amp;adm_ctx, skb, info, DRBD_ADM_NEED_CONNECTION); 3686 if (!adm_ctx.reply_skb) 3687 return retcode; 3688 3689 /* remote transport endpoints need to be globaly unique */ 3690 mutex_lock(&amp;adm_ctx.resource-&gt;adm_mutex); 3691 3692 retcode = adm_add_path(&amp;adm_ctx, info); 3693 3694 mutex_unlock(&amp;adm_ctx.resource-&gt;adm_mutex); 3695 drbd_adm_finish(&amp;adm_ctx, info, retcode); 3696 return 0; 3697 }  ","version":"Next","tagName":"h2"},{"title":"adm_add_path​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow-ii#adm_add_path","content":"首先先從 connection 中取得對應的 drbd_transport 的實作，不過這邊都沒有任何檢查，所以如果今天還沒有執行 add_peer 前就先執行 add_path，可能會有 Null pointer dereferences 的問題。 3538 static enum drbd_ret_code 3539 adm_add_path(struct drbd_config_context *adm_ctx, struct genl_info *info) 3540 { 3541 struct drbd_transport *transport = &amp;adm_ctx-&gt;connection-&gt;transport; 3542 struct nlattr *my_addr = NULL, *peer_addr = NULL; 3543 struct drbd_path *path; 3544 enum drbd_ret_code retcode; 3545 int err;  接下來就如同上述的步驟一樣，先從 netlink 中取出我們需要的資訊，在這個指令中，我們需要的是一條 path 兩端點的 address(ip:port)。接者透過check_path_usable檢查該參數，譬如是否存在，是否已經使用過。 3547 /* parse and validate only */ 3548 err = path_parms_from_attrs(NULL, info); 3549 if (err) { 3550 drbd_msg_put_info(adm_ctx-&gt;reply_skb, from_attrs_err_to_txt(err)); 3551 return ERR_MANDATORY_TAG; 3552 } 3553 my_addr = nested_attr_tb[__nla_type(T_my_addr)]; 3554 peer_addr = nested_attr_tb[__nla_type(T_peer_addr)];  一切準備完畢後，開始創立 strcut drbd_path，先從 kernel 要空間，接下來把兩端點的 address 都複製進去，最後就讓 transport class 自行去負責要怎麼處理了，於是呼叫了 transport-&gt;ops-&gt;add_path 去處理。 在本文的範例中使用的是 TCP 的方式，最後則是透過 dtt_add_path 去處理，詳細處理的流程之後會再仔細研究整個 TCP 層的架構。 3562 path = kzalloc(transport-&gt;class-&gt;path_instance_size, GFP_KERNEL); 3563 if (!path) 3564 return ERR_NOMEM; 3565 3566 path-&gt;my_addr_len = nla_len(my_addr); 3567 memcpy(&amp;path-&gt;my_addr, nla_data(my_addr), path-&gt;my_addr_len); 3568 path-&gt;peer_addr_len = nla_len(peer_addr); 3569 memcpy(&amp;path-&gt;peer_addr, nla_data(peer_addr), path-&gt;peer_addr_len); 3570 3571 kref_init(&amp;path-&gt;kref); 3572 3573 err = transport-&gt;ops-&gt;add_path(transport, path); 3574 if (err) { 3575 kref_put(&amp;path-&gt;kref, drbd_destroy_path); 3576 drbd_err(adm_ctx-&gt;connection, &quot;add_path() failed with %d\\n&quot;, err); 3577 drbd_msg_put_info(adm_ctx-&gt;reply_skb, &quot;add_path on transport failed&quot;); 3578 return ERR_INVALID_REQUEST; 3579 }  drbd_adm_connect 一切都準備完畢後，接下來就可以透過 drbd_adm_connect 真正地建立起兩端的連線。如同慣例，一開始都會先呼叫 drbd_adm_prepare 進行資源的整理，接下來就可以直接從 adm_ctx.connection 去取得先前創立的連線物件，然後判斷該連線目前的狀態。 當初創建好連線時，預設的狀態就是 C_STANDALONE。 3584 int drbd_adm_connect(struct sk_buff *skb, struct genl_info *info) 3585 { 3586 struct drbd_config_context adm_ctx; 3587 struct connect_parms parms = { 0, }; 3588 struct drbd_peer_device *peer_device; 3589 struct drbd_connection *connection; 3590 enum drbd_ret_code retcode; 3591 enum drbd_conn_state cstate; 3592 int i, err; 3593 3594 retcode = drbd_adm_prepare(&amp;adm_ctx, skb, info, DRBD_ADM_NEED_CONNECTION); 3595 if (!adm_ctx.reply_skb) 3596 return retcode; 3597 3598 connection = adm_ctx.connection; 3599 cstate = connection-&gt;cstate[NOW]; 3600 if (cstate != C_STANDALONE) { 3601 retcode = ERR_NET_CONFIGURED; 3602 goto out; 3603 }  接下來透過 first_path 確認該條 connection 至少有一條 path 存在，因為一個 connection 可以有多條 path，且這些 path 是透過 link list 的方式去紀錄的，所以只要判斷該 list 的第一個就知道目前有沒有至少一條 path 存在。 3605 if (first_path(connection) == NULL) { 3606 drbd_msg_put_info(adm_ctx.reply_skb, &quot;connection endpoint(s) missing&quot;); 3607 retcode = ERR_INVALID_REQUEST; 3608 goto out; 3609 }  最後透過 change_cstate 的方式來改變當前的狀態，然後透過一連串的呼叫宇改變，最後會在drbd_receive 內呼叫起 conn_connect 來進行真正的連線。 這中間的過程就不詳細描述，用兩張簡單的圖片大致說明即可。 首先透過第一張圖的流程，最後會跑到 queue_after_state_change_work 裡面，在裡面會創建一個 work，然後這個 work 裡面的 call back function會指向 w_after_state_change，最後把該 work 透過 drbd_queue_work 放入 resource 內的 work list。  1901 static void queue_after_state_change_work(struct drbd_resource *resource, 1902 struct completion *done) 1903 { 1904 /* Caller holds req_lock */ 1905 struct after_state_change_work *work; 1906 gfp_t gfp = GFP_ATOMIC; 1907 1908 work = kmalloc(sizeof(*work), gfp); 1909 if (work) 1910 work-&gt;state_change = remember_state_change(resource, gfp); 1911 if (work &amp;&amp; work-&gt;state_change) { 1912 work-&gt;w.cb = w_after_state_change; 1913 work-&gt;done = done; 1914 drbd_queue_work(&amp;resource-&gt;work, &amp;work-&gt;w); 1915 } else { 1916 kfree(work); 1917 drbd_err(resource, &quot;Could not allocate after state change work\\n&quot;); 1918 if (done) 1919 complete(done); 1920 } 1921 }  接下來如下圖，當 resource 一開始透過創立 resource 時，就會叫一起一隻 kernel thread，會專注於執行 drbd_worker 這隻 function，而這 function 內部則會不斷的把 resource 內部的 works 給拿出來執行。  2746 int drbd_worker(struct drbd_thread *thi) 2747 { 2748 LIST_HEAD(work_list); 2749 struct drbd_resource *resource = thi-&gt;resource; 2750 struct drbd_work *w; 2751 2752 while (get_t_state(thi) == RUNNING) { 2753 drbd_thread_current_set_cpu(thi); ................. 2793 2794 while (!list_empty(&amp;work_list)) { 2795 w = list_first_entry(&amp;work_list, struct drbd_work, list); 2796 list_del_init(&amp;w-&gt;list); 2797 update_worker_timing_details(resource, w-&gt;cb); 2798 w-&gt;cb(w, 0); 2799 } 2800 } ...................... 2826 return 0; 2827 }  最後呼叫該 work 的 call back function，最後會執行到 w_after_state_change，在這個 function內，最後會去把每個 connection 內部的 kernel thread 給叫起來，而這隻 kernel thread 則會呼叫 drbd_receiver 2782 static int w_after_state_change(struct drbd_work *w, int unused) 2783 { ............ 3247 for (n_connection = 0; n_connection &lt; state_change-&gt;n_connections; n_connection++) { ............ 3254 /* Upon network configuration, we need to start the receiver */ 3255 if (cstate[OLD] == C_STANDALONE &amp;&amp; cstate[NEW] == C_UNCONNECTED) 3256 drbd_thread_start(&amp;connection-&gt;receiver); 3257 ............ 3266 } ...... 3287 return 0; 3288 }  當 kernel thread 起來後，接下來就會呼叫 conn_connect 來進行後續的連線，當連線成功後，就會呼叫 drbdd 進入 while loop 內來處理。 所以接下來就繼續來觀察 conn_connect 底下到底怎麼做。 7646 int drbd_receiver(struct drbd_thread *thi) 7647 { 7648 struct drbd_connection *connection = thi-&gt;connection; 7649 7650 if (conn_connect(connection)) { 7651 blk_start_plug(&amp;connection-&gt;receiver_plug); 7652 drbdd(connection); 7653 blk_finish_plug(&amp;connection-&gt;receiver_plug); 7654 } 7655 7656 conn_disconnect(connection); 7657 return 0; 7658 }  7081 static void drbdd(struct drbd_connection *connection) 7082 { 7087 while (get_t_state(&amp;connection-&gt;receiver) == RUNNING) { 7088 struct data_cmd const *cmd; 7089 7090 drbd_thread_current_set_cpu(&amp;connection-&gt;receiver); 7091 update_receiver_timing_details(connection, drbd_recv_header_maybe_unplug); 7092 if (drbd_recv_header_maybe_unplug(connection, &amp;pi)) 7093 goto err_out; 7094 7095 cmd = &amp;drbd_cmd_handler[pi.cmd]; 7096 if (unlikely(pi.cmd &gt;= ARRAY_SIZE(drbd_cmd_handler) || !cmd-&gt;fn)) { 7097 drbd_err(connection, &quot;Unexpected data packet %s (0x%04x)&quot;, 7098 drbd_packet_name(pi.cmd), pi.cmd); 7099 goto err_out; 7100 } ............. 7131 } ... 7136 }  ","version":"Next","tagName":"h2"},{"title":"conn_connect​","type":1,"pageTitle":"Introduction","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow-ii#conn_connect","content":"首先，一開始會先設定當前的 protocol version，主要是用來區分 drbd8 以及 drbd9 用的，預設先當作 drbd 8 (version 80)。接下來則會改變當前的狀態，將 C_STANDALONE 轉換成 C_CONNECTING。 最後就呼叫 transport class去執行自己實作的 connect。 0665 static bool conn_connect(struct drbd_connection *connection) 0666 { ................ 0675 start: 0676 have_mutex = false; 0677 clear_bit(DISCONNECT_EXPECTED, &amp;connection-&gt;flags); 0678 if (change_cstate(connection, C_CONNECTING, CS_VERBOSE) &lt; SS_SUCCESS) { 0679 /* We do not have a network config. */ 0680 return false; 0681 } 0682 0683 /* Assume that the peer only understands protocol 80 until we know better. */ 0684 connection-&gt;agreed_pro_version = 80; 0685 0686 err = transport-&gt;ops-&gt;connect(transport); 0687 if (err == -EAGAIN) { 0688 if (connection-&gt;cstate[NOW] == C_DISCONNECTING) 0689 return false; 0690 goto retry; 0691 } else if (err &lt; 0) { 0692 drbd_warn(connection, &quot;Failed to initiate connection, err=%d\\n&quot;, err); 0693 goto abort; 0694 }  接下來去設定每個 socket 的 send/recevie timeout，詳細的用途可以參考SO_RCVTIMEO and SO_SNDTIMEO。 不過這邊可以注意的是，因為這邊底層是走 linux socket 的方式，所以是走上述的方法去設定，若今天改走 RDMA 的話，作法就會完全不同。 0696 connection-&gt;last_received = jiffies; 0697 0698 rcu_read_lock(); 0699 nc = rcu_dereference(connection-&gt;transport.net_conf); 0700 ping_timeo = nc-&gt;ping_timeo; 0701 ping_int = nc-&gt;ping_int; 0702 rcu_read_unlock(); 0703 0704 /* Make sure we are &quot;uncorked&quot;, otherwise we risk timeouts, 0705 * in case this is a reconnect and we had been corked before. */ 0706 drbd_uncork(connection, CONTROL_STREAM); 0707 drbd_uncork(connection, DATA_STREAM); 0708 0709 /* Make sure the handshake happens without interference from other threads, 0710 * or the challenge respons authentication could be garbled. */ 0711 mutex_lock(&amp;connection-&gt;mutex[DATA_STREAM]); 0712 have_mutex = true; 0713 transport-&gt;ops-&gt;set_rcvtimeo(transport, DATA_STREAM, ping_timeo * 4 * HZ/10); 0714 transport-&gt;ops-&gt;set_rcvtimeo(transport, CONTROL_STREAM, ping_int * HZ);  接下來嘗試去發送一些控制訊息給對面，譬如自己的DRBD版本的範圍，如下列drbd_send_features內所見 7332 static int drbd_send_features(struct drbd_connection *connection) 7333 { 7334 struct p_connection_features *p; 7335 7336 p = __conn_prepare_command(connection, sizeof(*p), DATA_STREAM); 7337 if (!p) 7338 return -EIO; 7339 memset(p, 0, sizeof(*p)); 7340 p-&gt;protocol_min = cpu_to_be32(PRO_VERSION_MIN); 7341 p-&gt;protocol_max = cpu_to_be32(PRO_VERSION_MAX); 7342 p-&gt;sender_node_id = cpu_to_be32(connection-&gt;resource-&gt;res_opts.node_id); 7343 p-&gt;receiver_node_id = cpu_to_be32(connection-&gt;peer_node_id); 7344 p-&gt;feature_flags = cpu_to_be32(PRO_FEATURES); 7345 return __send_command(connection, -1, P_CONNECTION_FEATURES, DATA_STREAM); 7346 }  0716 h = drbd_do_features(connection); 0717 if (h &lt; 0) 0718 goto abort; 0719 if (h == 0) 0720 goto retry;  中間又重新設定了一下 receive 的 timeout，而且只有針對 DATA_STREAM，意義不明。 最後呼叫 __drbd_send_protocol 將一些 net_conf 內的資料送過去。 0732 0733 transport-&gt;ops-&gt;set_rcvtimeo(transport, DATA_STREAM, MAX_SCHEDULE_TIMEOUT); 0734 0735 discard_my_data = test_bit(CONN_DISCARD_MY_DATA, &amp;connection-&gt;flags); 0736 0737 if (__drbd_send_protocol(connection, P_PROTOCOL) == -EOPNOTSUPP) 0738 goto abort;  最後面這段還不是很清楚在幹什麼，必須要更深入的理解細節，才能瞭解為什麼這邊又要跑一個 worker，裡面又會呼叫到 conn_connect2 來處理。 0767 if (connection-&gt;agreed_pro_version &gt;= 110) { 0768 if (resource-&gt;res_opts.node_id &lt; connection-&gt;peer_node_id) { 0769 kref_get(&amp;connection-&gt;kref); 0770 kref_debug_get(&amp;connection-&gt;kref_debug, 11); 0771 connection-&gt;connect_timer_work.cb = connect_work; 0772 timeout = twopc_retry_timeout(resource, 0); 0773 drbd_debug(connection, &quot;Waiting for %ums to avoid transaction &quot; 0774 &quot;conflicts\\n&quot;, jiffies_to_msecs(timeout)); 0775 connection-&gt;connect_timer.expires = jiffies + timeout; 0776 add_timer(&amp;connection-&gt;connect_timer); 0777 } 0778 }  Summary Kernel 這邊有非常多的 thread 在運行，同時還有很複雜的 state 狀態跑來跑去，要完整瞭解整個架構以及運作邏輯需要不少時間去測試。目前網路上幾乎沒有這方面的文件，就連官方網站也沒有文章說明底層的架構，這部分都只能依靠上層的應用說法與程式碼自己拼湊出這一切。整個 Coonection 內還包含了 DATA_STREAM與 DATA_STREAM，這部分的用途差異，實際上怎運過還必須要在更仔細地觀看相關函式以及 transport class TCP 底層的實作才有機會瞭解。 ","version":"Next","tagName":"h2"},{"title":"手把手打造仿 mininet 網路","type":0,"sectionRef":"#","url":"/docs/techPost/2017/setup-mininet-like-environment","content":"","keywords":"","version":"Next"},{"title":"Step1​","type":1,"pageTitle":"手把手打造仿 mininet 網路","url":"/docs/techPost/2017/setup-mininet-like-environment#step1","content":"首先，我們要在系統上創建 ove-eth0，關於 openvswitch 的安裝與啟動，本文就不再多敘述，網路上有滿多的文件都在講述其指令與教學。 因此這邊就直接使用 ovs-vsctl** 該指令直接來創造我們所需要的 switch。 ovs-vsctl add-br ovs-eth0 ifconfig ovs-eth0 up  這時候系統上的架構就如下圖般，什麼都沒有，只有一個 switch。 ","version":"Next","tagName":"h2"},{"title":"Step2​","type":1,"pageTitle":"手把手打造仿 mininet 網路","url":"/docs/techPost/2017/setup-mininet-like-environment#step2","content":"上述已經弄好了 switch 後， 我們接下來要創立兩個獨立的網路空間 network namespace，這邊使用 ip netns 指令來幫我們達成。 先使用 ip netns help 來看看有那些指令可以使用。 &gt;ip netns help Usage: ip netns list ip netns add NAME ip netns delete NAME ip netns identify PID ip netns pids NAME ip netns exec NAME cmd ... ip netns monitor  在本範例中我們只會用到 list, add 以及 exec 兩個指令，就如同字面意思的意義一樣，用來創造檢視,創造 network namespace 以及在該 netns 內執行對應的指令。 依序執行下列指令，創造好這兩個 netns 後，我們可以透過 ip netns list 確認的確有產生兩個 netns 。 ip netns add ns1 ip netns add ns2 ip netns list  接下來我們可以透過 ip netns exec ns1 bash 這個指令在 network namespace ns1 內執行 bash 這個指令，這樣我們就可以暫時切換裡面。 ip netns exec ns1 bash  接下來直接執行 ifconfig -a 查看系統上面的網路資訊，你會發現什麼都不見了，只剩下一個最簡單的 loopback 介面。 &gt; ifconfig -a lo Link encap:Local Loopback LOOPBACK MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  這就是 network isolation 的功用，將網路完全隔絕開來，不過我們暫時還沒有任何連線可以使用，所以先執行 exit 離開該 network namespace 回到我們的 Ubuntu吧。 上述的指令執行完畢後，我們的系統大概如下圖，有一點點的長進了。 ","version":"Next","tagName":"h2"},{"title":"Step3​","type":1,"pageTitle":"手把手打造仿 mininet 網路","url":"/docs/techPost/2017/setup-mininet-like-environment#step3","content":"我們已經將 software switch 以及相關的 network namespace 都準備好了，接下來我們要想辦法將這些東西串起來，打通整個連線。 這邊要使用的是 ip link 這個指令來處理，由於我們要在系統上創建的是一條虛擬的連結，稱之為 veth，這條虛擬連結要連接兩個 interface， 而這兩個 interface 則會分別給 switch 以及 ns 給使用，因此我們指令的原型大概如下 ip link add name $name type veth peer name $name2 上述指令代表會在系統中創建兩個 interface，名稱分別是 $name 以及 $name2，然後其中間透過 veth 方式串接起來，代表有任何封包從任何一端進入，都會從另外一端出來。 執行完下列指令後，可以透過 ifconfig 或是 ip link 看到剛創造出來的 interface ip link add name vet-n1 type veth peer name ovs-1 ip link add name vet-n2 type veth peer name ovs-2 ifconfig vet-n1 up ifconfig vet-n2 up ifconfig ovs-1 up ifconfig ovs-2 up  到這一步驟後，整個系統架構如下圖，已經有點樣子了，離目標只差一點點了。 ","version":"Next","tagName":"h2"},{"title":"Step4​","type":1,"pageTitle":"手把手打造仿 mininet 網路","url":"/docs/techPost/2017/setup-mininet-like-environment#step4","content":"經過前述的所有準備，該有的東西都有了，剩下的就是將上述創建的 interface 給放到正確的地方上，並且配上一個相同網域的 ip address，就可以讓 openvswitch 以 l2 briding 的方式把封包給轉發了。 這邊我們要繼續 ip 指令，首先我們要將剛剛創建的 vet-n1/vet-n2 這兩張 interface 給丟到 ns1/ns2 裡面，指令如下。 ip link set $interface netns $ns， 套到我們的環境的話，就是 ip link set vet-n1 netns ns1 ip link set vet-n2 netns ns2  當執行完這些指令後，再度透過 ip link 你會發現 vet-n1/vet-n2 這兩張 interface 完全消失了，已經被從 Ubuntu Host 本身給搬移到上述創造好的 network namespace n1/n2 裡面了。 接下來我們使用 ip netns exec 指令進入到 ns1/ns2 裡面去設定我們的網路了。 我們有下列事情要做 將剛剛獲得到的 vet-n1/vet-n2 改名成 eth0 (為了好看)將 eth0 以及 lo 叫起來幫 eth0 設定 ip 及網段。 所以指令大概如下 &gt; ip netns exec ns1 bash &gt; ip link set vet-n1 name eth0 &gt; ip addr add 10.0.0.101/24 dev eth0 &gt; ip link set eth0 up &gt; ip link set lo up &gt; exit  上述的指令會將 ns1 相關的事情都處理完畢，這時候再針對 ns2 進行一樣的處理，唯一記得的是 ip 的部分記得不要重複即可。 一切完畢後，目前系統上的架構如下圖 ","version":"Next","tagName":"h2"},{"title":"Step5​","type":1,"pageTitle":"手把手打造仿 mininet 網路","url":"/docs/techPost/2017/setup-mininet-like-environment#step5","content":"最後回到 Ubuntu(Host) 本身，最後就剩下 ovs-1/ovs-2 這兩張 interface 還沒處理了。 這邊我們透過 ovs-vsctl 的指令，將該兩張 interface 都接到 ovs-eth0 上面即可。 ovs-vsctl add-port ovs-eth0 ovs-1 ovs-vsctl add-port ovs-eth0 ovs-2 ip link set ovs-eth0 up ip link set ovs-1 up ip link set ovs-2 up  一切大功告成，整個系統的架構就如一開始的目標一樣了。 這時候就可以透過 ip netns exec ns2 ping 10.0.0.101 類似的指令去確認 ns1 以及 ns2 能不能互通，更複雜一點還可以進去執行除了 ping 以外的指令。 若今天 ovs-eth0 也有將系統上其他的網卡也加入近來，更可以讓 ns1/ns2 與外界網路連通，唯一要注意的是由於我們沒有採用 Controller 近來處理，所以預設的 openvswitch 只會使用 l2 briding 的方式去轉送封包，因此不同網段的封包會不通的。 Summary 本文中我們用了 network namespace 與 openvswitch 創造了一個類似 mininet 的環境，實際上 mininet 也是用一樣的方法去建置其模擬環境的。 我們除了學習怎麼使用這些工具外，若能對於其實作方法也有瞭解，更能夠幫助我們去思考該工具的極限以及其能力，同時也能夠加深我們自己的知識。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/cert-manager","content":"","keywords":"certificate cert-manager","version":"Next"},{"title":"Issuers​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#issuers","content":"Cert-Manager 裡面總共有兩種類型的 Issuers, 分別是 Issuer 以及 ClusterIssuers. 這兩個的差異只有在於其影響的 kubernetes namespace 的範圍而以，其餘的功能都一樣. 所以接下來的文章將會使用 Issuers 來當做範例介紹. Issuers 代表的是一個能夠從會簽署 x509 憑證的授權單位所取得憑證的一個資源，最 目前 Issuers 支援的 DNS Challenge 有兩種，分別是 HTTP-01 以及 DNS-01. 常見的後端代表就是想辦法從 Let's Encrypt 取得憑證. Issuers 是透過 Kubernetes CustomResourceDefinition(CRD) 所自定義的新資源 這意味你可以透過 kubectl get issuers 來檢視相關的設定 對於每個 kubernetes 叢集來說，系統內至少要部屬一個 Issuers 來完成憑證的功能Issuers 目前提供了四種後端接口, 這邊會稍微介紹一下彼此的差異，最後說明本文會基於哪種後端接口來展示 ","version":"Next","tagName":"h2"},{"title":"ACME​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#acme","content":"這個類型應該是最常見也是最多人在使用的，透過 Automated Certificate Management Environment(ACME)這個協定與已知的 CA 溝通來取得憑證。 與 CA 的溝通過程中，最困難也是最核心的部份就是 DNS Challenge, 使用者必須要向 CA 證明自己擁有想要簽署憑證的域名。 目前 Issuers 支援的 DNS Challenge 有兩種，分別是 HTTP-01 以及 DNS-01. HTTP-01​ 顧名思義，就是透過 HTTP 的方式來驗證是否擁有該網域，要採用這種方式基本上要滿足幾個條件 首先針對想要簽署的網域進行設定，譬如將 test.hwchiu.com 指向到一個 IP 地址該 CA 會嘗試透過 HTTP 的方式去連線 test.hwchiu.com. 其會預期得到一個對應的 HTTP 回應，該回應的格式可以參考 HTTP Challenge 透過上述兩個條件，我們可以觀察到幾個現象 HTTP-01 並不支援 wildcard 的 domain, 因為要透過 DNS 查詢對應的 IP 來取得 HTTP 回應為了讓 HTTP 封包可以順利的連線，通常客戶端都會需要有一個可存取的對外 IP 地址 當然這部分你若熟悉網路，要透過 DNAT 的方式將封包導入到私有 IP 的網頁伺服器上也是沒有問題 大部分人的人在使用 Let's Encrypt 時基本上都是透過 HTTP-01 這個方式來驗證，因為使用上其實相對簡單，基本的那些網頁伺服器以及對應的 HTTP 格式都有相關的軟體幫忙處理，大幅度簡化了管理員的設定。 本篇文章並不打算使用這個方式，因為過於簡單沒有什麼好設定的，所以我們來看看另外一種挑戰， DNS-01. DNS-01​ 相對於 HTTP-01 透過 HTTP 連線來驗證是否有該網域的擁有權， DNS-01 則是透過創造一個預期內容的 TXT 紀錄來證明你擁有該網域的擁有權。為了達成上述的事項，Issuers 目前有支援下列的 DNS 供應商來幫忙完成這個自動創建 TXT 紀錄的需求 Google CloudDNSAmazon Route53CloudflareAkamai FastDNSRFC2136ACME-DNS 透過 DNS-01 的機制就可以打破 HTTP-01 的限制，客戶端本身即使位於私有網路內也是沒有問題的。 而因為我自己的網域目前是放在 Cloudflare 上，因此本文就會採取 DNS-01 的方式並且使用 Cloudflare 作為測試的 DNS 提供者。 ","version":"Next","tagName":"h3"},{"title":"CA​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#ca","content":"相對於上述的 ACME 的使用方式， CA 類型的 Issuers 則是會使用一組事先準備好的簽署金鑰來發行憑證。這組簽署憑證必須要以 kubernetes secret 的形式存於 kubernetes cluster 中。 舉例來說，你可以先透過 openssl 的方式產生對應的檔案，並且把該檔案加入到 kubernetes secret 內，最後透過 CA 類型的 Issuers 來使用。 ","version":"Next","tagName":"h3"},{"title":"Valut​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#valut","content":"如果你系統內有安裝 HashCrop Valut 的話，可以透過這個類型的 Issuers 來與 Valut 溝通。 ","version":"Next","tagName":"h3"},{"title":"Self-Signed​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#self-signed","content":"這個選項目前會用到的機會不多，大致上有兩種情境 想要透過 kubernetes 打造一個 Public Key Infrastructur (PKI) 架構搭配後續會介紹的 Certificate 作為一個 root CA 供 CA 型態的 Issuers 使用 -. 可以參考 resource-validation-webhook 的使用範例來瞭解如何搭配使用 想要更加深入的瞭解這些選項可以直接到官網閱讀相關的資訊以及範例使用 ","version":"Next","tagName":"h3"},{"title":"Certificate​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#certificate","content":"看完了 Issuers 之後，我們來看看第二個也是最後一個透過 kubernetes CRD 所建立的物件，Certificate. Certificate 就如同其名稱一樣，代表的是一個憑證，其會嘗試使用 Issuers 來獲得對應的憑證與金鑰，當成功取得對應的資源後，就會將該對憑證與金鑰的資訊放到 kubernetes secret 裡面。Certificate 以及產生的 kubernetes secret 都跟 namespace 有綁定關係，所以對於不同的 namespace 的應用程式若想要擁有這張簽署的憑證，則都需要創立一個對應的 certificate 來使用。 ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#summary","content":"基本上整個 cert-manager 的關係都是由 Issuers/Certificate 所組成的，當然也有部屬安裝 cert-manager 時會佈署的 kubernetes deployment, 該 deployment 作為 Issuers/Certificate 之間的橋樑， 定期確認 Certificate 的合法性並且當憑證快要到期時還會嘗試更新。 透過這些資源的整合，Cert-Manager 就能夠於 kubernetes 叢集內提供可以從 Let's Encrypt 取得憑證並且定期自動更新以確保憑證不會過期的憑證供我們其他的應用程式使用。 Setup 看了上述的基本概念介紹後，我們接下來就要手把手的嘗試透過 Cert-Manager 來取得一個憑證，這中間的過程大概如下 部屬 cert-manager 相關的服務到 kubernetes 叢集中部署 Issuers部屬 Certificate透過檢查 Kubernetes Secret, 確認是否有拿到可用的憑證 整個系統的架構圖如下，基本上我們會有 Issuers/Certificate 基本元件，同時為了滿足 DNS-01, 這邊會需要一個額外的 kubernetes secret 供 Issuers 使用。此外 Certificate 最後則會產生一個 kubernetes secret 的合法憑證。  ","version":"Next","tagName":"h2"},{"title":"Cert-Manager​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#cert-manager","content":"安裝 Cert-Manager 的部分有兩種安裝方式，第一種就是最基本也是最熟悉的，根據自己的環境去部屬事先提供好了 Yaml 檔案。 這部分可以參考官方 Github 上面的檔案，基本上會根據叢集是否有啟用 RBAC (Role-Based Access Control) 來決定要部屬那個檔案。 另外一個安裝方式則是透過 Package 的概念來安裝，這邊採用的是 Helm 的方式來安裝，透過 Helm 的打包及管理，套件管理員將該服務所需要的 Yaml 都收集完畢並且打包，使用者只需要透過 Helm 的指令很簡單輕鬆的直接安裝完畢。 這邊我就直接使用 Helm 的方式來安裝 Cert-Manager. hwchiu➜~» helm install \\ --name cert-manager \\ --namespace kube-system \\ stable/cert-manager  想要安裝 Helm 可以參考官網的教學 安裝完畢後，可以透過 kubectl 的指令去確認是否需要的 deployment 有運行起來 hwchiu➜~» kubectl -n kube-system get deployment/cert-manager NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE cert-manager 1 1 1 1 5d  ","version":"Next","tagName":"h2"},{"title":"Issuers​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#issuers-1","content":"當 cert-manager 正確安裝後，我們接下來要開始處理 Issuers, 再我的範例之中，我採用的是 clusterIssuers 來建立一個供所有 namespace 都可以使用的 Issuers. 此外，由於我要採用的是 DNS-01 的方式來驗證網域的擁有權，同時我的 DNS 提供商是 CloudFlare. 因此我必須要先創立一個 kubernetes sercet 讓我的 Issuers 有能力操控我的 CloudFlar. ","version":"Next","tagName":"h2"},{"title":"CloudFlare​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#cloudflare","content":"這部分會依據你的 DNS 供應商有不同的做法，詳細的可以參考 Cert-Manager Supported DNS01 Providers 以 CloudFlare 爲範例，首先到你網域的控制台中去取得你的 Public Key. 可參考下圖得到該 Public Key 後，接下來要轉成 Base64 並且放到 kubernetes sercet.  假設該 Public Key 是 12345678910, 執行下列函式得到對應的 Base64 編碼結果 echo -n &quot;12345678910&quot; | base64  接下來我們要透過 yaml 的方式創建 kubernetes secret. 打開文件 sercet.yml 並且貼上下列內容 apiVersion: v1 kind: Secret metadata: name: cloudflare namespace: kube-system data: api: MTIzNDU2Nzg5MTA=  創建好上述的 yaml 檔案後，我們就透過 kubectl create -f secret.yml ","version":"Next","tagName":"h3"},{"title":"ClusterIssuers​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#clusterissuers","content":"接下來我們就要來創建我們的 ClusterIssuers 了， 這邊我們直接來看相關的 yaml 內容，有一些東西要注意 我們在 Issuer 內目前使用了 acme 的方式來認證server 的部分，一般來說會先用 staging 的內容來確認所有的資訊都正常且可以運作，正式環境中就會切換成 v02 的版本來使用於 acme 裡面，我們使用 dns01 的方式來驗證 我們採用的是 cloudflare 的方式， 這邊就是填寫你擁有網域的信箱接下來要使用剛剛創立的 kubernetes secret, name/key 就對應到 kubernetes secret 內的 metadata:name 以及 data/api. apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: cert-demo namespace: default spec: acme: #server: https://acme-v02.api.letsencrypt.org/directory server: https://acme-staging-v02.api.letsencrypt.org/directory email: your@mail.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: cert-demo # ACME DNS-01 provider configurations dns01: # Here we define a list of DNS-01 providers that can solve DNS challenges providers: - name: cf-dns cloudflare: email: your@mail.com # A secretKeyRef to a cloudflare api key apiKeySecretRef: name: cloudflare key: api  創建上述 ClusterIssuers 的檔案之後，可以透過 kubectl describe clusgerissuer/cert-demo 去確認狀態，譬如 ..... Status: Acme: Uri: https://acme-staging-v02.api.letsencrypt.org/acme/acct/7037688 Conditions: Last Transition Time: 2018-09-30T16:51:03Z Message: The ACME account was registered with the ACME server Reason: ACMEAccountRegistered Status: True Type: Ready Events: &lt;none&gt;  確認這邊是 True/Ready 之後, 對於 Issuers 的事情就告一段落了 ","version":"Next","tagName":"h3"},{"title":"Certificates​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cert-manager#certificates","content":"正式進入 Yaml 之前，有幾個重要的事情要先注意 每個 Certificate 可以設定 Subject Alternative Names(SANs), 亦即可以設定多個 domain name.針對設定的所有 domain name, Issuers 都必須要去處理。實際上在 Issuers 的設定中，可以設定多種方法，甚至是同時支援多個 DNS 供應商。因此對應Certificae 中的每個 domain name, 都要指派對應 Issuers 的可以用來處理該 Certificate 的物件，譬如 dns-01 內的某種 dns 供應商。每個 Certificate 最後會把得到的憑證資訊都寫入到 Kubernetes Secret 的資源內，所以我們必須要設定我們期望的名稱 有了上述的概念後，我們就可以來看一下 Yaml 的內容了 apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: demo-certi spec: secretName: demo-certi-tls issuerRef: name: cert-demo kind: ClusterIssuer dnsNames: - test.hwchiu.com acme: config: - dns01: provider: cf-dns domains: - test.hwchiu.com  這個範例中，我們要先指定該 Certificate 要使用的 Issuers. 這部分要透過 issuerRef 來指定剛剛創立的 ClusterIssuers. 接下來透過 secretName 去指定之後創立的 kubernetes secret 的名稱 最後要處理子域名的問題，在這個範例中只有一個，也就是 test.hwchiu.com. 所以在 acme 的部分就直接使用先前創立的 ClusterIssuers 內所描述的 DNS01 方式去進行認證 當這一切都準備就緒後就透過 kubectl apply 給部屬到叢集內。 接下來我們可以觀察整體運行的狀況，我們透過 kubctl descriBe certificate demo-creti 可以看一下 certificae 當前的狀態。 一個運行的範例如下 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CreateOrder 16s cert-manager Created new ACME order, attempting validation... Normal IssueCert 15s cert-manager Issuing certificate... Normal CertObtained 13s cert-manager Obtained certificate from ACME server Normal CertIssued 13s cert-manager Certificate issued successfully  當一切都完畢後，就可以透過 kubernetes get secret 看到 cert-manager 已經幫忙創立了一個 kubernetes secret，裡面已經包含了你需要的憑證。 如果有使用 ingress 相關的，可以直接在 Ingress 那邊透過 tls 的設定直接使用該 secret 幫你的 Ingress 套上這個憑證囉 Summary 使用任何的解決方案前，都要先思考一下到底自己遇到的是什麼問題，再從這個問題延伸去尋找方法來處理。接下來為了要更能夠熟悉該方法所帶來的好處以及壞處，就必須要花時間去瞭解其架構以及原理，最後透過動手操作的方式來實際體驗是否真的能夠解決問題。 本文的出發點很簡單，就是想要有一個解決方案能夠幫忙處理 Ingress 的憑證，然後因為沒錢所以希望使用 Let's Encrypt 的服務，但是礙於該服務所簽署的憑證有效期限很短，所以需要一直更新。 基於這個問題的情況下，我開始研究了 Cert-Manager，並且透過一系列的操作與體驗來理解如何使用 Cert-Manager 解決我的問題。 最後也希望大家都能夠保持類似的態度與想法來解決每一個問題，藉由這些步驟能夠更深刻的體會每個解決方案背後的原理以及設計思緒，將這些概念融入自己的思想之中以不停的成長學習。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/cni-compare","content":"","keywords":"k8s cni","version":"Next"},{"title":"Installation​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#installation","content":"Flannel 安裝非常簡單，實際上只需要部屬一個 Yaml 檔案即可。 該 Yaml 裡面其實會牽扯到不少元件，譬如 DaemonSet, InitContainer 等許多有趣的玩法，這邊就不再贅述，有興趣的可以想想看一個問題 你要如何讓任何加入到k8s的新節點都能夠自動安裝 CNI 需要的 binary 以及 config ? ","version":"Next","tagName":"h2"},{"title":"Features​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#features","content":"Flannel 功能非常簡單，就是且只有 Overlay Network，目標是提供Pod與Pod之間跨節點的溝通，提供 Layer3 IPv4 的能力來處理節點與節點之間傳輸的辦法。 目前其採用的實作方式有 VXLANUDP 封裝Host-GW 靜態路由設定 詳細的實作內容請參閱GitHub Flannel Calico 另外一個也是非常知名的 CNI Plugin 就是 Calico, 相對於 Flannel 來說， Calico 提供的功能則相對的多，而這些功能與特色都能夠對應不同的網路環境。 Calico 專案的開發相對於 Flannel 來得更加活躍且也不停的有各種功能出現，畢竟相對於 Flannel 單純想提供Pod x Pod 連線功能來說， Calico 想提供的則是更多的功能，自然而然的發展就會比較廣且活躍。 ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#installation-1","content":"安裝方便基本上可以很簡單，如同 Flannel 一樣透過一個 Yaml 就可以安裝基本的 CNI 資源到 Kubernetes 叢集中。此外也可以很複雜到需要安裝非常多的東西來提供更進階的功能。 這部分取決於你想要採用的功能。 舉例來說，如何建立存放 Calico 各節點溝通的資料就有兩種 etcdkubernetes API 詳細的安裝教學可以參閱官網 ","version":"Next","tagName":"h2"},{"title":"Features​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#features-1","content":"Calico 支援的平台非常多種，本身支援 CNI 介面使得支援 CNI 的平台都可以使用外，其也可以透過 Neutron Plugin 的方式安裝於 OpenStack 的環境中，算是我目前看到支援度非常豐富的網路功能解決方案 在其功能方面，基本上主要以 kubernetes 該平台能夠提供的功能來探討，可以分成兩個面向來看，分別是 Policy 以及 Network。 ","version":"Next","tagName":"h2"},{"title":"Policy​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#policy","content":"Network Policy 實際上，Kubernetes 本身有定義 Network Policy 的介面，但是實際上卻沒有實現這種能夠用來限制 Pod 與 Pod 之間連線的功能。 官網明確的說明這部分的實現要依賴 CNI Plugin 來處理。 而 Calico 就有實現這種介面，因此可以直接使用 Kubernetes Network Policy 的規範去設定相對應的 Yaml 來實現基本的ACL (Access Control List) Application Layer Policy 相對於基本的 Kubernetes Network Policy 外， Calico 本身透過 Kubernetes CustomResourceDefinetion(CRD) 提供了另外一層新的 GlobalNetworkPolicy. 該介面提供了基於 HTTP 方法或是路徑為基準的 ACL，本身的實現是透過 Service Mesh 的方式來進行 ACL 的處理，因此本身會依賴 lstio 的系統。在安裝上會需要更多的操作以及設定的細節來開啟此功能。 ","version":"Next","tagName":"h3"},{"title":"Network​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#network","content":"Overlay Network.Calico 對於網路連線方面也有提供類似 Flannel 的功能，透過 VXLAN 或是 IP in IP 這類型的協定來封裝連線封包已達到 Overlay Network 的需求Native Routing 除了上述的 Overlay Network 之外， Calico 最主打的就是第二種網路，完完全全的 Underlay Network, 不依賴任何的封包封裝協定。Calico 的想法很簡單，雖然 Overlay Network 很方便，但是實際上每次的封包封裝都會增加整體封包的複雜度，同時也會增加封包的處理時間。 因此 Calico 透過 IPTables 以及 Routing Table 的方式來處理 Layer3路由的問題，讓不同節點之間的 Pod 可以透過 Routing Table 的方式直接互相連間而不需要再包覆一層額外的標頭檔來處理封包。 這部分的原理牽扯到 Layer3 路由以及相關閘道的 MAC address 的原理，有興趣的可以參考官方說明The Calico Data Path: IP Routing and iptables3. Public IP Address AssignmentCalico 有額外自行開發一套 IPAM 來管理 IP 及分配，在此架構中所有的 IP 都會從 IP Pool 中去取得。 同時搭配 BGP 等動態路由協定的幫忙，所有的 Pod 都可以被賦予一個可被存取的 Public IP 地址，同時這些 Pod 在對外進行存取的時候也不需要進行 NAT 的轉換，可減少一次封包轉換所造成的效能耗損 Canal Canal 是一個比較特別的 CNI Plugin, 其目的在於整合 Flannel 以及 Calico 這兩個 CNI Plugin. 希望使用 Calico 的 Network Policy 功能以及 Flannel 的 Overlay Network 功能。 因此整體上並沒有什麼特別獨特的功能，不過隨者時間演進，目前 Canal 專案也有大變化。 從 Canal 於 Github canal 官網上面可以看到該專案在 Jan 27,2018 更新了說明來表示該專案已經停止開發，目前已經可以直接在 Calico 的官網直接看到 Calico 與 Flannel 的整合方法以及安裝方式，因此 Canal 這個獨立的 CNI Plugin 基本上已經沒有需要使用的必要了。 Nuage Nuage 是一個基於 SDN 概念開發的CNI Plugin，基底層使用了 OpenvSwitch 作為軟體交換機，同時使用了 OpenFlow 此協定來控制 OpenvSwitch. 此外，為了能夠更加聰明有智慧的去透過 OpenFlow 控制 OpenvSwitch, 也必須要有一個 OpenFlow Controller VSC 來作為一個中央控管的管理者。 ","version":"Next","tagName":"h3"},{"title":"Feature​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#feature","content":"Nugae 本身在網路連線方面，也有提供基於 VXLAN 協定的 Overlay Network 的應用。這部分應該是直接利用 OpenvSwitch 本身提供的 VXLAN 功能來完成。 此外針對 Nuage 也有實現 Network Policy, 其原理就是透過 Openflow 的規則下發到所有的 OpenvSwitch 來達到 Pod 與 Pod 之間的傳輸規則隔離。 下圖是一個簡單的示意圖來解釋 Network Policy 的運作原理，大致上就是當使用者透過 Client API 去寫入 Network Policy 的規則後，這些規則最後會被相關的應用程式給解讀並且轉達給 VSC, SDN Controller，最後 VSC 會把該規則對應到的資訊透過 OpenFlow 的方式寫入到對應的節點上面的 OpenvSwitch 上來達到 ACL 的功能。  Ciliunm Ciliunm 最厲害也是最主打的功能並不是網路方面的連接，反而是安全性以及平衡附載特點。 在其實作中採用了 eBPF 以及 XDP 這兩種這幾年逐漸受到重視的技術來處理封包。連線的封包能夠在網卡收到但是尚未送到 Kernel 前就先行處理，因此有更早期且更快速的效率來處理封包。 接下來就針對其各個特色功能來介紹 ","version":"Next","tagName":"h2"},{"title":"Networking​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#networking","content":"網路連線方面提供兩種功能，Overlay Network 以及 Native Routing. Overlay NetworkOverlay Network 本身也是依賴 VXLAN 以及 Geneve 這些已知的協定來處理，Ciliunm 直接使用 Kernel 相關的功能來達到這些封裝效果，因此其支援度以及支援性則是依賴於使用的 Kernel 版本。Native Routing 這個比較偏向給進階使用者使用的，叢集管理者必須要知道這方面的概念同時也要確保相關的 Routing Table 是可以運行的，同時也可以使用 OSPF/BGP 等相關的應用程式來輔助。 詳細的可以參考Ciliunm ConceptsLoad Balacing 負載平衡方面，Cilium 則使用了 BPF 的架構並且透過 Hashing 的方式來幫忙決定該連線最後的目標節點， ","version":"Next","tagName":"h2"},{"title":"Network Policy​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#network-policy","content":"Ciluum 除了實現了 Kubernetes Network Policy 這種基於 Layer4/Layer3 的防火牆外，本身也有額外實現了 Layer 7 的防火牆。 在 Layer7 方面支援了下列的協定 REST/HTTPgRPCKafka 已 Kafka 來說，可以使用下列的 Yaml 來描述相關的防火牆規則 apiVersion: &quot;cilium.io/v2&quot; kind: CiliumNetworkPolicy description: &quot;enable empire-hq to produce to empire-announce and deathstar-plans&quot; metadata: name: &quot;rule1&quot; spec: endpointSelector: matchLabels: app: kafka ingress: - fromEndpoints: - matchLabels: app: empire-hq toPorts: - ports: - port: &quot;9092&quot; protocol: TCP rules: kafka: - role: &quot;produce&quot; topic: &quot;deathstar-plans&quot; - role: &quot;produce&quot; topic: &quot;empire-announce&quot;  詳細更多的用法可以參閱其官網 來學習 OVN OVN, Open Virtual Network 是由 OpenvSwitch 的開發公司, Nicira 開發的一套 OpenvSwitch 管理工具。 最初開發 OVN 的用途就是希望能夠透過一個類似 SDN Controller 概念的應用程式來管理整個網路叢集中的 OpenvSwitch, 並且方便且有效率透過管理這些 OpenvSwitch 來達到各式各樣的網路功能，譬如簡單的 ACL 到多租戶的虛擬網路 如今 OVN 也透過 CNI 的介面以及相關的 Kubernetes API 來整合到 Kubernetes 環境中。 從功能面來說, OVN 的主軸還是在於網路功能的提供，除了最基本的 Overlay Network 之外，也能夠透過 OpenvSwitch 的架構來實現 如 kubernetes service(ClusterIP,NodePort) 等功能。 從目前的發展來看也許還沒有很強大的力量可以吸引使用者轉換過去，不過若能夠透過 OVN 配上 OpenvSwitch 來完成 Kubernetes 內部所有的 Networking 功能，則有招一日我認為若能夠將所有的 OpenvSwitch 都整合 DPDK 此框架則有機會可以一舉將整個 Kubernetes 內部所有網路傳輸的效能直接提到到 10Gbps 以上(數字上限很依賴調整，但是 10Gbps 基本上絕對沒問題). 有興趣玩玩的可以參考 官網，裡面也有附設相關的 Vagrant 讓開發者使用看看。 SR-IOV 相對於前述所有的 CNI 都提供一個全面性的網路功能，接下來的幾個 CNI 則是會跟網卡的硬體資訊有更強烈的整合。SR-IOV 的介紹可以參考 NFV 網路技術介紹 – SR-IOV 這篇文章來學習。 比較簡單的理解就可以想成硬體網卡端會直接映射一個虛擬網卡到對應的 Container 內部去使用，這種情況下該 Container 內往該虛擬網卡送出去的封包都會直接從對外層的實體網卡送出去，不會經過宿主機內部的任何軟體交換機處理。 好處來說就是效能會更高，速度更快。壞處來說就是基本上 Kubernetes 原先透過 IPTables 或是 IPvS 提供的任何 Kubernetes Networking Functions (Services/DNS) 都會失效。 此外 SR-IOV 也有 VF 數量上的限制，所以使用上絕對不會是每個 Pod 都會使用，而是要依據自己的需求以及對應的情境來使用。 ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#example","content":"一個簡單的 CNI 設定檔範例如下 { &quot;name&quot;: &quot;mynet&quot;, &quot;type&quot;: &quot;sriov&quot;, &quot;if0&quot;: &quot;enp1s0f1&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;10.55.206.0/26&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;10.55.206.1&quot; } }  使用上強烈建議要搭配 Intel 針對 SR-IOV 所開發的 Device Plugin 來處理 VF 的同步與管理，可以減少更多對硬體資訊的依賴性。 有興趣的可以參閱 Intel 所維護的 SRIOV-CNI SR-IOV 最早期的 Plugin 並不是 Intel 所開發的，但是 Intel 將其 Fork 回來並且加入了更多的功能，同時也在同一個 Plugin 之中去支援 DPDK 的功能 DPDK 與 SR-IOV 類似，都是針對特定用途且有網卡資訊依賴性的 CNI 插件，主旨都在提供一個更高速且更低傳輸延遲的網路環境。 DPDK 就是一個 Kernel ByPass 的應用程式架構，透過 Polling 的方式來盡可能的使用 CPU 去輪詢網卡來提升網卡的處理效率，在使用上則必須要指定對應的實體網卡資訊。 一旦網卡使用上了 DPDK, 所有未客製化的常見工具 (ifconfig, ip) 等都沒有辦法檢視到該網卡的資訊，因為該網卡已經從 Kernel 內消失了。 ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#example-1","content":"一個關於 DPDK 的 CNI 設定 Yaml 如下 { &quot;name&quot;: &quot;mynet&quot;, &quot;type&quot;: &quot;sriov&quot;, &quot;if0&quot;: &quot;enp1s0f1&quot;, &quot;if0name&quot;: &quot;net0&quot;, &quot;dpdk&quot;: { &quot;kernel_driver&quot;:&quot;ixgbevf&quot;, &quot;dpdk_driver&quot;:&quot;igb_uio&quot;, &quot;dpdk_tool&quot;:&quot;/opt/dpdk/usertools/dpdk-devbind.py&quot; } }  Intel 將 DPDK 相關的功能一起放在 SRIOV-CNI 裡面，可以到SRIOV-CNI Github參考用法 Bond-cni Bonding, 或是 Teaming 都是非常類似概念的一種實作，本文主要針對 Bonding 來介紹。 歡迎參考 Redhat 的文件來學習 Teaming 以及 Bonding 的差異If You Like Bonding, You Will Love Teaming 能夠將多張網卡抽象成一張網卡來使用，這種情況下該網卡能夠提供下列的功能 Load-Balancing 能夠針對 Layer2/Layer3/Layer4 等不同的規範讓不同的連線最後透過不同的實體網卡傳輸，舉例來說兩張 1G 的網卡綁在一起後是有機會能夠提供 2Gbps 的傳輸數率Fault-toleranceActive-Backup 當底層網卡有任何故障的時候，最上層的應用程式可以不需要意會到這種情況且依然有辦法繼續傳送網路封包。 ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#example-2","content":"相關的 CNI 設定如下 { &quot;name&quot;: &quot;mynet&quot;, &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;type&quot;: &quot;bond&quot;, &quot;mode&quot;: &quot;active-backup&quot;, &quot;miimon&quot;: &quot;100&quot;, &quot;links&quot;: [ { &quot;name&quot;: &quot;ens3f2&quot; }, { &quot;name&quot;: &quot;ens3f2d1&quot; } ] } }  這個 CNI Plugin 也是由 Intel 所開發的，不得不說 Intel 對於 On-premise 環境中的 Networking Solution 下足了功夫，缺少什麼就自己實現什麼並且開源，最後將這些各式各樣的 CNI Plugin 統整成為自己的解決方案。 有興趣了解更多的可以參閱其intel/bond-cni Multus/CNI-Genie/Knitter 前面講了非常多的 CNI Plugin，從全面性的網路功能到依賴特定網卡型號的用途都有，接下來要介紹的 CNI Plugin 可以說是一個非常特別類型的 CNI 用法。 舉 Kubernetes 為範例，對於每一個創建的 Pod 只會呼叫一次 CNI 來設定相關的網路功能，然而在某些場景應用中，會特別希望該 Pod 中有多個網路介面。我這邊使用下圖作為一個範例介紹 試想一個情境，有很多的應用程式容器化之後要運行在 Kubernetes 上，這些容器本身需要互相溝通協調，同時這些容器本身有非常強烈的網路效能需求，譬如 High Throughput/Low Latency. 在這些應用程式尚未容器化以前，其架構中常常會規劃成 Control Network 以及 Data Network. 這些應用程式透過 Control Network 來傳輸控制用的資料，而透過 Data Network 來傳輸真正的資料。 這些應用程式容器化遷移到 Kubernetes 之後，要如何在 Kubernetes 上滿足這些需求? 就算今天 Kubernetes 不用 IPTables 而改用 IPvS 的模式來處理相關的功能，整體傳輸的速度還是受限於 Linux Kernel而沒有辦法達到令人滿意的境界，勢必要另尋它路來處理。 這種需求的情況下，有一種特別的 CNI 就誕生了，這類型的 CNI 本身沒有提供任何的網路應用功能，而是作為一個呼叫者去銜接數個 CNI Plugins 來處理。 這邊借用 Multus 的一張圖片來說 最終目的就是希望每個 Container 被創立的時候都可以執行多次的 CNI Plugin. 因此整個架構就會是 Kubernetes --&gt; Special CNI -&gt; Other CNIs. 現在提供這種需求的 CNI 也不少，由於目的相同，大部分都是單純用法不同，因此我將這些 CNI 都放在一起。 Mutlus-CNI 是由 Intel 所主導開發的，看到這邊再來回想一下前面所介紹的 SRIOV/DPDK/Bonding 等也是由 Intel 所維護的相關 CNI Plugin。 不難想像 Intel 想要做的是一個整體的網路解決方案，先透過個別的 CNI 提供獨特的功能，最後透過 Multus 的方式將這些 CNI 全部串接起來來滿足使用者的需求。 ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-compare#example-3","content":"這邊舉一個 Intel 提供的範例，看看是如何透過 Multus 串接各式各樣的 CNI 來使用 { &quot;name&quot;: &quot;multus-demo-network&quot;, &quot;type&quot;: &quot;multus&quot;, &quot;delegates&quot;: [ { &quot;type&quot;: &quot;sriov&quot;, #part of sriov plugin conf &quot;if0&quot;: &quot;enp12s0f0&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;10.56.217.0/24&quot;, &quot;rangeStart&quot;: &quot;10.56.217.131&quot;, &quot;rangeEnd&quot;: &quot;10.56.217.190&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;10.56.217.1&quot; } }, { &quot;type&quot;: &quot;ptp&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;10.168.1.0/24&quot;, &quot;rangeStart&quot;: &quot;10.168.1.11&quot;, &quot;rangeEnd&quot;: &quot;10.168.1.20&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;10.168.1.1&quot; } }, { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;isDefaultGateway&quot;: true } } ] }  Summary 礙於人生苦短，時間有限，本篇文章沒有辦法將所有的 CNI Plugin 都介紹一遍，還有非常多的 CNI Plugin 沒有介紹到，譬如 Knitter, vhost-CNI, DANM 甚至是一些Plublic Cloud Provider所開源的 CNI 專案. 不過我相信對於大部分公有雲的使用者來說，其實使用公有雲本身提供的網路解決方案都能夠滿足大部分的需求，如果你有需要在 On-Premise 的架構下去設計這些 Kubernetes 平台並且需要符合特定的使用情境的話，那一定要好好的想清楚自己的需求，尋找一套合適的CNI來使用，真的不行的話非常推薦自己撰寫一套 CNI 出來客製化自己的需求。 Reference Scaling Kubernetes deployments with Policy-Based NetworkingCilium Network PolicyCNI - the Container Network Interface ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/cni-questions","content":"","keywords":"k8s cni","version":"Next"},{"title":"什麼是 CNI​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#什麼是-cni","content":"簡單的說，就是可以讓你的 Container 上網的一種方法，其中包括了讓你的 Container 能夠有對外存取的方法，IP 的設定甚至到 Container 內部關於 DNS 伺服器的設定都可以複雜的答案參考 [Container Network Interface] CNI Introduction 這篇文章，有比較詳細的介紹 Container Network Interface(CNI) 的概念 ","version":"Next","tagName":"h2"},{"title":"我現在使用 kubernetes, 到底要選擇哪個 CNI​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#我現在使用-kubernetes-到底要選擇哪個-cni","content":"這個問題非常容易聽到，但是基本上沒有辦法回答，因為網路的架構太過於複雜且龐大，沒有明確的需求之前沒有辦法得到一個較好的答案。如果你只是想要 Pod 之間可以正常連線，沒有其他的考量的話，就選擇 Flannel 吧。有其他網路使用相關的需求，甚至是 Network Policy 的話，就要在尋求其他的 CNI 解決方案可以參閱常見 CNI (Container Network Interface) Plugin 介紹 ","version":"Next","tagName":"h2"},{"title":"聽說 CNI 可以設定 DNS, 那為什麼我在 kubernetes 環境中這些設定都沒有被採用​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#聽說-cni-可以設定-dns-那為什麼我在-kubernetes-環境中這些設定都沒有被採用","content":"CNI 的確有提供 DNS 的選項，但是會不會採用要依賴使用該 CNI 的管理系統的決策，對於 Kubernetes 來說, 因為已經可以透過 Pod 的設定檔來處理相關的 DNS 設定了，所以 CNI 本身回傳的 DNS 設定就忽略掉，並不會採用。可以參考這篇文章學習如何透過 DNSPolicy 來個別設定 Pod 裡面的 DNS欄位, DNS setting in your Pod相關的 Gihub 討論可以參考這篇 Unable to add custom DNS to container using flannel plugin ","version":"Next","tagName":"h2"},{"title":"我有沒有辦法透過 CNI 去更彈性的設定 Pod 的 IP 地址​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#我有沒有辦法透過-cni-去更彈性的設定-pod-的-ip-地址","content":"在 CNI 裡面有 IPAM 相關的模組在進行 IP 地址的計算與分配，在預設的情況下，大部分的 CNI 都會使用 IPAM(host-local) 搭配使用者預先輸入的一組網段來分配 IP 地址給每個運行的 Pod.由於這邊沒有狀態紀錄的概念，所以相同 Deployment 產生的Pod每次都會拿到不一樣的 IP 是很正常的。如果想要做到一些特別的需求，譬如 Static IP，或是 Container 容器重啟可以拿到相同的 IP 地址，這部分目前都要依賴重新撰寫 IPAM 的模組來達成這個功能。 ","version":"Next","tagName":"h2"},{"title":"CNI 有沒有支援 HA​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#cni-有沒有支援-ha","content":"其實這個問題我聽到的時候也愣住了，我其實聽不太懂這個問題，所以忽略吧認真討論的話這個東西跟 CNI 架構無關，是你採用的那套 CNI 本身有沒有幫你實現網路相關的 HA 功能 ","version":"Next","tagName":"h2"},{"title":"我想要在 Pod 有多張網卡甚至是同時運行許多現有的 CNI，可以怎麼做?​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#我想要在-pod-有多張網卡甚至是同時運行許多現有的-cni可以怎麼做","content":"對於 Kubernetes 來說對於每個 Pod 都只會呼叫一次 CNI. 所以如果想要在一個 Pod 裡面有多張網卡或是執行多次 CNI 的話，有兩種方式 自行重新撰寫一個 CNI, 當該 CNI 被呼叫將相關的參數傳遞到想要呼叫的所有 CNI 來達成目的使用現有的解決方案來達成上述目標 目前我自己知道能夠處理這個問題的 CNI 專案有三個，如果有人看到更多的歡迎告訴我 Intel MultusHuawei GenieNokia DANM 這三個解決方案裡面我個人比較熟悉的是偏向 Intel Multus, 其功能跟 Huawei Genie 比較類似，都是專注於提供 CNI Chain 的概念，可以透過設定檔去描述多個 CNI 並且依序執行這些 CNI 來幫 Pod 提供網路功能相對於上述兩者而言， Nokia DANM 提供的功能更廣大，CNI Chain 只是其中一項功能而已，有興趣的朋友可以在閱讀其官方說明文件 ","version":"Next","tagName":"h2"},{"title":"我要如何知道我的 kubernetes 到底現在是用哪個 CNI​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#我要如何知道我的-kubernetes-到底現在是用哪個-cni","content":"先確認你的 kubelet 是採用 CNI，這部分可以透過觀察 kubelet 的參數來得知 root 1173 3.3 2.6 634128 105920 ? Ssl Oct16 164:30 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni  有三個跟 CNI 有關的參數，其中兩個分別描述 執行檔 以及 設定檔 的位置 2. 接下來到 ${cni-conf-dir} 的位置去看， kubernetes 會從裡面根據字母排序的方式找到第一個設定檔案來使用。 hwchiu@k8s-dev:~$ ll /etc/cni/net.d/total 12 drwxr-xr-x 2 root root 4096 Oct 8 15:10 ./ drwxr-xr-x 3 root root 4096 Oct 6 13:52 ../ -rw-r--r-- 1 root root 92 Oct 8 15:10 10-flannel.conf  ","version":"Next","tagName":"h2"},{"title":"要如何簡單偵錯 CNI 是否有運作正常​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#要如何簡單偵錯-cni-是否有運作正常","content":"這部分其實滿難的，以 Kubernetes 來說，除了每個 CNI 解決方案本身是否有相關的 Daemon 或是其他方式可以觀察 Log 之外，剩下的就是依賴 kubelet 本身呼叫 CNI 的結果。這部分可以透過 journalctl 來觀察 kubelet 的輸出訊息(前提是你有用 systemd) 控管使用 sudo journalctl -xe 觀察的話，可以專心觀察 cni.go 的輸出訊息 ","version":"Next","tagName":"h2"},{"title":"我的 Pod 網路有問題，我可以怎麼辦​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#我的-pod-網路有問題我可以怎麼辦","content":"放大招,砍掉 Kubernetes, 重新安裝先看看自己使用的是哪套 CNI, 然後搜尋看看那個 CNI 有沒有相關的 Issue手動Debug, 通常我遇到這種情況我會採取下列步驟 先透過 nslookup 等工具觀察 DNS 解析是否正常。透過 ping/telnet 等工具觀察連線是否正常透過 ip route/route 觀察路由目標是否正常透過 tcpdump 抓取封包，觀察封包的流向透過 iptables 肉眼觀察是否多餘或是缺少特定的規則導致連線起不來 上述 debug 的步驟有些會牽扯到 Host 主機，有些會牽扯到 Container，基本上要對整個網路跟封包傳輸的流程要有些概念才會比較知道自己在做什麼，以及該怎麼解讀看到的資訊。這部分需要時間與經驗去學習 ","version":"Next","tagName":"h2"},{"title":"Kubernetes 裡面我可以每個節點都用不同的 CNI 嗎​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#kubernetes-裡面我可以每個節點都用不同的-cni-嗎","content":"可以，因為 CNI 是跟者 kubelet 一起跑的，所以每台節點都是獨立設定，互相不影響在 CNI 的解決方案中，有的方案會讓管理者用起來很方便，不需要每台設定，譬如 Flannel 等則是會透過 DaemonSet 的方式自動幫每個節點安裝對應的 CNI Config。然而也有些 CNI 本身的使用上就需要特別去注意每台機器的資訊，特別是跟硬體網卡資訊有關聯的 CNI 解決方案，譬如 SR-IOV/DPDK/Host-Device假設每個節點都用不同的 CNI, 那你要擔心的可能是不同節點之間的 Pod 會不會因為 CNI 分配不同的 IP 位址，同時有沒有對應的 路由規則 來幫忙處理不同 Pod 之間的網路連線 ","version":"Next","tagName":"h2"},{"title":"我好想寫程式呀！ 有沒有辦法自己撰寫 CNI​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/cni-questions#我好想寫程式呀-有沒有辦法自己撰寫-cni","content":"CNI 本身其實滿容易撰寫的，只要瞭解其基本概念並且知道自己想要做什麼就差不多可以完成了CNI 本身沒有限定任何語言實現，只要可以提供一個 Binary 執行檔即可如果對於用 Golang 撰寫一個簡單的 Bridge CNI 可以參考下列文章 [Container Network Interface] Implement Your CNI In Golang Summary Container Network Interface(CNI) 本身概念不難，困難的其實都是網路本身，封包怎麼傳輸，不同節點之間怎麼路由，Overlay Network怎麼做，Network Policy 怎麼實現等諸多的網路問題實際上才是最令人頭疼的地方。 目前沒有看到任何一套 CNI 可以滿足所有的需求，所以在選擇 CNI 的部分還是要謹記自己的需求，從自己的需求出發，看看有沒有現成的解決方案可以採用，如果都沒有則需要評估是否需要自己開發一套 CNI 來實現所缺的功能。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/gpu-gke","content":"Preface 最近嘗試要再 GKE 上面開啟 GPU 的服務，然後實際上卻遇到了一些問題，因此用本篇文章來記錄遇到問題並且遇到問題的解決思路。 一開始有個需求時，很直覺的再 Google 上面直接用 GKE GPU 進行搜尋，的確也可以找到不少文章在講 GPU 與 Kubernetes 相關。 第一個搜尋結果則是 Google 官方的文章 Kubernetes Engine, GPUs 直接整理該文章中的重點流程. 從 GCP 的管理面板中去取得 GPU 資源創造 GPU Node Pool 供 GKE ㄐ透過 kubernetes DaemonSet 的方式安裝 NVIDIA Drivers 到所有 GPU Node 上接下來創造 Pod 即可正常使用 NVIDIA GPU 資源 Install 前面兩個步驟基本上不會有什麼問題，透過網頁介面點選或是參考網頁中的指令去創造即可順利完成。 比較困擾的則是第三個步驟，根據該篇文章顯示我們只需要部署一個 yaml 檔案即可完全 NVIDIA Drives 到所有 GPU 節點上 kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/nvidia-driver-installer/cos/DaemonSet-preloaded.yaml 但是我在另外一個 kubernetes 的文章中 Kubernetes scheduling-gpus若要再 GKE 上面安裝 GPU 資源的話，則需要根據情況部署下列 yaml 檔案。 On your 1.9 cluster, you can use the following commands to install the NVIDIA drivers and device plugin: # Install NVIDIA drivers on Container-Optimized OS: kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/DaemonSet.yaml # Install NVIDIA drivers on Ubuntu (experimental): kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/nvidia-driver-installer/ubuntu/DaemonSet.yaml # Install the device plugin: kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.9/cluster/addons/device-plugins/nvidia-gpu/DaemonSet.yaml 首先，第一篇文章表明只需要透過 DaemonSet 的方式去安裝 NVIDIA Drivers，而第二篇文章則需要部署兩個 DaemonSet，分別來處理 NVIDIA Drivers 以及 Device Plugin. 此外，針對第一個 NVIDIA Drivers 也提供不同作業系統的檔案(cos/ubuntu)，且就我的認知上 Device Plugin 本來就需要跑一個 DaemonSet 來監聽 Kubernetes 的事件來處理各式各樣的 Device 操作。 在百思不得其解的情況下，乾脆直接找到相關的 Git 專案來看看，有沒有更清楚的解釋，畢竟我們都知道文件更新的速度比不上程式碼修改的速度。 最後找到了這個 Git 專案 GoogleCloudPlatform/container-engine-accelerators 這個專案的文件直接解決了我關於 Device Plugin 的疑惑 In GKE, from 1.9 onwards, this DaemonSet is automatically deployed as an addon. Note that DaemonSet pods are only scheduled on nodes with accelerators attached, they are not scheduled on nodes that don't have any accelerators attached. 根據這份文件敘述，在 GKE (1.9版本之後)，會自動部署 Daemonset 到集群上面的 GPU Node 來處理 Device Plugin 相關的事宜。 由於我使用的 GKE 集群版本是 1.9 之後，所以只需要自己手動部署 DaemonSet 來安裝 NVIDIA Drivers 到所有的 GPU 節點上。 結合了上述兩者個文件，可以根據版本以及GPU 節點上面的作業系統選擇需要使用的 Yaml. 在 Github 的專案內，使用了 Brnach 的方式來切換不同的 kubernetes 版本，所以這邊就根據各自的需求來選擇。 想要獲得如官方範例的 raw.githubusercontent.com 的連結只要到該 Github 的檔案中點選右上的 Raw 按鈕則會獲得一個關於該檔案的存取連結。 譬如 kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/DaemonSet.yaml 由於該 yaml 使用的 namespace 是在 kube-system, 所以若要觀察其結果別忘記切換對應的 namespace 來觀察。 這邊只要該 Pod的狀態變成 Running 則代表該 NVIDIA Driver 已經安裝完畢了，就可以開始部署自己的 Pod 來使用這些 GPU 資源了。 題外話: 為什麼安裝一個 NVIDIA Driver 完成會是 Running 而不是其他的 Completed 等看起來結束的資訊? 這邊主要受限於目前 Kubernetes 資源的使用方法與 NVIDIA Driver 的安裝使用情境。 首先，我們希望 NVIDIA Driver 能夠安裝到每一台擁有 GPU 的節點上，所以這邊我們會希望使用 DaemonSet 來幫你部署對應的 Pod 到 GPU 節點。 然而 NVIDIA Driver 的安裝其實本身就是一個會結束的應用程式，如果正常使用上，當相關資源都安裝完畢，該 Pod 則會結束 並且將狀態切換成 Succeed. 此時 DaemonSet 則會為了確保其選擇的 Pod 是一個類似 Daemon 的程式，必須一直存活，則又會重新將該 Pod 叫起來繼續重新安裝。 如此則會發生一個反覆執行的無窮迴圈，為了解決這個問題，不少人在提出有沒有類似 DaemonJob 的資源可以使用? 希望可以在每個符合的節點上都運行一個Job就好，不需要一直反覆執行， 可以在下列的連結看到相關的討論串. 1.CronJob DaemonSet (previously ScheduledJob)2.[DaemonSet] Considering run-once DaemonSet (Job DaemonSet)3.Run Once DaemonSet on Kubernetes 在這個問題被正式解決前，有不少 Workarouund 可以處理這個情境，最常用的採用 Init-Container 的方式來完成。 藉由 Init-Container 來執行主要的安裝流程，然後再 Contianer 裡面使用則是只用了一個低資源沒做事情單純無窮迴圈卡住的應用程序。 當 Pod 開始運行的時候，先執行 Init-Contaienr 來進行真正的任務來安裝相關資源，接下來切換到 Contaienr 進行一個發呆的應用程式 確保 Pod 的狀態會維持在 Running 而不會被 Daemonset 給重啟。 這也是為什麼我們只要看到 Running 就可以視為 NVIDIA Driver 已經安裝完畢了。 另外，其實 Flannel (CNI) 也是使用這種方式來安裝相關的 CNI 資源(config/bianry) 到每台機器上。 回到正題，按照第一篇文章提到的說明文件，我們簡單部署一個 Pod 的應用程式來使用 GPU 資源 apiVersion: v1 kind: Pod spec: containers: - name: my-gpu-container resources: limits: nvidia.com/gpu: 1 看似簡單合理結果結果最後運行起來遇到問題，Pod 內的 GPU 應用程式在運行時會運行錯誤，錯誤訊息類似 &quot;libcuda.so.1 file&quot; is not found. 這看起來就是找不到 CUDA 相關的函式庫所以沒有辦法正常的啟用 GPU 應用程式。 這時候再重新檢視了一下該份 Google 文件，有個段落在講述 CUDA 的函式庫 CUDA® is NVIDIA's parallel computing platform and programming model for GPUs. The NVIDIA device drivers you install in your cluster include the CUDA libraries. CUDA libraries and debug utilities are made available inside the container at /usr/local/nvidia/lib64 and /usr/local/nvidia/bin, respectively. CUDA applications running in Pods consuming NVIDIA GPUs need to dynamically discover CUDA libraries. This requires including /usr/local/nvidia/lib64 in the LD_LIBRARY_PATH environment variable. 首先，你安裝 NVIDIA Driver 的同時， 也會一起安裝 CUDA 的函式庫 再來 CUDA 相關的函式庫與使用工具都可以在 Container 內的 /usr/local/nvidia/lib64 以及 /usr/local/nvidia/bin 找到與使用。 最後你必須要設定你應用程式的 LD_LIBRARY_PATH 來確保你的應用程式在尋找連結庫的時候不要忘記去找 /usr/local/nvidia/lib64. 有了上述這三個敘述，我重新檢視了一次自己運行的 GPU 應用程式。 首先, LD_LIBRARY_PATH 有正確的設定，但是我在系統上卻看不到任何 /usr/local/nvidia/ 任何檔案，連整個資料夾都沒有，也完全找不到任何 libcuda.so.1 的檔案。 所以我開始懷疑 NVIDIA Driver 到底有沒有正確的安裝成功? 我們重新看一次該安裝的 yaml 檔案，如下 apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-driver-installer namespace: kube-system labels: k8s-app: nvidia-driver-installer spec: selector: matchLabels: k8s-app: nvidia-driver-installer updateStrategy: type: RollingUpdate template: metadata: labels: name: nvidia-driver-installer k8s-app: nvidia-driver-installer spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cloud.google.com/gke-accelerator operator: Exists tolerations: - key: &quot;nvidia.com/gpu&quot; effect: &quot;NoSchedule&quot; operator: &quot;Exists&quot; volumes: - name: dev hostPath: path: /dev - name: nvidia-install-dir-host hostPath: path: - name: root-mount hostPath: path: / initContainers: - image: gcr.io/google-containers/ubuntu-nvidia-driver-installer@sha256:eea7309dc4fa4a5c9d716157e74b90826e0a853aa26c7219db4710ddcd1ad8bc name: nvidia-driver-installer resources: requests: cpu: 0.15 securityContext: privileged: true env: - name: NVIDIA_INSTALL_DIR_HOST value: /home/kubernetes/bin/nvidia - name: NVIDIA_INSTALL_DIR_CONTAINER value: /usr/local/nvidia - name: ROOT_MOUNT_DIR value: /root volumeMounts: - name: nvidia-install-dir-host mountPath: /usr/local/nvidia - name: dev mountPath: /dev - name: root-mount mountPath: /root containers: - image: &quot;gcr.io/google-containers/pause:2.0&quot; name: pause 我們可以觀察到下列幾個重點 Init-Container 的名稱是 nvidia-driver-installer該 DaemonSet 將節點上的 /home/kubernetes/bin/nvidia 透過 HostPath 方式掛載到 Init-Container 內的 /usr/local/nvidia. 接下來我們使用下列指令去觀察該 Init-Container nvidia-driver-installer 的安裝訊息。 kubectl logs -n kube-system -l name=nvidia-driver-installer -c nvidia-driver-installer 其輸出的資訊如下，而且 nvidia-smi 的資訊的確也有抓到 Tesla K80 這張卡，看起來一切都正常。 + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz + TOOLCHAIN_URL_FILENAME=toolchain_url + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk + ROOT_OS_RELEASE=/root/etc/os-release + KERNEL_SRC_DIR=/build/usr/src/linux + NVIDIA_DRIVER_VERSION=384.111 + NVIDIA_DRIVER_DOWNLOAD_URL_DEFAULT=https://us.download.nvidia.com/tesla/384.111/NVIDIA-Linux-x86_64-384.111.run + NVIDIA_DRIVER_DOWNLOAD_URL=https://us.download.nvidia.com/tesla/384.111/NVIDIA-Linux-x86_64-384.111.run + NVIDIA_INSTALL_DIR_HOST=/home/kubernetes/bin/nvidia + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia ++ basename https://us.download.nvidia.com/tesla/384.111/NVIDIA-Linux-x86_64-384.111.run + NVIDIA_INSTALLER_RUNFILE=NVIDIA-Linux-x86_64-384.111.run + ROOT_MOUNT_DIR=/root + CACHE_FILE=/usr/local/nvidia/.cache + set +x [INFO 2018-07-14 14:57:09 UTC] Running on COS build id 10323.85.0 [INFO 2018-07-14 14:57:09 UTC] Checking if third party kernel modules can be installed [INFO 2018-07-14 14:57:09 UTC] Checking cached version [INFO 2018-07-14 14:57:09 UTC] Found existing driver installation for image version 10323.85.0 and driver version 384.111. [INFO 2018-07-14 14:57:09 UTC] Configuring cached driver installation [INFO 2018-07-14 14:57:09 UTC] Updating container's ld cache [INFO 2018-07-14 14:57:14 UTC] Verifying Nvidia installation Sat Jul 14 14:57:15 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 384.111 Driver Version: 384.111 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 31C P0 62W / 149W | 0MiB / 11439MiB | 100% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ [INFO 2018-07-14 14:57:16 UTC] Found cached version, NOT building the drivers. [INFO 2018-07-14 14:57:16 UTC] Updating host's ld cache 不過至少我們知道了，該 nvidia-driver-installer 會將相關的資源都安裝到 /usr/local/nvidia 而該路徑則會對應到該 GPU 節點上面的 /home/kubernetes/bin/nvidia 資料夾。 這樣想了一下，我覺得我們的應用程式在使用的時候，應該要手動的將相關資源掛載進來，所以整個 yaml 檔案修改如下 apiVersion: v1 kind: Pod spec: containers: - name: my-gpu-container resources: limits: nvidia.com/gpu: 1 volumeMounts: - mountPath: /usr/local/bin/nvidia name: nvidia-debug-tools - mountPath: /usr/local/nvidia/lib64 name: nvidia-libraries volumes: - hostPath: path: /home/kubernetes/bin/nvidia/bin type: &quot;&quot; name: nvidia-debug-tools - hostPath: path: /home/kubernetes/bin/nvidia/lib64 type: &quot;&quot; name: nvidia-libraries 我們手動將 GPU 節點上面的檔案掛載到容器內，其對應的路徑如下 /home/kubernetes/bin/nvidia/bin -&gt; /usr/local/nvidia/lib64/home/kubernetes/bin/nvidia/lib64 -&gt; /usr/local/bin/nvidia 接下來到我們的應用程式內就可以正確地找到了相關的檔案，如 libcuda.so.1 以及相關的測試工具。 這時候執行一下 nvidia-smi 卻發現不能執行，會直接得到下列的錯誤 &quot;Failed to initialize NVML: Unknown Error&quot; 嘗試 Google 這類型的錯誤都沒有辦法找到答案來處理這個問題，最後決定還是自己來研究一下哪裡出錯了 這邊使用的工具是 strace，能夠顯示出該應用程式運行過程中呼叫到的所有 system call，是個非常強大好用的除錯工具。 跑了一下 strace 馬上就發現事情的不對勁，由該輸出結果可以看到類似下列的錯誤訊息(原諒我沒有完整 log) read(/dev/nvidiactl) no permission 這邊可以看到 nvidia-smi想要嘗試讀取 /dev/nvidiactl 結果卻沒有權限，這怎像都覺得一定是容器權限不夠，所以補上 Privileged 試試看。 就發現一切都順利了, nvidia-smi 也不會噴錯，而應用程式也能夠順利運行了。 附上最後的 yaml 檔案如下 apiVersion: v1 kind: Pod spec: containers: - name: my-gpu-container resources: limits: nvidia.com/gpu: 1 volumeMounts: - mountPath: /usr/local/bin/nvidia name: nvidia-debug-tools - mountPath: /usr/local/nvidia/lib64 name: nvidia-libraries securityContext: privileged: true volumes: - hostPath: path: /home/kubernetes/bin/nvidia/bin type: &quot;&quot; name: nvidia-debug-tools - hostPath: path: /home/kubernetes/bin/nvidia/lib64 type: &quot;&quot; name: nvidia-libraries Summary 總結一下上述的所有歷程 如果今天 GKE 的版本是 1.9 之後，我們只需要運行一個 DaemonSet 去安裝 NVIDIA Driver 即可， Device Plugin 會自己被運行CUDA 相關的資源都需要自己從 GPU 節點上掛載到所有運行的 Pod 中記得設定 privileged=true 到所有使用 GPU 的節點上。","keywords":"","version":"Next"},{"title":"Architecture","type":0,"sectionRef":"#","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow","content":"","keywords":"","version":"Next"},{"title":"post_parse​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#post_parse","content":"首先，當 drbdadm 這隻程式起來後，內部會先執行 post_parse 對設定檔進行一番解析，並且將解析到的資料給存到一個 d_resource 的物件中 1063 void post_parse(struct resources *resources, enum pp_flags flags) 1064 { 1065 struct d_resource *res; 1066 struct connection *con; .................. 1102 for_each_resource(res, resources) { 1103 struct d_host_info *host; 1104 struct mesh *mesh; 1105 1106 if (!(flags &amp; DRBDSETUP_SHOW)) { 1107 for_each_connection(con, &amp;res-&gt;connections) 1108 must_have_two_hosts(res, con); 1109 } 1110 1111 /* Other steps make no sense. */ 1112 if (!config_valid) 1113 continue; 1114 1115 STAILQ_FOREACH(mesh, &amp;res-&gt;meshes, link) 1116 create_connections_from_mesh(res, mesh); 1117 create_implicit_connections(res); 1118 for_each_connection(con, &amp;res-&gt;connections) 1119 set_host_info_in_host_address_pairs(res, con); 1120 for_each_host(host, &amp;res-&gt;all_hosts) { 1121 if (!host-&gt;node_id) 1122 derror(host, res, &quot;node-id&quot;); 1123 } 1124 }  值得注意是的後半部分網路部分的處理，目前 drbd.conf 支援的網路設定大致上有 Host 直接設定Connection 指名哪兩台 host 要互連Mesh 直接用參數內的 Host 創建一個 mesh 網路 所以從 1102 行開始，就針對每個 resource 的網路狀態去處理，最下面的部分主要分成四個部分去看 如果有設定 Mesh 網路，則透過 create_connections_from_mesh去創建所有的 connection接下來會根據設定檔去創建一個隱性的連線，我們的設定檔主要是依賴此 function 去運作的，因為我們沒有特別設定 connection 以及 mesh，所以會透過此 create_implicit_connection 去創建一條 connection 來使用接下來從所有的 connection 物件中(可能是手動設定，也有可能是上述創建的)，去設定相關的連線地址最後檢查是否所有的 host 都已經有 node_id 這個欄位，由於我們的設定檔也沒有寫 node_id，這個數值會在上述的 set_host_info_in_host_address_pairs 中去創立。 ","version":"Next","tagName":"h3"},{"title":"create_implicit_connection​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#create_implicit_connection","content":"接下來看一下create_implicit_connection怎麼處理的 0729 static void create_implicit_connections(struct d_resource *res) 0730 { 0731 struct connection *conn; 0732 struct path *path; 0733 struct hname_address *ha; 0734 struct d_host_info *host_info; 0735 int hosts = 0; 0736 0737 if (!STAILQ_EMPTY(&amp;res-&gt;connections)) 0738 return;  從這邊可以觀察到，如果你的drbd.conf中有使用到 connection 欄位的話，那這邊就直接返回，不需要幫忙產生任何 connection 使用 0740 conn = alloc_connection(); 0741 conn-&gt;implicit = 1; 0742 path = alloc_path(); 0743 path-&gt;implicit = 1; 0744 insert_tail(&amp;conn-&gt;paths, path);  初始化相關成員，主要是 struct connection 以及 struct path，之後會再分析這些結構彼此的關係，這邊只要先知道每個 connection底下都會有一串 path 即可。 0746 for_each_host(host_info, &amp;res-&gt;all_hosts) { 0747 if (++hosts == 3) { 0748 err(&quot;Resource %s:\\n\\t&quot; 0749 &quot;Use explicit 'connection' sections with more than two 'on' sections.\\n&quot;, 0750 res-&gt;name); 0751 break; 0752 } 0753 if (host_info-&gt;address.af &amp;&amp; host_info-&gt;address.addr &amp;&amp; host_info-&gt;address.port) { 0754 ha = alloc_hname_address(); 0755 ha-&gt;host_info = host_info; 0756 ha-&gt;proxy = host_info-&gt;proxy_compat_only; 0757 if (!host_info-&gt;lower) { 0758 ha-&gt;name = STAILQ_FIRST(&amp;host_info-&gt;on_hosts)-&gt;name; 0759 } else { 0760 ha-&gt;name = strdup(names_to_str_c(&amp;host_info-&gt;on_hosts, '_')); 0761 ha-&gt;address = host_info-&gt;address; 0762 ha-&gt;faked_hostname = 1; 0763 ha-&gt;parsed_address = 1; /* not true, but makes dump nicer */ 0764 } 0765 STAILQ_INSERT_TAIL(&amp;path-&gt;hname_address_pairs, ha, link); 0766 } 0767 } 0768 0769 if (hosts == 2) 0770 STAILQ_INSERT_TAIL(&amp;res-&gt;connections, conn, link); 0771 else 0772 free_connection(conn);  這邊要開始針對 host創立對應的 connection 結構，這邊要注意的是，若 drbd.conf中該 host 數量是兩台以上的時候，這時候 connection 就沒有那麼簡單的去決定要使用那些 host，所以在這邊就會跳過這個情形。 針對每個 host 將其資訊都收集起來放在 struct hname_address *ha 內，最後再放到 path裡面 所以到這個階段，一條 connection 底下有一個 path，而 path 底下有一個 list，裡面放了兩個 ha。 最後呼叫 MARCO STAILQ_INSERT_TAIL 將當前創好的 connection 給放到整個 resource 物件之中，這邊也是透過 linklist 的方式給綁進去。 ","version":"Next","tagName":"h3"},{"title":"set_host_info_in_host_address_pairs​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#set_host_info_in_host_address_pairs","content":"在 post_parse 中，當我們都準備好 connection 後，接下來會透過 set_host_info_in_host_address_pairs 要處理一些 host 相關的資訊，如 node_id。 可以從程式碼內看到，會掃過所有的 path，然後對所有的 path 再進行一次 _set_host_info_in_host_address_pairs 的呼叫，在本文的範例中，因為 PATH 只有一條，所以只會被呼叫一次。 0255 static void set_host_info_in_host_address_pairs(struct d_resource *res, struct connection *conn) 0256 { 0257 struct path *path; 0258 0259 for_each_path(path, &amp;conn-&gt;paths) 0260 _set_host_info_in_host_address_pairs(res, conn, path); 0261 }  ","version":"Next","tagName":"h3"},{"title":"_set_host_info_in_host_address_pairs​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#_set_host_info_in_host_address_pairs","content":"這邊的程式碼比較長，主要針對跟本文範例相關的邏輯為主 首先先掃過該 path 底下的hname_address，在之前的過程中，我們塞了兩個 struct hname_address進去，所以理論上這個 for 迴圈只會跑兩次而已。 由於先前創立 hname_address 的時候，也順便將其底下的 host_info 也準備好了，所以可以看到第一個 if 判斷旁邊也有相對應的註解。 這邊最主要的是使用 crc32 計算 address 的 hash使用，供後續產生 node_id 使用，同時把這些 host_info 都存起來 0141 static void _set_host_info_in_host_address_pairs(struct d_resource *res, 0142 struct connection *conn, 0143 struct path *path) 0144 { 0145 struct hname_address *ha; 0146 struct d_host_info *host_info; 0147 int addr_hash[2], i = 0; 0148 struct d_host_info *host_info_array[2]; 0149 0150 STAILQ_FOREACH(ha, &amp;path-&gt;hname_address_pairs, link) { 0151 if (ha-&gt;host_info) { /* Implicit connection have that already set. */ 0152 host_info = ha-&gt;host_info; 0153 if (i == 2) { 0154 err(&quot;LOGIC BUG in set_host_info_in_host_address_pairs()\\n&quot;); 0155 exit(20); 0156 } 0157 if (!host_info-&gt;address.addr) { 0158 err(&quot;\\&quot;connection-mesh\\&quot; (for \\&quot;%s\\&quot;) with a host (\\&quot;%s\\&quot;) &quot; 0159 &quot;that has no \\&quot;address\\&quot; defined\\n&quot;, 0160 res-&gt;name, ha-&gt;name); 0161 config_valid = 0; 0162 return; 0163 } 0164 addr_hash[i] = crc32c(0x1a656f21, 0165 (void *)host_info-&gt;address.addr, 0166 strlen(host_info-&gt;address.addr)); 0167 host_info_array[i++] = host_info; ...........  最後，若這條 connection 是透過 create_implicit_connection 產生的，則要對 connection 兩端的 host 去產生一個 node_id 來存放，這邊使用了 generate_implicit_node_id 來產生 node id，若剛好兩個 hash 都一樣的話，就會發生失敗，註解中有提到失敗的原因有可能兩個 host 採用了 proxy 的架構，所以 ip address 都會相同。這種情況下就重新透過crc32c搭配proxy的變數來重新計算一次node id。 0224 if (conn-&gt;implicit &amp;&amp; i == 2 &amp;&amp; !host_info_array[0]-&gt;node_id &amp;&amp; !host_info_array[1]-&gt;node_id) { 0225 /* This is drbd-8.3 / drbd-8.4 compatibility, auto created node-id */ 0226 bool have_node_ids; 0227 0228 have_node_ids = generate_implicit_node_id(addr_hash, host_info_array); 0229 0230 if (!have_node_ids) { 0231 /* That might be a config with equal node addresses, since it is 0232 127.0.0.1:xxx with a proxy... */ 0233 i = 0; 0234 path = STAILQ_FIRST(&amp;conn-&gt;paths); /* there may only be one */ 0235 STAILQ_FOREACH(ha, &amp;path-&gt;hname_address_pairs, link) { 0236 if (!ha-&gt;host_info) 0237 continue; 0238 0239 if (!ha-&gt;proxy) 0240 break; 0241 0242 addr_hash[i++] = crc32c(0x1a656f21, 0243 (void *)ha-&gt;proxy-&gt;outside.addr, 0244 strlen(ha-&gt;proxy-&gt;outside.addr)); 0245 } 0246 have_node_ids = generate_implicit_node_id(addr_hash, host_info_array); 0247 } 0248 if (!have_node_ids) { 0249 err(&quot;BAD LUCK, equal hashes\\n&quot;); 0250 exit(20); 0251 } 0252 }  當整個設定檔都解析完畢後，接下來就要處理真正的參數up r0了，根據下列程式碼 0326 /* */ struct adm_cmd disconnect_cmd = {&quot;disconnect&quot;, adm_drbdsetup, &amp;disconnect_cmd_ctx, ACF1_DISCONNECT}; 0327 static struct adm_cmd up_cmd = {&quot;up&quot;, adm_up, ACF1_RESNAME }; 0328 /* */ struct adm_cmd res_options_cmd = {&quot;resource-options&quot;, adm_resource, &amp;resource_options_ctx, ACF1_RESNAME};  可以清楚的看到，當第二個參數是 up 時，實際上會呼叫 adm_up 來進行後續的處理。 接下來看 adm_up 的介紹 ","version":"Next","tagName":"h3"},{"title":"adm_up​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#adm_up","content":"1974 /* The &quot;main&quot; loop iterates over resources. 1975 * This &quot;sorts&quot; the drbdsetup commands to bring those up 1976 * so we will later first create all objects, 1977 * then attach all local disks, 1978 * adjust various settings, 1979 * and then configure the network part */ 1980 static int adm_up(const struct cfg_ctx *ctx) 1981 { ......... 2021 return 0; 2022 }  可以觀察到，這隻 function 負責超多事情，基本上就是幫你把 object/disk/network 都處理完畢。這邊我們專注於 Network 相關的處理。 首先先呼叫 set_peer_in_resource 進行處理 1988 set_peer_in_resource(ctx-&gt;res, true);  ","version":"Next","tagName":"h3"},{"title":"set_peer_in_resource​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#set_peer_in_resource","content":"這邊會先掃過所有的 connection，然後對於每條connection，透過 set_peer_in_connection 去設定每條 connection 的 peer，同時也設定 connection 底下 path 的 peer address。 以本文的範例來說，該 resource r0 裡面包含兩台 host，分別是 hw1 以及 hw2。 一開始兩台 host 都必須要執行 drbdadm 來初始相關的功能，假設今天是 hw1 這台在執行。則對 hw1 來說，他看到 connection 的 peer 就要指向 hw2，反之亦然， hw2 所看到的 connection-&gt;peer 應該要指向 h1 才對。 0473 void set_peer_in_resource(struct d_resource* res, int peer_required) 0474 { 0475 struct connection *conn; 0476 int peers_addrs_set = 1; 0477 0478 for_each_connection(conn, &amp;res-&gt;connections) { 0479 struct path *path; 0480 set_peer_in_connection(res, conn, peer_required); 0481 0482 for_each_path(path, &amp;conn-&gt;paths) { 0483 if (!path-&gt;peer_address) 0484 peers_addrs_set = 0; 0485 } 0486 create_implicit_net_options(conn); 0487 } 0488 res-&gt;peers_addrs_set = peers_addrs_set; 0489 0490 if (!(peer_required &amp; DRBDSETUP_SHOW)) 0491 add_no_bitmap_opt(res); 0492 }  在設定完畢 peer 後，透過 create_implicit_net_options 去設定 network options 中的 _name 這個欄位而已。 最後用一個變數peer_addr_set來記住當前 resource 是否已經有設定過 peer 的 address了，因為有些 command 本身不需要 peer 的參與，所以會使用這個變數來作為一些邏輯的判斷。 最後來到了整個 adm_up 函式的重頭戲, 在一切資訊都準備完畢後，接下來要開始在兩端 host h1, h2 建立起連線，這邊透過 schedule_deferred_cmd 的方式去執行三個指令，分別是 new-peer, new-path 以及 connect，稍後這些指令都會透過 netlink 的方式送到 kernel space 去進行真正的連線操作。 1989 for_each_connection(conn, &amp;ctx-&gt;res-&gt;connections) { 1990 struct peer_device *peer_device; 1991 1992 if (conn-&gt;ignore) 1993 continue; 1994 1995 tmp_ctx.conn = conn; 1996 1997 schedule_deferred_cmd(&amp;new_peer_cmd, &amp;tmp_ctx, CFG_NET_PREP_UP); 1998 schedule_deferred_cmd(&amp;new_path_cmd, &amp;tmp_ctx, CFG_NET_PATH); 1999 schedule_deferred_cmd(&amp;connect_cmd, &amp;tmp_ctx, CFG_NET_CONNECT); 2000 2001 STAILQ_FOREACH(peer_device, &amp;conn-&gt;peer_devices, connection_link) { 2002 struct cfg_ctx tmp2_ctx; 2003 2004 if (STAILQ_EMPTY(&amp;peer_device-&gt;pd_options)) 2005 continue; 2006 2007 tmp2_ctx = tmp_ctx; 2008 tmp2_ctx.vol = peer_device-&gt;volume; 2009 schedule_deferred_cmd(&amp;peer_device_options_cmd, &amp;tmp2_ctx, CFG_PEER_DEVICE); 2010 } 2011 }  接下來看看 new-peer, new-path 以及 connection 實際上又做了些什麼事情。 在實際看這些指令做的事情以前，先來看看 schedule_deferred_cmd 怎麼處理這些指令的。 ","version":"Next","tagName":"h3"},{"title":"schedule_deferred_cmd​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#schedule_deferred_cmd","content":"此 function 主要是將相關的參數都收集起來放到 struct cfg_ctx 裡面，然後將這個要執行的指令透過 STAILQ_INSERT_TAIL 都方式放到一個全域的 Queue deferred_cmds 內。 0547 void schedule_deferred_cmd(struct adm_cmd *cmd, 0548 const struct cfg_ctx *ctx, 0549 enum drbd_cfg_stage stage) 0550 { 0551 struct deferred_cmd *d; 0552 0553 if (stage &amp; SCHEDULE_ONCE) { 0554 stage &amp;= ~SCHEDULE_ONCE; 0555 0556 STAILQ_FOREACH(d, &amp;deferred_cmds[stage], link) { 0557 if (d-&gt;ctx.cmd == cmd &amp;&amp; 0558 d-&gt;ctx.res == ctx-&gt;res &amp;&amp; 0559 d-&gt;ctx.conn == ctx-&gt;conn &amp;&amp; 0560 d-&gt;ctx.vol == ctx-&gt;vol) 0561 return; 0562 } 0563 } 0564 0565 d = calloc(1, sizeof(struct deferred_cmd)); 0566 if (d == NULL) { 0567 perror(&quot;calloc&quot;); 0568 exit(E_EXEC_ERROR); 0569 } 0570 0571 d-&gt;ctx = *ctx; 0572 d-&gt;ctx.cmd = cmd; 0573 0574 STAILQ_INSERT_TAIL(&amp;deferred_cmds[stage], d, link); 0575 }  整個程式的最後面則是會依賴 _run_deferred_cmds 將 queue 內的指令一個一個取出，然後透過 __call_cmd_fn 開始執行 ","version":"Next","tagName":"h3"},{"title":"_run_deferred_cmds​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#_run_deferred_cmds","content":"0698 int _run_deferred_cmds(enum drbd_cfg_stage stage) 0699 { 0700 struct d_resource *last_res = NULL; 0701 struct deferred_cmd *d = STAILQ_FIRST(&amp;deferred_cmds[stage]); 0702 struct deferred_cmd *t; 0703 int r; 0704 int rv = 0; 0705 0706 if (d &amp;&amp; adjust_with_progress) { 0707 printf(&quot;\\n%15s:&quot;, drbd_cfg_stage_string[stage]); 0708 fflush(stdout); 0709 } 0710 0711 while (d) { 0712 if (d-&gt;ctx.res-&gt;skip_further_deferred_command) { 0713 if (adjust_with_progress) { 0714 if (d-&gt;ctx.res != last_res) 0715 printf(&quot; [skipped:%s]&quot;, d-&gt;ctx.res-&gt;name); 0716 } else 0717 err(&quot;%s: %s %s: skipped due to earlier error\\n&quot;, 0718 progname, d-&gt;ctx.cmd-&gt;name, d-&gt;ctx.res-&gt;name); 0719 r = 0; 0720 } else { 0721 if (adjust_with_progress) { 0722 if (d-&gt;ctx.res != last_res) 0723 printf(&quot; %s&quot;, d-&gt;ctx.res-&gt;name); 0724 } 0725 r = __call_cmd_fn(&amp;d-&gt;ctx, KEEP_RUNNING); 0726 if (r) { ...  這邊可以注意的是 iterate_path 這個變數，如果這個變數為真的則，則該指令會針對 connection內所有的 paths 都進行一次， connection 則是在當初在 adm_up 時就會先透過 tmp_ctx.conn = conn 放進去。不過由於本文的設定檔只有一條 connection，且該 connection 上只有一個 path，所以這邊實際上也只會呼叫一次。 ","version":"Next","tagName":"h3"},{"title":"__call_cmd_fn​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#__call_cmd_fn","content":"0578 static int __call_cmd_fn(const struct cfg_ctx *ctx, enum on_error on_error) 0579 { 0580 struct d_volume *vol = ctx-&gt;vol; 0581 bool iterate_paths; 0582 int rv = 0; 0583 0584 iterate_paths = ctx-&gt;path ? 0 : ctx-&gt;cmd-&gt;iterate_paths; 0585 0586 if (ctx-&gt;cmd-&gt;disk_required &amp;&amp; 0587 (!vol-&gt;disk || !vol-&gt;meta_disk || !vol-&gt;meta_index)) { 0588 rv = 10; 0589 err(&quot;The %s command requires a local disk, but the configuration gives none.\\n&quot;, 0590 ctx-&gt;cmd-&gt;name); 0591 if (on_error == EXIT_ON_FAIL) 0592 exit(rv); 0593 return rv; 0594 } 0595 0596 if (iterate_paths) { 0597 struct cfg_ctx tmp_ctx = *ctx; 0598 struct path *path; 0599 0600 for_each_path(path, &amp;tmp_ctx.conn-&gt;paths) { 0601 tmp_ctx.path = path; 0602 rv = tmp_ctx.cmd-&gt;function(&amp;tmp_ctx); 0603 if (rv &gt;= 20) { 0604 if (on_error == EXIT_ON_FAIL) 0605 exit(rv); 0606 } 0607 0608 } 0609 } else { 0610 rv = ctx-&gt;cmd-&gt;function(ctx); 0611 if (rv &gt;= 20) { 0612 if (on_error == EXIT_ON_FAIL) 0613 exit(rv); 0614 } 0615 } 0616 return rv; 0617 }  最後要來看這些指令怎麼往下運行的，不論是 new-peer, new-path 或是 connect，其實最後都是依靠 drbdsetup 這隻程式在來運行，所以這邊基本上都是收集好參數後透過 system 的方式將該指令叫起來去執行。 1705 static int adm_connect(const struct cfg_ctx *ctx) 1706 { 1707 struct d_resource *res = ctx-&gt;res; 1708 struct connection *conn = ctx-&gt;conn; 1709 char *argv[MAX_ARGS]; 1710 int argc = 0; 1711 1712 argv[NA(argc)] = drbdsetup; 1713 argv[NA(argc)] = (char *)ctx-&gt;cmd-&gt;name; /* &quot;connect&quot; */ 1714 argv[NA(argc)] = ssprintf(&quot;%s&quot;, res-&gt;name); 1715 argv[NA(argc)] = ssprintf(&quot;%s&quot;, conn-&gt;peer-&gt;node_id); 1716 1717 add_setup_options(argv, &amp;argc, ctx-&gt;cmd-&gt;drbdsetup_ctx); 1718 argv[NA(argc)] = 0; 1719 1720 return m_system_ex(argv, SLEEPS_SHORT, res-&gt;name); 1721 } 1722 1723 static int adm_new_peer(const struct cfg_ctx *ctx) 1724 { 1725 struct d_resource *res = ctx-&gt;res; 1726 struct connection *conn = ctx-&gt;conn; 1727 1728 char *argv[MAX_ARGS]; 1729 int argc = 0; 1730 1731 bool reset = (ctx-&gt;cmd == &amp;net_options_defaults_cmd); 1732 1733 argv[NA(argc)] = drbdsetup; 1734 argv[NA(argc)] = (char *)ctx-&gt;cmd-&gt;name; /* &quot;new-peer&quot;, &quot;net-options&quot; */ 1735 argv[NA(argc)] = ssprintf(&quot;%s&quot;, res-&gt;name); 1736 argv[NA(argc)] = ssprintf(&quot;%s&quot;, conn-&gt;peer-&gt;node_id); 1737 1738 if (reset) 1739 argv[NA(argc)] = &quot;--set-defaults&quot;; 1740 1741 if (!strncmp(ctx-&gt;cmd-&gt;name, &quot;net-options&quot;, 11)) 1742 del_opt(&amp;conn-&gt;net_options, &quot;transport&quot;); 1743 1744 make_options(argv[NA(argc)], &amp;conn-&gt;net_options); 1745 1746 add_setup_options(argv, &amp;argc, ctx-&gt;cmd-&gt;drbdsetup_ctx); 1747 argv[NA(argc)] = 0; 1748 1749 return m_system_ex(argv, SLEEPS_SHORT, res-&gt;name); 1750 } 1751 1752 static int adm_path(const struct cfg_ctx *ctx) 1753 { 1754 struct d_resource *res = ctx-&gt;res; 1755 struct connection *conn = ctx-&gt;conn; 1756 struct path *path = ctx-&gt;path; 1757 1758 char *argv[MAX_ARGS]; 1759 int argc = 0; 1760 1761 argv[NA(argc)] = drbdsetup; 1762 argv[NA(argc)] = (char *)ctx-&gt;cmd-&gt;name; /* add-path, del-path */ 1763 argv[NA(argc)] = ssprintf(&quot;%s&quot;, res-&gt;name); 1764 argv[NA(argc)] = ssprintf(&quot;%s&quot;, conn-&gt;peer-&gt;node_id); 1765 1766 argv[NA(argc)] = ssprintf_addr(path-&gt;my_address); 1767 argv[NA(argc)] = ssprintf_addr(path-&gt;connect_to); 1768 1769 add_setup_options(argv, &amp;argc, ctx-&gt;cmd-&gt;drbdsetup_ctx); 1770 argv[NA(argc)] = 0; 1771 1772 return m_system_ex(argv, SLEEPS_SHORT, res-&gt;name); 1773 }  在 drbdsetup 中，可以看到關於這三個指令對應的資訊，這些指令的原型是 struct drbd_cmd，當 drbdsetup 被呼叫後，對應的指令就會跑到對應的 drbd_cmd中去執行，最後都會執行到 drbd_cmd 裡面的 function (fptr)來處理。 0229 struct drbd_cmd { 0230 const char* cmd; 0231 enum cfg_ctx_key ctx_key; 0232 int cmd_id; 0233 int tla_id; /* top level attribute id */ 0234 int (*function)(struct drbd_cmd *, int, char **); 0235 struct drbd_argument *drbd_args; 0236 int (*show_function)(struct drbd_cmd*, struct genl_info *, void *u_ptr); 0237 struct option *options; 0238 bool missing_ok; 0239 bool warn_on_missing; 0240 bool continuous_poll; 0241 bool set_defaults; 0242 bool lockless; 0243 struct context_def *ctx; 0244 const char *summary; 0245 };  0397 {&quot;connect&quot;, CTX_PEER_NODE, 0398 DRBD_ADM_CONNECT, DRBD_NLA_CONNECT_PARMS, 0399 F_CONFIG_CMD, 0400 .ctx = &amp;connect_cmd_ctx, 0401 .summary = &quot;Attempt to (re)establish a replication link to a peer host.&quot; },  0403 {&quot;new-peer&quot;, CTX_PEER_NODE, 0404 DRBD_ADM_NEW_PEER, DRBD_NLA_NET_CONF, 0405 F_CONFIG_CMD, 0406 .ctx = &amp;new_peer_cmd_ctx, 0407 .summary = &quot;Make a peer host known to a resource.&quot; },  0415 {&quot;new-path&quot;, CTX_PEER_NODE, 0416 DRBD_ADM_NEW_PATH, DRBD_NLA_PATH_PARMS, 0417 F_CONFIG_CMD, 0418 .drbd_args = (struct drbd_argument[]) { 0419 { &quot;local-addr&quot;, T_my_addr, conv_addr }, 0420 { &quot;remote-addr&quot;, T_peer_addr, conv_addr }, 0421 { } }, 0422 .ctx = &amp;path_cmd_ctx, 0423 .summary = &quot;Add a path (endpoint address pair) where a peer host should be reachable.&quot; },  在三個 case 中，三個指令對應的 function 其實都指向了 generic_config_cmd 這 function，而 generic_config_cmd 則再繼續呼叫 _generic_config_cmd 繼續往下處理 ","version":"Next","tagName":"h3"},{"title":"_generic_config_cmd​","type":1,"pageTitle":"Architecture","url":"/docs/techPost/2017/DRBD-v9-0-Network-Work-Flow#_generic_config_cmd","content":"到這一步後，就是根據先前指令中的一些資訊，組出對應的 netlink header，最後透過genl_send 將該命令透過 netlink 送到 kernel 去，然後再使用 genl_recv_msgs 的方式接收回來的訊息，確認事情完成後就結束。 1136 static int _generic_config_cmd(struct drbd_cmd *cm, int argc, char **argv) 1137 { 1138 struct drbd_argument *ad; 1139 struct nlattr *nla; 1140 struct option *options; 1141 int c, i; 1142 int rv; 1143 char *desc = NULL; /* error description from kernel reply message */ 1144 1145 struct drbd_genlmsghdr *dhdr; 1146 struct msg_buff *smsg; 1147 struct iovec iov; 1148 struct nlmsghdr *nlh; 1149 struct drbd_genlmsghdr *dh; 1150 struct timespec retry_timeout = { 1151 .tv_nsec = 62500000L, /* 1/16 second */ 1152 }; ...  Summary 本文到這邊目前已經大致瞭解 user space 的流程，接下來要探討 new-peer, new-path 以及 connect 這三個指令在 kernel 中的流程。 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/introduce-cni-i","content":"","keywords":"","version":"Next"},{"title":"Linux Bridge​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-i#linux-bridge","content":"In the default behavior, the docker will create a linux bridge docker0 when you install the docker.io/docker.ce into your system. and it will handle the network connectivity for every docker container (use the --net=bridge and it is docker default option) You can use the following command to see the linux bridge after you install the docker package. We can create our own linux bridge via the brctl command and you can get it by installing the bridge-utils package. $ apt-get install bridge-utils  Create our own linux bridge and assign a IP address to it. $ brctl addbr br0 $ ifconfig br0 up $ ifconfig br0 172.17.0.0 netmask 255.255.0.0  If you have installed the docker package, you can see there's a interface docker0 in the system and it's IP address is 172.17.0.0/16. If that, you should change your br0 IP address to other CIDR subnet. $ brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242b8582904 no $ ifconfig docker0 docker0 Link encap:Ethernet HWaddr 02:42:b8:58:29:04 inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  We can use the following figure to show the system view of the system now. The default ip address of the docker0 is 172.17.0.0/16 and it can be configured via the docker config. We won't discuss what is layer2 bridging here, the only thing we need to know is that docker will use this bridge to forward the packets between hosts and containers. ","version":"Next","tagName":"h2"},{"title":"Network Namespace​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-i#network-namespace","content":"Now, what will happen when we create a docker container? $$ docker run --name some-nginx -d -p 8080:80 some-content-nginx  First, the docker will create a docker container and also create a network namespace indise that container. The whole system looks like below figure. there're a linux bridge (docekr0) and a docker container (nginx). In our example, we won't use the docker but network namespace, so we can create a network namepsace here. $ ip netns add ns1  Up to now, the container(network namespace) doesn't have the network connectivity which measn any process inside that contaner can't setup a network connection with outside. ","version":"Next","tagName":"h2"},{"title":"Veth​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-i#veth","content":"In order to make the docker container nginx/netowkr namespace has the network connectivity, we need to connect two network namespaces togehter first. the linux host and the docekr container. since the network namespace is a logical concept in the linux system, we can use another linux technology veth to help us. The veth is represent to a virtual link and it can connect to two different network namespace, each veth pair is made up by two virtual network interfaceFor example, type the following command to create a veth pair. $ sudo ip link add ve_A type veth peer name ve_B $ ip link 15: ve_B@ve_A: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether be:8f:26:d9:22:50 brd ff:ff:ff:ff:ff:ff 16: ve_A@ve_B: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether a2:9b:75:06:51:30 brd ff:ff:ff:ff:ff:ff  In the above example, we create a veth pair and the virtual network interface of it is ve_A and ve_B. you can use the some network utils to see them, such as ip link, ifconfig. The system view loooks like beflow, we have a veth pair now but two sides of the veth pair still in the same network namespace.  Next, we need to move one side of the veth pair into the docker container, specifically, is the network namespace. Just like we say before, the veth pair is used to connect two network namespace. we can do that via the ip command. $ sudo ip link set ve_B netns ns1 $ sudo ip netns exec ns1 ip link set ve_B name eth0  Now, the ve_B is moved into the network namespace ns1 and rename as eth1, we can execute commands in the networl namespace to list the interface. $ sudo ip netns exec ns1 ifconfig -a eth0 ink encap:Ethernet HWaddr be:8f:26:d9:22:50 BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  and you should see the interface eth1with any IP configuration. At last, we need to attach another side of veth pair into the linux bridge docker0, just use the brctl command. brctkl addif docker0 ve_A  Good, We have setup differentes network namespace and connect it via the veth and linux bridge. ","version":"Next","tagName":"h3"},{"title":"ip management​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-i#ip-management","content":"The next thing we need to handle it to assign an IP addess to the docekr container/network namespace. Just like above, use the ip netns exec ns1 ifconfig eth1 xxxxxx netmask xxxxx to set the ip address to the interface eth1. The problem is how do we decide what IP address we use? Since we use the linux bridge for layer2 forwarding, we sholud put all the docker container/network namespace and bridge in the same subnet. Which means we should choose any IP address from 172.17.0.0/16. How to choose the IP address is designed by docker and you. You should avoid to use the duplicate IP address since it will cause the ARP problem. After choosing the IP address, set to the interface in the docker continer/network namespace $ sudo ip netns exec ns1 ifconfig eth1 172.16.x.x netmask 255.255.0.0  After that, you can repeat above example to create more network namespace with different IP address and try to use the command ping to test the network connectivity in the layer 2 network. ","version":"Next","tagName":"h3"},{"title":"iptables​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-i#iptables","content":"The last one we need to understand is iptables, and it's a optional step. For a docker container, if we want to access the container from outside network, we should use the -p flag to indicate the port mapping in the docker run command. For example, when we use the following command to create a docker container. $ docker run --name some-nginx -d -p 8080:80 some-content-nginx  It will also insert some rules into the iptables and those rules will do if the destination port number of a packet is 8080, forward it to the container some-content-nginx. modify the destination ip to the ip address of container some-content-nginxmodify the destination port number from 8080 to 80 But if we don't need to access it from outside? we don't the iptables rules to do that. that why I mean it's a optional step. Summary Accoding to the above example, we know that the docker network is based on the linux network namespace. What will happen when we run a docker container? setup a linux bridge (usually be created when you install docker)create a network namespacecreate a veth pair (virutal ether link)attach the veth pair to target network namespace.find a unique IP address and assign to the taget network namespace.setup the iptables rules if you want to access it from outside. In the next posts, I will talk about what is CNI and why we need CNI and how CNI works. ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/introduce-cni-ii","content":"","keywords":"","version":"Next"},{"title":"Why We Need CNI​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-ii#why-we-need-cni","content":"In the previous post, we have learn the procedure of the basic bridge network in the docker. Create a Linux BridgeCreate a Network NamespaceCreate a Veth PairConnect the bridge and network namespace with veth pairSetup the IP address to the network namespaceSetup the iptalbes rules for exporting the services (optional) However, That's the bridge network and it only provide the layer2 forwarding. For some use cases, it's not enough. More and more requirement, such as layer3 routing, overlay network, high performance , openvswitch and so on. From the docker point of view, it's impossible to implement and maintain all above requirements by them. The better solution is to open its interface and make everyone can write its own network service and that's how docker network works. So, there're so many plugins for the docker network now and every can choose what kind of the network they want. Unfortunately, docker isn't the only container technical, there're otehr competitors, such as rkt, lxc. Besides, more and more container cluster orchestration, docker swam, mesos, kubernetes and so on. Take a bridge network as an example, do we need to implement the bridge network for all container orchestration/solutions? do we need to write many duplicate code because of the not-unified interface between each orchestrator? That's why we need the Container Network Interface(CNI), The Container Network Interface(CNI) is a Cloud Native Computing Foundation projects, we can see more information here. With the CNI, we have a unified interface for network services and we should only implement our network plugin once, and it should works everywhere which support the CNI. According to the official website's report. those container runtimes solutions all supports the CNI rkt - container engineKubernetes - a system to simplify container operationsOpenShift - Kubernetes with additional enterprise featuresCloud Foundry - a platform for cloud applicationsApache Mesos - a distributed systems kernelAmazon ECS - a highly scalable, high performance container management service ","version":"Next","tagName":"h2"},{"title":"How CNI works​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-ii#how-cni-works","content":"Container Network Interface is a specifiction which defined what kind of the interface you should implement. In order to make it easy for developers to deveploe its own CNI plugin. the Container Network Interface project also provides many library for developing and all of it is based on the golang language. You can find those two libraries belowhttps://github.com/containernetworking/cnihttps://github.com/containernetworking/plugins ","version":"Next","tagName":"h2"},{"title":"What does CNI do​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-ii#what-does-cni-do","content":"In CNI specifiction, there're three method we need to implement for our own plugin. ADDDELETEVERSION ADD will be invoked when the container has been created. The plugin should prepare resources and make sure that container with network connectivity.DEKETE will be inboked when the container has been destroyed. The plugin should remove all allocated reousrces.VERSION shows the version of this CNI plugin. For each method, the CNI interface will pass the following information into your plugin ContainerIDNetnsIfNameArgsPathStdinData I will explain those fields detaily in the next tutorial. In here, we just need to know for the CNI plugin, we sholud use those information ContainerID, Network Namespace path and Interface Name and StdinData to make the container with network connectivity. Use the previous bridge-network as example. the network namespace will be created by the orchestrator and it will pass the path of that network namespace via the variable netns to CNI. After we crete the veth pair and connect to the network namespace, we should set the interface name to Ifname. For the IPAM (IP Adderss Management), we can get the information from the StdinData and calculate what IP address we should use in the CNI plugin. Kubernetes Now, We will see how kubernetes use CNI to create a network function for Pods. ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-ii#configuration","content":"In order to use the CNI, we need to config the kubelet to use the CNI method. There're three argurments we need to take care. cni-bin-dir: the directory of CNI binaries.cni-conf-dir: the directory of CNI config files, common CNI(flannel/calico..etc) will install its config into here.network-plugin: the type of network-plugin for Pods. In my kubernetes cluster (installed by kubeadm) vortex-dev:10:06:59 [~]vagrant $ps axuw | grep cni root 1864 4.9 2.1 569172 110108 ? Ssl 15:18 3:06 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni  You can see the arguments --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni of the kubelet. Now, Let we see the files under cni-bin-dir and cni-conf-dir. The cni-bin-dir contains all the CNI binary file and those files can be programmed by any language, just follow the CNI interface. vortex-dev:04:21:29 [~]vagrant $ls /opt/cni/bin/ bridge dhcp flannel host-local ipvlan loopback macvlan portmap ptp rainier sample tuning vlan  In the cni-conf-dir, we should put the CNI config here and kubernetes will use the config for your Pod. In my kubernetes cluster, I had installed the flannel CNI in it and the flannel will install its config here. vortex-dev:05:11:30 [~]vagrant $ls /etc/cni/net.d/ 10-flannel.conf vortex-dev:05:11:34 [~]vagrant $cat /etc/cni/net.d/10-flannel.conf { &quot;name&quot;: &quot;cbr0&quot;, &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;isDefaultGateway&quot;: true } }  ","version":"Next","tagName":"h2"},{"title":"How To Use it.​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-ii#how-to-use-it","content":"When kubelet receives a request to create a Pod in the node. First, it will search the cni-conf-dir in the alphabet order and inspect it. Take the 10-falnnel.conf as example. when the kubelet knows the type is flannel, it will try to call the flannel in the cni-bin-dir and that's /opt/cni/bin/flannel. vortex-dev:05:11:34 [~]vagrant $cat /etc/cni/net.d/10-flannel.conf { &quot;name&quot;: &quot;cbr0&quot;, &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;isDefaultGateway&quot;: true } }  ","version":"Next","tagName":"h2"},{"title":"Pause Container.​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-ii#pause-container","content":"Before kuberlet creates the Pod, it will create a pause conatiner first. And follows the CNI steps to setup the network fot that pause container.(Assueme we use the network-plugin=cni) Now, The pause container is running and has the network connectivity. The kubelet will create containers which is be described in the yaml file and attach those container to that pause container (in the docker command, we can use the --net=$containerID to do the same thing). By those procedure, we can maks sure all containers share the same network stack and any container crash won't destory the network stack since the network stack is hold sy the pause container. Combine the pause container and user containers, it's called Pod. And you can try to use the docker ps in your kubernetes node to see how many pause container in there. vortex-dev:05:19:30 [~]vagrant $sudo docker ps -a | grep pause 8838b9614a30 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 7 hours ago Up 7 hours k8s_POD_nfs-provisioner-5b75397b4807c54ad4fe92e2-6954c749cc-cn5jh_vortex_9f2f692c-a130-11e8-9450-02ddf6cab53d_0 0a232459f786 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 7 hours ago Up 7 hours k8s_POD_vortex-server-58895cd7c6-xvd8g_vortex_7d88347b-9f9a-11e8-8719-02ddf6cab53d_8 b0ca4ca2405d k8s.gcr.io/pause:3.1 &quot;/pause&quot; 7 hours ago Up 7 hours k8s_POD_kube-state-metrics-7d7d7b6bbc-fsf7b_vortex_7d83db65-9f9a-11e8-8719-02ddf6cab53d_7 63a1f3b8a35f k8s.gcr.io/pause:3.1 &quot;/pause&quot; 7 hours ago Up 7 hours k8s_POD_coredns-78fcdf6894-s8ts5_kube-system_c9ef514c-9a23-11e8-9c21-02ddf6cab53d_9 310b7a6daa54 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 7 hours ago Up 7 hours k8s_POD_cadvisor-zk8bk_vortex_7d726ff5-9f9a-11e8-8719-02ddf6cab53d_3 3f0141a5a9b6 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 7 hours ago Up 7 hours k8s_POD_network-controller-server-tcp-nnvgk_vortex_7d648d43-9f9a-11e8-8719-02ddf6cab53d_2 9cedcb482e69 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 7 hours ago Up 7 hours  Summary The Container Network Interface CNI made the network-service developer more easy to develop their own network plguin. They don't need to write duplicate code for different system/orchestrator. Just write once and run everywhere. And the CNI consists of a specification and many userful libraries for developers. The CNI only care the ADD and DELETE events. the CNI plugin shoould make sure the container with network connectivity when the ADD event has been triggered and remove all allocted resources when the DELETE event has been triggered. In the next tutorial, I will show how to write a simple bridge CNI plugin in golang. ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/introduce-cni-iii","content":"","keywords":"","version":"Next"},{"title":"Basic CNI​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-iii#basic-cni","content":"It provides some basic CNI, such as Bridge, MacVlan, Host Device.. And so on. You can chain those CNI into your own CNI and combine those into a more powerful CNI. ","version":"Next","tagName":"h2"},{"title":"IPAM​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-iii#ipam","content":"IPAM (IP Address Management) provides some method to handle the IP/Route management. It provides host-local, dhcp and static three methods now. In the host-local, you just need to provide a configuration file to describe what subnet/gateway you want to use and it will allocate a unused IP address from that subnet for your CNI. And the dhcp will runs a DHCP client in each container and send a dhcp request to get a IP address from the dhcp server. In this tutorial, we will implement a bridge CNI and explain those functions step by step. Before We Start Before we start to implement the CNI, we must know the interface/specification of the CNI. Your CNI will be invoked when the container is ready to create or has been terminated. Allocate resources for the container, including the IP address and the network connectivity.Remove all resources you allocated before when a container has been terminated. The caller will pass the following information into your CNI program Command (What kind of the event you should care) ADDDELETEVERSION ContainerID (The target ContainerID)NetNS (THe network namespace path of the container)IFNAME (The interface name should be created in the container)PATH (The current working PATH, you should use it to execute other CNI)STDIN (The configuration file of your CNI) Step By Step For each step, you can find a corresponding folder in my github repo and there's all golang files for each steps. ","version":"Next","tagName":"h2"},{"title":"Step1​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-iii#step1","content":"First, we need to provide two function for ADD and DELETE event which is used to allocate/recycle resource when the container has been start/terminated. We use the framework provided by the The ContainerNetworking/CNI and it will encapsulate Package main Import ( &quot;github.com/containernetworking/cni/pkg/skel&quot; &quot;github.com/containernetworking/cni/pkg/version&quot; ) func cmdAdd(args *skel.CmdArgs) error { return nil } func cmdDel(args *skel.CmdArgs) error { return nil } func main() { skel.PluginMain(cmdAdd, cmdDel, version.All) }  In this framework, it encapsulates all information we need into a predefined type skel.CmdArgs type CmdArgs struct { ContainerID string Netns string IfName string Args string Path string StdinData []byte }  Use the go build to build the binary and assume our execution file is example and then we should provide a basic configuration which should contains useful information for our CNI. Maybe we call the file configuration its contents looks like { &quot;name&quot;: &quot;mynet&quot;, &quot;BridgeName&quot;: &quot;test&quot;, &quot;IP&quot;: &quot;192.0.2.1/24&quot; }  Now, We can use the following command to execute our CNI program. sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 \\ CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 \\ CNI_PATH=`pwd` \\ ./example &lt; config  For that go CNI framework, those infromation should be passed by the environement and we can get that from the CmdArgs. Actually, we have done the basic CNI program but it does nothing. A good CNI should make a container network connectivity and assign a valid IP address to the container and we will do that in the foloowing tutorial. ","version":"Next","tagName":"h2"},{"title":"Step 2​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-iii#step-2","content":"Now, we will create a linux bridge for the container and the logical flow looks like Read the bridge information from the config.Get the bridge name we want to use.Create the bridge if it doesn't exist in the system. Since the frametwork store the config content in the CmdArgs object as a []byte form. we should create a structure to decode those []byte data. type SimpleBridge struct { BridgeName string `json:&quot;bridgeName&quot;` IP string `json:&quot;ip&quot;` }  and decode the config content in the CmdAdd function. func cmdAdd(args *skel.CmdArgs) error { sb := SimpleBridge{} if err := json.Unmarshal(args.StdinData, &amp;sb); err != nil { return err } fmt.Println(sb)  There're many ways for creating the Linuxu Bridge, we can use the system commadn brctl addbr via the os.Exec or use the netlink to create. We choose the netlink method here since the os.Exec is too easy for developer. First, we should import the netlink package &quot;github.com/vishvananda/netlink&quot;and we will use the type netlink.Bridge to describe the bridge we want. In the following example, we will do three things. Prepare the netlink.Bridge object we want.Create the BridgeSetup the Linux Bridge.  br := &amp;netlink.Bridge{ LinkAttrs: netlink.LinkAttrs{ Name: sb.BridgeName, MTU: 1500, // Let kernel use default txqueuelen; leaving it unset // means 0, and a zero-length TX queue messes up FIFO // traffic shapers which use TX queue length as the // default packet limit TxQLen: -1, }, } err := netlink.LinkAdd(br) if err != nil &amp;&amp; err != syscall.EEXIST { return err } if err := netlink.LinkSetUp(br); err != nil { return err }  Now. The CmdAdd function should look like below. func cmdAdd(args *skel.CmdArgs) error { sb := SimpleBridge{} if err := json.Unmarshal(args.StdinData, &amp;sb); err != nil { return err } fmt.Println(sb) br := &amp;netlink.Bridge{ LinkAttrs: netlink.LinkAttrs{ Name: sb.BridgeName, MTU: 1500, // Let kernel use default txqueuelen; leaving it unset // means 0, and a zero-length TX queue messes up FIFO // traffic shapers which use TX queue length as the // default packet limit TxQLen: -1, }, } err := netlink.LinkAdd(br) if err != nil &amp;&amp; err != syscall.EEXIST { return err } if err := netlink.LinkSetUp(br); err != nil { return err }  Use the aforementioned command to call the binary again and you should see the linux bridge test has been created. If youu don't have the brctl command, use the apt-get install bridge-utils to to install the bridge tools. ","version":"Next","tagName":"h2"},{"title":"Step3​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-iii#step3","content":"In the next step, we will creat a veth for connecting the linux bridge and the taget container. The logical flow are Get the bridge object from the Bridge we created beforeGet the namespace of the containerCreate a veth on the container and move the host-end veth to host ns.Attach a host-end veth to linux bridge This step is more complicate then previous steps. since we will handle the network namespace here. Fortunately, the CNI project has provided convenience function to handle the veth and it can cover the (3) action itom above. First, we use the netlink.LinkByName method to lookup the netlink object.  l, err := netlink.LinkByName(sb.BridgeName) if err != nil { return fmt.Errorf(&quot;could not lookup %q: %v&quot;, sb.BridgeName, err) }  and the we need to make sure that object is netlink.Bridge, so we do the type casting.  newBr, ok := l.(*netlink.Bridge) if !ok { return fmt.Errorf(&quot;%q already exists but is not a bridge&quot;, sb.BridgeName) }  Second, since the CmdArgs already provide the network namespace path of the container, we can use the method from the ns package to load the object of the network namespace. import `&quot;github.com/containernetworking/plugins/pkg/ns&quot;` netns, err := ns.GetNS(args.Netns) if err != nil { return err }  For each NetNS object, it implement a function Do which take a function as its parameter and that function's parameter is the caller's network namespace. The do function will switch the network namespace to NetNS object itself and call the function(parameter) and feed the original network namespace as parameter. See the following example to learn more about do function. var handler = func(hostNS ns.NetNS) error { hostVeth, containerVeth, err := ip.SetupVeth(args.IfName, 1500, hostNS) } if err := netns.Do(handler); err != nil { return err }  First , we create a function handler which calls the ip.SetpuVeth to create a veth pair on caller's network namespace and move one side of veth pair to its third parameter(hostNS) When we call the netns.Do(handler), it will call the function handler in netns's network namepsace and pass the caller's network namespace to the function handler. Which will result in that there will be a veth pair between the host's network namespace and netns's netowkr namespace. In order to store the information about that veth pair, we can use the current.Interface{} object to store the data. First, we need to import the library import &quot;github.com/containernetworking/cni/pkg/types/current&quot;  and then create a variable represent to host side network interface in the function handler. hostIface := &amp;current.Interface{} var handler = func(hostNs ns.Netns) error { hostVeth, _, err := ip.SetupVeth(args.IfName, 1500, hostNS) if err != nil { return err } hostIface.Name = hostVeth.Name return nil }  Now, we can get the interface name of veth pair in the host side by hostIface.Name and then we will attach that link to the Linux Bridge we created before. Get the link object from the interface name by function call netlink.LinkByNameConnect the link to bridge by function call netlink.LinkSetMaster hostVeth, err := netlink.LinkByName(hostIface.Name) if err != nil { return err } if err := netlink.LinkSetMaster(hostVeth, newBr); err != nil { return err }  There is one important thing we need to care is the OS thread. since we will switch the netns to handle the namespace things. We must make sure the OS won't switch the thread during the namespace operations. Use the function runtime.LockOSThread() in the golang predefined function init(). func init() { // this ensures that main runs only on main thread (thread group leader). // since namespace ops (unshare, setns) are done for a single thread, we // must ensure that the goroutine does not jump from OS thread to thread runtime.LockOSThread() }``` See the whole example program in https://github.com/hwchiu/CNI_Tutorial_2018/tree/master/tutorial/step3 and you can directly run the `run.sh` in your linux machine to see the following output. ```shell= Ready to call the step3 example {test 192.0.2.1/24} The CNI has been called, see the following results The bridge and the veth has been attatch to bridge name bridge id STP enabled interfaces test 8000.aa6e12faa09b no vethff65a064 The interface in the netns eth10 Link encap:Ethernet HWaddr 7e:23:e2:e5:8f:c4 inet6 addr: fe80::7c23:e2ff:fee5:8fc4/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1 errors:0 dropped:0 overruns:0 frame:0 TX packets:1 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:90 (90.0 B) TX bytes:90 (90.0 B) lo Link encap:Local Loopback LOOPBACK MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  We have successfully create a linux bridge and connect to the other network namespace via the veth pair and the interface in that namepsace is eth10 which has been defiend in the config file. ","version":"Next","tagName":"h2"},{"title":"Step4​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/introduce-cni-iii#step4","content":"In this step, we will setup the IP address into the target network namespace. To make the problem easy, we had set the target IP address in the config and we can get via the sp.IP type SimpleBridge struct { BridgeName string `json:&quot;bridgeName&quot;` IP string `json:&quot;ip&quot;` }  The function we used to assign the IP address is netlink.AddrAddSo the workflow is Generate a IP object from the config.Call the nelink.AddrAdd in the target network namespace. The parameter of netlink.AddrAdd is netlink.Addr and see its structure below. type Addr struct { *net.IPNet Label string Flags int Scope int Peer *net.IPNet Broadcast net.IP PreferedLft int ValidLft int }  We can use the net package provided by official golang to generate the net.IPNet type and its a CIDR form (IP address and the Mask). Since the IP address in our config is a string192.0.2.15/24, we use the net.ParseCIDR to parse the string and return a pointer of net.IPNet So, modify the previous handler to assign the IP address when we create a veth. Since the net.IPNet object get from the net.ParseCIDR is the subnet not a real IP addrees, we should reassign the IP address to its IP field again. var handler = func(hostNS ns.NetNS) error { hostVeth, containerVeth, err := ip.SetupVeth(args.IfName, 1500, hostNS) if err != nil { return err } hostIface.Name = hostVeth.Name ipv4Addr, ipv4Net, err := net.ParseCIDR(sb.IP) if err != nil { return err } link, err := netlink.LinkByName(containerVeth.Name) if err != nil { return err } ipv4Net.IP = ipv4Addr addr := &amp;netlink.Addr{IPNet: ipv4Net, Label: &quot;&quot;} if err = netlink.AddrAdd(link, addr); err != nil { return err } return nil }  See the whole example program in https://github.com/hwchiu/CNI_Tutorial_2018/tree/master/tutorial/step4 and you can directly run the run.sh in your linux machine to see the following output. Ready to call the step4 example {test 192.0.2.15/24} The CNI has been called, see the following results The bridge and the veth has been attatch to bridge name bridge id STP enabled interfaces test 8000.a6f55b2927c0 no vethd611bb3b The interface in the netns eth10 Link encap:Ethernet HWaddr aa:a0:96:45:65:c5 inet addr:192.0.2.15 Bcast:192.0.2.255 Mask:255.255.255.0 inet6 addr: fe80::a8a0:96ff:fe45:65c5/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:2 errors:0 dropped:0 overruns:0 frame:0 TX packets:1 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:168 (168.0 B) TX bytes:90 (90.0 B) lo Link encap:Local Loopback LOOPBACK MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  And you can see we have already set the IP address to the interface eth10. You can use the following command to mamually set the IP address to the linux bridge and use the ping command to check the network connectiviy between the host and the target network namespace. sudo ifconfig test 192.0.2.1 sudo ip netns exec ns1 ping 192.0.2.1  Summary In this tutorial, we have implemented a simple Linux Bridge CNI (only Add function) in golang. We create the linux bridge and use the veth to connect the linux bridge with the target netowrk namespace. Besides, we also fethc the information we want from the pre-defined config file which means we can more flexible to change the behavior of your own CNI implementation. To make the problem simple, we don't use any complicated method to acquire a unique address from the config but you can desing you own algorithm to do that. If you want to learn more about the IP related operations, you can go to the host-local to learn more. ","version":"Next","tagName":"h2"},{"title":"11個保護你 Kubernetes 集群的技巧與觀念(上)","type":0,"sectionRef":"#","url":"/docs/techPost/2018/k8s-security-11tips-i","content":"","keywords":"","version":"Next"},{"title":"The Control Plane​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(上)","url":"/docs/techPost/2018/k8s-security-11tips-i#the-control-plane","content":"The Control Plane 所代表的就是 Kubernetes 的控制面，其擁有的權力跟能力非常的巨大，從最基本資源的創建與監控 (Pod/Deployment/Service)，資源的調度(Pod Schedule) 以及包括所有 Kubernetes 上資源的存取 (Secret/ConfigMap)。 由於 The Control Plane 這邊能做的事情實在太多，一旦讓非系統管理者有機會接觸到這個部分，就有機會對整個集群造成意想不到不可挽回的事情。 因此第一個章節所要描述的就是如何安全的防護你的 kubernetes，讓任何惡意攻擊者沒有機會去操作你的 kubernetes 集群。 ","version":"Next","tagName":"h2"},{"title":"TLS Everywhere​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(上)","url":"/docs/techPost/2018/k8s-security-11tips-i#tls-everywhere","content":"這邊的原則非常簡單，只要任何內部元件之間的溝通有支援 TLS, 就使用 TLS，沒有任何不用的理由 透過 TLS 能夠確保這條連線受到保護，除了可以驗證 Server/Client 彼此的深分外，也能夠避免傳輸內容被竊聽。 事實上有提供 TLS 功能連線的元件也都有提供所謂的類 Insecure 方式連線，就是不依賴 TLS 來進行連線而是直接進行純文字傳送。 對於很多人來說，其實會覺得要設定 Secure 連線很麻煩，在 Server 端要產生準備好憑證，而且對於每個連線的 Client 端都要準備好對應的憑證，讓整個連線可以正常運行。 我自己是覺得除了進行研究方便快速測試之外，其他的情境應該都要盡可能的使用 Secure 連線來確保連線安全。 那在 Kubernetes 中，到底有多少個元件之間有 TLS 連線的存在? 根據 Lucas Käldström 於 kubeadm Cluster Creation Internals: From Self-Hosting to Upgradability and HA 所描繪的流程圖，我們可以清楚地看到在 kubernetes 元件彼此之間的溝通，除了 Kubernetes 本身元件之外，也包含了第三方的插件的開發。  Master Node​ 首先，Master 上面的元件可以分成兩種交流方式，分別是 gRPC 以及 Protobuf這邊我認為是因為 etcd 本身不是屬於 kubernetes 自行開發的元件，所以在與之溝通上就會必須依賴本身已經存在的格式與規範。 由於 gRPC 本身是基於 HTTP2 的方式來傳輸封包，所以透過 TLS 加密是完全沒有問題的 而上圖中的 Controller Manager, API Server 以及 Scheduler 這些 Kubernetes 自行開發的組件彼此之間透過 Protobuf 來規範這些格式，都可以在這邊 design-proposals 看到開發與設計的規範。 目前上述三個元件互相溝通也是支援 TLS 連線的，如果夠熟悉 kubernetes 手把手建置過程的讀者就知道整個集群內有非常多的 key/cert 等資訊要設定與配置，非常的複雜。 Cross Nodes​ 在 kubernetes 的架構下，除了所謂的 master 節點外，還有所謂的 minion 節點，而 minion 則會透過機器上的 kubelet 與 API Server 進行溝通。 如同上述的 Protobuf 的走法，這邊也有公開的設計規範以及 API，最後溝通的部分也支援 TLS 加密。 圖片中還有提到 CNI,CRI, OCI 等這部分則是容器相關資源標準化介面，從Network,RunTime等不同標準，實際上使用者會使用哪些都是可以自行抽換的。 此外，不少網路的 CNI 也都有透過 etcd 的方式去存取資料，這意味者對於每個 CNI 也都必須要幫忙準備相關的 etcd 憑證來建立 TLS 連線，千萬不可以為了方便就捨棄安全性。 TLS 帶來的問題​ TLS 聽起來很美好，實際上在部屬的時候卻會帶來不少困難，最明顯的就是動態增加節點需要額外的心力去處理憑證相關的問題。 為了解決這個問題，可以參考這篇由 Todd Rosner 撰寫的這篇文章 Kubernetes TLS bootstrapping, 裡面有詳細介紹 Kubernetes TLS bootstrapiing 的原理與設計以及如何用來解決動態擴充節點的問題，非常值得一讀。 ","version":"Next","tagName":"h3"},{"title":"Enable RBAC with Least Privilege, Disable ABAC, and Monitor Logs​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(上)","url":"/docs/techPost/2018/k8s-security-11tips-i#enable-rbac-with-least-privilege-disable-abac-and-monitor-logs","content":"Kubernetes 在 Authorization 方面也有許多的安全問題需要注意，想要瞭解全文可以直接參考官網文件 Authorization Overview 簡單來說，目前 kubernetes 支援的 Authorization 總共有四種方法，分別是 NodeRBACABACWebhook 本篇文章不會探討這四種的差異及比較，之後有時間會再寫文章來介紹這幾種並且分享看法。 這邊要知道的就是請盡量使用 RBAC 並且在設定權限的時候，不要把權限全開，請根據用到的權限開啟特定的權限來達到最大的保護。 如果對於 RBAC 想要詳細暸解，可以參考這篇文章 RBAC Support in Kubernetes 有寫過 RBAC 的讀者就會知道，其實寫 RBAC 非常的麻煩，必須要不停的 try and error 去找出到底自己的應用程式用到了哪些權限，一個一個的補上去。 這過程非常疲倦但是為了最高的安全性，還是要請大家準確地執行。 不過為了解決這個問題，這邊特別介紹一個工具 audit2rbac只要你的 kuberntes 是 v.10.0 之後，就可以開啟一個 beta 的功能 audit log. 而上述的工具可以幫你解析這些 audit log 來判斷你的權限還缺少哪些，然後幫你產生對應的 RBAC 檔案，聽起來滿好用的，這部分還需要找時間來實際玩玩看，看看是否真的如敘述般的好用。 當然有興趣的讀者也可以先行嚐鮮來使用看看這個工具 ","version":"Next","tagName":"h3"},{"title":"Use Third Party Auth for API Server​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(上)","url":"/docs/techPost/2018/k8s-security-11tips-i#use-third-party-auth-for-api-server","content":"原文認為透過一個類似 SSO(Single Sign On) 這種集中的方式能夠有效的去控管使用者的權限，特別是當使用者有任何調度更動時。 對於 Kubernetes 來說，如果整合第三方服務的認證，譬如 Google/GitHub，就可以在有大量使用者有任何異動之時不需要一直重新調整設定 Kubernetes API server 除了上述服務外，作者還介紹了 OIDC(OpenID Connect Identity) 與 kubernetes 的一些使用情境，可以在這個專案 k8s ODIC helper 這邊看到使用方法。 這邊我本身也還沒有很熟悉，需要花更多時間來理解這邊的觀念到時候再跟大家介紹這些應用與概念。 ","version":"Next","tagName":"h3"},{"title":"Separate and Firewall your etcd Cluster​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(上)","url":"/docs/techPost/2018/k8s-security-11tips-i#separate-and-firewall-your-etcd-cluster","content":"原文作者說道, etcd 在 kubernetes 集群內扮演一個非常重要也是非常關鍵的角色，其儲存了所有 kubernetes 集群內的所有資訊，包含了各式各樣的設定以及 kubernetres 的 secrets 資源。 為了妥善保護好 etcd，作者認為 etcd 本身在安全防護上要跟 kubernetes集群是分開處理的。 對於 etcd 這樣的角色來說，只要讓攻擊者有寫入的能力，就意味者攻擊可以扮演一個 root 的角色來操控整個 kubernetes 集群。 若是攻擊者只有讀取的能力，也是有可能會讓攻擊者透過 etcd 取得道各式各樣機密的資訊。 這邊舉一個範例來說明 etcd 被攻擊會有多大的影響。 kubernetes scheduler 會透過 etcd 來找尋還沒有運行起來的 pod, 然後尋找一個可行的 pod 來運行。如果今天有使用者想要創立一個 pod, 則該 pod 會先在 API server 端進行驗證，確認一切參數都合法且符合條件後，就會把該 pod 的資訊寫入到 etcd. 綜合以上兩點，如果今天有惡意使用者直接透過 etcd 去修改 pod 本身的屬性，譬如PodSecurityPolicies 這方面跟安全性/權限有關的任何設定，就可以直接對這個 pod 產生非預期的結果，這邊就會有很大的安全性漏洞。 要怎麼保護 etcd? 作者認為 一定要上 TLS 加密etcd 應該要部署在特定的節點上為了減少私鑰被偷取並且在一般的 worker 節點上使用，管理者可以針對 etcd 集群與 API server 進行防火牆的設定，只有 API server 才能夠跟 etcd 集群連線，避免任何 worker 節點上有任何機會去存取 etcd ","version":"Next","tagName":"h3"},{"title":"Rotate Encryption Keys​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(上)","url":"/docs/techPost/2018/k8s-security-11tips-i#rotate-encryption-keys","content":"對於 Security 來說，定期的轉換你使用的金鑰與憑證能夠降低問題發生時的最大風險程度。 舉例來說，若今天不小心金鑰遺失了，若系統上面沒有去定期改變，則攻擊者可以無時無刻的都對系統進行非預期的操作，然而若金鑰有定期去改變，則攻擊者在一定時間後就沒有辦法繼續對集群操作了。 對 kubernetes 來說，針對 kubelet 這隻應用程式來說，目前有提供這樣的功能，能夠定期的去轉換，這邊可以參考官方文件 Certificate Rotation但是對於採用對稱性加密的 API Server 來說則是沒有提供這個能力，這邊我暫時還不清楚差異性，對於安全性沒有這麼熟悉。 原文則表示這部分需要手動的去定時轉換來提高安全性，下列這篇文章有介紹該如何操作 Kubernetes Secrets Encryption，並且提到若你採用的集群是 GKE/AKS 這種雲端服務商提供的 Kubernetes 平台，則背後都會自己幫你進行這方面的保護。 ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(上)","url":"/docs/techPost/2018/k8s-security-11tips-i#summary","content":"其實原文只有短短的敘述過去，但是其實每篇內容都有非常詳細的資訊可以繼續擴展，能的話建議讀者花點時間把文章內的連結都讀了一遍，我相信會對 kubernetes 內部的各種機制有更深層的瞭解，可以學到更多的東西。 ","version":"Next","tagName":"h2"},{"title":"11個保護你 Kubernetes 集群的技巧與觀念(下)","type":0,"sectionRef":"#","url":"/docs/techPost/2018/k8s-security-11tips-ii","content":"","keywords":"","version":"Next"},{"title":"Preface​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#preface","content":"本篇文章的原文為 11 Ways Not to Get Hacked 本文作者將這 11 個技巧與觀念分成三大部分來 分別是 The Control PlaneWorkloadsThe Future 本篇文章延續 11個保護你 Kubernetes 集群的技巧與觀念(上) 繼續探討原文作者後半部分的概念 在上篇文章中，我們專注於 The Control Plane 這邊相關的安全管理，而本篇文章我們則會專注於後續的兩個方向，分別是 Workloads 以及 The Future。 ","version":"Next","tagName":"h2"},{"title":"Workloads​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#workloads","content":"相對於 The Control Plane 著重於整個集群架構上的安全問題， Workloads 則是著重於運行於 kubernetes 內的各種工作服務，如 Pods,Deployments, Jobs 等 雖然這些工作服務在部署階段是受到 kubernetes 經過檢查確認合法才會往下執行的，但是由於這些工作服務都會直接面向使用者，若這些工作容器內本身的權限過高且遭受到非預期的攻擊時，就會衍生出其他的安全性問題。 因此作者認為，針對這些容器相關的部署，其權限能夠愈低愈好，針對其用途開啟特定權限，不能為了方便偷懶就將全部的權限開啟。 ","version":"Next","tagName":"h2"},{"title":"Use Linux Security Features and PodSecurityPolicies​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#use-linux-security-features-and-podsecuritypolicies","content":"針對 Linuux 相關的容器來說，Linux Kernel 其實提供了不少安全控管的機制，譬如 Linux CapabilityAppArmorSELinuxseccomp Linux Capability​ Linux Capability 是針對 Thread 為單位的權限控管，部分跟系統底層的操作都需要特定的 capability 才能夠操作，譬如擁有 NET_ADMIN 能力的則可以透過相關指令去修改系統上的網路設定(Routing/Interface), 而擁有 NET_RAW 的則有能力可以去聽取 Raw Socket 的封包。 在 Kubernetes 內也可以透過 SecurityContext 對每個 Container 去進行相關的權限設定. 類似用法如下 pods/security/security-context-4.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo-4 spec: containers: - name: sec-ctx-4 image: gcr.io/google-samples/node-hello:1.0 securityContext: capabilities: add: [&quot;NET_ADMIN&quot;, &quot;SYS_TIME&quot;]  AppArmor​ AppArmor 則是一種基於 Process 程序為單位去限制其存取檔案及操作的一種機制。 針對每個 Process 可以去描述只能存取哪些檔案，藉此降低該程序被攻擊時所造成的傷害. 對於 Kubernetes來說，自從 1.4.0 開始就有支援 AppArmor 相關的功能，可以事先在每個節點上安裝 AppArmor 並且透過 Profile 的方式去設定 AppArmor 的權限，然後每個 Pod 透過 Annotation 的方式去掛載特定的 AppArmor 來限制該容器的權限。 詳細的範例可以參考官網的範例Kubernetes AppArmor apiVersion: v1 kind: Pod metadata: name: hello-apparmor annotations: # Tell Kubernetes to apply the AppArmor profile &quot;k8s-apparmor-example-deny-write&quot;. # Note that this is ignored if the Kubernetes node is not running version 1.4 or greater. container.apparmor.security.beta.kubernetes.io/hello: localhost/k8s-apparmor-example-deny-write spec: containers: - name: hello image: busybox command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo 'Hello AppArmor!' &amp;&amp; sleep 1h&quot; ]  SELinux​ Security-Enhanced Linux (SELinux) 主旨在於提供更強安全性的 Linux 系統，藉由取代原先的 自主式存取控制 (Discretionary Access Control, DAC) 為 委任式存取控制 (Mandatory Access Control, MAC) 的方法 來針對特定的應用程式與特定的使用者達成更細部的權限管理，可達成即使是 root 身份在執行特定檔案/指令時也會被降權成非root權限。 對應於 Kubernetes 來說，首先要先在節點上面開啟 SELinux 相關的功能，並且正確設定完畢。 接下來對於要採用該 SELinux 的 Pod 中去設定 SecurityContext，並且透過 seLinuxOptions 的方式來設定相關的參數 整體的參數結構可以參考 Kubernetes API Reference而使用範例可參考如下 ... securityContext: seLinuxOptions: level: &quot;s0:c123,c456&quot;  Seccomp​ seccomp-bpf 是一套以 process 程序為單位去限制其能夠呼叫 Linux Kernel System Call 的機制。 一般的應用程式大部分可能都只會用到 Read,Write 等基本功能，再來可能會牽扯到 Fork/Clone 這類型，更甚至會牽扯到網路相關的網卡綁定。 因此透過 seccomp 的功能去限制能夠執行的 System Call 也能夠大幅的降低當應用程式被攻擊時所造成的傷害。 對應於 kubernetes 來說，該功能目前還是屬於 Alpha 階段，目前的使用方式大概是 在對應的節點上放置相關的 seccomp 的檔案，用來描述相關的設定在運行的 Pod 的檔案內，透過 Annotation 的方式將該 Pod 與對應的 seccomp 設定檔連結來達到限制的功用。 詳細的操作可以參考Kubernetes Pod-Security-Policy 除了上述提到跟 Linux Kernel 提供的安全機制外，Kubernetes v1.11 本身也提供了一個新的機制 Pod Securirt Policy(Beta). 該機制提供了一種Cluster-Wide的安全性設定，能夠將相關的安全性設定設定成一份全域的設定檔案，透過 RBAC 以及 ServiceAccount 的綁定，就可以讓所有的 Pod 自動套用這些安全性設定，減少了每個單獨設定的繁瑣程序。 相像的文件可以參考Kubernetes Pod-Security-Policy ","version":"Next","tagName":"h3"},{"title":"Statically Analyse YAML​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#statically-analyse-yaml","content":"相對於前面提到這種RunTime時期的安全性檢查，這邊要介紹的則是一種相輔相成的概念，Statiscally Analyse YAML，針對部署服務的 Yaml 設定檔案去分析，評估該 Yaml 內的設定是否足夠安全。 作者認為敏感機密的資訊不應該被放在任何的 Yaml 內容之中，若採用 ConfigMap 以及 Secret 等方式來存放任何機密資訊的話，也必須要將這些資料透過一些工具來進行加密，譬如 Valut(with CoreOS's operator, git-crypt, sealed secrets 以及 cloud procider KMS 此外，這邊也提出了另外一套我覺得滿有趣的工具 kubectl-kubesec該工具是一個 kubelet 的 plugin, 能夠針對部署資源的 Yaml 檔案將行掃描，根據安全性相關的設定給予評分，讓管理者能夠知道盡可能地將安全性相關的權限給濃縮到最小，減少被惡意攻擊時能夠造成的傷害。 舉例來說針對 kubernetes-dashboard 的部署，得到下列結果。 kubernetes-dashboard kubesec.io score 7 ----------------- Advise 1. containers[] .securityContext .runAsNonRoot == true Force the running image to run as a non-root user to ensure least privilege 2. containers[] .securityContext .capabilities .drop Reducing kernel capabilities available to a container limits its attack surface 3. containers[] .securityContext .readOnlyRootFilesystem == true An immutable root filesystem can prevent malicious binaries being added to PATH and increase attack cost 4. containers[] .securityContext .runAsUser &gt; 10000 Run as a high-UID user to avoid conflicts with the host's user table 5. containers[] .securityContext .capabilities .drop | index(&quot;ALL&quot;) Drop all capabilities and add only those required to reduce syscall attack surface  其建議大概如下 不要用 Root去運行，應該修改 Container 改成 non-root 就可以執行Linux Capabilityes 應該要移除掉沒有用到的部分針對 / Root File System 應該只能夠唯獨就好執行 User 的 UID 應該要超過 10000，可以避免該 UID 與本機端的 Host 衝突(主要是因為 User Namespace 還沒有完全實作於 k8s container 中) ","version":"Next","tagName":"h3"},{"title":"Run Containers as a Non-Root User​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#run-containers-as-a-non-root-user","content":"作者說道，大部分使用 Root 去運行的 Container 應用程式其實根本都不需要 Root 這麼大的權限，以 Root 身份去執行擁有的權限已經超過的該應用程式真正需要的權限，這樣的設定會讓任何惡意的攻擊者在成功突入後有更大的權限去對系統造成更大的傷害。 首先，目前的 Linux 在針對檔案權限的部分預設還是採用 自主式存取控制 (Discretionary Access Control, DAC) 這種依據檔案的 User/Group UID/GID 對應的方式來判別是否有權限進行讀寫。 同時，在 Kubernetes 的環境下，創建各式各樣的 Container 的同時並沒有啟動User namespace 的隔離，這意味者容器內使用者的ID (UID) 實際上是可以對應到外面節點上的使用者資料集中。 以實際的案例來說明上述的情況就是若以 Root 的身份去運行一個容器於 kubernetes 集群之中，在對應的工作節點上可以觀察到實際上也是用一個Root的身份去運行該容器的應用程式。 以下面的範例來說，我們在 kubernetes 節點上可以觀察到 coredns 這個 pod 實際上是用 root 身份在節點上去運行對應的應用程式。 $ps axuw |grep dns vagrant 4656 0.0 0.0 12916 936 pts/3 S+ 07:38 0:00 grep --color=auto dns root 7886 0.2 0.5 45224 21924 ? Ssl Jul27 5:56 /coredns -conf /etc/coredns/Corefile root 8011 0.2 0.5 45224 22180 ? Ssl Jul27 6:04 /coredns -conf /etc/coredns/Corefile  雖然目前已經有很多的技術被提出來避免 Container 內的應用程式可以突破限制進而影響外部宿主機上面的資源，但是作者認為能夠避免使用 Root 去執行還是避免使用，畢竟安全性這種東西沒有完美的一天。 此外，大部分的容器應用程式都會使用 Root 的身份執行第一個應用程式，也就是 PID=1 的應用程序。如果該應用程式被攻擊者掌控了也就意味者攻擊者獲得了 Root的權限，因此攻擊者就能夠執行更進一步的操作。 文中特別提及一篇文章 Running Non-Root Containers On Openshift, 說到 Bitnami 將他們用到的 Container 從 Root 轉移到 Non-Root 的過程以及方法，並且說明為什麼要轉移到 Non-Root 的執行環境。 最後，作者在此提到了 kubernetes v1.11 的新功能 (PodSecuurityPolicy), 使用了一個簡單的 Yaml 來描述如何限制所有的 Pod 都必須要以 Non-Root 的身份來執行 # Required to prevent escalations to root. allowPrivilegeEscalation: false runAsUser: # Require the container to run without root privileges. rule: 'MustRunAsNonRoot'  最後作者提到，到 User Namespace 隔離的功能完成之前，使用 Non-Root 身份去運行容器依然是一個必要且不可以避免的選擇，為了整個集群以及容器的安全性，還是需要多花時間進行轉換。 ","version":"Next","tagName":"h3"},{"title":"Use Network Policies​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#use-network-policies","content":"講完了跟容器部署的安全性之後，接下來要探討的是Pod之間互相存取的安全性問題 在預設的情況下，kubernetes 集群內的Pod彼此都可以互相透過網路存取，即使是不同 namespace 的容易也可以。 如果想要限制Pod間的網路存取，這邊可以引入 kubernetes 提供的功能 Network Policy, 透過 Network Policy, 管理者可以限制Pod間的存取規則，除了單純的 Pod 之外，還可以限制連接埠,Namespace 以及相關的網路協定 (TCP/UDP). 但是因為 Network Policy 實際上並不是 kubernetes 自行實作的內容，這部分要仰賴 kubernetes 集群使用的 CNI(Container Network Interface). 如果想要嚐鮮的人可以考慮使用看看 Calico, Weave 等有支援 Network Policy 的 CNI。 就如同之前所提供的概念一樣，針對安全性的部分都是盡可能的縮小權限，非必要的就不要開啟。 套用在 Network Policy 這邊則是非必要的連線都不允許通過，所以一開始可以先講所有的連線全部都阻擋起來，再根據需要的連線一條一條的規則打開。 這樣的做法聽起來有效，然而卻非常的複雜與繁瑣，因此這邊又介紹另外一個有趣的工具netassert,一套基於 nmap的工具，可以根據設定去測試當前的封包走向是否運作，藉此減少撰寫 NetworkPolicy 時要不停的打流量來測試當前的 Network Policy是否如設定般運作。 ","version":"Next","tagName":"h3"},{"title":"Scan Images and Run IDS​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#scan-images-and-run-ids","content":"最後一個要探討的安全部分則是針對 Image 本身進行安全性的探討。 在 kubernetes 的流程中，當 Pod 本身的 yaml 送進到 kubernetes 集群中後，會受到 Admission Controllers 一系列的檢查，去確保這次資源的部屬是合法的。 這中間的檢查，有一個非常有趣的就是 webhook. 透過 webhook, 我們可以透過第三方的應用程式去檢查當前的 Image 安全與否，若不安全就阻擋下來，讓該 Pod 沒有機會被部署到 kubernetes 集群內。 關於 webhook 的使用方法，可以參考官方文件的說明 kuubernetes webhook. 至於檢查 Image 這個行為，我們暫時稱作 IDS(Intrusion Detection System), 這邊作者提供了兩套相關的工具來提供這類型的服務，分別是 Clair 以及 Micro Scanner. 感謝網友SCLin指正，上述的說法有誤，Clair 以及 Micro Scanner 這兩個服務都是包在 Image 階段執行 Vulnerability assessment 以及 static code analysis 的工具。 而真正符合 IDS(Intrustion Detection System) 的工具則是Sysdig's Falco這個 Project. 該 Project 的介紹就是 Behavioral Activity Monitoring, 該 IDS 工具能夠偵測下列的行為 A shell is run inside a containerA container is running in privileged mode, or is mounting a sensitive path like /proc from the host.A server process spawns a child process of an unexpected typeUnexpected read of a sensitive file (like /etc/shadow)A non-device file is written to /devA standard system binary (like ls) makes an outbound network connection 除了上述工具外，作者也提到另外一個服務的概念, grafeas, 該服務會維護一個資料庫，該資料庫內會以每個 Image 的 Hash tag 作為索引資料，來記住對應的 Image 是否有任何安全性的問題。 所以可以藉由此工具來掃描當前所屬的 Image 是否內部繼承了任何有問題的 Image 來判斷是否有任何安全性問題。 至於 Zero Day 這種類型的安全性問題也有相關的工具可以幫忙處理，如twistlock,aquasec 以及 Sysdig Secure ","version":"Next","tagName":"h3"},{"title":"The Future​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#the-future","content":"最後，在學習了 Control-Plane 以及 Workload 這種兩類型相關的安全性防護後，作者認為接下來的趨勢則是 Service Mesh. Service Mesh 簡單來說就是在不修改所有已經存在的微服務容器前提下，透過一些第三方的應用程式讓你在不同容器中間建立一條加密的連線，將這些服務串起來。 此外這些容器的連線中也有所謂的 Zero Trust連線，藉由 TLS 相關的加密處理安全性問題，減少了需要 Network Policy 額外的封包處理。 基本上現在最火紅的 Service Mesh 服務就是 Istio，這部分的概念與用法非常龐大，短短一篇文章沒有辦法訴說完畢，之後若有時間可以再好好的寫一篇文章來介紹 Istio 的概念與用法。 ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"11個保護你 Kubernetes 集群的技巧與觀念(下)","url":"/docs/techPost/2018/k8s-security-11tips-ii#summary","content":"總算將這一篇安全性相關的文章給唸完了，裡面的內容其實很短，但是這 11 個重點其實每一個都可以獨立寫成一篇文章來仔細探討。 作者將整個安全性分成三個部分，總共 11 點來探討，這 11 點除了介紹概念之外，也都提供了不少相關 kubernetes 的用法或是額外的工具來協助各式各樣的安全性問題。 最後，我認為安全性真的很重要，如果今天你要提供一個kubernetes平台供外部使用者使用的話，在最後進行到 Production 前一定要花時間注意當前集群內的安全性問題，存 Control-Plane 先確保基礎架設的安全，接下來從 Workloads 方面去限制相關資源部署的權限問題，使用最小的權限來執行所需要的應用程式，降低問題發生時的傷害。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-dns","content":"","keywords":"k8s dns","version":"Next"},{"title":"The Reason Why We Need This​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#the-reason-why-we-need-this","content":"一般的使用情境下，我們的kubernetes 的集群使用方式就如同圖片中紫色/粉紅色(Pod3)區塊一樣，所有的 Pod 如果有任何要存取DNS的需求，都會透過集群內K8S-DNS來處理對應的請求與回覆。 然而在 NFV 的使用情境下，網路變成一個很重要的區塊，整體的效能都取決於該應用程式的設計與整個及集群的網路架構設計。 這部分的應用程式通常都會追求高輸出或是低延遲，同時也要避免這些流量會跟其他無關的流量使用相同的網路線才傳輸，造成該應用程式沒有辦法得到最好的效能。 在這種情況下，通常整個集群就會將網路設計成兩種架構，分別是Control Network, Data Network 兩個完全不同用途的網路架構。 在 Kubernetes 的架構下， Control Network 就類似圖示中的 Cluster Network，負責整個集群之間的溝通。 對應到圖中綠色/橘色(Pod1,Pod2)這兩個區塊則是所謂的 Data Network. 其網路卡本身也是獨立出來，不會與本來的 kubernetes 集群互相衝突，其之間的流量就透過獨立的網路傳輸。 獨立出來的網路架構就意味者，這些特殊應用的Pod基本上沒有辦法跟Kubernetes集群內的Kube-DNS 互連，網路本身就隔絕了這些連線。 此外，這些應用程式可能會在外部有自己的 DNS Server 來使用，所以在這種類型下，我們會希望這些應用程式 (Pod2/Pod3) 能夠使用自定義的 DNS Server 來使用，而 並非集群內建的 DNS Server 。 Pod 透過上述的介紹，已經可以大概了解我們的需求，希望能夠針對 Pod 內去進行 DNS 客製化的設定。 在目前 kubernetes v1.11.0 裡面，對於 Pod 來說，DNS 相關的選項有兩個，分別是 DNSConfig 以及 DNSPolicy。 其中 DNSConfig 代表的是客製化的 DNS 參數，而 DNSPolicy 則是要如何幫 Pod設定預設的 DNS。 接下來我們來看看對於 Pod 來說，到底有哪些 DNS 的設定可以使用 底下範例使用的全部的 kubernetes yaml 檔案都可以在 Hwchiu KubeDemo 找到 ","version":"Next","tagName":"h2"},{"title":"DNSConfig​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#dnsconfig","content":"DNSConfig 意味可以讓操作者延伸當前 Pod 內關於 DNS 的設定，這邊要特別注意的是，我使用的字眼是 延伸 而非 設定，這是因為透過下個章節的 DNSPOlicy, 每個 Pod 都會有一組預設的 DNS 設定。 透過 DNSConfig 我們可以繼續往上疊加相關的 DNS 參數到 Pod 之中。 目前總共支援三個參數可以設定，分別是 nameservers:searches:options: 這三個參數其實就是對應到大家熟悉的 /etc/resolv.conf 裡面的三個參數，這邊就不針對 DNS 進行介紹，不熟悉的朋友可以自行在去 Google 學一下這些參數。 在 Kubernetes裡面，這三個變數都歸屬於 dnsConfig 下面，而 dnsConfig 則是歸屬於 PodSpec 底下，因為 Pod 內所有的 Container 都共享相同的 Network Namespace, 所以網路相關的設定都會共享。 這邊提供一個簡單的 yaml 範例，也可以在 這邊找到 apiVersion: v1 kind: Pod metadata: name: ubuntu-setting namespace: default spec: containers: - image: hwchiu/netutils command: - sleep - &quot;360000&quot; imagePullPolicy: IfNotPresent name: ubuntu restartPolicy: Always dnsConfig: nameservers: - 1.2.3.4 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: &quot;2&quot; - name: edns0  在部屬上述 yaml 檔案後，透過下列指令可以觀察到系統上面 DNS 相關的設定變得非常多。 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ kubectl exec ubuntu-setting cat /etc/resolv.conf  nameserver 10.96.0.10 nameserver 1.2.3.4 search default.svc.cluster.local svc.cluster.local cluster.local ns1.svc.cluster.local my.dns.search.suffix options c vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$  針對 nameserver 可以觀察到多了 1.2.3.4, 而 search 則是多了 ns1.svc.cluster.local my.dns.search.suffix 這兩個自定義的數值，最後 options 則增加了我們範例中的 ndots:2 edns0 DNSConfig 非常簡單直覺，如果你有自己需要的 DNS 參數需要使用，就可以透過這個欄位來設定。 ","version":"Next","tagName":"h2"},{"title":"DNSSPolicy​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#dnsspolicy","content":"前面提過， DNSConfig 提供的是延伸 Pod 內預設的 DNS 設定，而 DNSPolicy 就是決定 Pod 內預設的 DNS 設定有哪些。 目前總共有四個類型可以選擇 NoneDefaultClusterFirstClusterFirstHostNet 接下來針對這四個選項分別介紹 ","version":"Next","tagName":"h2"},{"title":"None​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#none","content":"None 的意思就如同字面上一樣，將會清除 Pod 預設的 DNS 設定，於此狀況下， Kubernetes 不會幫使用者的 Pod 預先載入任何自身邏輯判斷得到的 DNS 設定。 但是為了避免一個 Pod 裡面沒有任何的 DNS 設定存在，因此若使用這個 None的話，則一定要設定 DNSConfig 來描述自定義的 DNS 參數。 使用下列 Yaml 檔案來進行測試 apiVersion: v1 kind: Pod metadata: name: ubuntu-none namespace: default spec: containers: - image: hwchiu/netutils command: - sleep - &quot;360000&quot; imagePullPolicy: IfNotPresent name: ubuntu restartPolicy: Always dnsPolicy: None dnsConfig: nameservers: - 1.2.3.4 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: &quot;2&quot; - name: edns0  部屬完畢後，透過下列指令觀察該 Pod 內的 DNS 設定，可以觀察到跟之前 DNSConfig 的結果有一點差異，這時候只有顯示我們在 Yaml 裡面所設定的那些資訊，集群本身預設則不會幫忙加入任何 DNS 了。 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ kubectl exec ubuntu-none cat /etc/resolv.conf nameserver 1.2.3.4 search ns1.svc.cluster.local my.dns.search.suffix options ndots:2 edns0  ","version":"Next","tagName":"h3"},{"title":"Default​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#default","content":"Default 代表的是希望 Pod 裡面的 DNS 設定請繼承運行該 Pod 的 Node 上的 DNS 設定。 簡單來說就是，該 Pod 的 DNS 設定會跟節點機器完全一致。 接下來使用下列Yaml檔案ˋ來進行測試 apiVersion: v1 kind: Pod metadata: name: ubuntu-default namespace: default spec: containers: - image: hwchiu/netutils command: - sleep - &quot;360000&quot; imagePullPolicy: IfNotPresent name: ubuntu restartPolicy: Always dnsPolicy: Default  首先，我們先觀察本機上面的 DNS 設定，這邊因為我的 kubernetes 集群只有一台，所以我可以確保該 Pod 一定會運行在我這台機器上。 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ cat /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.0.2.3  這時可以觀察到，機器上本來的DNS設定非常簡單，只有單純的 10.0.2.3。 接下來我們觀察該 Pod 內的 DNS 設定 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ kubectl exec ubuntu-default cat /etc/resolv.conf nameserver 10.0.2.3  可以看到這兩個的 DNS 設定是完全一致的，該 Pod內的 DNS 設定已經直接繼承自該運行節點上的設定了。 ","version":"Next","tagName":"h3"},{"title":"ClusterFirst​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#clusterfirst","content":"相對於上述的 Default 設定， ClusterFisrt 是完全相反的操作，會預先把 kube-dns 的資訊當做預設參數寫入到該 Pod 內的 DNS 設定。 特別注意的是， ClusterFirst 是預設的行為，若沒有在 Pod 內特別描述 PodPolicy, 則預設會以 ClusterFirst 來看待 接下來使用下列Yaml檔案來觀察結果看 apiVersion: v1 kind: Pod metadata: name: ubuntu-clusterfirst namespace: default spec: containers: - image: hwchiu/netutils command: - sleep - &quot;360000&quot; imagePullPolicy: IfNotPresent name: ubuntu restartPolicy: Always dnsPolicy: ClusterFirst  首先，因為 ClusterFirst 使用的是 kube-dns 的 clusterIP 作為 DNS 的位置，所以我們先透過下列指令觀察 kube-dns 的 IP 位置 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ kubectl -n kube-system get svc kube-dns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 19h  根據上述結果可以觀察到，當前 kube-dns 使用的 clusterIP 是 10.96.0.10。 接下來去觀察對應 Pod 內的 DNS 設定 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ kubectl exec ubuntu-clusterfirst cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$  這邊可以看到一些資訊 nameserver 對應到的就是 kube-dns 的 clusterIPsearch 這邊則會產生不少個關於 kubernetes 集群內使用的規則，這邊要特定注意到 default 這個數值會隨者該 Pod 所屬的 namespace 有所改變 在 kubernetes 中，每個 service 都會打開一個 $service.$namespace.svc.cluster.local 的 DNS. 對於 Pod 來說, 相同 namespace 可以直接用 $service 直接查詢到，若是不同的 namespace則要輸入 $service.$namespace這邊就是依賴這些 search 來達成這些功能的 此外，ClusterFirst 有一個衝突，如果你的 Pod 有設定 HostNetwork=true 的話，則 ClusterFirst 就會變成 Default 來使用。 ","version":"Next","tagName":"h3"},{"title":"HostNetwork​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#hostnetwork","content":"使用下列 Yaml 來觀察測試一下 apiVersion: v1 kind: Pod metadata: name: ubuntu-hostnetwork-policy-default namespace: default spec: containers: - image: hwchiu/netutils command: - sleep - &quot;360000&quot; imagePullPolicy: IfNotPresent name: ubuntu hostNetwork: true restartPolicy: Always dnsPolicy: ClusterFirst  vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ kubectl exec ubuntu-hostnetwork-policy-default cat /etc/resolv.conf nameserver 10.0.2.3  可以觀察到，這個情況下， DNS 的設定會被設定回節點上的設定。 這邊稍微來解釋一下這個設計上的原理以及流程 因為設定 HostNetwork=true, 會讓該 Pod 與該節點共用相同的網路空間(網卡/路由等功能)相關的 /etc/hosts 就不會被掛載到 Pod 裡面這種情況下就會使用節點本身的 DNS 設定，也就是 Default 的概念 在這種情況下，就會有人想要問，我如果刻意的想要這樣設定不行嘛? 原先的設計中，是沒有辦法刻意處理的，原因是當 Pod yaml 檔案送出去後，在發現沒有設定 PodPolicy 的情況下，會自動幫你把該PodPolicy 補上 ClusterFirst 的數值。 然後最後面的程式處理邏輯中，其實並沒有辦法分別下列兩種情況 HostNetwork 我希望走 Host DNS HostNetwork &amp; PodPolicy=ClusterFirst. 我希望走 ClusterIP DNS 上述兩種情況對於後端的程式來看都長得一樣HostNetwork &amp; PodPolicy=ClusterFirst.因此完全沒有辦法分辨 我們可以直接從 Kubernetes 程式碼 來閱讀一下其運作流程 func getPodDNSType(pod *v1.Pod) (podDNSType, error) { dnsPolicy := pod.Spec.DNSPolicy switch dnsPolicy { case v1.DNSNone: if utilfeature.DefaultFeatureGate.Enabled(features.CustomPodDNS) { return podDNSNone, nil } // This should not happen as kube-apiserver should have rejected // setting dnsPolicy to DNSNone when feature gate is disabled. return podDNSCluster, fmt.Errorf(fmt.Sprintf(&quot;invalid DNSPolicy=%v: custom pod DNS is disabled&quot;, dnsPolicy)) case v1.DNSClusterFirstWithHostNet: return podDNSCluster, nil case v1.DNSClusterFirst: if !kubecontainer.IsHostNetworkPod(pod) { return podDNSCluster, nil } // Fallback to DNSDefault for pod on hostnetowrk. fallthrough case v1.DNSDefault: return podDNSHost, nil } // This should not happen as kube-apiserver should have rejected // invalid dnsPolicy. return podDNSCluster, fmt.Errorf(fmt.Sprintf(&quot;invalid DNSPolicy=%v&quot;, dnsPolicy)) }  這邊可以看到一旦是 DNSClusterFirst 的情況下，若有設定 HostNetwork, 最後就會直節回傳 podDNSHost 節點的 DNS 設定回去。 為了解決上述的問題，所以引進了一個新的型態 ClusterFirstHostNet ","version":"Next","tagName":"h3"},{"title":"ClusterFirstHostNet​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#clusterfirsthostnet","content":"ClusterFirstHostNet 用途非常簡單，我希望滿足使用 HostNetwork 同時使用 kube-dns 作為我 Pod 預設 DNS 的設定。 根據上面的程式碼也可以觀察到  case v1.DNSClusterFirstWithHostNet: return podDNSCluster, nil  其實只要將 DNSPolicy 設定為 ClusterFirstHostNet, 就會一律回傳 kube-dns 這種 clusterIP 的形式。 這邊我們使用 下列 Yaml 檔案 apiVersion: v1 kind: Pod metadata: name: ubuntu-hostnetwork-policy namespace: default spec: containers: - image: hwchiu/netutils command: - sleep - &quot;360000&quot; imagePullPolicy: IfNotPresent name: ubuntu hostNetwork: true restartPolicy: Always dnsPolicy: ClusterFirstWithHostNet  部屬完畢後執行下列指令觀察該 Pod 的狀態 vagrant@vortex-dev:~/kubeDemo/dns/dnsSetting$ kubectl exec ubuntu-hostnetwork-policy cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5  可以發現這時候的 DNS 設定就會使用的是 ClusterIP 的設定了。 Summary 目前 Pod 有提供兩種設定 DNS 的參數來使用者來管理 DNS 相關的參數。 DNSPolicyDNSConfig ","version":"Next","tagName":"h2"},{"title":"DNSPolicy​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#dnspolicy","content":"代表的是該 Pod 預設的 DNS 設定，目前總共有四種類型可以使用 Default 代表的是繼承自運行節點的 DNS 設定 None 代表的是完全不設定任何 DNS 的資訊，所有的 DNS 都依賴 DNSConfig 欄位來描述 (若採用 None, 一定要設定 DNSConfig) ClusterFirst 代表是使用 kube-dns 提供的 clusterIP 作為預設的 DNS 設定同時若使用者沒有在 Pod 內去描述 DNSConfig ，則預設會使用 ClusterFirst 這個設定 ClusterFirstWithHostNet 這個選項就是特別針對 HostNetwork=true 創立的可以提供使用節點網路但是同時又使用 kube-dns 提供的 clusterIP 作為其 DNS 的設定。 ","version":"Next","tagName":"h2"},{"title":"DNSConfig​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns#dnsconfig-1","content":"DNSConfig 可以讓使用者直接輸入 DNS 相關的參數，該參數會擴充該 Pod原本的 DNS 設定檔案。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-dns-ii","content":"","keywords":"","version":"Next"},{"title":"維持 /etc/resolv.conf​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-ii#維持-etcresolvconf","content":"首先觀察到，在所有的集群內，只要 /etc/resolv.conf 有資料的話，則 dnsmasq 的 /etc/resolv.conf 都會是一致的  但是接下來若將 /etc/resolv.conf 給清空，這時候不同集群表現出來的結果卻完全不同了。 ","version":"Next","tagName":"h2"},{"title":"清空 /etc/resolv.conf​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-ii#清空-etcresolvconf","content":"第一種案例如下，dnsmasq 內的則是自動的被捕上了 8.8.8.8 以及 8.8.4.4  第二種案例如下，dnsmasq 內的則是自動的被捕上了跟 kubernetes kube-dns 有關的資訊，這些資料看起來就跟 dnsPolicy:clusterFirst 完全一樣  歸納結果 總和以上原因，目前可以至少知道，只要 /etc/resolv.conf 有資料的話， dnsmasq 內的 /etc/resolv.conf 就會與該資料一致。 但是若 /etc/resolv.conf 沒有資料的話， 則 dnsmasq 內的 /etc/resolv.conf 目前卻出現兩種可能性。 思路(二) 接下來為了釐清這個問題，就開始認真的翻文件以及相關的程式碼，最後終於找到了影響的原因 相關的程式碼會在下篇文章解釋一切的來龍去脈 另外一個會影響數值的就是 dockerd 本身啟動的參數 根據上述兩個有差異的 kubernetes 集群去分析，發現這兩個 kubernetes 集群內本身運行的 dockerd 有參數上的差異 其中一個是非常乾淨，單純的設定連線資訊，另外一個則是設定了很多DNS 的數值 如下圖 而且可以發現這些參數的數值都跟上述 dnsmaq 裡面的數值完全一致  歸納結果 目前觀察的結果，當 kubernetes 創見 Pod 且 dnsPolicy=default 時，其內部容器 /etc/resolv.conf 的數值會受到兩個參數影響 該節點上本身的 /etc/resolv.conf該節點上 dockerd 運行的 dns 參數 node\\dockerd\t有設定 DNS\t沒設定 DNS有數值\tnode\tnode 沒有數值\tdockerd\t8.8.8.8 基本上只要節點上的 /etc/resolv.conf 有資料，就直接採用 若節點上的 /etc/resolv.conf 沒有資料，則會根據 dockerd 本身有沒有額外的 DNS 參數 來決定，若有則使用，沒有則採用 8.8.8.8 以及 8.8.4.4 將上述的結果用更完整的說法就是kubernetes 會先嘗試使用 /etc/resolv.conf 的資料，但是若發現 /etc/resolv.conf 是空的，這時候就會去依賴 dockerd 幫忙產生的 /etc/resolv.conf ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-dns-iii","content":"","keywords":"","version":"Next"},{"title":"kube runtime​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iii#kube-runtime","content":"","version":"Next","tagName":"h2"},{"title":"createPodSandbox​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iii#createpodsandbox","content":"整個函式如下 // createPodSandbox creates a pod sandbox and returns (podSandBoxID, message, error). func (m *kubeGenericRuntimeManager) createPodSandbox(pod *v1.Pod, attempt uint32) (string, string, error) { podSandboxConfig, err := m.generatePodSandboxConfig(pod, attempt) if err != nil { message := fmt.Sprintf(&quot;GeneratePodSandboxConfig for pod %q failed: %v&quot;, format.Pod(pod), err) glog.Error(message) return &quot;&quot;, message, err } // Create pod logs directory err = m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755) if err != nil { message := fmt.Sprintf(&quot;Create pod log directory for pod %q failed: %v&quot;, format.Pod(pod), err) glog.Errorf(message) return &quot;&quot;, message, err } podSandBoxID, err := m.runtimeService.RunPodSandbox(podSandboxConfig) if err != nil { message := fmt.Sprintf(&quot;CreatePodSandbox for pod %q failed: %v&quot;, format.Pod(pod), err) glog.Error(message) return &quot;&quot;, message, err } return podSandBoxID, &quot;&quot;, nil }  ","version":"Next","tagName":"h3"},{"title":"generatePodSandboxConfig​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iii#generatepodsandboxconfig","content":"generatePodSandboxConfig 這個函式很簡單，基本上就是從 Pod 本身 yaml 檔案裡面去讀取設定，根據這些設定來處理相關的資訊。 我們在意的部份就是 DNS 相關，所以以下就擷取 DNS 相關的函式內容 // createPodSandbox creates a pod sandbox and returns (podSandBoxID, message, error). func (m *kubeGenericRuntimeManager) generatePodSandboxConfig(pod *v1.Pod, attempt uint32) (*runtimeapi.PodSandboxConfig, error) { .... dnsConfig, err := m.runtimeHelper.GetPodDNS(pod) if err != nil { return nil, err } podSandboxConfig.DnsConfig = dnsConfig .....  這邊可以看到呼叫了 GetPodDNS 來取得對應的設定，然後我們就一路往下追 ","version":"Next","tagName":"h3"},{"title":"GetPodDNS​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iii#getpoddns","content":"func (c *Configurer) GetPodDNS(pod *v1.Pod) (*runtimeapi.DNSConfig, error) { dnsConfig, err := c.getHostDNSConfig(pod) if err != nil { return nil, err } dnsType, err := getPodDNSType(pod) if err != nil { glog.Errorf(&quot;Failed to get DNS type for pod %q: %v. Falling back to DNSClusterFirst policy.&quot;, format.Pod(pod), err) dnsType = podDNSCluster } switch dnsType { case podDNSNone: // DNSNone should use empty DNS settings as the base. dnsConfig = &amp;runtimeapi.DNSConfig{} case podDNSCluster: ... case podDNSHost: // When the kubelet --resolv-conf flag is set to the empty string, use // DNS settings that override the docker default (which is to use // /etc/resolv.conf) and effectively disable DNS lookups. According to // the bind documentation, the behavior of the DNS client library when // &quot;nameservers&quot; are not specified is to &quot;use the nameserver on the // local machine&quot;. A nameserver setting of localhost is equivalent to // this documented behavior. if c.ResolverConfig == &quot;&quot; { switch { case c.nodeIP == nil || c.nodeIP.To4() != nil: dnsConfig.Servers = []string{&quot;127.0.0.1&quot;} case c.nodeIP.To16() != nil: dnsConfig.Servers = []string{&quot;::1&quot;} } dnsConfig.Searches = []string{&quot;.&quot;} } } ... return c.formDNSConfigFitsLimits(dnsConfig, pod), nil }  這邊的邏輯非常簡單 先透過 getHostDNSConfig 取得當前 /etc/resolv.conf 的內容，並且存於變數 dnsCOnfig接下來透過 getPodDNSType 該函式從 Pod Yaml 內讀取對應的 dnsPolicy 資訊，來決定當前 Pod 的 dnsType第三步驟就是最重要的，根據當前 dnsType 來決定要如何處理 dnsConfig. 由於 podDNSCluster 的程式碼太多了，我這邊就先忽略掉，我們只要專注看 DNSHost 的案例。 裡面非常簡單，如果你當初運行 kubelet 的時候有特別設定不要使用 /etc/resolv.conf 的話，就會把 dnsConfig 補上一個 127.0.0.1 的資訊，不過這個 Case 我暫時還沒想到，可能就是機器本身就是一個 DNSServer 的情況吧。將 dnsConfig 回傳出去 所以在正常情況下，我們的 dnsConfig 理論上就會是節點上 /etc/resolv.conf 內的資料。 到這邊為止，就是 kube-runtime 自行處理的部份，並且將得到的 dnsConfig 放置到變數 podSandboxConfig.DnsConfig 之中。 接下來我們來看一下 dockershim 裡面的 RunPodSandbox 會怎麼處理這些 DNS 的資訊。 ","version":"Next","tagName":"h3"},{"title":"DocekrShim​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iii#docekrshim","content":"","version":"Next","tagName":"h2"},{"title":"RunPodSandbox​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iii#runpodsandbox","content":"由於 RunPodSandbox 的函式非常長，這邊就擷取跟 DNS 相關的部份來觀察 func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) { config := r.GetConfig() .... // Rewrite resolv.conf file generated by docker. // NOTE: cluster dns settings aren't passed anymore to docker api in all cases, // not only for pods with host network: the resolver conf will be overwritten // after sandbox creation to override docker's behaviour. This resolv.conf // file is shared by all containers of the same pod, and needs to be modified // only once per pod. if dnsConfig := config.GetDnsConfig(); dnsConfig != nil { containerInfo, err := ds.client.InspectContainer(createResp.ID) if err != nil { return nil, fmt.Errorf(&quot;failed to inspect sandbox container for pod %q: %v&quot;, config.Metadata.Name, err) } if err := rewriteResolvFile(containerInfo.ResolvConfPath, dnsConfig.Servers, dnsConfig.Searches, dnsConfig.Options); err != nil { return nil, fmt.Errorf(&quot;rewrite resolv.conf failed for pod %q: %v&quot;, config.Metadata.Name, err) } } .... return resp, nil }  其實這邊的程式碼已經有了滿不錯的註解來解釋相關的行為。 1.之前透過 kube run-time 得到的 dns 設定都只是一個存在於記憶體內的一堆資料而已，其實都還沒有跟真正的容器有任何關係。 2.透過 dockershim 的操作，這時候其實已經將對應的容器創見完畢，而且該容器內/etc/resolv.conf的內容是完全依賴 docker container 決定 3.kubernetes 這邊的邏輯非常簡單，如果我之前有產生過任何 dns 的設定，就直接將 docker 產生出來的 /etc/resolv.conf 給直接覆蓋掉。 所以我們可以看一下 rewriteResolvFile 這個函式 ","version":"Next","tagName":"h3"},{"title":"rewriteResolvFile​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iii#rewriteresolvfile","content":"// rewriteResolvFile rewrites resolv.conf file generated by docker. func rewriteResolvFile(resolvFilePath string, dns []string, dnsSearch []string, dnsOptions []string) error { ... var resolvFileContent []string for _, srv := range dns { resolvFileContent = append(resolvFileContent, &quot;nameserver &quot;+srv) } if len(dnsSearch) &gt; 0 { resolvFileContent = append(resolvFileContent, &quot;search &quot;+strings.Join(dnsSearch, &quot; &quot;)) } if len(dnsOptions) &gt; 0 { resolvFileContent = append(resolvFileContent, &quot;options &quot;+strings.Join(dnsOptions, &quot; &quot;)) } if len(resolvFileContent) &gt; 0 { resolvFileContentStr := strings.Join(resolvFileContent, &quot;\\n&quot;) resolvFileContentStr += &quot;\\n&quot; glog.V(4).Infof(&quot;Will attempt to re-write config file %s with: \\n%s&quot;, resolvFilePath, resolvFileContent) if err := rewriteFile(resolvFilePath, resolvFileContentStr); err != nil { glog.Errorf(&quot;resolv.conf could not be updated: %v&quot;, err) return err } } return nil }  這邊也不能理解，就是根據之前存放於 podSandboxConfig.DnsConfig 內各式各樣關於 DNS 的設定組合起來，然後直接覆蓋掉本來的 /etc/resolv.conf. 結論 到這邊，我們已經把整個問題給釐清一半了 重新看一次之前的結論 kubernetes 會先嘗試使用節點上 /etc/resolv.conf 的資料，但是若發現 /etc/resolv.conf 是空的，這時候就會去依賴 dockerd 幫忙產生的 /etc/resolv.conf 我們的推論跟我們程式碼觀察的結果是完全吻合的，再 dnsPolicy=default 的前提下，只要 kubernetes 只要能夠獲得合法的 /etc/resolv.conf 就會使用，否則直接使用 docekr container 所創造的 /etc/resolv.conf. 但是這邊還有留下一個謎點，到底 dockerd 的設定是如何影響 /etc/resolv.conf 以及 8.8.8.8/8.8.4.4 是如何出現的? 由於再寫下去本篇文章會愈來愈長，所以決定將 docker container 相關的程式碼分析再寫一篇文章來處理。 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-dns-iiii","content":"","keywords":"","version":"Next"},{"title":"ContainerStart​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#containerstart","content":" func (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error { ... // check if hostConfig is in line with the current system settings. // It may happen cgroups are umounted or the like. if _, err = daemon.verifyContainerSettings(container.OS, container.HostConfig, nil, false); err != nil { return errdefs.InvalidParameter(err) } // Adapt for old containers in case we have updates in this function and // old containers never have chance to call the new function in create stage. if hostConfig != nil { if err := daemon.adaptContainerSettings(container.HostConfig, false); err != nil { return errdefs.InvalidParameter(err) } } return daemon.containerStart(container, checkpoint, checkpointDir, true) }  這邊的邏輯基本上就是處理一些 Container 相關設定，最後直接呼叫一個私有函式 containerStart 來進行更進一步的處理 ","version":"Next","tagName":"h2"},{"title":"containerStart​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#containerstart-1","content":" // containerStart prepares the container to run by setting up everything the // container needs, such as storage and networking, as well as links // between containers. The container is left waiting for a signal to // begin running. func (daemon *Daemon) containerStart(container *container.Container, checkpoint string, checkpointDir string, resetRestartManager bool) (err error) { .... if err := daemon.conditionalMountOnStart(container); err != nil { return err } if err := daemon.initializeNetworking(container); err != nil { return err } .... if daemon.saveApparmorConfig(container); err != nil { return err } ... err = daemon.containerd.Create(context.Background(), container.ID, spec, createOptions) if err != nil { return translateContainerdStartErr(container.Path, container.SetExitCode, err) } // TODO(mlaventure): we need to specify checkpoint options here pid, err := daemon.containerd.Start(context.Background(), container.ID, checkpointDir, container.StreamConfig.Stdin() != nil || container.Config.Tty, container.InitializeStdio) ... container.SetRunning(pid, true) container.HasBeenManuallyStopped = false container.HasBeenStartedBefore = true daemon.setStateCounter(container) daemon.initHealthMonitor(container) ... }  這個函式非常的重要，可以看一下該函式的註解 // containerStart prepares the container to run by setting up everything the // container needs, such as storage and networking, as well as links // between containers. The container is left waiting for a signal to // begin running. 再這個函式內會創建好相關的容器，並且會將該容器用到的相關資源(儲存/網路)等都準備好 ，由於我們要觀察的是 DNS 相關的資訊，所以我們要繼續往 initializeNetworking 的方向往下追。 ","version":"Next","tagName":"h2"},{"title":"initializeNetworking​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#initializenetworking","content":" func (daemon *Daemon) initializeNetworking(container *container.Container) error { var err error if container.HostConfig.NetworkMode.IsContainer() { ... } if container.HostConfig.NetworkMode.IsHost() { ... } if err := daemon.allocateNetwork(container); err != nil { return err } return container.BuildHostnameFile() }  接下來都是根據各種 Networking 相關的設定來處理，前面兩個的判斷處理則是根據當初創建該容器時，有沒有指令 --network=xxxx 來特別處理，若有設定 --network=host 或是 --network=container:xxxx 會有一些額外的處理。 因為這些情境跟我們的 Kubernetes 的使用方法不同，我們不會走到這邊的判斷，而是直接會走到下面的 allocateNetwork 來開始準備網路相關的資訊。 ","version":"Next","tagName":"h2"},{"title":"allocateNetwork​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#allocatenetwork","content":"func (daemon *Daemon) allocateNetwork(container *container.Container) error { ... // always connect default network first since only default // network mode support link and we need do some setting // on sandbox initialize for link, but the sandbox only be initialized // on first network connecting. defaultNetName := runconfig.DefaultDaemonNetworkMode().NetworkName() if nConf, ok := container.NetworkSettings.Networks[defaultNetName]; ok { cleanOperationalData(nConf) if err := daemon.connectToNetwork(container, defaultNetName, nConf.EndpointSettings, updateSettings); err != nil { return err } } ... return nil }  再這個函式內，首先會嘗試使用預設的網路型態，這邊指的就是在創建容器所下的參數 --net=xxx，而預設的類型就是 bridge. 所以接下來就很直覺的去呼叫 connectToNetwork 來進行下一階段的處理 ","version":"Next","tagName":"h2"},{"title":"connectToNetwork​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#connecttonetwork","content":"func (daemon *Daemon) connectToNetwork(container *container.Container, idOrName string, endpointConfig *networktypes.EndpointSettings, updateSettings bool) (err error) { start := time.Now() if container.HostConfig.NetworkMode.IsContainer() { return runconfig.ErrConflictSharedNetwork } if containertypes.NetworkMode(idOrName).IsBridge() &amp;&amp; daemon.configStore.DisableBridge { container.Config.NetworkDisabled = true return nil } .... sb := daemon.getNetworkSandbox(container) createOptions, err := buildCreateEndpointOptions(container, n, endpointConfig, sb, daemon.configStore.DNS) if err != nil { return err } .... if sb == nil { options, err := daemon.buildSandboxOptions(container) if err != nil { return err } sb, err = controller.NewSandbox(container.ID, options...) if err != nil { return err } updateSandboxNetworkSettings(container, sb) } .... return nil }  connectToNetwork 並不是只有創建新的容器時才會使用，所以這邊還會進行一些相關的參數檢查。 由於我們是第一次串件該容器，所以容器所對應的沙盒 SandBox 會是空的。 最後根據空的沙盒，決定透過 controller.NewSanbox 去創建一個沙盒。 值得注意的是，這邊有一個 buildSandboxOptions 會把其他用到的參數都重新整理一次，然後傳入到 controller.NewSandbox 這邊去處理。 ","version":"Next","tagName":"h2"},{"title":"NewSandbox​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#newsandbox","content":"func (c *controller) NewSandbox(containerID string, options ...SandboxOption) (Sandbox, error) { if containerID == &quot;&quot; { return nil, types.BadRequestErrorf(&quot;invalid container ID&quot;) } var sb *sandbox ... // Create sandbox and process options first. Key generation depends on an option if sb == nil { sb = &amp;sandbox{ id: sandboxID, containerID: containerID, endpoints: []*endpoint{}, epPriority: map[string]int{}, populatedEndpoints: map[string]struct{}{}, config: containerConfig{}, controller: c, extDNS: []extDNSEntry{}, } } sb.processOptions(options...) ... if err = sb.setupResolutionFiles(); err != nil { return nil, err } return sb, nil }  特別注意一下，當我們到這邊執行相關函式的時候，我們的專案已經從docker-ce遷移到libnetwork了。 這個函式非常的長，描述的創建新的 Sandbox 期間需要注意的所有事項，這邊首先會透過sb.processOptions 去設定相關變數的數值，這邊要特別注意的是其實每一個 options 本身是對應到一個 function pointer。 如果我們當初有透過 dockerd 去設定相關的 DNS 設定的話，這邊其實實際上會賦值到 sb.config.dnsList, sb.config.dnsSearchList 以及 sb.config.dnsOptionsList func (sb *sandbox) setupResolutionFiles() error { if err := sb.buildHostsFile(); err != nil { return err } if err := sb.updateParentHosts(); err != nil { return err } return sb.setupDNS() }  這邊其實非常簡單，我們的主要目標終於出現了!!!setupDNS 意思就如同名稱一樣直接，就是設定該沙盒內DNS的設定。 ","version":"Next","tagName":"h2"},{"title":"setupDNS​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#setupdns","content":"func (sb *sandbox) setupDNS() error { var newRC *resolvconf.File .... originResolvConfPath := sb.config.originResolvConfPath if originResolvConfPath == &quot;&quot; { // if not specified fallback to default /etc/resolv.conf originResolvConfPath = resolvconf.DefaultResolvConf } currRC, err := resolvconf.GetSpecific(originResolvConfPath) if err != nil { if !os.IsNotExist(err) { return err } // it's ok to continue if /etc/resolv.conf doesn't exist, default resolvers (Google's Public DNS) // will be used currRC = &amp;resolvconf.File{} logrus.Infof(&quot;/etc/resolv.conf does not exist&quot;) } if len(sb.config.dnsList) &gt; 0 || len(sb.config.dnsSearchList) &gt; 0 || len(sb.config.dnsOptionsList) &gt; 0 { var ( err error dnsList = resolvconf.GetNameservers(currRC.Content, types.IP) dnsSearchList = resolvconf.GetSearchDomains(currRC.Content) dnsOptionsList = resolvconf.GetOptions(currRC.Content) ) if len(sb.config.dnsList) &gt; 0 { dnsList = sb.config.dnsList } if len(sb.config.dnsSearchList) &gt; 0 { dnsSearchList = sb.config.dnsSearchList } if len(sb.config.dnsOptionsList) &gt; 0 { dnsOptionsList = sb.config.dnsOptionsList } newRC, err = resolvconf.Build(sb.config.resolvConfPath, dnsList, dnsSearchList, dnsOptionsList) if err != nil { return err } // After building the resolv.conf from the user config save the // external resolvers in the sandbox. Note that --dns 127.0.0.x // config refers to the loopback in the container namespace sb.setExternalResolvers(newRC.Content, types.IPv4, false) } else { // If the host resolv.conf file has 127.0.0.x container should // use the host resolver for queries. This is supported by the // docker embedded DNS server. Hence save the external resolvers // before filtering it out. sb.setExternalResolvers(currRC.Content, types.IPv4, true) // Replace any localhost/127.* (at this point we have no info about ipv6, pass it as true) if newRC, err = resolvconf.FilterResolvDNS(currRC.Content, true); err != nil { return err } // No contention on container resolv.conf file at sandbox creation if err := ioutil.WriteFile(sb.config.resolvConfPath, newRC.Content, filePerm); err != nil { return types.InternalErrorf(&quot;failed to write unhaltered resolv.conf file content when setting up dns for sandbox %s: %v&quot;, sb.ID(), err) } } // Write hash if err := ioutil.WriteFile(sb.config.resolvConfHashFile, []byte(newRC.Hash), filePerm); err != nil { return types.InternalErrorf(&quot;failed to write resolv.conf hash file when setting up dns for sandbox %s: %v&quot;, sb.ID(), err) } return nil }  這個函式會針對一些跟 DNS 相關的參數來進行處理，包含了 dnsServerdnsSearchdnsOptionsresolveConf 這邊的運作邏輯如下 先根據參數resolveConf來讀取當前 DNS 的全部設定如果使用者有自行設定 DNS 的參數，就會全面使用這邊的設定，完全忽略(1)載入的設定 2.1 這邊最後會呼叫 resolvconf.Build 將參數的設定直接覆寫到容器內的 /etc/resolv.conf如果使用者沒有自行設定 DNS 的話，就會針對 (1) 載入的設定進行一次過濾 3.1 針對 127.0.0.1/8 之類的進行過濾 FilterResolveDNS3.2 過濾後若發現是空的，則補上 8.8.8.8/8.8.4.4 這樣就來看一下 FilterResolveDNS 怎麼處理 DNS ","version":"Next","tagName":"h2"},{"title":"FilterResolveDNS​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-dns-iiii#filterresolvedns","content":" // FilterResolvDNS cleans up the config in resolvConf. It has two main jobs: // 1. It looks for localhost (127.*|::1) entries in the provided // resolv.conf, removing local nameserver entries, and, if the resulting // cleaned config has no defined nameservers left, adds default DNS entries // 2. Given the caller provides the enable/disable state of IPv6, the filter // code will remove all IPv6 nameservers if it is not enabled for containers // func FilterResolvDNS(resolvConf []byte, ipv6Enabled bool) (*File, error) { cleanedResolvConf := localhostNSRegexp.ReplaceAll(resolvConf, []byte{}) // if IPv6 is not enabled, also clean out any IPv6 address nameserver if !ipv6Enabled { cleanedResolvConf = nsIPv6Regexp.ReplaceAll(cleanedResolvConf, []byte{}) } // if the resulting resolvConf has no more nameservers defined, add appropriate // default DNS servers for IPv4 and (optionally) IPv6 if len(GetNameservers(cleanedResolvConf, types.IP)) == 0 { logrus.Infof(&quot;No non-localhost DNS nameservers are left in resolv.conf. Using default external servers: %v&quot;, defaultIPv4Dns) dns := defaultIPv4Dns if ipv6Enabled { logrus.Infof(&quot;IPv6 enabled; Adding default IPv6 external servers: %v&quot;, defaultIPv6Dns) dns = append(dns, defaultIPv6Dns...) } cleanedResolvConf = append(cleanedResolvConf, []byte(&quot;\\n&quot;+strings.Join(dns, &quot;\\n&quot;))...) } hash, err := ioutils.HashData(bytes.NewReader(cleanedResolvConf)) if err != nil { return nil, err } return &amp;File{Content: cleanedResolvConf, Hash: hash}, nil }  首先先呼叫 ReplaceAll 把所有 localhost 127.0.0.0/8 相關的 IP 都清空。清空之後，若發現這時候沒有 DNS 的話，直接透過 dns := defaultIPv4Dns 補上預設的 DNS (8.8.8.8/8.8.4.4) summary 當每次創建新容器時，最後會依賴到 libnetwork 內跟 DNS 相關的參數來設定 如果使用者有自行設定 DNS 的參數，就會全面使用這邊的設定，完全忽略(1)載入的設定 這邊最後會呼叫 resolvconf.Build 將參數的設定直接覆寫到容器內的 /etc/resolv.conf 如果使用者沒有自行設定 DNS 的話，就會針對 (1) 載入的設定進行一次過濾 針對 127.0.0.1/8 之類的進行過濾 FilterResolveDNS過濾後若發現是空的，則補上 8.8.8.8/8.8.4.4 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-fedora","content":"","keywords":"","version":"Next"},{"title":"Install​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-fedora#install","content":"Since we are use the Fedora now, the helm has not been takend by any dnf repos, we need to download the binary from the official website and you can use the following script to download the binary automatically. $ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &gt; get_helm.sh $ chmod 700 get_helm.sh $ ./get_helm.sh  You will be informed to use the helm init to initial the helm environment in your kubernetes cluster after executing the get_helm.sh, but don't do that now. Since we use the kuberadm to install the kubernetes and it use the RBAC mode as default. We need to add another parameter for the RBAC mode. You need to prepare the RBAC config for your helm chart service before you using it to install any packages. And you can find the whole tutoral here After create the RBAC config, use the follwing command to init your helm helm init --service-account tiller  Test Now, using the following command to install the nginx ingress package by helm. helm install --name my-release stable/nginx-ingress  Use the kubectl to see all the kubernetes resource we installed by the helm kubectl get all | grep my-release  ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-service-i","content":"","keywords":"","version":"Next"},{"title":"ClusterIP​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-i#clusterip","content":"ClusterIP 的意思就是只有叢集內的應用程式/節點可以透過該組 FQDN 去存取背後的服務。 在此情況下，除了透過kubernetes去部屬的應用程式外，預設情況下都沒有辦法透過該FQDN去存取，即使你直接使用了kubernetes dns來問到對應的IP地址也沒有辦法。 這邊指的是預設情況下，如果夠懂網路以及背後原理，當然還是有辦法可以從外面存取到這些服務的 ","version":"Next","tagName":"h2"},{"title":"NodePort​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-i#nodeport","content":"NodePort 本身包含了 ClusterIP 的能力，此外多提供了一種能力讓非叢集的應用程式/節點也有辦法存取叢集內的應用程式。 舉例來說，我們可以部屬多個網頁伺服器，然後透過 NodePort 的方式讓外部的電腦(瀏覽器）來存取這些在 kubernetes 叢集內的網頁伺服器。 由前面我們知道，kubernetes service 務提供的 FQDN 只能供叢集內的應用程式去存取。 那要如何達到非叢集的應用程式也能夠存取叢集內的應用程式? 這邊就如同其字面NodePort一樣，任何非叢集內的應用程式都可以透過存取叢集節點上的特定Port轉而存取到叢集內的應用服務。 詳細的運作原理留到下篇文章在好好的跟大家探討與分享 最後用一張圖片來說明 ClusterIP 以及 NodePort 兩者的關係 在圖示中，紫色的K8S App就是所謂的叢集內應用程式，而紅色的HostApp就是所謂非叢集的應用程式。 ClusterIP: 只有紫色的應用程式以及叢集內的節點可以存取NodePort: 紫色跟紅色的應用程式都可以存取，只是存取的方式些許不同。注意的是該非叢集內的應用程式可以運行在任何節點上，只要有辦法透過網路與kubernetes叢集內集點相連即可。 How To Use It 接下來使用kubeDemo專案內的內容來展示一下如何使用 ClusterIP 以及 對應的 NodePort 服務。 在此範例中，我採用 Nginx 作為一個後端的服務，然後用 ubuntu 當做一個叢集內的應用程式，想要透過 Service 的方式存取到 Nginx  首先，我們先部屬相關的應用程式Ngnix 以及用來測試用的 ubuntu vortex-dev:04:10:45 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl apply -f services/deployment/nginx.yml deployment.apps/k8s-nginx created vortex-dev:04:16:17 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl apply -f services/application/ubuntu.yml pod/ubuntu created  部屬完畢後，接下來我們要來部屬相關的 Cluster-IP 以及 NodePort 兩個服務 ","version":"Next","tagName":"h2"},{"title":"Deploy ClusterIP​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-i#deploy-clusterip","content":"vortex-dev:04:29:45 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl apply -f services/service/nginx-cluster.yml service/k8s-nginx-cluster created  仔細研究一下 services/service/nginx-cluster.yml 檔案的內容 apiVersion: v1 kind: Service metadata: name: k8s-nginx-cluster labels: run: k8s-nginx-cluster spec: ports: - port: 80 protocol: TCP selector: run: k8s-nginx  這邊用的是非常簡單範例 這邊沒寫 Type, 預設就會是 ClusterIP.該 service 會透過 selector 去找名稱是 k8s-nginx 的nginx的應用服務，並且告知該應用服務是使用 TCP:80 去連線該 service 本身是名稱是 k8s-nginx-cluster 接下來透過 kubectl describe 來觀察一下該 service. vortex-dev:04:32:55 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl describe svc k8s-nginx-cluster Name: k8s-nginx-cluster Namespace: default Labels: run=k8s-nginx-cluster Annotations: kubectl.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;run&quot;:&quot;k8s-nginx-cluster&quot;},&quot;name&quot;:&quot;k8s-nginx-cluster&quot;,&quot;namespace&quot;:&quot;default&quot;}... Selector: run=k8s-nginx Type: ClusterIP IP: 10.98.51.150 Port: &lt;unset&gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.244.0.88:80,10.244.0.89:80,10.244.0.90:80 Session Affinity: None Events: &lt;none&gt;  這邊先注意的就是 Name 以及 Namespace 這兩個欄位，因為該 service 會用 \\$Name.$Namespace 的方式吐出一個可以使用的 FQDN 供其他應用程式使用 此範例中就是 k8s-nginx-cluster.default。 為了驗證這個情境，我們嘗試透過剛剛部屬的 Ubuntu 去嘗試對 NGINX 存取網頁看看 vortex-dev:04:48:50 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl exec ubuntu curl -- -s k8s-nginx-cluster.default &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt; &lt;style&gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome to nginx!&lt;/h1&gt; &lt;p&gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&lt;/p&gt; &lt;p&gt;For online documentation and support please refer to &lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt; Commercial support is available at &lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;  非常順利的存取到網頁了，這時候如果想要從節點本身(非叢集應用程式)去存取看看呢? vortex-dev:04:54:37 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $curl k8s-nginx-cluster.default curl: (6) Could not resolve host: k8s-nginx-cluster.default  會發現根本連該 FQDN 的 DNS 解析都沒有辦法。 實際上 ClusterIP 是因為 kube-dns 的關係沒有辦法解析該位置 但是若嘗試直接使用解析過後的IP位置去存取 叢集內的節點透過解析後的地址是可以存取到目標的。 只是一般人不會想要直接使用該 IP，而是更依賴使用 FQDN 的方式。 ","version":"Next","tagName":"h2"},{"title":"Deploy NodePort​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-i#deploy-nodeport","content":"接下來我們嘗試部屬看看 NodePort 的 service vortex-dev:04:29:45 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl apply -f services/service/nginx-node.yml service/k8s-nginx-node created  仔細研究一下 services/service/nginx-cluster.yml 檔案的內容 apiVersion: v1 kind: Service metadata: name: k8s-nginx-node labels: run: k8s-nginx-node spec: ports: - port: 80 protocol: TCP selector: run: k8s-nginx type: NodePort  這邊可以觀察到 特別標示該 Type 是 NodePort該 service 本身是名稱是 k8s-nginx-node 接下來透過 kubectl describe 來觀察一下該 service. vortex-dev:04:32:55 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl describe svc k8s-nginx-node Name: k8s-nginx-node Namespace: default Labels: run=k8s-nginx-node Annotations: kubectl.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;run&quot;:&quot;k8s-nginx-node&quot;},&quot;name&quot;:&quot;k8s-nginx-node&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec... Selector: run=k8s-nginx Type: NodePort IP: 10.99.157.45 Port: &lt;unset&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;unset&gt; 32293/TCP Endpoints: 10.244.0.88:80,10.244.0.89:80,10.244.0.90:80 Session Affinity: None External Traffic Policy: Cluster Events: &lt;none&gt;  這邊要注意的是除了之前的 Name/Namespace 之外，多了 NodePort 的欄位出現了，這邊後面的 32293 就是代表可以透過任意叢集節點上面的 TCP:32293 去存取到內部的 Nginx 服務器 我們直接使用 172.17.8.100:32293 嘗試看看 vortex-dev:05:03:44 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $curl 172.17.8.100:32293 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt; &lt;style&gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome to nginx!&lt;/h1&gt; &lt;p&gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&lt;/p&gt; &lt;p&gt;For online documentation and support please refer to &lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt; Commercial support is available at &lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;  Summary 本章節中，我們介紹了 Kubernetes Serive, 為什麼需要 Service 以及 Service 如何解決我們的問題 同時介紹了常用的 ClusterIP 以及 NodePort 這兩種類型的差異以及概念 最後透過幾個簡單的範例展示下如何使用 ClusterIP/NodePort 讓我們能夠更方便的透過 service 去存取我們的後端服務 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-service-ii","content":"","keywords":"","version":"Next"},{"title":"Endpoints​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-ii#endpoints","content":"在 kubernetes 內有一個名為 endpoints 的資源，其代表的是 service 所關注目標服務實際上真正運行Pod的 IP 地址 vortex-dev:02:00:28 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl get endpoints NAME ENDPOINTS AGE k8s-nginx-cluster 10.244.0.88:80,10.244.0.89:80,10.244.0.90:80 9h k8s-nginx-node 10.244.0.88:80,10.244.0.89:80,10.244.0.90:80 9h kubernetes 172.17.8.100:6443 11d  可以從上述的輸出結果看到每個 service 都會對應到多組的 endpoints，所以當叢集內的容器有任何 IP 更動的時候，這邊的數據都會自動更新，以確保 service 有辦法存取後端真正的服務 有在使用 Service 的讀者，以後若有遇到 service 不通的情況，可以嘗試先看看該 service 是否有對應的 endpoints，沒有的話可能是 selector 寫錯或是目標服務根本沒有運行起來。 ","version":"Next","tagName":"h2"},{"title":"Custom Chain​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-ii#custom-chain","content":"kubenetes 使用 iptables 時為了更有效管理不同的功能與規則的歸屬，建立的大量的 custom chain ","version":"Next","tagName":"h2"},{"title":"iptables-save​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-ii#iptables-save","content":"這是一個 iptables 相關的指令，我個人很喜歡用它來觀察 iptables 的規則，本文的所有範例都會是使用該指令進行展示 ClusterIP 我們已經知道 ClusterIP 的作用範圍只有叢集內的應用程式/節點，所以在本段落我們會著重於三個概念來理解 叢集內節點是的存取比較尷尬，的確可以透過 ClusterIP 地址來存取，但是預設情況下是沒有辦法解析FQDN取得對應的 ClusterIP 地址。 如何透過 FQDN 輾轉存取到目標容器們(Endpoints)如何做到只有叢集內的應用程式/節點才可以存取假設有多個目標容器(Endpoints), 這中間的選擇方式是怎麼處理? ","version":"Next","tagName":"h2"},{"title":"Access By FQDN​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-ii#access-by-fqdn","content":"我們都知道 Service 本身會提供一組對應的 FQDN 供應用程式使用 實際上這組FQDN 只有 kube-dns 能夠理解，而且其對應的 IP 地址其實就是每個 Service 提供的 ClusterIP這邊的ClusterIP剛好跟 Type 的ClusterIP 名稱一樣，但是這邊要表示的真的是個IP地址 vortex-dev:05:36:40 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE k8s-nginx-cluster ClusterIP 10.98.51.150 &lt;none&gt; 80/TCP 1d k8s-nginx-node NodePort 10.99.157.45 &lt;none&gt; 80:32293/TCP 1d  在此範例中，可以看到不論是 ClusterIP 或是 NodePort 實際上都會有一組 Cluster-IP 的 IP 地址。 這個Cluster-IP最大的特性就是他是一個虛擬的IP地址，在整個kubernetes叢集內是找不到任何一張網卡擁有這個IP地址的。 所有針對該Cluster-IP發送的封包，在滿足特定的條件下，都會被透過DNAT(Destination Network Address Translation) 進行轉換，在service 其實就會是被轉換到其中一個 EndPoints 的真正 IP 地址 vortex-dev:05:43:50 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $sudo iptables-save -t nat | grep nginx-cluster | grep DNAT -A KUBE-SEP-7MBJVYFMXTKOJUKD -p tcp -m comment --comment &quot;default/k8s-nginx-cluster:&quot; -m tcp -j DNAT --to-destination 10.244.0.88:80 -A KUBE-SEP-ARZAHNE3T3EMMTGB -p tcp -m comment --comment &quot;default/k8s-nginx-cluster:&quot; -m tcp -j DNAT --to-destination 10.244.0.90:80 -A KUBE-SEP-O3CWA7STMVCKFPRY -p tcp -m comment --comment &quot;default/k8s-nginx-cluster:&quot; -m tcp -j DNAT --to-destination 10.244.0.89:80 vortex-dev:05:43:54 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $kubectl get endpoints k8s-nginx-cluster NAME ENDPOINTS AGE k8s-nginx-cluster 10.244.0.88:80,10.244.0.89:80,10.244.0.90:80 1d  透過 iptables 的觀察，我們可以看到在某些 custom chain 中會透過 DNAT 的方式把封包的目標IP 位址轉換到這些endpoints 擁有的IP地址。 實際上這個 custom chain 就是 KUBE-SEP-XXXX, 每個 Endpoints 都有一條屬於自己的 custom chain. 而 KUBE-SEP 我想其含意應該就是 KUBE Service Endpoint 吧。 剛剛我們對iptables規則的理解，每個規則都是符合條件,做一件事情，因此背後有多少個endpoints,實際上就會有多少條規則在處理DNAT。 到這邊我們還有一些疑問還沒有解開，只要先記住下面的結論就好。 每個service的FQDN都會對應到一組Cluster-IP IP 地址，該地址其實是虛擬IP地址。送往該Cluster-IP 的封包在滿足特定的情況下，會透過DNAT的方式轉換成其中一個endpoints容器上的真實IP地址 ","version":"Next","tagName":"h2"},{"title":"Cluster Only​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-ii#cluster-only","content":"現在我們要來討論一下，到底所謂的只有叢集內的應用程式/節點才可以存取clusterIP這到底是怎麼運作的。 我們複習一下前面的某個敘述 送往該Cluster-IP 的封包在滿足特定的情況下，會透過DNAT的方式轉換成其中一個endpoints容器上的真實IP地址 這邊提到要滿足特定的情況才會走到DNAT轉到對應的EndPoints。 所以只有叢集內的應用程式/節點才可以存取 其實就是 特定的情況 我們都知道 iptables 的規則可以根據封包的一些資訊來做比對，所以我們能不能做出一種規則是 只有 封包的來源IP地址是來自叢集內的應用程式/節點，符合這種規則的才有資格去進行 DNAT 進行轉發 實際上使用的概念是更簡單，這邊透過 iptables build-in chain 裡面的 OUTPUT/PREROUTING 兩個 chain 來達成只有叢集內的應用程式/節點這個功能 這邊我直接講明 OUTPUT: 本地節點送出去的封包都會先走到這邊PREROUTING: 本地網卡收到封包後會走到這邊，包含了Contaienr出來的封包都會走到 接下我們來透過 iptables 的指令來觀察一下這些規則。 根據前面的查詢，我們知道 ClusterIP 地址是 10.98.51.150, vortex-dev:04:24:49 [~/go/src/github.com/hwchiu/kubeDemo](master)vagrant $sudo iptables-save | grep k8s-nginx-cluster .... -A KUBE-SERVICES -d 10.98.51.150/32 -p tcp -m comment --comment &quot;default/k8s-nginx-cluster: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-3FL7SSXCKTCXAYCR ....  上述的規則我們來仔細看一下每個參數的意義 -A KUBE-SERVICES 這是一個 Custom Chain, 所有跟 Kubernetes Service 有關的第一到防線規則都在這邊 -d 10.98.51.150/32 目標位置是 ClusterIP 的話 -p tcp 目標是 TCP 協定 -m comment 就是註解 -m tcp --dport 80 使用外掛模組來解析TCP裡面的資訊，希望 TCP port 是80 -j KUBE-SVC-3FL7SSXCKTCXAYCR 上述所有條件都符合，就會跳入另外一個custom chain來執行後續任務 後面的部份我們先不管他，我們來看一下什麼情況下的封包會進入到 KUBE-SERVICES 這個 custom chain. $sudo iptables-save -c | grep KUBE-SERVICES :KUBE-SERVICES - [0:0] [2376:171145] -A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES [3706:223392] -A OUTPUT -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES ...  這邊可以看到有兩條規則，分別對應到原生的 OUTPUT 以及 PREROUTING，直接透過 -j 直接跳入到 KUBE-SERVICES 來進行後續處理。 其實夠熟悉 iptables 的朋友應該已經可以猜到，在此規則狀況下，我只要有辦法讓流向ClusterIP的封包透過一些網路規則的方式流向到叢集內的節點，依然可以順利的存取背後的服務。 只是因為這些ClusterIP本身不存在網路之中，所以需要針對整個網路的路由表規則額外設定 這部份就是額外有興趣的人可以自己研究，這邊就不再多敘述。 我們先用下圖來幫助目前的概念做一個整理 橘色底的代表是封包的來源，在此案例中其實就代表叢集內的節點/應用程式綠色底代表的是iptables build-in chain，主要用來處理叢集內應用程式/節點上的封包傳輸藍色的則是kubernetes 的 custom chain.紫色的則是代表 iptables 的描述規則紅色則是我們知道最後會在 KUBE-SEP-XXX 透過 DNAT 把封包轉換到其中一個endpoints之中。???則是代表我們還沒有研究到的，只知道這中間還有一部分的謎團等待解開  每個service的FQDN都會對應到一組ClusterIP IP 地址，該地址其實是虛擬IP地址。透過 iptablse 的 OUTPUT/PREROUTING，其有能力去匹配所有叢集內的應用程式/節點所送出的封包最後透過去比對封包的目的地位址是否是 ClusterIP 來決定要不要往下跳到其他custom chain 去處理。封包最後會透過DNAT的方式轉換成其中一個endpoints容器上的真實IP地址 ","version":"Next","tagName":"h2"},{"title":"Loab Balancing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-ii#loab-balancing","content":"現在我們要來看看最後一個部分了，到底要怎麼從眾多的 Endpoints 中挑選出一個可用的 Pod 來使用。 根據前面的分析，當我們的封包符合叢集內使用的規則後，會跳到一個KUBE-SVC-3FL7SSXCKTCXAYCR 的 custom chain. 實際上 KUBE-SVC-XXXX 的 custom chain 就是用來處理挑選 Endpoints 用的，會根據每個 kubernetes service 創造一條屬於其的 chain. 我們先重新認真看一下這條規則 -A KUBE-SERVICES -d 10.98.51.150/32 -p tcp -m comment --comment &quot;default/k8s-nginx-cluster: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-3FL7SSXCKTCXAYCR  當封包滿足叢集內的條件時，就會跳到一個名為KUBE-SVC-3FL7SSXCKTCXAYCR的 custom chain. 這時候來仔細檢視其內容 -A KUBE-SVC-3FL7SSXCKTCXAYCR -m comment --comment &quot;default/k8s-nginx-cluster:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-POVAFWTN5ECIRK7J -A KUBE-SVC-3FL7SSXCKTCXAYCR -m comment --comment &quot;default/k8s-nginx-cluster:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-AQWRPA7WRPWQAWLR -A KUBE-SVC-3FL7SSXCKTCXAYCR -m comment --comment &quot;default/k8s-nginx-cluster:&quot; -j KUBE-SEP-XPSDT7KEI65EZ2WI  我們可以觀察到裡面有幾個重點 有 -m statistic, random, probability 這些跟機率相關的文字。滿足特定條件後，都會跳到 KUBE-SEP-XXXXX 這些 custom chain. 這就如同我們之前所觀察到會執行 DNAT 的 custom chain 了 接下來說明一下到底那群跟機率有關的規則是怎麼運作的。 我們先前已經說明過，iptables的運作方式是符合條件, 就做一件事情所以並沒有很簡單的在一條規則內，幫你選出對應的Endpoints. 於是這邊的作法是，假設我有三個 Endpoint，挑選的流程如下 請問機率大神，給我一個數字 若該數字&lt;0.33, 則使用第一個endpoints否則重新問機率大神，從剩下的 endpoints 挑選 請問機率大神，再次給我一個數字 若該數字&lt;0.5, 則使用第二個 endpoints否則直接使用地三個 endpoints. 用下圖的方式來重新解釋這個流程，假設今天有四個 Endpoints 要選擇 一開始要從四個裡面選擇，所以機率只有 1/4, 若符合了就採用第一個 Endpoint因為前面沒有符合，所以接下來要從三個裡面繼續選擇下一個 endpoints，這時候的機率就是1/3,但是因為要走到這步必須(1)沒有成功，所以機率是(3/4*1/3), 就是 1/4一此類推，每個 Endpoints 的機率都是 1/4運氣最不好的 endpoints 必須要進行 n-1 次的規則比對 (n是endpoints的數量)運氣最好的只需要一次比對就可以找到。 當找到要使用的 Endpoints 的時候，就會跳到對應的 KUBE-SEP-XXXX 去進行 DNAT 的轉換。 Summary 最後一塊拼圖也已經完成了，到這邊已經可以大概知道是如何透過 iptables 來完成 clusterIP 的轉發。 在這種實作架構中，每個節點的 iptables 都會自行去負責尋找 endpoints 來處理，而ClusterIP 這個不存在的IP地址只是幫助我們讓iptables有個好依據來處理。 就用下圖來幫這篇文章做個最後的結尾。 下篇文章在來仔細看看 NodePort 以及 SessionAffinity 這些功能如何透過 iptables 來實現。  ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-service-iii","content":"Preface 本文章是屬於 kubernetes service 系列文之一，該系列文希望能夠與大家討論下列兩個觀念 什麼是 Kubernetes Service, 為什麼我們需要它？ 它能夠幫忙解決什麼問題Kubernetes Service 是怎麼實現的?， 讓我們用 iptables 來徹徹底底的理解他 相關文章:[Kubernetes] What is Service[Kubernetes] How To Implement Kubernetes Service - ClusterIP[Kubernetes] How To Implement Kubernetes Service - SessionAffinity 本文銜接上篇文章，繼續透過對 iptables 的分析來研究 kubernetes service 中 NodePort 的實作原理。 NodePort 的功能就如同字面上的意思一樣,Node Port, 提供了一種透過存取叢集節點上事先定義好的Port Number 就可以輾轉存取到後端的真正服務。 作為一個靠腦力生存的人，每次遇到全新概念的時候，都要問問自己幾個問題 這個概念是想要解決什麼問題?什麼時候會用到?如果是我，我會怎麼實作? Why We Need NortPort NodePort 本身是屬於 kubernetes service的一環，自然就是要提供一個方式可以讓外部來存取集群內的服務，而且可以不用去理會後面這些容器們的真實IP地址。 既然已經有前面的 ClusterIP 提供了一種叢集內存取的方式，什麼情況下我們會需要 NodePort 這種透過存取節點的方式? 這邊使用一個下列的範例來解釋可能的情況 以下只是一種範例，但是未必是最佳解 假設今天有一個試驗環境，在Cloud Provider(Google/Azure/AWS...etc)中架設了一個kubernetes叢集，裡面透過 nginx 的方式部屬了一個網頁伺服器。 與此同時，我希望該叢集能夠提供下列的特性供我使用 我希望管理人員可以不需要去擔心該 nginx 的狀態，其網頁服務能夠一直正常運作。我可以在任意地方直接連接到該 nginx 提供的網頁伺服器服務 為了滿足第一個條件，我們可以透過 kubernetes deployment 的方式去確保 nginx 的容器處於一種運行的狀態。 為了滿足第二個條件，我們可以透過 kubernetes service 的方式去連接上述的 nginx 容器們並且提供一種接口讓外部存取 在這種情況下，只要kubernetes叢集內的節點擁有一個固定的對外IP地址，同時 kubernetes server 透過 NodePorts 的方式提供該nginx往外存取的能力。 這種情況下，我們就可以在任何地方，透過直接存取該節點的對外IP地址，然後間接透過NodePort的功能存取到集群內的nginx服務。 How It Works 已經有了前述關於 ClusterIP 運作的概念後，其實要探討 NodePort 是如何實現的就非常簡單了。 我們先快速複習一下 ClusterIP 的運作流程 封包若來自叢集上的應用程式/節點，則跳到 `KUBE-SVC如果封包的目標IP地址是 ClusterIP 所提供的虛擬IP地址, 則跳到 KUBE-SVC-XXXXKUBE-SVC-XXX 裡面根據機率的方式，選到一個 Endpoints，最後跳到 KUBE-SEP-XXXKUBE-SEP-XXX 裡面執行 DNAT, 將封包的目標地址改成真正的容器地址，然後轉發 有了上述的概念，我們如果要支援 NodePort 這種能夠透過節點IP的方式存取的話。 想了一下，其實就是把上述的(1)/(2)改掉就好，能夠跳到 KUBE-SVC-XXX的話，後續就完全一致了。 接下來，我們繼續使用kubeDemo來進行相關的服務部屬以及iptables規則研究。 首先，我們先在環境內部署相關的 nginx 以及 kubernetes service(NodePort) vortex-dev:06:36:12 [~/go/src/github.com/hwchiu/kubeDemo/services](master)vagrant $kubectl apply -f deployment/nginx.yml deployment.apps/k8s-nginx created vortex-dev:06:36:18 [~/go/src/github.com/hwchiu/kubeDemo/services](master)vagrant $kubectl apply -f service/nginx-node.yml service/k8s-nginx-node created 這邊就不再敘述太多跟 service/endpoints 相關的資訊與位置，直接從 iptables 的角度出發。 我個人非常喜歡 kubernetes的一點就是所有的 iptables 的規則都會下註解，所以其實可以很輕易的透過 grep 的方式找到相關的規則。 以上述的範例來說，我們在 default namespace 中部屬了一個 k8s-nginx-node 的 kubernetes service. 所以透過 grep default/k8s-nginx-node 的方式就可以過濾出所有跟這個Service有關的所有規則。 vortex-dev:03:17:35 [~]vagrant $sudo iptables-save | grep default/k8s-nginx-node -A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/k8s-nginx-node:&quot; -m tcp --dport 30136 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/k8s-nginx-node:&quot; -m tcp --dport 30136 -j KUBE-SVC-RD5DSC6PXE26GCYZ -A KUBE-SEP-VRKO3GZ2XUCPVWY5 -s 10.244.0.115/32 -m comment --comment &quot;default/k8s-nginx-node:&quot; -j KUBE-MARK-MASQ -A KUBE-SEP-VRKO3GZ2XUCPVWY5 -p tcp -m comment --comment &quot;default/k8s-nginx-node:&quot; -m tcp -j DNAT --to-destination 10.244.0.115:80 -A KUBE-SEP-YNJKNN6SS5424R7C -s 10.244.0.113/32 -m comment --comment &quot;default/k8s-nginx-node:&quot; -j KUBE-MARK-MASQ -A KUBE-SEP-YNJKNN6SS5424R7C -p tcp -m comment --comment &quot;default/k8s-nginx-node:&quot; -m tcp -j DNAT --to-destination 10.244.0.113:80 -A KUBE-SEP-ZGMDZ7UNNV74OV5B -s 10.244.0.114/32 -m comment --comment &quot;default/k8s-nginx-node:&quot; -j KUBE-MARK-MASQ -A KUBE-SEP-ZGMDZ7UNNV74OV5B -p tcp -m comment --comment &quot;default/k8s-nginx-node:&quot; -m tcp -j DNAT --to-destination 10.244.0.114:80 -A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.98.128.179/32 -p tcp -m comment --comment &quot;default/k8s-nginx-node: cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ -A KUBE-SERVICES -d 10.98.128.179/32 -p tcp -m comment --comment &quot;default/k8s-nginx-node: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-RD5DSC6PXE26GCYZ -A KUBE-SVC-RD5DSC6PXE26GCYZ -m comment --comment &quot;default/k8s-nginx-node:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-YNJKNN6SS5424R7C -A KUBE-SVC-RD5DSC6PXE26GCYZ -m comment --comment &quot;default/k8s-nginx-node:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ZGMDZ7UNNV74OV5B -A KUBE-SVC-RD5DSC6PXE26GCYZ -m comment --comment &quot;default/k8s-nginx-node:&quot; -j KUBE-SEP-VRKO3GZ2XUCPVWY5 我們快速的掃過所有的規則，根據 custom chain 來看，分成四個部分 KUBE-NODEPORTSKUBE-SEP-XXXXKUBE-SERVICESKUBE-SVC-XXXX 這邊目前只有 KUBE-NODEPORTS 還沒有看過，剩下的都跟 ClusterIP 是一樣的功能的。 NodePort 的功能基於 ClusterIP 之上再添加新功能，所以本來 Cluster 該有的規則對於 NodePort 來說都不會少 KUBE-NODEPORTS 我們仔細觀察 KUBE-NODEPORTS 相關的兩條規則 -A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/k8s-nginx-node:&quot; -m tcp --dport 30136 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/k8s-nginx-node:&quot; -m tcp --dport 30136 -j KUBE-SVC-RD5DSC6PXE26GCYZ 第一條規則的目標是 -j KUBE-MARK-MASQ, 這部份是跟 SNAT 有關的，這個之後有機會再寫額外的文章來介紹 SNAT 相關的功能以及處理方式。 這邊只要知道這是修改封包的來源IP位址即可 第二條規則比較重要，我們可以觀察到 其比對條件是 -m tcp --dport 30136.符合條件後執行的行為是 -j KUBE-SVC-RD5DSC6PXE26GCYZ vortex-dev:03:34:14 [~]vagrant $kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE k8s-nginx-node NodePort 10.98.128.179 &lt;none&gt; 80:30136/TCP 1d 根據查詢 kubernetes svc 的結果，我們可以觀察到透過存取 30136/TCP 的方式就可以存取 NodePOrt. 而這個資訊與我們前面看到的 KUBE-NODEPORTS 這邊的規則完全一樣 最後可以發現當規則一致時，就會跳到 KUBE-SVC-XXX 去進行 endpoints 的挑選以及相關的 DNAT 功能。 哪接下來的問題只剩下一個 到底封包什麼時候會進入到 KUBE-NODEPORTS ? 只要釐清這個問題，剩下的處理方式就都跟 ClusterIP 完全一樣了。 這時候我們就要一條一條 iptables 的規則來慢慢查詢 我偷懶直接使用 -j KUBE-NODEPORTS 的方式來查詢，到底誰會跳入 KUBE-NODEPORT vortex-dev:03:43:42 [~]vagrant $sudo iptables-save | grep &quot;\\-j KUBE-NODEPORTS&quot; -A KUBE-SERVICES -m comment --comment &quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS 這個規則非常有趣，首先我們可以觀察到，他在 KUBE-SERVICES 這個 custom chain 裡面。 接下來可以觀察他的註解 kubernetes service nodeports; NOTE: this must be the last rule in this chain 然後看一下比對條件以及執行目標 -m addrtype --dst-type LOCAL-j KUBE-NODEPORTS 第一個比對條件我們從文字上來解讀，只要封包的目標IP地址是屬於本節點上的任何網卡IP。 只要符合上述規則，就會跳到 KUBE-NODEPORT 裡面進行比對，然後就按照前述的去處理了。 對於 --dst-type LOCAL 有興趣的人可以嘗試閱讀下列這個檔案https://elixir.bootlin.com/linux/v4.7.8/source/net/netfilter/xt_addrtype.c#L119https://elixir.bootlin.com/linux/v4.7.8/source/include/uapi/linux/rtnetlink.h#L203看看 kernel 內大致上是怎麼處理這系列操作的 到這邊我們整理一下所有的思路。 NodePort 也是倚賴 KUBE-SERVICES，當封包目標是本地端的 IP 位置的時候，就會跳到 KUBE-NODEPORT 裡面去比對 protocol/port 來進行後續跟 ClusterIP 相同的處理所有的 kubernetes NodePort service 都會共用同一個 KUBE-NODEPORT, 因此所有的 NodePort 使用的 Port 都不能一樣 我們用下列這張圖來總結 NodePort 的運作 PortBinding 由於 NodePort 會使用到節點上面的 Port 來提供服務 但從 iptables 的規則觀察下，其實 NodePort 所用到的 Port 就是一個虛擬的 Port，譬如上述範例的 30136。 為了避免有任何應用程式之後將 NodePort 要用到的 Port 給拿去使用，導致整個有任何非預期的行為出現 譬如某服務想要用 30136 port, 但是所有的封包都被 iptables 導走了，導致該服務一直沒有辦法接收到真正的連線 為了解決這個問題就是不要讓任何應用程式有機會使用到 30136 的連接埠，因此每個節點上面的 kube-proxy 就會幫忙做這件事情。 一旦 NodePort 成功建立後，就會將該 Port 給使用走，讓其他的應用程式沒有機會使用。 這部份我們可以透過 netstat 的指令來觀察 vortex-dev:04:08:18 [~]vagrant $sudo netstat -ltpn | grep 30136 tcp6 0 0 :::30136 :::* LISTEN 10181/kube-proxy 這點跟 docker 的想法是很類似的，不過 docker 所啟用的 docker-proxy 其實也會幫忙 forward 這些封包，而不是單純的搶占避免服務失效而已。 vortex-dev:01:08:39 [~/go/src/github.com/linkernetworks/vortex/vendor](hwchiu/VX-62)vagrant $sudo docker run -d -p 5566:80 nginx f4b6b72ad82c170a92cd6ea272fc8d665b69835b8508d20e1ac2b220b2ba5b31 vortex-dev:01:08:43 [~/go/src/github.com/linkernetworks/vortex/vendor](hwchiu/VX-62)vagrant $ps axuw | grep docker-p root 21499 0.0 0.0 59068 2852 ? Sl 01:08 0:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 5566 -container-ip 172.18.0.2 -container-port 80 Summary 本章節我們仔細的討論了 NodePort 各種面向的概念，最後發現其實 NodePort 的規則非常簡單，建立於 ClusterIP 之上。 只要能夠掌握 ClusterIP 是如何運作的，回過頭來看 NodePort 就不難理解這整個過程。 最後繼續使用這張圖作為總結，希望大家這時候都能夠順利的看懂這張圖要表達的一切概念","keywords":"","version":"Next"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-service-iiii","content":"","keywords":"","version":"Next"},{"title":"Setup​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-iiii#setup","content":"接下來，我們繼續使用kubeDemo來進行相關的服務部屬以及iptables規則研究。 vortex-dev:01:37:28 [~/go/src/github.com/hwchiu/kubeDemo/services](master)vagrant $kubectl apply -f service/nginx- nginx-affinity.yml nginx-cluster.yml nginx-node.yml vortex-dev:01:37:28 [~/go/src/github.com/hwchiu/kubeDemo/services](master)vagrant $kubectl apply -f service/nginx-affinity.yml service/k8s-nginx-affinity created  我們用下列指令確認一下剛剛部屬的 kubernetes service 是否真的有設定 sessionAffinity vortex-dev:01:40:58 [~/go/src/github.com/hwchiu/kubeDemo/services](master)vagrant $kubectl get service k8s-nginx-affinity -o jsonpath='{.spec.sessionAffinity}' ClientIP  ","version":"Next","tagName":"h2"},{"title":"IPTABLES​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-iiii#iptables","content":"按照慣例，最簡單的觀察方式就是直接觀察 iptables 的規則，這邊直接透過 k8s-ngins-affinity 這個關鍵字來查詢所有相關的規則 $sudo iptables-save | grep k8s-nginx-affinity -A KUBE-SEP-HDMJEKA4BFKBU6OK -s 10.244.0.145/32 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -j KUBE-MARK-MASQ -A KUBE-SEP-HDMJEKA4BFKBU6OK -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.145:80 -A KUBE-SEP-Q5HAFBJX4HVXF6EM -s 10.244.0.144/32 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -j KUBE-MARK-MASQ -A KUBE-SEP-Q5HAFBJX4HVXF6EM -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-Q5HAFBJX4HVXF6EM --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.144:80 -A KUBE-SEP-YFKOY7G33LWKGTLC -s 10.244.0.143/32 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -j KUBE-MARK-MASQ -A KUBE-SEP-YFKOY7G33LWKGTLC -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-YFKOY7G33LWKGTLC --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.143:80 -A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.109.59.245/32 -p tcp -m comment --comment &quot;default/k8s-nginx-affinity: cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ -A KUBE-SERVICES -d 10.109.59.245/32 -p tcp -m comment --comment &quot;default/k8s-nginx-affinity: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-UBXGHWUUHMMRNNE6 -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-YFKOY7G33LWKGTLC --mask 255.255.255.255 --rsource -j KUBE-SEP-YFKOY7G33LWKGTLC -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-Q5HAFBJX4HVXF6EM --mask 255.255.255.255 --rsource -j KUBE-SEP-Q5HAFBJX4HVXF6EM -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -j KUBE-SEP-HDMJEKA4BFKBU6OK -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-YFKOY7G33LWKGTLC -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-Q5HAFBJX4HVXF6EM -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -j KUBE-SEP-HDMJEKA4BFKBU6OK  稍微看了一下可以發現規則數量變多了，每個 Endpoints 本身多出兩條的規則出來，所以此範例中因為有三個Endpoints，所以總共會多出六條新的規則。 -A KUBE-SEP-HDMJEKA4BFKBU6OK -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.145:80 -A KUBE-SEP-Q5HAFBJX4HVXF6EM -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-Q5HAFBJX4HVXF6EM --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.144:80 -A KUBE-SEP-YFKOY7G33LWKGTLC -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-YFKOY7G33LWKGTLC --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.143:80 -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-YFKOY7G33LWKGTLC --mask 255.255.255.255 --rsource -j KUBE-SEP-YFKOY7G33LWKGTLC -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-Q5HAFBJX4HVXF6EM --mask 255.255.255.255 --rsource -j KUBE-SEP-Q5HAFBJX4HVXF6EM -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -j KUBE-SEP-HDMJEKA4BFKBU6OK  在我們開始研究這些規則之前，我們還是要先來問自己一句話如果是我們自己來實作這個功能，我們會怎麼實作? 假設需求就是 ClientIP ，相同來源IP地址所建立的新連線都要分配到相同的 EndPoints 來使用 直覺下，我們可以用類似 Cache 的概念來完成這個功能，其流程如下 收到新的連線請求, 檢查該來源IP地址是否存在 Cache 中若存在，直接使用該 Cache 內關於的目標 Endpoints 來使用若不存在，則嘗試從 EndPoints 內挑選出一個目標，並且將結果記錄到 Cache 之中. 所以可以將該 cache 分成 Read/Wrtie 兩個功能面向來看待，以下圖來表示  上述的流程看起來滿直觀且合理的，但是這些流程在 iptables 的規則中到底要怎麼完成? ","version":"Next","tagName":"h2"},{"title":"recent modules​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-iiii#recent-modules","content":"我們將前面6條新規則縮減到兩條來單獨觀察就好(因為每個EndPoints會有兩條) -A KUBE-SEP-HDMJEKA4BFKBU6OK -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.145:80 -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -j KUBE-SEP-HDMJEKA4BFKBU6OK  為了加深各位的印象並且能夠順利的解讀 ClusterIP 的原理，需要再次複習一下這張圖片，並且確保知道下圖中各個項目的含意。 ","version":"Next","tagName":"h2"},{"title":"Save​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-iiii#save","content":"首先我們觀察第一條規則，其位於 KUBE-SEP 這個位置，這個其實就是真正執行 DNAT 的 custom chain. 這邊做的事情與我們假想的流程完全一致, 當選出欲使用的 Endoints 並進行 DNAT 轉換之時，順便將該結果記錄到 Cache 內。若不存在，則嘗試從 EndPoints 內挑選出一個目標，並且將結果記錄到 Cache 之中. 我們來仔細看一下這條規則 -A KUBE-SEP-HDMJEKA4BFKBU6OK -p tcp -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --set --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.145:80  裡面新增加的部份則是 -m recent --set --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource  這邊我們要介紹一個新的 iptables 的擴充模組 recent. 但是礙於篇幅沒有辦法詳細介紹其所有用法以及原理。 我們可以將 recent 想成他提供一個簡單的類似 key/value 的 cache 機制，同時支援 Read/Write 等操作來存取該 Cache. 這邊就針對這參數進行一個簡單的介紹 -m recent: 使用擴充模組 recent--set: 這次的行為想要進行儲存的動作，將某些 key/value 寫進到 recent cache 內--name KUBE-SEP-XXXXXXXX: 這邊對應的就是存到 cache 內的 Value.--mask 255.255.255.255: 這個搭配下一個參數使用--rsource: 這邊代表是的我要用什麼當做 key, 這邊使用的是 souruce 就是所謂的封包來源IP地址,既然有IP地址，就可以搭配前面的mask來調整IP位址的範圍，這個範例中就是/32的設定，意味IP要完全一樣才行。 所以歸納一下，若今天已經選定了一個Endpoints要來使用，首先會先跳到屬於該 Endpoints 專屬於的 custom chani KUBE-SEP-HDMJEKA4BFKBU6OK. 在進行 DNAT 之前，會先透過 recent cache 的方式去紀錄下列的對映關係 [來源IP地址] =&gt; KUBE-SEP-HDMJEKA4BFKBU6OK 將上述的概念重新整理，目前的已知拼圖如下  另外，之前有提到過 iptables 的每個指令都是符合特定規則，執行特定行為. 所以其實 recent 模組內關於 Set/Write 相關的操作永遠都是回傳符合，讓上層的規則可以繼續往下執行。 畢竟針對 Set/Write 這類型操作本身就沒有要比對任何東西，只是被拿來進行其他的操作而已。 ","version":"Next","tagName":"h3"},{"title":"Read​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-service-iiii#read","content":"看完了第一題規則後，接下來來看一下最後一條，其實也就是第二條規則 -A KUBE-SVC-UBXGHWUUHMMRNNE6 -m comment --comment &quot;default/k8s-nginx-affinity:&quot; -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-HDMJEKA4BFKBU6OK --mask 255.255.255.255 --rsource -j KUBE-SEP-HDMJEKA4BFKBU6OK  這條規則其實就是 Cache 裡面關於 Read 的操作，但是這邊有一個點要注意，因為 iptables 的規則就是一條一條根據比對條件來判斷要不要執行特定行為. 所以這邊沒有辦法用程式化的方式去從 Cache 裡面取得對應的 EndPoints 名稱。 我們先記住，該 recent 提供的方式是詢問請問該Key有沒有資料，有的話是不是這個Value。 在這種情況下，我們可以想像一下其運作原理。 針對每一條 KUBE-SVC-XXX 裡面的規則，依序每個 EndPoints 執行下列操作 請問Cachue裡面有沒有 來源IP位址 =&gt; 當前EndPoints 的紀錄? 有的話就直接跳到對應的 Endooints 的custom chain去執行DNAT.如果沒有的話，嘗試第二個 Endpoints所有的 Endpoints 嘗試後都沒有結果，那就透過機率的方式選擇一個可用的 Endpoints 有了這些概念後，我們從參數的部分來直接看一下 iptables 實際上的下法 -m:recent 使用擴充模組 recent--rcheck: 這邊我們執行 READ 的指令，要檢查 cache 內是否有對應的 key/value--name: value, 就是 Endpoints 對應到的 custom-chain name--rsouce/--mask: key, 封包的來源 IP--seconds: 每個 cache 內的記錄都會有一個過期的時間，這個時間的意思是只有上次設定該 ket/value 的時間距離現在N秒內的才算數，已這個範例來說就是 10800秒 內的 cache 記錄才算數，如果是超過 10800秒 前記錄的，就當失效。--reap: 這個是指每次查詢的時候，會將已經超過有效時間 的規則一併清除。 把這整個流程全部重新組合後，我們用下面的這張圖來描述關於整個 SessionAffinity 的概念與實作seconds 相關的概念我就沒有加入到該圖片中了，因為篇幅有限，針對主要觀念去描述即可。  Summary 看到這邊，我們大概瞭解如何透過 iptables 的功能來達成 SessionAffinity:ClientIP 的功能，透過 iptables 的擴充模組 recent 提供類似 key/value 的 cache 機制來紀錄 來源IP地址 與 Endpoints 的關係。 如果對於 iptables 擴充模組有興趣的讀者，之後我會撰寫一些文章介紹 iptables 的架構以及如何自己撰寫一個 iptables 的擴充模組。 最後我們將本篇文章所學的概念與一直以來使用的關係圖給整合起來，當設定 SessionAffinity 時，我們會在KUBE-SVC 嘗試透過 Recent/Cache 的方式找到是否有使用過的 Endpoints之後再真正執行 DNAT 的 KUBE-SEP-XXXX 時會不停的更新 Recent/Cache 內的資料以及時間，避免該筆資料過期。  ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-storage-i","content":"","keywords":"","version":"Next"},{"title":"PersistentVolume(PV)​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#persistentvolumepv","content":"PersistentVolume(PV) 代表的就是背後儲存系統的叢集資源，因此在每個 PV 的描述中都要去描述該 PV 最後要銜接的儲存系統的相關資訊，譬如如果是 NFS 的話，就需要去描述相關的 Server IP 以及 ExportPath 等 NFS 相關的資訊。 PV 作為一個與儲存系統銜接的抽象層，除了包含儲存相關的參數外，也要提供一些相同的參數供上層使用。 PV 的實現則要根據這些參數嘗試去跟後端的儲存系統要求配置符合相關需求的儲存空間供容器使用。 接下來看一下對於一個代表儲存系統於叢集內資源的代表，其本身有哪些相關的參數可以使用 ","version":"Next","tagName":"h2"},{"title":"VolumeMode​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#volumemode","content":"首先是一個我覺得大家比較少注意也比較不會去使用的參數，所謂的 VolumeMode 代表的是到底是 FileSystem 還是 Block Device. 一般我們在使用的都是已經格式化成為 FileSystem 的類型，譬如 ext4,btrfs,zfs. 然而有些應用會想要直接使用 BlockDevice 來使用，譬如使用 /dev/sda5 之類的方式。 預設的情況下都還是 FileSystem，因此沒有特別需求的話這個參數不太需要處理。 ","version":"Next","tagName":"h3"},{"title":"Capacity​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#capacity","content":"理想的狀況下，為了有效地控制與管理儲存空間，能夠事先規劃所需的容量大小並且請求相對應的儲存空間來使用是個比較好的應用方式。 於此前提下， Kubernetes 提供了透過 Capacity 的方式來表明這個 PV 希望儲存後端所提供的儲存空間至少要滿足這對應的大小. 以目前規劃的藍圖來說，除了單純的儲存空間大小外，未來也希望能夠規劃以效能為考慮的 IOPS (Input/Otput Per Secsion), 詳細的可以參閱Kubernetes Resource Model 此外，要特別注意的是，並不是所有的儲存空間都能夠依照這個需求來滿足對應的條件，譬如以 NFS 本身其實就不支援這個選項。(不支援的意思是可以設定，但是實作上不會採用) ","version":"Next","tagName":"h3"},{"title":"AccessMode​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#accessmode","content":"AccessMode 則是設定使用端可以如何使用這個儲存空間，主要面向的對象是 讀寫 的設定，以及是否可以同時多個節點進行讀寫。 設定分成三種 ReadWriteOnce 請求到的該塊空間只能同時給一個節點使用，節點上的各種容器使用可以同時 進行讀取或是寫入的動作。ReadOnlyMany 請求到的該塊空間可以同時給多個節點使用，但是節點上的各種容器使用都只能基於讀取這種沒有寫入的操作。ReadWrtieMany 請求到的該塊空間可以同時給多個節點使用，且大家要進行讀取或是寫入等動作都是沒有問題的。 這邊比較有趣的是所有的同步單位都是基於節點而非基於容器，我個人的理解是因為 Kubernetes 底層的實作會是在節點上根據需求產生一個對應的儲存空間，接者該儲存空間則是會透過容器本身的映射方式，掛載到多個容器內去進行寫入存取。 此外，如果使用情境是如 ReadWriteMany 所敘述，該儲存空間可以再多個節點上同時進行讀寫，要如何確保檔案內容的一致性? 這邊的則是對應的儲存系統背後要自己負責處理，Kubernetes 本身不涉及任何儲存系統的實現與保護。 因此能夠支援 ReadWriteMany 的儲存空間其實並不多，如 NFS 或是一些分散式檔案系統 Ceph/GlusterFS. ","version":"Next","tagName":"h3"},{"title":"Reclaim Policy​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#reclaim-policy","content":"對於 PV 來說，Reclaim Policy 的含義則是當使用自己的PVC被刪除時，這個PV要如何去處理要來的儲存空間。 舉例來說，假如我希望這塊儲存空間的資料可以重複被利用，我希望這些資料的刪減都是由管理員或是使用者自行處理，則我們應該要將該 Policy 設定成 Retain. 然而其他的情況下，可能會希望當 PVC 被移除後，就將對應的 PV 一併刪除，直接將該儲存空間移除，因為沒有需要重複利用的情境。 那就可以將該 Policy 宣告為 Delete 使用上還是根據使用情境來設定，沒有一定的對錯與最好的選擇 ","version":"Next","tagName":"h3"},{"title":"Class​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#class","content":"Class 類似標籤的概念，這邊先談簡單的概念，可以透過對 PV 設定該 Class(實際上在 Yaml 中叫做 StorageClassName）來限制只有含有相同標籤的 PVC 可以使用。 譬如我們可以針對這些 PV 設定不同的標籤，有可能是 SSD 或是慢速的HDD, 然後最上層的 PVC 可以再根據需求與標籤來從特定的 PV 中選出一個符合規則的儲存空間供上層使用。 實際上 Class 對於 StorageClass 那邊還有別的用途，這個部分等到後面再來詳談。 ","version":"Next","tagName":"h3"},{"title":"Claim​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#claim","content":"這是一個在官方文件中沒有提到的參數，但是某些情況下還滿好用的，可以透過該參數去描述我只有特定的 PVC 能夠使用該 PV, 這部分實際上是透過 PVC 的 namespace 以及 meta name 來進行一個映射，並且透過這個方式可以將特定的 PV 與 PVC 給綁定。 詳細的用法可以參閱 can-a-pvc-be-bound-to-a-specific-pv ","version":"Next","tagName":"h3"},{"title":"Mount Options​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#mount-options","content":"上述的所有選項都比較偏向儲存空間的能力與特性，而這個 Mount Options 則是描述當 Kubernetes 叢集中的節點想要掛載這個目標的儲存空間時，是否有一些額外的參數可以匹配與使用. 關於 Mount Options 的討論與構想，可以直接參閱 Support Volume Mount Options #168 ","version":"Next","tagName":"h3"},{"title":"Others​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#others","content":"看了這麼多參數，那到底要如何去描述背後的儲存空間設定? 實際上在 Yaml 中並沒有一個固定的 key 去描述到底該如何使用背後的儲存空間。反而是每個儲存設定都會有一個屬於自己的 key，然後再該 key 底下去描述對應的資源與參數。 舉例來說使用 NFS 跟使用 GlusterFS 則會完全不同，可以參考下列兩個 Yaml 檔案 apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2  apiVersion: v1 kind: PersistentVolume metadata: name: gluster-default-volume annotations: pv.beta.kubernetes.io/gid: &quot;590&quot; spec: capacity: storage: 2Gi accessModes: - ReadWriteMany glusterfs: endpoints: glusterfs-cluster path: myVol1 readOnly: false persistentVolumeReclaimPolicy: Retain  這邊的原因主要是因為 kubernetes 後端的實現(golang) 在進行物件的解析的時候，是採用下列的結構, 有興趣的人可以研究一下其背後實現原理的。 type PersistentVolumeSource struct { // GCEPersistentDisk represents a GCE Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. Provisioned by an admin. // More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk // +optional GCEPersistentDisk *GCEPersistentDiskVolumeSource `json:&quot;gcePersistentDisk,omitempty&quot; protobuf:&quot;bytes,1,opt,name=gcePersistentDisk&quot;` // AWSElasticBlockStore represents an AWS Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. // More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore // +optional AWSElasticBlockStore *AWSElasticBlockStoreVolumeSource `json:&quot;awsElasticBlockStore,omitempty&quot; protobuf:&quot;bytes,2,opt,name=awsElasticBlockStore&quot;` // HostPath represents a directory on the host. // Provisioned by a developer or tester. // This is useful for single-node development and testing only! // On-host storage is not supported in any way and WILL NOT WORK in a multi-node cluster. // More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath // +optional HostPath *HostPathVolumeSource `json:&quot;hostPath,omitempty&quot; protobuf:&quot;bytes,3,opt,name=hostPath&quot;` // Glusterfs represents a Glusterfs volume that is attached to a host and // exposed to the pod. Provisioned by an admin. // More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md // +optional Glusterfs *GlusterfsPersistentVolumeSource `json:&quot;glusterfs,omitempty&quot; protobuf:&quot;bytes,4,opt,name=glusterfs&quot;` // NFS represents an NFS mount on the host. Provisioned by an admin. // More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs // +optional NFS *NFSVolumeSource `json:&quot;nfs,omitempty&quot; protobuf:&quot;bytes,5,opt,name=nfs&quot;` // RBD represents a Rados Block Device mount on the host that shares a pod's lifetime. // More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md // +optional RBD *RBDPersistentVolumeSource `json:&quot;rbd,omitempty&quot; protobuf:&quot;bytes,6,opt,name=rbd&quot;` // ISCSI represents an ISCSI Disk resource that is attached to a // kubelet's host machine and then exposed to the pod. Provisioned by an admin. // +optional ISCSI *ISCSIPersistentVolumeSource `json:&quot;iscsi,omitempty&quot; protobuf:&quot;bytes,7,opt,name=iscsi&quot;` // Cinder represents a cinder volume attached and mounted on kubelets host machine // More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md // +optional Cinder *CinderPersistentVolumeSource `json:&quot;cinder,omitempty&quot; protobuf:&quot;bytes,8,opt,name=cinder&quot;` // CephFS represents a Ceph FS mount on the host that shares a pod's lifetime // +optional CephFS *CephFSPersistentVolumeSource `json:&quot;cephfs,omitempty&quot; protobuf:&quot;bytes,9,opt,name=cephfs&quot;` // FC represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod. // +optional FC *FCVolumeSource `json:&quot;fc,omitempty&quot; protobuf:&quot;bytes,10,opt,name=fc&quot;` // Flocker represents a Flocker volume attached to a kubelet's host machine and exposed to the pod for its usage. This depends on the Flocker control service being running // +optional Flocker *FlockerVolumeSource `json:&quot;flocker,omitempty&quot; protobuf:&quot;bytes,11,opt,name=flocker&quot;` // FlexVolume represents a generic volume resource that is // provisioned/attached using an exec based plugin. // +optional FlexVolume *FlexPersistentVolumeSource `json:&quot;flexVolume,omitempty&quot; protobuf:&quot;bytes,12,opt,name=flexVolume&quot;` // AzureFile represents an Azure File Service mount on the host and bind mount to the pod. // +optional AzureFile *AzureFilePersistentVolumeSource `json:&quot;azureFile,omitempty&quot; protobuf:&quot;bytes,13,opt,name=azureFile&quot;` // VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine // +optional VsphereVolume *VsphereVirtualDiskVolumeSource `json:&quot;vsphereVolume,omitempty&quot; protobuf:&quot;bytes,14,opt,name=vsphereVolume&quot;` // Quobyte represents a Quobyte mount on the host that shares a pod's lifetime // +optional Quobyte *QuobyteVolumeSource `json:&quot;quobyte,omitempty&quot; protobuf:&quot;bytes,15,opt,name=quobyte&quot;` // AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. // +optional AzureDisk *AzureDiskVolumeSource `json:&quot;azureDisk,omitempty&quot; protobuf:&quot;bytes,16,opt,name=azureDisk&quot;` // PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine PhotonPersistentDisk *PhotonPersistentDiskVolumeSource `json:&quot;photonPersistentDisk,omitempty&quot; protobuf:&quot;bytes,17,opt,name=photonPersistentDisk&quot;` // PortworxVolume represents a portworx volume attached and mounted on kubelets host machine // +optional PortworxVolume *PortworxVolumeSource `json:&quot;portworxVolume,omitempty&quot; protobuf:&quot;bytes,18,opt,name=portworxVolume&quot;` // ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes. // +optional ScaleIO *ScaleIOPersistentVolumeSource `json:&quot;scaleIO,omitempty&quot; protobuf:&quot;bytes,19,opt,name=scaleIO&quot;` // Local represents directly-attached storage with node affinity // +optional Local *LocalVolumeSource `json:&quot;local,omitempty&quot; protobuf:&quot;bytes,20,opt,name=local&quot;` // StorageOS represents a StorageOS volume that is attached to the kubelet's host machine and mounted into the pod // More info: https://releases.k8s.io/HEAD/examples/volumes/storageos/README.md // +optional StorageOS *StorageOSPersistentVolumeSource `json:&quot;storageos,omitempty&quot; protobuf:&quot;bytes,21,opt,name=storageos&quot;` // CSI represents storage that handled by an external CSI driver (Beta feature). // +optional CSI *CSIPersistentVolumeSource `json:&quot;csi,omitempty&quot; protobuf:&quot;bytes,22,opt,name=csi&quot;` }  ","version":"Next","tagName":"h3"},{"title":"PersistentVolumeClaim(PVC)​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#persistentvolumeclaimpvc","content":"PersistentVolume 是與後端儲存空間連接的叢集資源，而 PersistentVolumeClaim(PVC) 則是銜接 Pod 與 PV 的中介抽象層，可以說是容器本身對於儲存需求的資源請求。 使用上，Pod 本身會去描述想要使用哪個 PVC，並且希望把該 PVC 掛載到 Pod 內使用。 而 PVC 本身則是會根據自身的需求，去找尋是否有符合的PV可以使用，並且將其該 PVC 與對應的 PV 進行一個 Binding 的動作。 ","version":"Next","tagName":"h2"},{"title":"Resourecs​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#resourecs","content":"如同上面所述， PV 所描述的是叢集資源，而PVC描述的則是資源請求，在參數方面基本上 PV 跟 PVC 都會有相同的描述。不論是 Capacity, AccessMode, VolumeMode. 因此 PVC 只會去找尋能夠符合其條件的 PV 來進行挑選的動作。 ","version":"Next","tagName":"h3"},{"title":"Binding​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#binding","content":"Binding 這邊拉出來特別解釋一下，實際上 PV 與 PVC 的概念PV 代表的是從後端儲存設備根據資源需求所取得的儲存空間。PVC 則是對於希望從眾多的PV中選出一個符合需求的 PV來使用，這個挑選的過程就稱為 Binding. 舉例來說，今天系統中創建了三個 PV, 每個 PV 都宣稱自己有 50Gb 的空間可以使用。這時候有個 PVC 的條件是希望找到一個至少有 20Gb 空間的 PV 來使用。 那實際上這三個 PV 都可以被使用，至於最後是哪個PV被選到，我這邊沒看到太多詳細且可證實的說法，之後若有看到會再補充。 如果今天有特別的需求，希望特定的 PVC 可以挑選到特定的 PV, 這邊可以透過前述講過的 Claim 參數來達成，只要於 PV 的設定中去描述對應 Claim 的 namespace/name, 並且與目標的 PVC 中去使用這些數值，就可以確保該 PVC 一定會用到該 PV 了。 ","version":"Next","tagName":"h3"},{"title":"PV/PVC​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#pvpvc","content":"我們重新再檢視一次剛剛看過的圖表 透過前述對於 PV 與 PVC 概念的闡述，我們現在可以知道 Pod 裡面會描述 Volume, 而 Volume 則是會描述對應的 PVC 名稱PVC 實際上會根據自己的資源去系統中找到符合需求的叢集儲存資源PV, 並且將這兩者給 Binding 起來PV 則是會負責跟對應的後端儲存設備進行連接，詳細的描述要如何使用後端儲存資源 將這個工作流程用實際上操作的用法來看，系統管理員與叢集使用者要如何協同合作來使用呢? 舉一個簡單的範例來說 首先，系統管理員必須要先根據系統情境與架構創建好相對應的 PV使用者根據自己的需求，去系統上創建對應的 PVC, 並且期望該 PVC 能夠找到符合需求的 PV 來使用最後使用者部署相關的 Pod 於容器中時，就能夠將該 PVC 給掛載到容器中使用 這個流程中聽起來順暢但是實際上卻有一個用起來不夠方便的地方，就在於系統管理員必須要先創立相關的 PV 來使用，而且對於系統管理員來說，他可能並不知道使用者要的 PVC 資源到底是什麼，因此並沒有很好的規劃剛剛好的空間來使用。 我們將這種需要系統管理員事先設定 PV 且讓 PVC 根據需求自己去尋找 PV 的類型稱為 Static。 而相對於 Static，就會有所謂的 Dynamic 的概念出現，因此接下來就是要來介紹 StorageClass 的使用了。 ","version":"Next","tagName":"h2"},{"title":"StorageClass​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#storageclass","content":"Dynamic 的概念最大的優勢就是，系統管理員不需要事先設定 PV, 而 PV 的創建都是根據需求創造出來的，這有兩個好處 系統管理員不需要事先設定相關的 PV創建出來的 PV 都能夠完全符合 PVC 真正的資源需求，不會有浪費的情形 為了滿足這個想法，於是我們要來使用 StorageClass 了，對於 StorageClass 來說，我們先來觀看一下整體的使用流程。 系統管理員根據需求與架構，創建對應的 StorageClas 物件，該物件也需要描述後端的儲存空間，但是不需要描述相對應的資源大小使用者接下來根據需求創建 PVC, 而這些 PVC內描述我要使用特定的 StorageClasskubernetes 觀察到有 PVC 想要使用 StorageClass 且發現該 StorageClass 物件存在，就會透過 StorageClass 的屬性以及 PVC 的需求創造一個完全符合該 PVC 需求的 PV最後該 PVC 與該 PV 就可以綁定並且供 Pod 使用 接下來細看一下 StorageClass 的參數以及一些概念 ","version":"Next","tagName":"h2"},{"title":"Provisioner​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#provisioner","content":"由於現在 PV 的創建是自動的，但是因為 PV 本身就跟後端的儲存空間有直接的連接關係，因此我們在 StorageClass 也要有能力去描述後端的儲存空間資訊。 所以實際上在使用的時候必須要在該 StorageClass 的Yaml 中透過 Provisioner 去描述該 StorageClass 背後使用的儲存系統是哪一套解決方案。 然而要注意的是，並不是所有 PV 目前支援的儲存系統都有對應的 StorageClass Provisioner 支援。 支援的部分除了官方自己已經合併到主線以外，也有第三方的解決方案再擴充相關的 StorageClass Provisioner. 詳細的支援列表可以參考 Kubernetes StorageClass Provisioner ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#parameters","content":"前面都有看到 PV 內對於每個儲存欄位都有自己的一個結構去描述相關的參數，而在 StorageClass 內實際上並沒有這種格式來使用，然而不同的儲存系統可能也要有不同的相關參數來使用，因此在 StorageClass 這邊則是採用了一個固定的格式 Parameters ，裡面則是各式各樣的 key:value 的形式， kubernetes 會將這些資訊全部傳送到 Provisioner，讓 Provisioner 自行解讀來處理。 以下是一個 StorageClass 的範例 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gluster-vol-default provisioner: kubernetes.io/glusterfs parameters: resturl: &quot;http://192.168.10.100:8080&quot; restuser: &quot;&quot; secretNamespace: &quot;&quot; secretName: &quot;&quot; allowVolumeExpansion: true  ","version":"Next","tagName":"h3"},{"title":"Binding​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-i#binding-1","content":"之前有提到過，PVC 在找尋 PV 的時候會尋找資源符合的來使用，但是如果使用的是透過 StorageClass 來幫你創建的 PV 的話，實際上系統會幫忙再 PV 的描述中使用 Claim 的欄位，確保你的 PVC 與創建出來的 PV 可以完全地符合，且也只會使用這個 PV. 此外，透過 StorageClass 所創建出來的 PV，其 StorageClassName 的欄位就會自動被捕上該 StorageClass 的名稱，對於資訊的補充也更加完整。 所以就 PV/PVC 裡面的 StorageClass 的用法，若系統上沒有 StorageClass 的資源的話，可以單純用來限制 PVC 與 PV 尋找的範圍。若有 StorageClass 的話，就可以讓 StorageClass 來幫忙創建與維護相關的 PV.  最後，重新檢視一次最初的那張圖片 左邊是最基本的用法，自行創建 PV 與 PVC, 並且將所需要的資源與參數都填寫完畢後，由容器本身去選擇要使用哪個 PVC. 右邊則是套用上 Storage Class 這種動態創建 PV 的用法，將 PV 的創造與維護 (PV的刪除也是依賴 Reclaim Policy) 讓 kubernetes 本身幫忙維護。 而創建出來的 PV 所擁有的系統資源以及存取模式等都會跟 PVC 完全一致，確保不會有額外的系統資源浪費。 下一張會跟大家分享一下 NFS 這個儲存設備到底在 Kubernetes 裡面可以怎麼使用，除了從最基本的 PV/PVC 一般常見的用法外，也會跟大家分享一下如何使用第三方的 NFS Provisioner 來透過 StorageClass 使用 NFS 做為容器的儲存空間 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/kubernetes-storage-ii","content":"","keywords":"","version":"Next"},{"title":"Pod​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#pod","content":"如果是要在 Pod 內直接使用 NFS， 非常簡單，只要在 Volume 的欄位描述相關的資訊即可。 這種使用方式就是簡單，但是不方便，每次都要描述相關的 IP/Path 等資訊，所以一般使用上還是會採用 PV/PVC 的架構。 spec: containers: - name: busybox image: hwchiu/netutils:latest volumeMounts: - name: nfs-volume mountPath: /nfs volumes: - name: nfs-volume nfs: server: 172.17.8.101 path: /nfsshare  ","version":"Next","tagName":"h2"},{"title":"PV/PVC​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#pvpvc","content":"如果要使用 PV/PVC 的架構，流程如下 創立 PV 資源，與事先架設好的 NFS Server 綁定創立 PVC，去尋找符合需求的 PV創立 Pod, 透過 VolumeMounts 的方式去掛載創立好的 PVC. ","version":"Next","tagName":"h2"},{"title":"PV/PVC/Pod​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#pvpvcpod","content":"PV 必須要描述跟 NFS Server 最直接的資訊，譬如 IP 位置以及相關的路徑 apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 1Mi accessModes: - ReadOnlyMany nfs: server: 172.17.8.101 path: &quot;/nfsshare&quot;  PVC 中直接因為這個範例很簡單，也沒有其他的 PV 存在，因此只要條件滿足就可以選擇到對應的 PV 並且連接起來 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadOnlyMany storageClassName: &quot;&quot; resources: requests: storage: 1Mi  Pod 中透過Volumes 的資訊去綁定對應的 PVC 即可 apiVersion: v1 kind: Pod metadata: name: hwchiu labels: app: hwchiu spec: containers: - name: busybox image: hwchiu/netutils:latest volumeMounts: - name: nfs-volume mountPath: /nfs volumes: - name: nfs-volume persistentVolumeClaim: claimName: nfs  基本上按照上述範例，就可以順利在 Pod 裡面的 /nfs 資料夾使用到外部 NFS 儲存伺服器的資料了 ","version":"Next","tagName":"h3"},{"title":"Limitation​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#limitation","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#description","content":"NFS 使用起來看起來非常順利簡單，實際上卻有一些限制，接下來好好探討一下這些限制 在上述的 PV 的創建過程中，我們可以看到我設立了  capacity: storage: 1Mi accessModes: - ReadOnlyMany  然而實際上，我們到真正的 Pod 裡面執行下列指令，卻是可以正常執行的 dd if=/dev/zero of=/nfs/ii count=40960  該指令會在 /nfs/ 底下寫入一個將近 200Mb 的檔案。 這個操作完全忽略了上述的設定 大小 1MiReadOnly 的權限設定 這部分主要是因為 NFS 本身架構導致的結果。 首先， NFS 本身並不是根據需求而動態分配空間出來的一種檔案系統，本身檔案系統有多大，你可以存取的人就有多大可以使用。 因此過往在使用經驗上，通常都會仔細設計每個分享出來的資料夾架構以及透過其他的系統工具來限制該資料夾的能夠使用的空間大小。 此外，對於 NFS 來說，控管權限的部分分成兩個部分來看 MountOptionUnix 檔案權限系統 (0777) 首先，在 MountOption 的部分，我們可以在 NFS Server/NFS Client 的部分設定 RO/RW 等不同的權限，但是這些權限範圍過大，基本上就是控制該 NFS Client 是否有讀寫的權限而已 至於 Unix 檔案權限系統方面，基本上就是根據使用者的 UID/GID 以及該檔案本身的 UID/GID/OTHER相關的的權限來控制。 所以有一個很有趣的現象就是，假設你什麼都沒有處理的情況下，你叫起的 Pod 預設都是以 root 的身份去執行，意味該使用者擁有對 NFS 分享資料夾下所有檔案的所有權利，Read/Write/Rename/Delete... 都可以執行。 所以要好好地針對 NFS 的檔案權限去使用的人，必須要有 完整了解整個 UNIX 檔案權限的知識完整規劃不同權限 UID/GID 等架構每個對應的 Pod 運行的時候都要處理對應的 RunAsUser 等設定，來切換對應的使用者 當你完成了上面這些設定，你就會發現實際上還是不能通，最主要的問題是在UNIX 裡面， UID/GID 實際上是一堆數字，該檔案權限系統比對的也是數字。 但是在你的 Pod 裡面，你的 /etc/password 不一定有該數字對應的使用者資料，所以你今天透過 RunAsUser 去換一個使用者名稱 hwchiu 來運行這個 Pod. 但是因為該 Pod 內並沒有找不到這個使用者的名稱，也沒辦法找到對應的 UID. 最後你就會被系統給強制轉換成 Root. 所以一切的檔案權限又回到原點了。 這部分非常麻煩處理，過往可能會想要透過 NIS(YP) 或是修改 /etc/nsswitch.con 的方式來調整查詢的順序。 但是這些舉動對於 Pod 來說非常麻煩及極度困難。 因此有想要完整的在 Kubernetes 提供 NFS Clinet 服務且也要支援外部權限系統的人，要好好的思考這些步驟該怎麼處理。 ","version":"Next","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#kubernetes","content":"針對上述的限制，kubernetes 內也有一些額外的選項可以處理。 首先關於 ReadOnly 這些的選項，我們可以透過下列的參數  nfs: server: 172.17.8.101 path: &quot;/nfsshare&quot; readOnly: true  讓 Kubernetes 知道NFS在掛載的時候可以請幫忙提供 RO 的參數。 接下來在使用 PVC 的時候也要給予類似的參數  volumes: - name: nfs-volume persistentVolumeClaim: claimName: nfs readOnly: true  這樣的話我們在使用的 Pod 內使用mount指令可以觀察到 RO而非RW的選項 root@hwchiu:/# mount | grep nfs 172.17.8.101:/nfsshare on /nfs type nfs4 (ro,relatime,vers=4.0,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.17.8.101,local_lock=none,addr=172.17.8.101) root@hwchiu:/# touch /nfs/test touch: cannot touch '/nfs/test': Read-only file system root@hwchiu:/#  Kubernetes-incubator: nfs-client 前面我們提到了透過 PV/PVC 的方式來事先設定好相關的儲存資源，並且分配讓對應的 Pod 來使用。 這種情況下，基本上每個 Pod 都可以共享相同的資料夾，並且看到相同的內容。如果該 NFS Server 有不一樣的 Export Path 且有不同的用途，通常就要創立不同的 PV/PVC 組合來供最上層的 Pod 來使用。 如果今天想要透過 StorageClass 這種動態分配的方式來使用 NFS Server 有沒有辦法? 預設的 Kubernetes 是沒有提供這類型的功能，但是我們可以透過額外的套件功能來幫我們滿足這個功能 也就是接下來要介紹的 kubernetes-incubator 內的專案 NFS-Provisioner: nfs-client 詳細的資訊可以參考其 Kubernetes-Incubator External-Storage NFS-Client ","version":"Next","tagName":"h3"},{"title":"Introduction​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#introduction","content":"該專案的目標很簡單，讓你可以透過 StorageClass 動態產生 PV/PVC 供上層的 Pod 使用. 其原理很簡單，其實就是透過 Kubernetes 內建的 Provisioner 介面去額外實現一個 Provisioner Controller. 接下來當我們部署一個 StorageClass 的時候，可以於 Provisioner 的欄位去指向事先創立好的 Provisioner. 這樣當未來有任何 PVC 的需求要指向 StorageClass, 最後就會將該請求傳送到該 Deploykent(Provisioner) 內去處理。 ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/kubernetes-storage-ii#installation","content":"我們最主要需求就是運行起一個 Provisioner Controller. 這部分 nfs-client 有提供 helm 來安裝 helm install stable/nfs-client-provisioner \\ --name nfs-client --set nfs.server=172.17.8.101 \\ --set nfs.path=/nfsshare  這邊我們需要針對 nfs.server 以及 nfs.path 去進行設定。當一切安裝完畢後，我們透過 helm status nfs-client 來觀察一下所有安裝的 kubernetes 資源 ~$ helm status nfs-client LAST DEPLOYED: Sun Dec 30 15:16:04 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==&gt; v1/ServiceAccount NAME AGE nfs-client-nfs-client-provisioner 1m ==&gt; v1/ClusterRole nfs-client-nfs-client-provisioner-runner 1m ==&gt; v1/ClusterRoleBinding run-nfs-client-nfs-client-provisioner 1m ==&gt; v1/Role leader-locking-nfs-client-nfs-client-provisioner 1m ==&gt; v1/RoleBinding leader-locking-nfs-client-nfs-client-provisioner 1m ==&gt; v1/Deployment nfs-client-nfs-client-provisioner 1m ==&gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE nfs-client-nfs-client-provisioner-986bcfb76-svqb4 1/1 Running 0 1m ==&gt; v1/StorageClass NAME AGE nfs-client 1m  這邊我們可以觀察的是 StorageClass 以及 Deployment. ~$ kubectl describe storageclass nfs-client Name: nfs-client IsDefaultClass: No Annotations: &lt;none&gt; Provisioner: cluster.local/nfs-client-nfs-client-provisioner Parameters: archiveOnDelete=true AllowVolumeExpansion: True MountOptions: &lt;none&gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: &lt;none&gt;  可以看到 Provisioner 這邊指定的是 cluster.local/nfs-client-nfs-client-provisioner, 而這個數值我們可以透過檢視該 Deployment 的環境變數來觀察到 ~$ kubectl describe deployment nfs-client-nfs-client-provisioner | grep PRO PROVISIONER_NAME: cluster.local/nfs-client-nfs-client-provisioner  接下來我們就可以透過部署 PVC 以及對應的 Pod 來使用了 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim annotations: volume.beta.kubernetes.io/storage-class: &quot;nfs-client&quot; spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi  apiVersion: v1 kind: Pod metadata: name: hwchiu labels: app: hwchiu spec: containers: - name: busybox image: hwchiu/netutils:latest volumeMounts: - name: nfs-volume mountPath: /nfs volumes: - name: nfs-volume persistentVolumeClaim: claimName: test-claim  這邊可以注意的是，當對應的 PVC 被創建之後，我們可以在 NFS Server 那邊觀察到，對應的 /nfsshare 資料夾裡面又被創建了一個新的資料夾 default-test-claim-pvc-e0ae384c-0c45-11e9-be59-02e1e1d1e477 這個含義就是該 Deployment(Provisioner) 針對該 PVC 的需求，動態的創建了一個資料夾，並且把該資料夾當作 mount path 給之後的 Pod 去使用。 所以使用不同 PVC 的 Pod 即使背後都是相同的 NFS Server, 也可以因為 StorageClass 動態規劃的幫忙而不會互相看到彼此的資料。 Summary 最後用一個簡單的架構圖來進行一下比較 原生的 Kubernetes NFS 的支援下，NFS Server 怎麼設定， 使用的 NFS Client(Pod) 就只能怎麼使用，掛載以及使用的資料夾比較固定。基本上要依賴 NFS Server 端的設計來滿足各種不同的使用情境，此外管理者必須要自己先行創立對應的 PV/PVC 供要使用的 Pod 來使用。 如果想要使用 StorageClass 這種動態分配空間的使用方式的話，可以參考一些孵化中的專案，NFS-Client 就是其中一個，透過自行實作 Provisioner，根據需求在整個 NFS Server 的目錄下創建更多更多的小目錄供對應的 PVC 去使用。 舉例來說，今天可以創建多個 PVC, 譬如 DB-Data, Container-Log 等不同的需求，然後 Provisioner 就會創建不同的資料夾供這些應用使用，最後每個 Pod 可以根據自己的需要來掛載特定的資料夾，而且互相彼此不會看到彼此資料。 唯一要注意的時，這邊也是有權限以及容量的問題，這部分就是 NFS 的基本限制。 ","version":"Next","tagName":"h3"},{"title":"[Golang] aggregate in mongo","type":0,"sectionRef":"#","url":"/docs/techPost/2018/mgo-aggregate","content":"","keywords":"","version":"Next"},{"title":"Structure​","type":1,"pageTitle":"[Golang] aggregate in mongo","url":"/docs/techPost/2018/mgo-aggregate#structure","content":"這邊就採用 golang 簡單的描述一下我們的結構，這些結構會用在 mongodb 裡面 type User struct { ID bson.ObjectId `bson:&quot;_id&quot;` Name string `bson:&quot;name&quot;` Pods []Pod `bson:&quot;pods&quot;` } type Pod struct { ID bson.ObjectId `bson:&quot;_id&quot;` CreatedBy bson.ObjectId `bson:&quot;createdBy&quot;` Name string `bson:&quot;name&quot;` }  ","version":"Next","tagName":"h2"},{"title":"Find​","type":1,"pageTitle":"[Golang] aggregate in mongo","url":"/docs/techPost/2018/mgo-aggregate#find","content":"第一種方法其實非常直覺 就是先取得該 User 的實例去 Pod 的 Collection 內進行搜尋，找出所有的 Pod 其記錄的User是 UserA. 假設我們已經有該 User 的物件了，接下來很簡單透過 find 的方式找出所有由該 User 所創造的 Pod. 並且把找到的結果都放到 User 物件裡面的 Pods 變數 c := db.C(&quot;pods&quot;) c.Find(bson.M{&quot;createdBy&quot;: user.ID}).All(&amp;user.Pods)  ","version":"Next","tagName":"h2"},{"title":"Aggregate​","type":1,"pageTitle":"[Golang] aggregate in mongo","url":"/docs/techPost/2018/mgo-aggregate#aggregate","content":"在 mgo裡面，如果想要做到 Aaggregate 類似這種 Join 的方式，其實可以透過一個叫做 pipeline 的函式來完成 為了使用 pipeline 的，我們必須要先描述我們想要如何去尋找這些資料，這些資料是由bson.M{} 陣列組成的。 該陣列內，至少要提供兩種不同的查詢方式 $match: 這邊用來描述我們要如何查詢 User 物件，在此範例中我們使用特定的User.ID來描述該 User$lookup: 用來描述要如何 Aggregate, 其中有四個欄位要填寫 from: 用來進行 Aggregate 的 collection 名稱，此範例中我們設定為 pods 這個 collection.localField: 在該 pipeline 要使用的 collection 內要使用的欄位，這邊我們會使用 User 裡面的 _id.foreignField: 在 from 這個 collection 裡面要使用的欄位，此範例就是 Pods 裡面的 createdBy.as: 這個是用來存放查詢後的資料，首先其本身必須是個Array，同時該欄位必須定義在 User 結構裡面。 程式碼如下，該程式碼的意思很簡單，我們對 users 這個 collection 裡面透過 $match 去尋找特定的 User，同時透過 $lookup 的方式到 pods 該 collection 裡面去把 User.ID == Pods.CreatedBy 相同的 pods 都找出來，最後放到 user.pods 裡面 c := db.C(&quot;users&quot;) pipeline := []bson.M{ {&quot;$lookup&quot;: bson.M{&quot;from&quot;: &quot;pods&quot;, &quot;localField&quot;: &quot;_id&quot;, &quot;foreignField&quot;: &quot;createdBy&quot;, &quot;as&quot;: &quot;pods&quot;}}, {&quot;$match&quot;: bson.M{&quot;_id&quot;: user.ID}}, } var resp User err := c.Pipe(pipeline).One(&amp;resp) if err != nil { fmt.Println(err) }  ","version":"Next","tagName":"h2"},{"title":"Performance​","type":1,"pageTitle":"[Golang] aggregate in mongo","url":"/docs/techPost/2018/mgo-aggregate#performance","content":"因為這次的範例非常簡單，所以並不能用來代表在所有的使用情境下的結果，只能用來表示如本文的情境下的效率問題。 在上述的範例專案內，一開始會先隨機產生一個使用者，並且隨機產生30000 筆 Pod 資料，並且嘗試比較不同數量的情況下，透過 Find 以及 Aggregate 實際上的效能問題。 下列的時間都是 (ms) Methond\\Numbers\t1000\t5000\t10000\t30000\t50000Find\t5.7\t18\t35\t111\t250 Aggregate\t7.4\t22\t77\t180\tNaN 這邊要特別注意的是，當 Pods 的數量過高的時候，使用 pipeline 來處理就會得到下列的錯誤訊息 Total size of documents in pods matching { $match: { $and: [ { createdBy: { $eq: ObjectId('5b86dac74807c532d70bea52') } }, {} ] } } exceeds maximum document size 根據官網描述，預設的情況下，只能存放 16MB 的資料。 這意味者資料過多的情況下，使用者要注意這個現象，避免資料存取失敗。 ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"[Golang] aggregate in mongo","url":"/docs/techPost/2018/mgo-aggregate#summary","content":"本文假設的情境非常簡單，基本上兩種方法都可以完成，但是在資料多寡的情況下，花費的時間就有所區別。 若是有更複雜的需求，在 pipeline 裡面還可以設定除了 $lookup 以及 $match 以外的用法，透過一次的呼叫就把資料給查詢完畢。 至少如果只是本文這種很簡單的情境，其實自己額外查詢即可，不論在簡潔性跟效率上我認為都更高。 所以還是一樣，不同情境下，每個功能都會有不同的使用方法跟考量點。 這邊就讓各位自己去評估囉 ","version":"Next","tagName":"h2"},{"title":"Reference​","type":1,"pageTitle":"[Golang] aggregate in mongo","url":"/docs/techPost/2018/mgo-aggregate#reference","content":"https://docs.mongodb.com/manual/aggregation/ ","version":"Next","tagName":"h2"},{"title":"[netfilter] Introduction to ebtables","type":0,"sectionRef":"#","url":"/docs/techPost/2018/netfilter-eiptables-i","content":"","keywords":"","version":"Next"},{"title":"Preface​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#preface","content":"再之前的 kubernetes server 系列文中有稍微介紹過 iptables 的一些用法，以及如何透過 iptables 來完成 kubernetes service. 若有任何不熟悉的概念可以重新閱讀一次該系列文章. [Kubernetes] How To Implement Kubernetes Service - ClusterIP 回到主題，這次想要跟大家慢慢介紹的就是 iptables 這個常見也常用的工具。 網路上其實已經可以搜尋到非常多關於 iptables 相關的文章。 不論是基本介紹，或是一些相關用法，其實都有滿多的資源可以學習，不過我認為這些文章都散落各地，所以想要整理一下這些資訊並且統整起來做一個一系列的iptables文章。 這個系列文的內容大致上如下 iptables/ebtables 的基本架構介紹，包含下列各種組成的概念 Target/Chain/Table/Match 透過 docker 預設網路Bridge的情況來解釋，容器與外界網路，容器與容器彼此之間的網路傳輸，實際上再 iptables 中到底會怎麼運作，如果想要處理這些封包，該怎麼設定相關規則。介紹相關 iptables 常見的使用問題最後則是會跟大家介紹，如何自己手動撰寫一個 iptables 擴充模組，讓你的iptables擁有獨一無二的功能 本文則是系列文第一篇，將著重於netfilter的架構介紹，讓大家對於netfilter 的概念有個基本的認知與瞭解。 最後則會介紹一下 ebtables 的概念以及封包傳遞過程。 相關系列文章 [netfilter] Introduction to iptables[netfilter] Dig Into Docker Bridge Network By iptables/ebtables ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#introduction","content":"一般我們常常在講的 iptables，其實背後真正的專案以及相關技術都跟 netfilter 密切相關。 netfilter 的官方網站 上面有非常詳細的介紹，同時有非常多相關的專案。 譬如從使用者會使用的指令到給使用者開發的函式庫全部都該網站上面找到，而今天我們的主角 iptables 則是屬於給使用者使用的指令. 下圖是一個可以簡單描述整個 iptables/netfilter 的架構圖。 整個系統的組成分成三大部分，分別是 user-space,kernel-space 以及 network interface cards.  ","version":"Next","tagName":"h2"},{"title":"User Space​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#user-space","content":"大家最常見也最熟悉的 iptables 以及比較少人知道的 ebtalbes 都是所謂的 user-space 的應用程式。 這類的應用程式提供了一個友善且易於操作的環境讓使用者/系統管理者有辦法去對kernel的 netfilter 進行操作。 除了 netfilter 專案本身提供的工具之外，使用者也可以自行的透過相關的程式語言函式庫來與 kernel 內的 netfilter 溝通。 與kernel的溝通方面，一般都是採用 system call 的方式來溝通，針對一些特別的應用甚至可以採用 netlink 的方式去接收封包來進行二次處理。 對二次處理有興趣的人可以搜尋 NFQUEUE 相關的資源 ","version":"Next","tagName":"h3"},{"title":"Kernel Space​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#kernel-space","content":"整個 netfilter 的精華與操作基本上都是在 kernel 層級去完成的。再此 netfilter 的子系統內，包含了所有使用者透過 iptables 所傳達的所有指令，譬如丟棄所有來自a.b.c.d的封包 這種類似的規則，全部都會存在 kernel 內。 這邊值得注意的是，因為 kernel 每次重新開機後，上次存在記憶體內的資訊都會消失，這意味者所有iptables所傳達的命令都會消失，因此才會有所謂的 iptables save/restore 等相關的指令然後搭配上 systemd/upstart 等機制來確保每次重新開機後相關的規則都會重新部屬上去。 ","version":"Next","tagName":"h3"},{"title":"Network Interface Card​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#network-interface-card","content":"大家會使用 iptables 一定都是想要針對特定的網路封包進行一些處理，而這些網路封包必然伴隨網路設備 Network Interface Card 一同出現。 這些設備可以是實體的網路卡，也可以是系統上虛擬出來的，譬如loopback,docker0(bridge0),tuntap,veth 等各式各樣的虛擬網卡。 當這些網卡跟 kernel 透過某種關係連結後，從這些網卡進/出的封包就會受到 netfilter 的影響，進而可以透過 iptables 所傳達的規則進行控制與約束 ","version":"Next","tagName":"h3"},{"title":"Workflow​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#workflow","content":"網卡與系統掛勾，此時 kernel 知道有哪些網卡Physical/Virtaul 正在系統上使用者透過 iptables/ebtables 等工具將相關的規則寫入到 kernel 裡面，此時 kernel netfilter subsystem 內就會有對應的規則。當有任何封包從任何網卡內進/出時，這些封包就會被上述已經創立的規則進行比對，並且進行對應的動作。 ","version":"Next","tagName":"h3"},{"title":"iptables/ebtables​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#iptablesebtables","content":"有了上述的基本流程後，我們接下來要專注再user-space 的規則方面，瞭解該怎麼解讀每個規則。 大部分的人比較少聽到 ebtalbes, 然而為了更加理解容器本身的網路運作原理，因此這邊還是要跟大家介紹一下 ebtalbes 這個工具。 ","version":"Next","tagName":"h2"},{"title":"Component​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#component","content":"規則組成方面，基本上 iptables/ebtables 的概念是一致的，所以為了節省空間，決定一起介紹。 這邊我們先隨便拿一個常見的 iptables 規則來當做範例 iptables -t nat -A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE  上述的規則用中文來說就是我希望來源IP不屬於 10.244.0.0/16, 且目的IP屬於 10.244.0.0/16的封包，都進行 MASQUERADE(SNAT) 這個動作 但是對於整個系統來說，該指令其實可以分成四大部分 Table: -t natChain: -A POSTROUTINGMatch: ! -s 10.244.0.0/16 -d 10.244.0.0/16TARGET: -j MASQUERADE 接下來會針對這四個部分進行說明與介紹 ","version":"Next","tagName":"h3"},{"title":"Table​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#table","content":"首先，我們已經知道整個 iptables/ebtables/netfilter 的運作是由各式各樣的規則所組成的。 然而每條規則的用途不會完全相同，為了妥善管理與使用，會希望將相同用途的規則放置在相同的 Table 裡面。 舉例來說，最常見也是大部分指令的預設值, filter其功能就如同其名稱一樣，用來進行 filter 相關的規則，譬如將封包丟棄，讓封包通過等常見行為都放置在這個 filter 的 table 中。 針對上述的範例, -t nat 的意思就是這條規則要放置在 nat 這個Table之中。 ","version":"Next","tagName":"h3"},{"title":"Chain​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#chain","content":"Chain 的概念比較複雜，分為所謂的 Build-in Chain 以及使用者自己創立的 Chain. 使用者創立的 Chain 比較偏向用來管理，而且必須要從 Build-in Chain 這邊進去，所以這邊還是會比較專注於 Build-in Chain 的介紹。 Chain 相對於 Table 來說，我覺得可以想成是一個狀態點，該狀態是用來描述該封包當前的階段。 用比較口語話的說法就是這條iptables規則會再什麼時間點去封包進行匹配。舉 INPUT chain 來說，這個狀態就代表封包準備進入到系統後但是還沒有被對應的應用程式接收。 以底層的實現來說， Table 與 Chain 彼此是互相連結綁定的。 只有特定相關邏輯的 Table 可以再特定時間點上 Chain 去組合成一個常見的 iptables 規則 譬如 filter Table 就可以運行再 INPUT Chain 但是卻不能運行再 PREROUTING Chain. 實際上會有哪些 Chain 以及哪些 Table 會因為 iptables/ebtables 本身的架構而有所不同，因此會在下面的部份再詳細介紹。 針對上述的範例 -A POSTROUTING 來看一下，他的意思就是我希望當前這條規則要放到 POSTROUTING 這條 chain 裡面，同時因為 Chain 裡面的規則本身是有次序性的, 所以再寫入規則的時候 可以使用 -I (insert) 或是 -A (append) 的概念來決定這條新規則的位置。 ","version":"Next","tagName":"h3"},{"title":"Match​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#match","content":"Match 的概念就非常簡單，每個規則都可以去描述符合該規則的封包格式，這部份除了預設的封包格式之外，也有不少擴充模組可以使用。 譬如最常見的就是-m tcp --dport 53 這種針對 TCP 封包且目標連接埠是53的規則 回到上述的範例,! -s 10.244.0.0/16 -d 10.244.0.0/16 這個規則希望可以比對封包的來源/目的 IP 位置，同時藉由 ! 這個符號來反轉比對結果。 因此就是所有來源不是 10.244.0.0/16 但是卻要送往 10.244.0.0/16 的封包。 此外，在 Match 方面，一條規則可以同時使用多個擴充模組來更加強化要比對的規則。 ","version":"Next","tagName":"h3"},{"title":"Target​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#target","content":"最後一個部分，就是當規則符合之後，要進行什麼樣的動作。 常見的有 Acceptr,Drop 這種給 filter Table 使用的動作。 複雜的甚至可以是修改封包格式，譬如 Source Network Address Translation(SNAT), Destination Network Address Translation(DNAT) 這種。 回到上述的範例, -j MASQUERADE，這邊會透過 -j 的方式描述後面要銜接的參數就是 Target, 而 MASQUERADE 則是一種讓kernel幫你維護的 SNAT 動作。 ","version":"Next","tagName":"h3"},{"title":"ebtables​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#ebtables","content":"ebtalbes 相對於 iptables 來說使用的情境比較少，主要是其針對的目標範圍都著重於 Layer2 這層次，也就是 Ethernet Frame 這邊。 對於大部分的使用者以及管理者來說，通常都是以 IP 為基本單位來進行管理的，然而部分的網路元件設計者，譬如 Virtlet, a Kubernetes runtime server 再其架構中就有使用到 ebtables 來幫忙完成特定的功能。 此外，為了下一篇文章能夠更理解整個 docker bridge network 的傳輸過程，對於 ebtables 還是要有點基本概念，這樣才可以對整體架構有個完整的認識 ","version":"Next","tagName":"h2"},{"title":"Table​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#table-1","content":"Table 方面，目前的 ebtalbe 總共有三張 tables, 分別是 filter,nat 以及 broute. filter 則是專注於 Accept/Drop 相關過濾的行為nat 這邊則專注於針對 MacAddress 相關的轉換broute 這邊可以用來決定封包到底要進行 Layer2 Bridging 或是 Layer3 Routing。算是一個比較特別的規則。 ","version":"Next","tagName":"h3"},{"title":"Chain​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#chain-1","content":"Chain 的話，再 ebtables 中總共有六條 chain. brouting: 這個 chain 的執行順序非常的早，基本上是ebtalbes 裡面最早的 chain. 只要當任何 linux bridge 上面的任何一個 port 有收到任何 Frame 進來後，就會到這個Chain裡面去進行比對. 這邊通常不太會去設定，而是都會依賴後續的 Bridging Decision透過Mac Address 去決定封包到底該怎麼走。 因為 Linux Bridge 本身會有 STP(Spanning Tree Protocol) 的運作，因此只有歸類於可轉發的連接埠才需要去進行這些封包比對。 input: 如果今天封包的目標 Mac Address 是 Linux Bridge 本身的話，那封包就會進入到 input chain 來處理。最常見的範例就是 docker 容器想要對不同網段進行存取，會先進入到 gateway 也就是所謂的 linux bridge 本身。 output: 針對要從 Linux Bridge 離開的封包都會進行處理的Chain，這類型的封包大致上有兩種可能。第一種是主機本身產生的封包，目的就是要從該linux bridge底下的某些 Port 轉發出去，或是該封包是從 linux bridge 的某些 Port 轉發到其他的 Port. forward: 針對會被Linux Bridge進行Layer2 Bridging 轉發的封包所進行的 Chain. 基本上同網段的容器間傳輸的封包都會到這個階段。 prerouting: 這個 Chian 就是其名稱的解讀，Pre-Routing, 再封包進入到 Linux Bridge 後，但是還沒有碰到 Bridging Decision 前可以進行的階段。 postrouting: 這個 Chian 就是其名稱的解讀，Post-Routing, 再封包準備離開 Linux Bridge 前，但是還沒有碰到 真正的透過網卡送出去前可以進行的階段。 其實對於 ebtables 來說，用 pre-forwarding 以及 post-forwarding 會更貼切，畢竟 routing 是更偏向 Layer3 路由方面的規則。 這邊因為 iptables 長期的習慣所以在命名方面也就遷就於此 ","version":"Next","tagName":"h3"},{"title":"Match​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#match-1","content":"在比對的規則來說， ebtalbes 專至於 Layer2 相關的處理，譬如 unitcast/multicast/broadcast，甚至是 vlan/stp 等 封包都可以處理。 此外，除了常見的封包內容外，也可以透過-m這個方式去使用擴充模組來達到更靈活的比對功能。 ","version":"Next","tagName":"h3"},{"title":"Target​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#target-1","content":"就如同前面所描述的，預設的 Target 其實都會跟對應的 Table 有關，譬如 ACCEPT/DROP 就會在 filter/brouting 這些Table. 雖然都叫做 ACCEPT/DROP, 其兩者的意思在 filter/brouting 的用途卻是不一樣的，有興趣的人可以直接參考 man page 來學習更多的用法與概念。 除此之外，也有許多的擴充模組提供更多強大的功能，譬如可以透過 arp 等相關的 Target, 針對 ARP 相關的封包直接讓系統幫你回應對應的 ARP Reply。 ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#summary","content":"看到這邊，我們用一張圖片，將上述的概念重新整合一次，將 Table/Chain 與 ebtables 進行整合。  首先，關於該圖片中我大致上分成三個層次，分別 user-space 的任何網路應用程式kernel-space 裡面偏向 Layer3 相關的處理，這部份其實是iptables的主戰場，但是因為整體架構的原因，這邊只會稍微帶過kernel-space 裡面偏向 Layer2 相關的處理，這邊就是偏向 ebtalbes 的處理 首先當封包從跟Linux Bridge有關的網卡，左邊的nic 近來之後，首先會先進入到 brouting 這個 chain，brouting 這個 chain 裡面只支援 brouintg Table, 若有任何規則要將該封包直接透過routing處理的話，封包就會直接拉到 kernel-space/layer3 層級來處理， 這個範例中先用一個 Magic 的概念來代表任何跟 Layer3 有關的操作。 接者封包會跑到 Prerouting 這邊，該chain裡面只有nat可以執行，這意味到這個階段頂多只能對封包進行內容的修改，還沒有辦法丟棄。 這邊要注意，這時候還沒決定到底封包該怎麼轉送，所以可以透過修改封包的目標 Mac Address 來影響到底之後該怎麼轉送封包，因此這也是這邊 Chian 為什麼叫做 PreRouting. 接下來就會到所謂的 Bridging Decision. 這邊其實就會運行 Learning Bridging 相關的算法，根據 Mac Address 來決定轉發的方向，若目標對象是 Linux Bridge 本身，則會透過 input chain 一路往上送到 Layer3 去處理。 對於 input 來說，可以透過 filter 來進行封包的處理，決定哪些封包要過，哪些不要過。 如果只是該 Linux Bridge 下不同連接埠的轉發的話，就會走 Forward 這邊，最後透過 Postrouting 這邊進行後續的修改，最後就從目標的網卡將該封包送出去。 如果今天是主機上面的應用程式要送封包出去，且該封包目的地最後會透過 Linux Bridge 來轉發 則該封包最後經過 Layer3 神祕的處理後，最後會跑回到 Layer2 這邊，並且經過 output chain 以及 postrouting 依序的處理，最後也走到網卡出去。 本文到這邊我們先建立一個基本的概念，到底 iptables/ebtables 的組成元件以及對應的概念。 同時也稍微觀看了一下 ebtables 本身的封包傳輸流程。 然而只有單單的 ebtables 是沒有辦法理解以及解釋整個 docker bridge network 的運作及傳輸。 因此我們會在下篇文章以相同的觀點來分析 iptables, 並且將 iptables 以及 ebtables 給整合起來分析整體的運作流程。 到時候在分析 docker bridge network 的時候，能夠有更詳細的概念與背景去瞭解整體封包的傳輸過程 ","version":"Next","tagName":"h3"},{"title":"Reference​","type":1,"pageTitle":"[netfilter] Introduction to ebtables","url":"/docs/techPost/2018/netfilter-eiptables-i#reference","content":"ebtables man pageebtables/iptables interaction on a Linux-based bridge ","version":"Next","tagName":"h2"},{"title":"[netfilter] Introduction to iptables","type":0,"sectionRef":"#","url":"/docs/techPost/2018/netfilter-eiptables-ii","content":"","keywords":"","version":"Next"},{"title":"Preface​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#preface","content":"這次想要跟大家慢慢介紹的就是 iptables 這個常見也常用的工具。 網路上其實已經可以搜尋到非常多關於 iptables 相關的文章。 不論是基本介紹，或是一些相關用法，其實都有滿多的資源可以學習，不過我認為這些文章都散落各地，所以想要整理一下這些資訊並且統整起來做一個一系列的iptables 文章。 這個系列文的內容大致上如下 iptables/ebtables 的基本架構介紹，包含下列各種組成的概念 Target/Chain/Table/Match 透過 docker 預設網路Bridge的情況來解釋，容器與外界網路，容器與容器彼此之間的網路傳輸，實際上再 iptables 中到底會怎麼運作，如果想要處理這些封包，該怎麼設定相關規則。介紹相關 iptables 常見的使用問題最後則是會跟大家介紹，如何自己手動撰寫一個 iptables 擴充模組，讓你的iptables擁有獨一無二的功能 本文延續前一篇 ebtables 的介紹，將使用相同的概念來闡述 iptables(ipv4) 的概念，包含了 Tarble/Chain/Match/Target 等功能。 相關系列文章 [netfilter] Introduction to ebtables[netfilter] Dig Into Docker Bridge Network By iptables/ebtables ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#introduction","content":"為了能夠更充分理解本文所描式的各個觀念，強烈建議先閱讀前篇文章 來理解整個規則裡面的四大部分，Table/Chain/Match/Target 這邊再次做個快速的複習 Table: 相同用途的 rules 會放在相同的 Table 中，常見的有用來當防火牆的 filter，或是修改封包內容的 nat.Chain: 封包比對的時間點，不同時間點下能夠進行不同的動作。這意味每個Chain 下能夠搭配的 Table 是有限制的。Match: 每個規則都要描述當前規則希望匹配的封包內容，除了基本的比對欄位外，還可以用各式各樣擴充模組來匹配不同的封包內容。Target: 當封包比對成功後，要執行什麼樣的動作，不同於 Match 可比對多個內容，每個規則都只能採用一個 Target 來採取動作。 ","version":"Next","tagName":"h2"},{"title":"iptables​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#iptables","content":"iptables 的用途非常的廣，以 docker 為範例來說，從基本的容器對外上網，這邊會需要 SNAT 來轉換封包。或是透過 docker run -p 1234:80 xxxx 這種方式讓外界能夠透過 DNAT 的方式來存取容器內的特定連接埠。 上述的這些顯而易見的操作實際上背後是牽扯到了非常複雜的封包傳輸，為了理解這部份，我們要先來檢視一下 iptables 裡面四大部分的介紹 ","version":"Next","tagName":"h2"},{"title":"Table​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#table","content":"Table 方面，目前的 iptables 總共有五張 tables, 分別是 filter,nat,raw,mangle 以及 security. filter: 跟 ebtables 一樣， filter Table 也是 iptables 系列指令的預設 table, 用來存放如 ACCEPT/DROP 等相關防火牆功能的規則。nat: 就是如同其名稱一樣，Network Address Translation(nat)，對於來源或是目的的 IP 地址進行修改的動作都是再這邊進行的。 實際上再 Linux Kernel 內有一套叫做 conntrack 的機制去維護所有經過本機的網路連線。 基本上只有新建立的連線才會進入到 nat 這個 table 去處理。 畢竟以 SNAT 這種會需要動態產生一個 Port 來進行轉發的動作，其實每條連線只要進行一次就好，後續該連線的封包就讓 kernel 幫你默默的執行就好。 之後有機會再來討論一下 conntrack 的機制與架構，以及其能夠提供什麼樣的資訊給系統管理者/使用者 3. raw: 這個 chain 比較少使用，其用途是用來特別處理不想要讓 kernel: 幫你管理 conntrack 的封包。 4. mangle: 除了nat能夠修改封包的 IP 地址外， mangle 也會用來進行一些封包的修改。然而其修改會比較偏向一些 metadata 標籤概念的欄位，讓其他的規則可以透過檢視這些標籤來得知該封包先前有符合某些條件，藉由這些更多的條件判斷來決定該怎麼處理封包。 舉例來說，再 iptables 裡面有所謂的 mark 的概念，這個32bit的欄位並不屬於 OSI 裡面的任何一層的封包格式，而是 linux kernel 裡面用 sk_buff 該描述封包結構中自己新增的欄位。 透過這個欄位我們可以在不同的階段去追蹤相同的封包，來達到更複雜的處理。 譬如再 FORWRAD Chain 我想要知道這個封包是不是之前有再 PREROUUTING 被處理過，就可以用該 mark 來處理。 6. security: 這個 table 更少出現，必須伴隨者 SELinux 的使用來提供更多安全相關的功能，主要牽扯到 Mandatory Access Control (MAC) 規則以及 Discretionary Access Control (DAC) 這兩者的管理，有興趣的可以看看最初的 commit。 上述裡面，基本上 raw/mangle/security 這三個 table 比較少使用，所以後續會比較著重在 filter/nat 這兩個 table 為主。 ","version":"Next","tagName":"h3"},{"title":"Chain​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#chain","content":"Chain 的話，再 ibtables 中總共有五條 chain. PREROUTING: 這個 Chian 就是其名稱的解讀，Pre-Routing, 再封包進入到 Linux Kernel 後，但是還沒有碰到 Routing Decision 前可以進行的階段。 這邊舉一個現實會使用到 PREROUTING 的使用情境，很多人在家裡可能會有架設 server/nas 等各種服務的可能，然而因為 IP 地址數量的限制，這些背後的機器都會使用私有的 IP 地址，譬如 192.168.0.0/16, 這種情況下為了讓外界能夠順利的存取到這些內部的 server/nas，常見的作法都是會在家裡對外上網的那台 router 設定譬如 PortFORWARDing/虛擬伺服器 等功能， 將特定的連接埠轉發到內部 server 的私有IP地址及連接埠。 這功能實際上就是在 PREROUTING 這個階段會進行 DNAT，將封包的目的IP位址與連接埠都轉換到內部server的IP地址與連接埠。 最後透過 Routing Decision 來往後轉發 INPUT: 如果該封包根據 Routing Decision 後封包是要進入到本機系統，譬如系統上的應用程式，譬如 www server。 則INPUT就是查詢完畢到封包被應用程式接收的中間階段。 如果今天機器上架設了一個 nginx server, 並且聽再 0.0.0.0:80. 則任何送到該機器網卡上面且連接埠是80 的封包最後都會經過 INPUT chain 來處理。 所以也可以在這邊透過其他的選項丟棄掉不想要連接到 nginx server 的封包。 FORWARD: 如果該封包根據 Routing Decision 後封包是要幫忙轉發。則FORWARD就是查詢完畢到封包要從網卡送出去的中間階段。 實際上，預設的 linux kernel 是沒有 FORWARD 的功能的，必須要將 kernel 關於 ip_FORWARD 的開關打開才可以使用。 所以才會看到很多篇文章都在講解需要 echo 1 &gt; /proc/sys/net/ipv4/ip_FORWARD 這種方式打開 kernel 內關於轉發的功能。 詳細的程式碼有興趣可以參考下列連結,(我這邊隨便找了一個 Linux Kernel 4.3 的程式碼) Read The ip_FORWARDCheck the device ip_FORWARD configCheck the config to decide the routing OUTPUT: 針對要從 Linux Kernel 離開的封包都會進行處理的階段，這類型的封包是主機本身產生的封包，目的就是要從某些網卡轉發出去。 舉例來說系統上的 nginx www server 要回應使用者的需求，這些回應的封包就會走 OUTPUT chain 出去。 POSTROUTING: 這個 Chian 就是其名稱的解讀，Post-Routing, 再封包準備從系統出去前，但是還沒有碰到真正的透過網卡送出去前可以進行的階段。 這邊繼續使用家裡架設的 server/nas 當作範例，因為IP地址不夠的問題，所以內部這些server/nas要出去的封包其來源IP地址必須要修改成對外Router的IP地址。 而這個行為我們稱為所謂的 Source Network Address Translation (SNAT)，而這個操作都是在 POSTROUTING 這邊去執行的。 ","version":"Next","tagName":"h3"},{"title":"Match​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#match","content":"在比對的規則來說， iptables 專至於 Layer3 相關的處理，譬如 IP 的來源/目的地址，以及當前封包使用的Layer4協定，譬如 tcp, udp, udplite, icmp, esp, ah, sctp。 此外，除了常見的封包內容外，也可以透過-m這個方式去使用擴充模組來達到更靈活的比對功能。 譬如常見的 -m tcp --dport=1234 這個額外的 tcp 模組能夠檢查 TCP 封包裡面的欄位。 因為原生的 iptables 只有檢查到所謂的 protocol 協議而已，並沒有再細部的去解析封包內容，因此若需要細緻到該協議的內容，都需要依賴擴充模組來完成。 ","version":"Next","tagName":"h3"},{"title":"Target​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#target","content":"就如同前面所描述的，預設的 Target 其實都會跟對應的 Table 有關，譬如 ACCEPT/DROP 就會在 filter 這些Table. 再 iptagles 裡面有四個預設的 Target ACCEPT: 該封包判定通過，直接離開這個 Chain.DROP: 丟棄該封包，直接離開這個 Chain.QUEUE: 可以把封包從 kernel-space 透過 netlink 的方式送到 user-space 去後搭配 DPI(Deep packet inspection) 進行封包檢測來判斷當前封包的類型與種類OTHER_CHAIN: 使用者可以自己創見新的 chain 然後透過 -J $CHAIN_NAME 的方式讓封包跳到不同的 custom_chain 去進行比對。RETURN: 直接返回上一層的 chain, 通常是會搭配 -j $CHAIN_NAME 一起使用。 此外再 iptables 有非常多有趣的 Target 可以執行 SNAT/DNAT: 這種修改封包 IP 地址的行為NFQUEUE: 擴充原先的 QUEUE，提供更多的 queue number 供 user-space 選擇。log: 單純記錄封包資訊，並且從 kernel 輸出，可以傭 dmesg 去觀察該記錄。由於該 Target 的實作，其本身並不會做到類似 ACCEPT/DROP 這種馬上決定該封包去留的行為，而是會繼續讓封包往下一個規則嘗試比對。 下一篇文章就會大量使用到 log 這個 target 來幫助我們觀察再容器間封包傳輸時，到底有哪些 iptables/ebtables 會被呼叫到。 ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#summary","content":"有了上述的基本概念後，我們把這些概念重新整合一次，將 Table/Chain 與 iptables 進行整合，同時為了簡單清楚，我們就只專注於 nat/filter 這兩張 Table 即可。  首先，當封包從網卡進入後，首先會經過 conntrack 的管理，讓系統幫你進行連線追蹤的相關工作。 這邊的說法都是精簡的，因為去掉了 raw/mangle/security 這些 Table 的關係，實際上 raw 本身的運作會比 conntrack 還要快。 接者就是所謂的 PREROUTING, 再系統根據封包的目的地IP地址進行選擇前，我們可以在 PREROUTING 透過 DNAT 的方式修改封包的目的IP地址，藉此改變封包的傳送對象。 最後就是所謂的 Routing Decision 了，這部份會在 kernel 內透過查詢 routing table 的方式 可以透過 ip route, route 等相關的指令查詢系統上當前的 routing 規則。 Routing 查詢完畢之後，會有兩個走向，一個是將封包透過 Socket 的方式讓 上層的應用程式 去收取封包，這種情況下就會走過 INPUT chain 的階段處理。 管理者就可以在 INPUT 這邊實現簡單的防火牆，來針對特定的封包給予通過或是丟棄。 若查詢 routing 後決定要將封包轉發同時系統也有透過 /proc/xxxxxx/ip_FORWARD 進行設定，則此時就該封包就會開始進入到 FORWARD 這個階段進行處理, 此時也可以透過 filter 進行簡單的防火牆過濾，決定封包的去留。 走完 FORWARD 後就是所謂的 POSTROUTING 了，這邊可以進行所謂的 SANT, 將封包的來源 IP 地址修改以順利讓該封包能夠建立一條順利的網路連線。 實際上，再 iptables 的規則中，有兩種的 SNAT 的實現方法，分別是 -j SNAT xxx.xxx.xxx.xxx:xxx 以及 -j MASQUERDAE. 因為 SNAT 再運作的時候其實需要考慮連接埠的轉換，每一條出去的連線都要搭配一個連接埠來作為回傳連線的匹配對象，所以傳統的 SNAT Targer 需要特別指定該次 SNAT 轉換後用的IP地址與連接埠。 不過這種情況實在是會讓整個系統變得不好用，所以後來發展出了 MASQUERADE 這種動態 SNAT 的方式讓 kernel 自己幫你選擇要使用的 IP 以及連接埠。 最後，若使用者的網路應用程式需要往外傳送封包，則該封包會先進入到 OUTPUT Chain, 這邊也可以透過 filter 進行防火牆的操作。 最後封包就會走入 POSTROUTING 進行後續的處理。 ","version":"Next","tagName":"h3"},{"title":"Summary With Ebtables.​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#summary-with-ebtables","content":"前述我們已經看到了 iptables 的運作流程，而前篇文章我們也看過了 ebtables 的運作流程。 現在我們需要將這兩者的邏輯給結合，構造出一個更複雜的網路系統。  這張圖裡面我們依然分成三個部分來看待，分別是 User-Application, Kernel-Space/Layer3 以及 Kernel-Space/Layer2. 然後圖中針對 iptables 以及 ebtalbes 使用不同的顏色來表示彼此的 Chain/Table。 接下來要來好好的解釋這張圖的概念，在開始前我們先有一些相關的理解。 iptables 無所不在，縱使在 Layer2 Bridging 的世界中，還是會牽扯到 iptables 的運作。(實際原因我不清楚，但是我想跟 conntrack 有關，任何的封包連線 linux kernel 都想要追蹤, 此外，我想與透過 IP 地址方式去操作管理平易近人也有關)所有封包的 起頭/終點 一定是 (網卡/應用程式),中間會經過 Layer2 也會經過 Layer3，這部份完全看你封包來源網卡與封包目標網卡屬於什麼層級而決定怎麼走 好了，我們可以好好的來重新審視這張圖，一開始我們就先從封包的進入點，也就是網卡這邊來看。 首先當封包進入網卡的時候，會先進入所謂的 Bridge Check 這個階段，這時候會決定封包要走到 Layer3 處理，還是 Layer2 處理。底下會針對這兩個 Case 探討 其實這個階段非常有趣，各位可以想想看，當你看到一個封包，你怎麼知道這個封包到底是要 Routing 還是要 Bridge? 實際上在 Linux Kernel 來說，是透過所謂的 netdev_rx_handler_register 來註冊每張網卡收到封包後該怎麼處理。 以 Linux Bridge 來看，當透過 brctl addif br1 xxx 這個方式把 xxx 加入到 br1 這個 bridge 時，就會把 xxx 這個網卡的接收封包函式設定成 bridge 有關，所以之後近來的封包就會走 Layer2 的方式去跑，反之亦然其他按照相同道理就會走 Layer3 的流程。 有興趣觀看原始碼的可以參考下列連結register handler functioncall ebtables ","version":"Next","tagName":"h2"},{"title":"Layer3​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#layer3","content":"如果今天封包走到了 Layer3 這邊來處理，那處理的流程基本上就跟本文前半部分描述的雷同，唯一不同點只有當進行完畢 Routing Decision 後，在選擇 FORWARDd 的階段，若轉送目的地網卡對應到的是屬於本機上面的 Linux Bridge 網卡，則封包最後又會走到 Layer2 那層，在這情況下就會在經過 iptables 後又會馬上轉接 ebtables，最後就會送到網卡出去。 ","version":"Next","tagName":"h3"},{"title":"Layer2​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#layer2","content":"如果封包一開始進入點就是 Linux Bridge 的網卡，這時候可以在 brouting chain 進行一次檢查，如果這時候有透過 ebtables特別將封包送到 Layer3 處理的話，那流程就會如同上述一樣，從 conntrack 一路走到 Routing Decision. 如果繼續決定走 layer2 來處理封包的話，那流程就會跟前篇文章講解 ebtables 敘述的流程一樣。只是這邊要特別注意的是，實際上 iptables 也會混雜在 layer2 的處理過程中，所以在真正進行 Bridge Decision 前也會遇到 iptables PREROUTING 進行處理。 如果透過 Bridge Decision 查詢目標的 MAC Address 後決定將封包轉送到 Linux Bridge 本身的話，那最後就會走向 Layer3 上層的走法，否則則會繼續在 Layer2 這邊將封包往其他的 Bridge Port 去轉發。 在 Bridge Port 轉發的過程中，也是會牽扯到 iptables 相關的規則。所以若只是單純的兩個 Bridge 底下的封包互相轉傳的話，其實也是可以透過 iptables 使用 IP 去控制封包轉送，或是透過 ebtables 透過 MAC Adddress 去控制封包轉送。 ","version":"Next","tagName":"h3"},{"title":"Application​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#application","content":"最後來看，如果是從本機應用程式送出去封包的走向，首先封包一定會經過 Layer3 相關的轉發，經過 Routing Decision 時就會知道目的地的網卡為何，如果目標網卡是屬於 Linux Bridge, 則最後該封包又會一路走到 Layer2 的部份，此時又可以透過 iptables/ebtalbes 兩者來處理封包。 如果目的網卡不是上述的，那基本上就會直接走完 iptables 的過程，最後透過網卡轉發出去。 其實比較正確的比對方式應該是該網卡本身會怎麼處理封包，在 Linux Kernel 裡面會針對每個網卡net device去設定相關的收送函式，當有封包要從該網卡送出去時就會呼叫對應的函式，這時候裡面就會決定應該要怎麼處理封包，進而去呼叫對應的 iptables/ebtalbes 相關的處理。 所以一些特別的網卡，不論是 IPSec/VXLan/Tun/Tap 等實際上怎麼運行都還是要看 kernel 內真正的實作來決定到底封包會怎麼走。 到這邊已經將 iptables 以及 ebtables 兩者的關係給結合起來，可以觀察到實際上會經過的規則是非常的多。 下篇文章我們會嘗試使用真正的容器環境，搭配一些擴充模組來實際觀察這些容器不同方向的封包傳輸實際上會牽扯到哪些相關的 TABLE/CHAIN. ","version":"Next","tagName":"h3"},{"title":"Reference​","type":1,"pageTitle":"[netfilter] Introduction to iptables","url":"/docs/techPost/2018/netfilter-eiptables-ii#reference","content":"ebtables man pageebtables/iptables interaction on a Linux-based bridge ","version":"Next","tagName":"h2"},{"title":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","type":0,"sectionRef":"#","url":"/docs/techPost/2018/netfilter-eiptables-iii","content":"","keywords":"","version":"Next"},{"title":"Preface​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#preface","content":"這次想要跟大家慢慢介紹的就是 iptables 這個常見也常用的工具。 網路上其實已經可以搜尋到非常多關於 iptables 相關的文章。 不論是基本介紹，或是一些相關用法，其實都有滿多的資源可以學習，不過我認為這些文章都散落各地，所以想要整理一下這些資訊並且統整起來做一個一系列的iptables 文章。 這個系列文的內容大致上如下 iptables/ebtables 的基本架構介紹，包含下列各種組成的概念 Target/Chain/Table/Match 透過 docker 預設網路Bridge的情況來解釋，容器與外界網路，容器與容器彼此之間的網路傳輸，實際上再 iptables 中到底會怎麼運作，如果想要處理這些封包，該怎麼設定相關 規則。介紹相關 iptables 常見的使用問題最後則是會跟大家介紹，如何自己手動撰寫一個 iptables 擴充模組，讓你的iptables擁有獨一無二的功能 本文延續前一篇 ebtables 的介紹，將使用相同的概念來闡述 iptables(ipv4) 的概念，包含了 Tarble/Chain/Match/Target 等功能。 相關系列文章 [netfilter] Introduction to ebtables[netfilter] Introduction to iptables ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#introduction","content":"本文是結合前述兩篇理論文章後的實戰文，要透過對 iptables/ebtables 的操作來實際觀察封包於不同的情境之中傳輸實際上會經過哪些 iptables/ebtables 的控管。 ","version":"Next","tagName":"h2"},{"title":"Software Requirement​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#software-requirement","content":"在實際觀察前，我們需要先建立好一個容易測試的環境，我自己測試的環境如下 Linux: 4.4.0-128-genericUbuntu: Ubuntu 16.04.4 LTSDocker: 17.06.2-ce 整個測試用的所有程式以及相關腳本都可以在 iptables experience 這邊找到 ","version":"Next","tagName":"h3"},{"title":"Environment​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#environment","content":"本文所有的測試情境都會基於下圖的環境。左邊是以上帝視角的視野來觀察整個測試環境，右邊則是採用 User-Space/Kernel-Space 此角度來觀察測試環境  首先會先準備兩個 Container 容器，這兩個容器分別扮演 Nginx Server 以及 Ping Clinet 的角色。 此外，主機上面本身也要擁有 Ping 的能力，若沒有的需要進行安裝，否則本文後續的測試會沒有辦法繼續。 最後我們會嘗試進行三個不同類型的封包傳輸，觀察這些封包實際上會受到哪些 iptables/ebtables 的影響。 Container Bash 透過 ping 指令連線到 Container Nginx外網連到 Container 內的 Nginx本機透過 ping 連到 Container 內的 Nginx ","version":"Next","tagName":"h3"},{"title":"Setup Docker Containets​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#setup-docker-containets","content":"首先，確認主機本身已經有安裝 Docker 相關的服務，接者執行下列程式來運行兩個 Docker 容器於主機上。 #!/bin/bash docker rm -f nginx docker rm -f netutils docker run -d -p 5566:80 --name nginx nginx docker run -d --name netutils hwchiu/netutils  同時為了確保能夠正常運作所有指令，可執行下列指定將相關指令安裝起來 apt-get install -y ebtables iputils-ping  此外，為了詳細的觀察 iptables/ebtables 對連線封包的傳輸，我們要使用 Log 相關的 Target 來操作這些，最後這些 Log 的資訊都會從 Kernel 打印出來， 我們可以透過 Dmesg 的方式去觀察這些封包。 基本上在 iptables 是採用 -j LOG 的方式來處理，然而在 ebtables 則是直接採用 --log 這種原生的方式來處理，其隱性的使用 -j CONTINUE 去繼續處理封包。 實際上我們可以用 dmesg -c 的方式，每次呼叫都只會顯示新出現的部分，這樣會更容易幫助我們觀察封包 在 Log 的指令中，透過 --log-prefix 的方式去列印更多的資訊，可以幫助我們更好觀察。 範例指令 iptables -t mangle -I PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-PREROUTE' --log-level debug ebtables -t filter -I INPUT --log --log-prefix 'ctc/ebtable/filter-input' --log-level debug  ","version":"Next","tagName":"h3"},{"title":"Container To Container​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#container-to-container","content":"在這個測試情境中，我們要觀察容器與容器之間的傳輸，如下圖。  在這邊我們要從一個容器使用 ping -c1 去傳送一個封包到另外一個容器。 然後藉由 dmesg 這個指令來觀察果。 由於我自己所下的 iptables/ebtables 的規則非常簡單，所以強烈建議系統上不要有任何其他的容器正在有任何的網路傳輸，否則 Kernel 輸出會讓你很難理解每個訊息的先後順序。 ","version":"Next","tagName":"h2"},{"title":"Setup ebtables​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#setup-ebtables","content":"我用來建立跟刪除 ebtables 規則的腳本如下 #!/bin/bash insert() { ebtables -t broute -I BROUTING --log --log-prefix 'ctc/ebtable/broute-BROUTING' --log-level debug ebtables -t nat -I PREROUTING --log --log-prefix 'ctc/ebtable/nat-PREROUTE' --log-level debug ebtables -t nat -I POSTROUTING --log --log-prefix 'ctc/ebtable/nat-POSTROUTE' --log-level debug ebtables -t filter -I INPUT --log --log-prefix 'ctc/ebtable/filter-input' --log-level debug ebtables -t filter -I OUTPUT --log --log-prefix 'ctc/ebtable/filter-output' --log-level debug ebtables -t filter -I FORWARD --log --log-prefix 'ctc/ebtable/filter-forward' --log-level debug } delete() { ebtables -t broute -D BROUTING --log --log-prefix 'ctc/ebtable/broute-BROUTING' --log-level debug ebtables -t nat -D PREROUTING --log --log-prefix 'ctc/ebtable/nat-PREROUTE' --log-level debug ebtables -t nat -D POSTROUTING --log --log-prefix 'ctc/ebtable/nat-POSTROUTE' --log-level debug ebtables -t filter -D INPUT --log --log-prefix 'ctc/ebtable/filter-input' --log-level debug ebtables -t filter -D OUTPUT --log --log-prefix 'ctc/ebtable/filter-output' --log-level debug ebtables -t filter -D FORWARD --log --log-prefix 'ctc/ebtable/filter-forward' --log-level debug } check() { count=`ebtables-save | grep ctc| wc -l` if [ &quot;$count&quot; == &quot;0&quot; ]; then echo &quot;Delete Success&quot; else echo &quot;Delete Fail, Use the ebtables-save to check what rules still exist&quot; fi } if [ &quot;$1&quot; == &quot;d&quot; ]; then delete check else insert fi  執行裡面的 insert 函式就可以對 ebtables 的所有 Table/Chain 組合都寫一條規則，注意的是我採用的是 -I xxx 意味者將該規則放到第一條，避免我們的規則因為其他的規則沒有被順利執行。 實驗結束時可以透過 delete 函式去移除相關的規則 ","version":"Next","tagName":"h3"},{"title":"Setup iptables​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#setup-iptables","content":"iptables 的概念非常雷同，但是因為系統本身有太多網路流量在傳輸，所以我有特別設定 -s 172.18.0.0/16 -d 172.18.0.0/16 這規則來確保只有封包的來源與目的地都是屬於容器之間的才會去紀錄。 此外，我特別下了一條 mangle 的規則是因為 nat 在 PREROUTING/POSTROUTING 會因為 conntrack 的幫忙導致看不出來有被執行多次，所以特別多用一個 mangle 來幫忙釐清。 #!/bin/bash insert() { iptables -t mangle -I PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-PREROUTE' --log-level debug iptables -t mangle -I POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-POSTROUTE' --log-level debug iptables -t nat -I PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-PREROUTE' --log-level debug iptables -t nat -I POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-POSTROUTE' --log-level debug iptables -t filter -I INPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-input' --log-level debug iptables -t filter -I OUTPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-output' --log-level debug iptables -t filter -I FORWARD -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-forward' --log-level debug } delete() { iptables -t mangle -D PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-PREROUTE' --log-level debug iptables -t mangle -D POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-POSTROUTE' --log-level debug iptables -t nat -D PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-PREROUTE' --log-level debug iptables -t nat -D POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-POSTROUTE' --log-level debug iptables -t filter -D INPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-input' --log-level debug iptables -t filter -D OUTPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-output' --log-level debug iptables -t filter -D FORWARD -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-forward' --log-level debug } check() { count=`iptables-save | grep ctc| wc -l` if [ &quot;$count&quot; == &quot;0&quot; ]; then echo &quot;Delete Success&quot; else echo &quot;Delete Fail, Use the iptables-save to check what rules still exist&quot; fi } if [ &quot;$1&quot; == &quot;d&quot; ]; then delete check else insert fi  ","version":"Next","tagName":"h3"},{"title":"Test​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#test","content":"當上述規則的設定完畢後，我們先執行數次 dmesg -c 去確保目前沒有任何 kernel 所輸出的新訊息。 接者執行下列指令，請先確保(172.18.0.2)是 容器 nginx 的 IP 地址 docker exec netutils ping 172.18.0.2 -c1  接下來馬上執行 sudo dmesg -ct 來顯示資料。(透過 -t 只是不想要顯示時間，排版比較好看) 該輸出資料如下，我們將該資料分成兩部分，因為 ping -c1 實際上會牽扯到 ICMP Request 以及 ICMP Reply. 如果你的環境中有看到 proto=0x0806, 這是所謂的 APR 封包，本文暫時不討論 ARP。 ctc/ebtable/broute-BROUTING IN=vethd709394 OUT= MAC source = 02:42:ac:12:00:03 MAC dest = 02:42:ac:12:00:02 proto = 0x0800 ctc/ebtable/nat-PREROUTE IN=vethd709394 OUT= MAC source = 02:42:ac:12:00:03 MAC dest = 02:42:ac:12:00:02 proto = 0x0800 ctc/iptable/mangle-PREROUTEIN=docker0 OUT= PHYSIN=vethd709394 MAC=02:42:ac:12:00:02:02:42:ac:12:00:03:08:00 SRC=172.18.0.3 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=7179 DF PROTO=ICMP TYPE=8 CODE=0 ID=8896 SEQ=1 ctc/iptable/nat-PREROUTEIN=docker0 OUT= PHYSIN=vethd709394 MAC=02:42:ac:12:00:02:02:42:ac:12:00:03:08:00 SRC=172.18.0.3 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=7179 DF PROTO=ICMP TYPE=8 CODE=0 ID=8896 SEQ=1 ctc/ebtable/filter-forward IN=vethd709394 OUT=veth5ead5c7 MAC source = 02:42:ac:12:00:03 MAC dest = 02:42:ac:12:00:02 proto = 0x0800 ctc/iptable/filter-forwardIN=docker0 OUT=docker0 PHYSIN=vethd709394 PHYSOUT=veth5ead5c7 MAC=02:42:ac:12:00:02:02:42:ac:12:00:03:08:00 SRC=172.18.0.3 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=7179 DF PROTO=ICMP TYPE=8 CODE=0 ID=8896 SEQ=1 ctc/ebtable/nat-POSTROUTE IN= OUT=veth5ead5c7 MAC source = 02:42:ac:12:00:03 MAC dest = 02:42:ac:12:00:02 proto = 0x0800 ctc/iptable/mangle-POSTROUTEIN= OUT=docker0 PHYSIN=vethd709394 PHYSOUT=veth5ead5c7 SRC=172.18.0.3 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=7179 DF PROTO=ICMP TYPE=8 CODE=0 ID=8896 SEQ=1 ctc/iptable/nat-POSTROUTEIN= OUT=docker0 PHYSIN=vethd709394 PHYSOUT=veth5ead5c7 SRC=172.18.0.3 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=7179 DF PROTO=ICMP TYPE=8 CODE=0 ID=8896 SEQ=1  透過我們事先描述好的 log-prefix, 我們可以很清楚的觀察到 iptables/ebtables 比對的過程。 這些規則我只針對幾個有趣的部分介紹一下 前面封包的IN=代表的都是本機上用來將 Docker容器與 Linux Bridge 連結的 veth 虛擬連線。可以觀察到前面幾個訊息的 OUT= 都是空的，這是因為還沒有進行 Bridge Decision, 還沒有辦法知道封包到底目標的網卡是誰。可以看到在 ebtables 這邊的 in= 都是 vethxxx 而 iptables 的都是 docker0, 這是因為兩者層及不同，關注的點不一樣，實際在上 iptables 中可以透過 physical 相關的參數拿到 vethxxxx.經過 FORWARD 之後，可以觀察到 IN 的訊息都不見了，這是因為在 PREROUTING 這邊可以進行 SNAT 之類的選項，可以改變封包的送端是誰，所以這時候 IN= 的資料其實就是一個不確定性，而且也沒那麼重要了。 ctc/ebtable/broute-BROUTING IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:ac:12:00:03 proto = 0x0800 ctc/ebtable/nat-PREROUTE IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:ac:12:00:03 proto = 0x0800 ctc/iptable/mangle-PREROUTEIN=docker0 OUT= PHYSIN=veth5ead5c7 MAC=02:42:ac:12:00:03:02:42:ac:12:00:02:08:00 SRC=172.18.0.2 DST=172.18.0.3 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=59995 PROTO=ICMP TYPE=0 CODE=0 ID=8896 SEQ=1 ctc/ebtable/filter-forward IN=veth5ead5c7 OUT=vethd709394 MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:ac:12:00:03 proto = 0x0800 ctc/iptable/filter-forwardIN=docker0 OUT=docker0 PHYSIN=veth5ead5c7 PHYSOUT=vethd709394 MAC=02:42:ac:12:00:03:02:42:ac:12:00:02:08:00 SRC=172.18.0.2 DST=172.18.0.3 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=59995 PROTO=ICMP TYPE=0 CODE=0 ID=8896 SEQ=1 ctc/ebtable/nat-POSTROUTE IN= OUT=vethd709394 MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:ac:12:00:03 proto = 0x0800 ctc/iptable/mangle-POSTROUTEIN= OUT=docker0 PHYSIN=veth5ead5c7 PHYSOUT=vethd709394 SRC=172.18.0.2 DST=172.18.0.3 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=59995 PROTO=ICMP TYPE=0 CODE=0 ID=8896 SEQ=1  針對 ICMP Reply 回傳的部分，我們首先可以觀察到 訊息數量不對稱，少了兩個比對的訊息少的分別是 iptable/nat-PREROUTEIN 以及 ctc/iptable/nat-POSTROUTEIN. 因為 nat 相關的操作都會被 conntrack 進行快取幫忙做掉了。 我們將封包了來回搭配之前的圖表整理一下。 v 因為容器與容器之間的傳輸，基本上都是在 Linux Bridge 底下進行傳輸，所以 ping 產生的 ICMP Request 以及 ICMP Reply 都會走相同的路線來傳輸。 ","version":"Next","tagName":"h3"},{"title":"Localhost To Container​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#localhost-to-container","content":"這次的情境更為簡單，本機上面直接透過 ping 這個應用程式去連結到容器內部，這邊會直接使用容器的 IP 地址來進行溝通。 情境圖如下，相對於上述的 Container To Container, 這次的封包不是完全的 Layer2 轉發就可以處理的，會牽扯到本機上面的 Ping 程式，這意味者 Layer3 的部分也會出現。 ","version":"Next","tagName":"h2"},{"title":"Setup ebtables​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#setup-ebtables-1","content":"基本上 ebtables 的指令與前述相同，沒有部份需要修改，可以繼續使用前述的 ebtables 指令。 #!/bin/bash insert() { ebtables -t broute -I BROUTING --log --log-prefix 'ctc/ebtable/broute-BROUTING' --log-level debug ebtables -t nat -I PREROUTING --log --log-prefix 'ctc/ebtable/nat-PREROUTE' --log-level debug ebtables -t nat -I POSTROUTING --log --log-prefix 'ctc/ebtable/nat-POSTROUTE' --log-level debug ebtables -t filter -I INPUT --log --log-prefix 'ctc/ebtable/filter-input' --log-level debug ebtables -t filter -I OUTPUT --log --log-prefix 'ctc/ebtable/filter-output' --log-level debug ebtables -t filter -I FORWARD --log --log-prefix 'ctc/ebtable/filter-forward' --log-level debug } delete() { ebtables -t broute -D BROUTING --log --log-prefix 'ctc/ebtable/broute-BROUTING' --log-level debug ebtables -t nat -D PREROUTING --log --log-prefix 'ctc/ebtable/nat-PREROUTE' --log-level debug ebtables -t nat -D POSTROUTING --log --log-prefix 'ctc/ebtable/nat-POSTROUTE' --log-level debug ebtables -t filter -D INPUT --log --log-prefix 'ctc/ebtable/filter-input' --log-level debug ebtables -t filter -D OUTPUT --log --log-prefix 'ctc/ebtable/filter-output' --log-level debug ebtables -t filter -D FORWARD --log --log-prefix 'ctc/ebtable/filter-forward' --log-level debug } check() { count=`ebtables-save | grep ctc| wc -l` if [ &quot;$count&quot; == &quot;0&quot; ]; then echo &quot;Delete Success&quot; else echo &quot;Delete Fail, Use the ebtables-save to check what rules still exist&quot; fi } if [ &quot;$1&quot; == &quot;d&quot; ]; then delete check else insert fi  ","version":"Next","tagName":"h3"},{"title":"Setup iptables​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#setup-iptables-1","content":"於 iptables 方面，我們新增了一條 mangle OUTPUT 來協助觀察封包的轉送，主要原因是本機的 ping 應用程式送出 ICMP Request 會牽扯到 OUTPUT Chain。 #!/bin/bash insert() { iptables -t mangle -I OUTPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-OUTPUT' --log-level debug iptables -t mangle -I PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-PREROUTE' --log-level debug iptables -t mangle -I POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-POSTROUTE' --log-level debug iptables -t nat -I PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-PREROUTE' --log-level debug iptables -t nat -I POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-POSTROUTE' --log-level debug iptables -t filter -I INPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-input' --log-level debug iptables -t filter -I OUTPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-output' --log-level debug iptables -t filter -I FORWARD -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-forward' --log-level debug } delete() { iptables -t mangle -D OUTPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-OUTPUT' --log-level debug iptables -t mangle -D PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-PREROUTE' --log-level debug iptables -t mangle -D POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/mangle-POSTROUTE' --log-level debug iptables -t nat -D PREROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-PREROUTE' --log-level debug iptables -t nat -D POSTROUTING -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/nat-POSTROUTE' --log-level debug iptables -t filter -D INPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-input' --log-level debug iptables -t filter -D OUTPUT -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-output' --log-level debug iptables -t filter -D FORWARD -s 172.18.0.0/16 -d 172.18.0.0/16 -j LOG --log-prefix 'ctc/iptable/filter-forward' --log-level debug } check() { count=`iptables-save | grep ctc| wc -l` if [ &quot;$count&quot; == &quot;0&quot; ]; then echo &quot;Delete Success&quot; else echo &quot;Delete Fail, Use the iptables-save to check what rules still exist&quot; fi } if [ &quot;$1&quot; == &quot;d&quot; ]; then delete check else insert fi  ","version":"Next","tagName":"h3"},{"title":"Test​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#test-1","content":"當上述的規則都準備完畢之後，我們就可以開始來進行測試了。 由於這次是使用本機上面的 ping 指令來傳輸封包，所以測試的指令更為簡單 接者執行下列指令，請先確保(172.18.0.2)是 容器 nginx 的 IP 地址 ping 172.18.0.2 -c1  接下來馬上使用 sudo dmesg -ct 來觀察結果，然後我將結果分成 ICMP Request 以及 ICMP Reply 兩個部分來觀察。 ctc/iptable/mangle-OUTPUTIN= OUT=docker0 SRC=172.18.0.1 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=15859 DF PROTO=ICMP TYPE=8 CODE=0 ID=30580 SEQ=1 ctc/iptable/filter-outputIN= OUT=docker0 SRC=172.18.0.1 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=15859 DF PROTO=ICMP TYPE=8 CODE=0 ID=30580 SEQ=1 ctc/iptable/mangle-POSTROUTEIN= OUT=docker0 SRC=172.18.0.1 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=15859 DF PROTO=ICMP TYPE=8 CODE=0 ID=30580 SEQ=1 ctc/iptable/nat-POSTROUTEIN= OUT=docker0 SRC=172.18.0.1 DST=172.18.0.2 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=15859 DF PROTO=ICMP TYPE=8 CODE=0 ID=30580 SEQ=1 ctc/ebtable/filter-output IN= OUT=veth5ead5c7 MAC source = 02:42:db:a1:f2:79 MAC dest = 02:42:ac:12:00:02 proto = 0x0800 ctc/ebtable/nat-POSTROUTE IN= OUT=veth5ead5c7 MAC source = 02:42:db:a1:f2:79 MAC dest = 02:42:ac:12:00:02 proto = 0x0800  由於封包是直接從本機的 Ping 出發的，所以會先從 Layer3 開始傳送封包，因此第一個遇到的就會是 iptables 相關的規則這邊可以觀察到因為封包是從本機出去的，所以其實 IN= 的欄位一直都是空的，因為其實也不重要。因為目標容器是在 Linux Bridge 底下，所以封包會先查到 docker0, 最後依賴 Layer2 去轉發送出去。 ctc/ebtable/broute-BROUTING IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:db:a1:f2:79 proto = 0x0800 ctc/ebtable/nat-PREROUTE IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:db:a1:f2:79 proto = 0x0800 ctc/iptable/mangle-PREROUTEIN=docker0 OUT= PHYSIN=veth5ead5c7 MAC=02:42:db:a1:f2:79:02:42:ac:12:00:02:08:00 SRC=172.18.0.2 DST=172.18.0.1 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=55481 PROTO=ICMP TYPE=0 CODE=0 ID=30580 SEQ=1 ctc/ebtable/filter-input IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:db:a1:f2:79 proto = 0x0800 ctc/iptable/filter-inputIN=docker0 OUT= PHYSIN=veth5ead5c7 MAC=02:42:db:a1:f2:79:02:42:ac:12:00:02:08:00 SRC=172.18.0.2 DST=172.18.0.1 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=55481 PROTO=ICMP TYPE=0 CODE=0 ID=30580 SEQ=1  ICMP Reply 的方向是從容器回到本機的 Ping 應用程式，因此進入點就是Linux Bridge, 這意味者一定是從 ebtables/broute-BROUTING 開始查詢完相關的 Bridging Table 以及 Routing Table, 最後決定要將封包送到 Ping 的應用程式，因此會走到 INPUT Chain 這邊來處理。 最後我們將上述的流向給合併起來觀看，在這個範例之中因為 ICMP Request 以及 ICMP Reply 是不同的走向。所以在下圖中。我們紫色的代表是 ICMP Request 的走向，而藍色代表的是 ICMP Reply 的走向。  ","version":"Next","tagName":"h3"},{"title":"Wan To Container​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#wan-to-container","content":"終於到了最後一個情境，這個情境也是最多人常用的情境。我們的容器本身再創立的時候，會透過 -p 5566:80 的方式將本機的 5566 連接埠串通到容器內的 80 連接埠. 接者外部的網路透過 5566 連結埠來存取對應的容器內容。 因此在這個範例中，我們打算從外部網路透過 5566 連結埠來存取是先建立好的 Nginx 容器。 接者透過 iptables/ebtables 的記錄來觀察在這種情境下，封包會怎麼傳輸。 此外，由於我們還有透過 5566 連結埠轉換到 80連結埠的需求，所以在我們觀察的 iptables/ebtables 的結果中，應該也要可以看到封包資訊的變換(IP/Port/MAC Address) ","version":"Next","tagName":"h2"},{"title":"Setup ebtables​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#setup-ebtables-2","content":"在 ebtables 方面，規則基本上沒有太多變化，繼續依照之前的用法即可。 #!/bin/bash insert() { ebtables -t broute -I BROUTING --log --log-prefix 'ctc/ebtable/broute-BROUTING' --log-level debug ebtables -t nat -I PREROUTING --log --log-prefix 'ctc/ebtable/nat-PREROUTE' --log-level debug ebtables -t nat -I POSTROUTING --log --log-prefix 'ctc/ebtable/nat-POSTROUTE' --log-level debug ebtables -t filter -I INPUT --log --log-prefix 'ctc/ebtable/filter-input' --log-level debug ebtables -t filter -I OUTPUT --log --log-prefix 'ctc/ebtable/filter-output' --log-level debug ebtables -t filter -I FORWARD --log --log-prefix 'ctc/ebtable/filter-forward' --log-level debug } delete() { ebtables -t broute -D BROUTING --log --log-prefix 'ctc/ebtable/broute-BROUTING' --log-level debug ebtables -t nat -D PREROUTING --log --log-prefix 'ctc/ebtable/nat-PREROUTE' --log-level debug ebtables -t nat -D POSTROUTING --log --log-prefix 'ctc/ebtable/nat-POSTROUTE' --log-level debug ebtables -t filter -D INPUT --log --log-prefix 'ctc/ebtable/filter-input' --log-level debug ebtables -t filter -D OUTPUT --log --log-prefix 'ctc/ebtable/filter-output' --log-level debug ebtables -t filter -D FORWARD --log --log-prefix 'ctc/ebtable/filter-forward' --log-level debug } check() { count=`ebtables-save | grep ctc| wc -l` if [ &quot;$count&quot; == &quot;0&quot; ]; then echo &quot;Delete Success&quot; else echo &quot;Delete Fail, Use the ebtables-save to check what rules still exist&quot; fi } if [ &quot;$1&quot; == &quot;d&quot; ]; then delete check else insert fi  ","version":"Next","tagName":"h3"},{"title":"Setup iptables​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#setup-iptables-2","content":"不同於 ebtables，在 iptables 這邊的修改比較多，原因如下 此情境屬於 Wan To Container, 這意味牽扯到不同網段的傳輸因為我操作的環境算是很乾淨，所以我針對 Wan IP 以及 Container IP 來作為封包的條件在我的環境中，我的 Wan 是 172.17.0.1 而 Container 是 172.18.0.2. 所以我規則會針對 172.17.0.0/16 以及 172.18.0.0/16 來設定。 #!/bin/bash insert() { iptables -t mangle -I PREROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-PREROUTE' --log-level debug iptables -t nat -I PREROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-PREROUTE' --log-level debug iptables -t mangle -I PREROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-PREROUTE' --log-level debug iptables -t nat -I PREROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-PREROUTE' --log-level debug iptables -t filter -I FORWARD -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/filter-forward' --log-level debug iptables -t filter -I FORWARD -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/filter-forward' --log-level debug iptables -t mangle -I FORWARD -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-FORWARD' --log-level debug iptables -t mangle -I FORWARD -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-FORWARD' --log-level debug iptables -t mangle -I POSTROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-POSTROUTE' --log-level debug iptables -t mangle -I POSTROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-POSTROUTE' --log-level debug iptables -t nat -I POSTROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-POSTROUTE' --log-level debug iptables -t nat -I POSTROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-POSTROUTE' --log-level debug } delete() { iptables -t mangle -D PREROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-PREROUTE' --log-level debug iptables -t nat -D PREROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-PREROUTE' --log-level debug iptables -t mangle -D PREROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-PREROUTE' --log-level debug iptables -t nat -D PREROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-PREROUTE' --log-level debug iptables -t filter -D FORWARD -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/filter-forward' --log-level debug iptables -t filter -D FORWARD -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/filter-forward' --log-level debug iptables -t mangle -D FORWARD -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-FORWARD' --log-level debug iptables -t mangle -D FORWARD -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-FORWARD' --log-level debug iptables -t mangle -D POSTROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-POSTROUTE' --log-level debug iptables -t mangle -D POSTROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/mangle-POSTROUTE' --log-level debug iptables -t nat -D POSTROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-POSTROUTE' --log-level debug iptables -t nat -D POSTROUTING -p tcp -d 172.17.0.0/16 -j LOG --log-prefix 'wtc/iptable/nat-POSTROUTE' --log-level debug } check() { count=`iptables-save | grep wtc| wc -l` if [ &quot;$count&quot; == &quot;0&quot; ]; then echo &quot;Delete Success&quot; else echo &quot;Delete Fail, Use the iptables-save to check what rules still exist&quot; fi } if [ &quot;$1&quot; == &quot;d&quot; ]; then delete check else insert fi  ","version":"Next","tagName":"h3"},{"title":"Test​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#test-2","content":"在測試方面，我一開始本來是採用 curl 的方式去連線 nginx 容器，但是其實 curl 做了太多事情了，除了一開始的 TCP 三方交握連線外，還包含了 HTTP GET。 對於我們只想要單純觀察 Wan To Controller 這來回的連線來說，這其實做了太多事情了。 為了簡化整個觀察結果，我最後決定採用 telnet 的方式，單純建立 TCP 連線就好。而整個 TCP 的三方交握連線其實是三個封包的傳輸，所在觀察的結果中可以觀察到三個部分的連線。 然而在我們的觀察目標中，我們只需要觀察前兩個連線就好，畢竟這樣已經足夠讓我們去觀察 Wan To Controller 的傳輸過程中， iptables/ebtables 會如何影響我們的連線。 待一切規則都準備好後，在你外網的機器上，執行下列指令。 這邊要注意的是，我測試機器的對外IP 地址是 172.18.8.211，而我本身主機的IP地址是 172.17.8.1. 這邊請調整成自己的環境 [18:13:20] hwchiu ➜ ~» telnet 172.17.8.211 5566 Trying 172.17.8.211... Connected to 172.17.8.211. Escape character is '^]'.  這時候馬上透過 dmesg -ct 去收集封包，可以得到類似下列的結果，我將結果整理，只收集 TCP 三方教握的前兩方傳輸就好 ctc/iptable/mangle-PREROUTEIN=enp0s8 OUT= MAC=08:00:27:ff:b2:c4:0a:00:27:00:00:02:08:00 SRC=172.17.8.1 DST=172.17.8.211 LEN=64 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=63584 DPT=5566 WINDOW=65535 RES=0x00 CWR ECE SYN URGP=0 ctc/iptable/nat-PREROUTEIN=enp0s8 OUT= MAC=08:00:27:ff:b2:c4:0a:00:27:00:00:02:08:00 SRC=172.17.8.1 DST=172.17.8.211 LEN=64 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=63584 DPT=5566 WINDOW=65535 RES=0x00 CWR ECE SYN URGP=0 ctc/iptable/mangle-FORWARDIN=enp0s8 OUT=docker0 MAC=08:00:27:ff:b2:c4:0a:00:27:00:00:02:08:00 SRC=172.17.8.1 DST=172.18.0.2 LEN=64 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=63584 DPT=80 WINDOW=65535 RES=0x00 CWR ECE SYN URGP=0 ctc/iptable/filter-forwardIN=enp0s8 OUT=docker0 MAC=08:00:27:ff:b2:c4:0a:00:27:00:00:02:08:00 SRC=172.17.8.1 DST=172.18.0.2 LEN=64 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=63584 DPT=80 WINDOW=65535 RES=0x00 CWR ECE SYN URGP=0 ctc/iptable/mangle-POSTROUTEIN= OUT=docker0 SRC=172.17.8.1 DST=172.18.0.2 LEN=64 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=63584 DPT=80 WINDOW=65535 RES=0x00 CWR ECE SYN URGP=0 ctc/iptable/nat-POSTROUTEIN= OUT=docker0 SRC=172.17.8.1 DST=172.18.0.2 LEN=64 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=63584 DPT=80 WINDOW=65535 RES=0x00 CWR ECE SYN URGP=0 ctc/ebtable/filter-output IN= OUT=veth5ead5c7 MAC source = 02:42:db:a1:f2:79 MAC dest = 02:42:ac:12:00:02 proto = 0x0800 ctc/ebtable/nat-POSTROUTE IN= OUT=veth5ead5c7 MAC source = 02:42:db:a1:f2:79 MAC dest = 02:42:ac:12:00:02 proto = 0x0800  如同前述一樣，我這邊也針對一些有趣的封包內容進行討論 因為我們是透過 docker run -p 5566:80, 所以我們傳輸的 5566 連接埠會被轉換成 80 連接埠。 可以觀察到第三個規則 mangle-FORWARD 之後，DPT=5566 都變成了 DPT=80. 主要是因為經過了 nat-PREROUTING 後就被更動了。同上面的理由，可以觀察到封包的目標IP地址從原本的DST=172.17.8.211 被轉換成容器的DST=172.18.0.2.因為是從Wan To Controller, 所以封包會先從 iptalbes 開始跑，最後跑道 Linux Bridge 後才會進入到 ebtables最一開始封包的 MAC Address 的發送是 0a:00:27:00:00:02 -&gt;08:00:27:ff:b2:c4. 但是一但到了 ebtables 那層，也就是經過 docker0 之後，你可以觀察到這時候的 MAC address 的流向變成 02:42:db:a1:f2:79 -&gt; 02:42:ac:12:00:02. 這邊原理實際上跟 IP 封包傳輸有關，這邊不多敘述。 ctc/ebtable/broute-BROUTING IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:db:a1:f2:79 proto = 0x0800 ctc/ebtable/nat-PREROUTE IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:db:a1:f2:79 proto = 0x0800 ctc/iptable/mangle-PREROUTEIN=docker0 OUT= PHYSIN=veth5ead5c7 MAC=02:42:db:a1:f2:79:02:42:ac:12:00:02:08:00 SRC=172.18.0.2 DST=172.17.8.1 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=80 DPT=63584 WINDOW=28960 RES=0x00 ECE ACK SYN URGP=0 ctc/ebtable/filter-input IN=veth5ead5c7 OUT= MAC source = 02:42:ac:12:00:02 MAC dest = 02:42:db:a1:f2:79 proto = 0x0800 ctc/iptable/mangle-FORWARDIN=docker0 OUT=enp0s8 PHYSIN=veth5ead5c7 MAC=02:42:db:a1:f2:79:02:42:ac:12:00:02:08:00 SRC=172.18.0.2 DST=172.17.8.1 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=80 DPT=63584 WINDOW=28960 RES=0x00 ECE ACK SYN URGP=0 ctc/iptable/filter-forwardIN=docker0 OUT=enp0s8 PHYSIN=veth5ead5c7 MAC=02:42:db:a1:f2:79:02:42:ac:12:00:02:08:00 SRC=172.18.0.2 DST=172.17.8.1 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=80 DPT=63584 WINDOW=28960 RES=0x00 ECE ACK SYN URGP=0 ctc/iptable/mangle-POSTROUTEIN= OUT=enp0s8 PHYSIN=veth5ead5c7 SRC=172.18.0.2 DST=172.17.8.1 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=80 DPT=63584 WINDOW=28960 RES=0x00 ECE ACK SYN URGP=0  這邊是封包的回程，是由 Container To Wan 的方向 封包是從 ebtables 開始，因為是從 Linux Bridge 底下的網卡收到容器傳出來的封包，接下來透過各種轉發，最後從 iptabes 那邊轉發出去到外部網路。觀察最後一個 mangle-postrouting 可以看到 IP/Port/Mac 都還沒有被轉換，這些都會在 nat-postrouting 這邊去處理，但是因為 conntrack 的關係，這些操作會在 kernel 給快取執行掉了。若要真的觀察可以透過 tcpdump 的方式去監聽封包。 在看到了這兩種的情境後，我們將彼此的流向圖給整理一下，如下圖。 圖中藍色的連線則是 Wan To Container 而紫色則是 Container To Wan 的封包流向。 基本上因為牽扯到 Layer3/Layer2 的處理，封包都會經過 iptables 的 filter Table/FORWARD Chain. 如果有要針對防火牆去處理的話，可以在這邊去執行。  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"[netfilter] Dig Into Docker Bridge Network By iptables/ebtables","url":"/docs/techPost/2018/netfilter-eiptables-iii#summary","content":"在本文中，我們嘗試透過 iptables/ebtables 本身的 log 模組來協助我們釐清於不同的拓墣情境中，封包之間的轉送會怎麼經過 iptables/ebtables。 總共有三種拓墣環境，分別是 Container To ContainerLocalhost to ContainerWan To Container 針對每個環境，我們都觀察封包的來回兩種狀態，除了觀察 iptables/ebtables 的走向之外，也順便觀察封包內容的變化。 只有真正的瞭解整個封包的傳輸行為，以及對應低 iptables/ebtables 的走向，未來在管理 iptables/ebtables 才能夠更精準的去設定相關的規則來滿足自己的需求。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/paper-tensorflow-with-rdma","content":"","keywords":"","version":"Next"},{"title":"System & Library​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/paper-tensorflow-with-rdma#system--library","content":"目前現存的 dataflow 相關應用程式再底層傳輸資料時，大抵上分成兩種方式傳輸，一種是採用 RPC 的方式去設計收送的資料格式，另外一種則是基於檔案區塊(File Block)進行傳輸。 舉例來說， Tensorflow 以及早期版本的 Spark 就是使用 RPC 的方式進行跨節點的資料傳輸。後期版本的 Spark 以及 Hadoop (MapReduces) 則是將檔案輸出到本地的檔案系統，然後再將該檔案區塊傳輸到其他節點。 作者認為，RPC 於傳輸有者效能上的缺失，主要歸咎於 RPC 的請求/回復(Request/Response) 本身不但有大量的資料複製，同時也有可能有加解密(Encoding/Decoding)。 這些行為都會嚴重的消耗 CPU運算，並且降低點到點的傳輸速度與提高延遲性。 至於檔案區塊(File Block)通常都會將檔案存到外部的檔案系統，即使是 RAMFS(Memory Based File System) 也是會牽扯到很多資料複製的行為。 作者認為上述的問題主要在於API的設計行為，此行為導致計算(computation)與傳輸(communication)兩個子系統都自行管理自己的記憶體空間，彼此沒有共享。 這種情況下很難真正去實現點對點的 Zero Copy 的資料傳輸，(這邊也呼應前面的 rSOCKET的設計依然會有非必要的資料複製行為)。 作者認為在這種架構下，不論這兩個子系統都設計了多良好的緩衝區管理(Buffer Management)，都至少會有一次的資料複製行為。 舉例來說，假如我們採用了最廣為流行的 TCP/IP 系統，在 Linux Kernel 內部會採用 sk_buff 的結構來儲存網路封包，而且在底層大部分針對網路封包操作的行為都是基於指標去操作，雖然這類看起來都是指標操作的行為的確避免了任何資料複製的動作。 但是資料要從 User Space 轉送到 Kernel Space 還是需要經過一次的轉換。 這類型的問題不論是在各種 RPC 或是 函式庫(Library)內都有出現。 ","version":"Next","tagName":"h2"},{"title":"Dataflow​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/paper-tensorflow-with-rdma#dataflow","content":"對於 Dataflow 來說，作者認為若在高速網路環境中同時滿足下列兩種情形，則記憶體複製就有機會是整體的效能瓶頸。 作者認為若應用程式的緩衝區太大，沒有辦法符合當前系統架構上的 L1/L2/L3 快取。很少有單一應用程式有辦法把整個網路環境中的頻寬都吃滿，造成壅塞。 首先，針對第一個情形來說，我們要先探討複製大檔案跟複製很多小檔案的差距。 作者認為當經過一些運算時候，後者(很多小檔案)會更有機會留在快取中繼續使用，而大檔案很容易就會發生 Cache-Miss的事件而最終要從 Memory 中讀取。 作者也提到，對很多 RDMA 相關的應用程式(譬如 FaSST )來說，針對小檔案的檔案複製，會特別處理。該應用程式內會特別準備一塊空間來處理這些小檔案的複製，而這塊空間是個 page-locked 的記憶體空間。藉由這個行為減少每次對於小檔案都要去進行所謂的 pinning memroy(從虛擬記憶體空間映射到實際記憶體空間)產生的消耗。 接下來探討第二個情形，有一些應用程式(譬如 KV )是特別關注在封包延遲性方面的，這種應用程式通常都不會消耗整個網路頻寬，其要求的點都是在於每秒能夠處理多少封包，而大部分情況下，Key/Value這些資料都是小封包，大概是數KB左右。 此外，作者還提供了一個數據來強調這個情況。 當傳輸的緩衝區大小是 4KB 左右的，這時候大概傳輸可以達到 20-30 GB/s，而若緩衝區的大小是超過 4MB 時，這時候的傳輸只剩下 2-4 GB/s,主要是緩衝區太大的時候，這些封包都沒有辦法符合快取的大小，導致大量複製行為最終使得傳輸速度下降。 所以結論一下，只要當緩窗區的大小過大的時候，這時候很容易因為 Cache Miss而產生各種複製的行為，而這些複製行為就會導致整體效能下降。 為了解決這個問題，作者提出了一個 Unified Memory Alloccator 的機制，這個機制會去同時控管計算(computation)與傳輸(communication)兩個子系統內的緩衝區配置。 這個機制有兩大重點 實作不同類型的記憶體配置，譬如當前記憶體是要給 RDMA 使用，還是當前本系統的 DMA。解析 dataflow graph 中的資訊來決定當前節點的資料是要使用何種記憶體配置。 此外，不論是 RDMA 或是 DMA 所產生的緩衝區都會同時在計算(computation)與傳輸(communication)兩個子系統內共同使用，直到兩個系統都不再使用該空間時，才會將該空間給釋放出來。 所以看到這邊可以大概想像一下作者到底要怎麼做了。 分析整個 dataflow graph 中的資料走向，盡可能的讓相同使用的資料只要使用一份緩衝區空間即可，然後透過 RDMA 或是 DMA 等技術來傳遞資料，減少整個過程中的資料複製行為。 IMPLEMENTATION 作者採用的 dataflow 是基於 tensorflow，因此該 Unified Memory Alloccator 本身也是實作在 tensorflow 裡面，可以直接到下列位置觀看作者與 tenforflow 維護者的溝通以及程式碼的修改。 作者新增了一種 memory allocator 到整體的程式碼內，要使用時只要打開相關選項即可(前提是要先針對有 RDMA 重新編譯整個專案)。 在作者的實作的記憶體分配器中，會自動的去解析 tensorflow 的 computational grpah 以及 distributed graph partition，所以只有滿足下列兩種條件的張量 tensor 才會去該記憶體分配器中被選擇使用 RDMA。 必須是source node(出發端點) 或是 sink node(結尾端點)該操作必須是 send/receive 同時要跨實體機器 這也很合理，因為跨機器間的傳輸會用到網路，所以才會需要用到 RDMA 來傳輸資料。 在最原始的 TenforFlow 的版本中，使用了基於 HTTP/2 的 gRPC 格式作為 Tensor 間的傳輸。而作者修改了這邊的程式碼，使得這邊會直接跳過 RPC 的呼叫，改使用 RDMA 來傳輸資料。 此外，為了完整支援能夠透過 RDMA 來使用本地端或是遠端的 GPU 資源，作者還使用了 GPU direct RDMA 的技術來完成這些事情。藉由這個技術任何 PCI-e 的第三方裝置(譬如 NIC )都可以直接讀取 GPU 上面的記憶體空間。 此外作者也觀察到當 GPU direct RDMA 的路徑會經過 CPU-socket (譬如處於不同的 NUMA NODE) 會發生嚴重的效能問題。 因為在這種情況下，這些資料會先被送到記憶體內，然後在被複製出來，最後才會真正的送到 NIC 或是 GPU 上去處理，導致了不必要的消耗。 簡單來說，作者透過分析然後決定那些記憶體空間要共用，那些用DMA，那些用RDMA，同時透過 GPU direct RDMA 的技術直接存取遠方機器上面 GPU 的記憶體，藉此降低大量的資料複製行為。 EVALUATION ","version":"Next","tagName":"h2"},{"title":"Environment (Hardware)​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/paper-tensorflow-with-rdma#environment-hardware","content":"4 servers 都連接上 Mellanox MSN2100-BB2F 40Gbe RoCE Switch (For RDMA)每個 server 都配置下列硬體 Mellanox MT27500 40GbE NICDual6-core Intel Xeon E5-2603v4 CPU4 NVidia Tesla K40m GPUs256 GB DDR4-2400MHz Switch 本身有配置 PFC 來控管傳輸流量降低封包掉落機率 ","version":"Next","tagName":"h3"},{"title":"Environment (Software)​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/paper-tensorflow-with-rdma#environment-software","content":"作者訓練了分散式版本的 VGG16 CNN 模型。模組的參數大小是 528 MB採用了 synchronous 模式 parameter servers 的數量跟 Workers 的數量一至，同時 針對每一台機器上面的 Worker 都會同時使用 CPU 與 GPU 來進行運算，而 parameter server (PS) 則只會使用 CPU 配上系統上的記憶體來收集資訊。 ","version":"Next","tagName":"h3"},{"title":"Target​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/paper-tensorflow-with-rdma#target","content":"作者修改後的 TensorFlow官方未修改的 TensorFlow v1.2.1 Yahoo 自行修改後的 TensorFlow 也支援 RDMA，但是機制沒有作者完善，還是有不少的複製行為 ","version":"Next","tagName":"h3"},{"title":"Metric​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/paper-tensorflow-with-rdma#metric","content":"作者想觀察的是完成訓練所消耗的時間， ","version":"Next","tagName":"h3"},{"title":"Result​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/paper-tensorflow-with-rdma#result","content":"比較的結果來看，(1) 比 (2) 快上了 2.43 倍, 而 (1) 比 (3) 快上了 1.21 倍16顆 GPU 與 1顆 GPU 的比較起來，效能提升了 13.8倍 Summary 作者觀察到在 dataflow 中之間的傳輸大小不小，同時這些資料會在系統中有大量的資料複製行為，因此引進了 DMA，RDMA 以及 GPU direct RDMA 等技術來減少整體的資料複製行為，並且也將整體的程式碼完全貢獻回 TensorFlow 內，未來任何人想要嘗試這個機制也可以直接使用。 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/tools-ncdu","content":"","keywords":"","version":"Next"},{"title":"MacOS (Brew)​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/tools-ncdu#macos-brew","content":"brew install ncdu ","version":"Next","tagName":"h2"},{"title":"Ubuntu (apt system)​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/tools-ncdu#ubuntu-apt-system","content":"apt-get install ncdu Usage Change to any directory you want to inspect the file size and then exectue the commnd ncdu. First, ncdu will recursively collect the file information under the current directory. It will takes a time depends on how big of your directory structure. You will see the following picture in your terminal. After the collection has done, it will display the size of each file and directory(total size if it's directory) under the current directory. Format The output forwat is clear. First colume: The file size and it will automatically transfer to humand readble size. Second colume the percentage of specified file/directory to the whole root directory, it use the numder of sharp symbol to show the ratio by default and you can toggle the shortcut g to display by numbrer. Third colume The file/directory name  Operation ","version":"Next","tagName":"h2"},{"title":"Navagation​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/tools-ncdu#navagation","content":"The basic operation is navagation, use the arrow key(up/down) or k/j to move the cursoe up and down respectively. ","version":"Next","tagName":"h2"},{"title":"Open​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/tools-ncdu#open","content":"The amazing feature I think better than the legacy command du is nctu supports the open opeartion. You can use the to arrow key(right/left) to open the directory and use it as the root directory or go back to the previous root directory. With the help of this feature, we don't need to execute the command du many times to see the whole inforatiom. ","version":"Next","tagName":"h2"},{"title":"Delete​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/tools-ncdu#delete","content":"Besides, ncdu also provides the delete option to let your delete the file or whole directory in the current window. You can see the instruction help by the key ?.  Summary I used to use the command du to inspect the current file/directory size and also use the command sort to sort the result by the du command. There're some problem about that usage and bothered me for a long time. If command du shows the output with human-readble foramt, it's hard for sorting, but if it shows the size as numeric format, it's good for sorting but not for reading. In the ncdu, that problem doesn't exsit and the ncdu also support the delete operation and the way to change the current root directory. That's why I switch to use the ncdu once I had found this powerful tool. ","version":"Next","tagName":"h2"},{"title":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","type":0,"sectionRef":"#","url":"/docs/techPost/2018/travisci-k8s","content":"","keywords":"","version":"Next"},{"title":"GitHub​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#github","content":"放置應用程式原始碼的地方，本案例中使用 GitHub 這個程式碼託管網站。 ","version":"Next","tagName":"h3"},{"title":"TravisCI​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#travisci","content":"一套自動化測試的服務，與 GitHub 可以連動，當你的 GitHub 專案有收到任何程式碼合併請求(Pull Request) 等更動的時候，可以透過 TravisCI幫你的應用程式進行測試，並且將測試的結果回傳到 GitHub 讓專案管理員瞭解該次的程式碼修改是否有順利通過所有測試。 ","version":"Next","tagName":"h3"},{"title":"Minikube​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#minikube","content":"一套用來建置 Kubernetres 集群的工具，在我們的方案中我們會在 TravisCI 的環境中透過此工具來產生一個本地的 kubernetes 集群。接者我們 TravisCI 內的應用程式就獲得一個獨立的 kubernetes 集群來使用了。 本篇文章不會介紹關於 Gitnub 以及 TravisCI 的基本操作，而是著重在這兩者與 kubernetes 的整合。 因此相關操作請自行學習。 架構介紹 接下來將使用下圖來介紹整個測試方案的運作流程。  假設開發者的 GitHub 專案已經與 TravisCI 進行連接開發者日以繼夜的撰寫程式碼，對 GitHub 專案發出程式碼合併更動的請求GitHub 這邊收到通知，通知 TravisCI 準備進行測試TravisCI 根據需求創建一個全新的測試環境出來 這邊則會產生一個虛擬機器出來 在上述產生的測試環境中，使用 Minikube 該元件來產生一個全新的 kubernetes 集群確認集群創建完畢，相關服務的運行中後，便可以針對 GitHub 專案內的程式碼進行測試測試的結果回報給 TravisCI，然後 TravisCI 會再回報給 GitHub 專案，讓開發者可以瞭解這次的修改是否有通過所有的測試。 示範案例展示 本次示範所使用的程式碼都可以於 kubeTravisDemo 內找到 而該專案對應的 TravisCI 結果也可以在 TravisCI kubeTravisDemo 瀏覽 ","version":"Next","tagName":"h3"},{"title":"應用程式​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#應用程式","content":"本篇開頭提到，有些應用程式會需要與 Kubernetes 有緊密的連結操作，而本次展示的專案則使用了 Client-go 這個套件開發了一個很簡單能夠自行產生 Pod 的應用程式。 並且為該功能撰寫了一個簡單的測試，該測試會透過該函式嘗試去產生 Kubernetes Pod 並且確保該 Pod 有成功產生，最後將其刪除。 如果想要了解怎麼透過 client-go 撰寫 kubernetes 相關應用程式，可參閱下列投影片 Kubernetes library 開發實戰 with client-go 該測試程式碼大致上如下 .... err = createPod(clientset, podName) assert.NoError(t, err) pod, err := clientset.CoreV1().Pods(&quot;default&quot;).Get(podName, metav1.GetOptions{}) assert.NotNil(t, pod) assert.NoError(t, err) err = clientset.CoreV1().Pods(&quot;default&quot;).Delete(podName, &amp;metav1.DeleteOptions{}) assert.NoError(t, err) ...  如果在一個沒有 kubernetes 集群的環境中，該測試程式碼則沒有辦法測試(因為沒有真的集群可以去進行kubernetes操作) 雖然有 Fake-client 可以進行相關的測試，但是有部份的操作是需要真的集群去運行才可以進行的，這類型的就沒有辦法用 Fake-client 來測試。 ","version":"Next","tagName":"h2"},{"title":"TravisCI​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#travisci-1","content":"當應用程式準備好之後，我們就要在 GitHub 專案中描寫我們如何使用 TravisCI 的測試環境。 該環境描述檔案採用 yaml 的格式，名稱為 .travis.yml，可以在 Travis CI Getting started 看到相關文件. 這邊有幾件事情要注意 Travis-CI 產生的是虛擬機器，預設沒特別開啟情況下，我們不能在虛擬機器內再開啟另外一個虛擬機器，所以我們要使用 Docker 的方式來創造 kubernetes 集群. 這部分可以透過 minikube --vm-drive=none 來達成在 minikube 的部署方式中，有 localkube 以及 kubeadm 兩種方式來部署，由於目前 minikube 主推 kubeadm 並且也說明未來會拋棄 localkube，因此我們的部屬方式決定採用 kubeadm.由於 kubeadm 本身會依賴 systemd 去進行相關的 systemd service 運行，而 Ubuntu 14.04 預設依然使用 upstart 而非 systemd，因此我們必須要標明我們希望使用的 OS 是 Ubuntu 16.04(xenial) 有了這幾個基本注意事項後，我們可以撰寫一個 .travis.yml 來符合我們的需求 language: go sudo: required dist: xenial services: - docker env: - CHANGE_MINIKUBE_NONE_USER=true before_install: - go get -u github.com/kardianos/govendor before_script: - sudo mount --make-rshared / - curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ - curl -Lo minikube https://github.com/kubernetes/minikube/releases/download/v0.28.1/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ - sudo minikube -v 9 start --vm-driver=none --bootstrapper=kubeadm --kubernetes-version=v1.10.0 --extra-config=apiserver.authorization-mode=RBAC - minikube update-context - until kubectl get nodes minikube | grep &quot; Ready&quot;;do kubectl get nodes; sleep 1; done - until kubectl -n kube-system get pods -lk8s-app=kube-dns -o jsonpath=&quot;{.items[0].status.phase}&quot; | grep &quot;Running&quot; ;do sleep 1;echo &quot;waiting for kube-addon-manager to be available&quot;; kubectl get pods --all-namespaces; done script: - go test -v ./...  上述檔案內，跟 kubernetes 有關的部分落在 dist, env 以及 before_script 這三大區塊內。 ","version":"Next","tagName":"h2"},{"title":"Dist​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#dist","content":"就如同上面說明，希望 TravisCI 所配置的機器是使用 Ubuntu 16.04, 免除了 Systemd 相關依賴的自行安裝手續 事實上非常難裝且有滿多問題，沒心力不要嘗試在 14.04 內去透過 systemd/kubeadm 安裝 kubernetes ","version":"Next","tagName":"h3"},{"title":"Env​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#env","content":"這邊特別設置一個環境變數是給 minikube 使用的，因為一邊透過 kubectl 操作 kubernetes 集群實際上是透過 $HOME/.kube/config(default) 內的設計去跟集群獲得授權來操作，那在 minikube 的環境中，我們透過 CHANGE_MINIKUBE_NONE_USER=true 這個環境變數可以讓產生出來的可以被任何使用者帳號讀取使用，否則預設是 root:root 才有權限存取 ","version":"Next","tagName":"h3"},{"title":"Before_Script​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#before_script","content":"這邊先針對 / 這個 Mount Point 設定成 rshard，供 kube-dns 使用 想瞭解 rshard 可以參考 Kubernetes Mount Propagation 這篇文章，有詳細的說明 Volume 間的 rshard/rslave 以及後來新的 feature volume bidirectional 的介紹 接下來就是下載 kubectl 以及 minikube 兩個相關的執行檔接者透過 minikube 來創建本地的 kubernetes 集群 -v 9 是顯示更多的 log，可以方便安裝期間偵錯--vm-driver=none 採用 docker 的方式而不是 VM 的方式來創建 kubernetes--bootstrapper=kubeadm 指定使用 kubeadm 進行 kubernetes 叢集的安裝--kubernetes-version=v1.10.0 這邊則是指定 kubernetes 的版本 最後是預設將授權的方式使用 RBAC, 相關的 issue 可以參閱 minikube issue #1722到了這一步後我們的 kubernetes 集群就正式啟動了，但是因為一些核心的服務，如 kube-dns 等可能還沒有正常運作，這邊就根據你的需求看看是否需要確認該服務已經啟動後才進行應用程式的測試後續的部分都是透過一些腳本語言的方式確認 kubernetes 集群已經正常運作，這部分有很多寫法，依照自己喜好即可。 ","version":"Next","tagName":"h3"},{"title":"script​","type":1,"pageTitle":"使用 Travis CI 為你的 Kubernetes 應用程式打造自動化測試","url":"/docs/techPost/2018/travisci-k8s#script","content":"當上述的腳本都執行完畢後，意味者集群可以開始運行，我們可以進行自己應用程式的測試該測試則會真的對剛創立的 kubernetes 集群進行 Pod 的創立/查詢/刪除 等行為。 相關測試 針對上述使用的示範專案以及相關的 TravisCI, 首先我們先來看消耗時間，畢竟如果每次測試都花上太多時間，其實也是會消磨大家的耐心的 這邊直接擷取 TravisCI 的測試報告, 該測試總共花費的時間是 2 min 48 sec  左邊顯示的是相關指令，而右邊則是顯示該指令花費的時間。 花費了將近 50 秒在進行 go 相關 library 的安裝，這是應用程式需要的，跟 `kubernetes 無關安裝 kubectl/minikube 等相關工具，這邊花費了大概 13 秒左右`啟動 minikube 大概花費了 53 秒最後等待 kubernetes及集群中的 kube-dsn 啟動則花費了 23 秒左右. 零零總總算一下，大概總共有 90 秒的時間再啟動一個 kubernetes 的集群，目前大概是耗費了整體測試的 90/168=53%但是這個時間是固定的，基本上當本體的應用程式測試的時間愈複雜，花費的時間愈久，這個創立集群花的時間也就微不足道了。 其他 Q: 我就愛用 ubuntu-14.04 就是沒有 systemd 該怎麼辦? A: 為了生命良好，先改用 bootstrapper=localkube Q: 只能用在 TravisCI 嗎? 其他的測試服務如 CircleCI,Jenkins 能不能這樣? A: 基本上概念一樣，你可以用各種方式去安裝kubernetes集群，事實上我也有在 Jenkins 做過類似的事情，此外除了 minikube 外你可也可以用 Vagrant + kubeadm + Jenkins 去做到類似的事情 Q: 我的測試功能可能會要有更複雜的需求跟更複雜的環境，可以怎麼辦 A: TravisCI 本身虛擬出來的環境還是比較死，不夠靈活，可以改用 Jenkins 等自己有辦法掌握虛擬機器內容的服務 Q: 有辦法支援 kubernetes v1.11.0 嗎 A: 這要依賴 minikube 官方的支援 參考 https://blog.travis-ci.com/2017-10-26-running-kubernetes-on-travis-ci-with-minikubehttps://github.com/travis-ci/travis-ci/issues/7260 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2018/travisci-step-job-stage","content":"","keywords":"","version":"Next"},{"title":"install/script​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#installscript","content":"整個 TravisCI 的生命週期是由兩個主要步驟來構成 install: 用來安裝任何相依性套件的階段script: 真正用來進行 CI 相關測試的階段 除了上述這兩個步驟外，也有所謂類似 Environment 這種非必要選項可以讓整個 TrasivCI設定更簡潔 譬如可以讓 TravisCI 幫你準備相關的環境程式語言環境與特定版本，譬如說 golang 1.8, 1.9 對於 install/script 這兩個執行步驟來說，本身為了讓運作邏輯更加細膩，所以又衍生出了前後步驟的概念 before_install: 該步驟會在 install 前運行，主要是用來準備任何 install 步驟所需要的資源，譬如透過 apt-get update 等更新套件倉庫。before_script: 該步驟會在 script 前運行，如同 before_install 一樣，為了 script 進行資源準備來滿足真正測試所需，譬如資料庫的建置after_script: 當 script 運行完畢後會執行的步驟，實際上還有所謂的after_success 以及 after_failure 更細部的針對測試的結果來區分的步驟。 根據相關人員在 Github Issue 的回答， before_install 以及 before_script 的使用時機如下 before_install runs before the install step, which is meant to install any required packages or dependencies. You can prepare things before you run this step, or you can e.g. run sudo apt-get update to refresh the apt indexes. before_script runs before the actual test/build script runs. It's commonly used to run any preparation steps required to get the build running, for instance copy database configurations, set up any additional environment configuration, and so on. ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#example","content":"以下示範一個非常簡單的 .travis.yml 設定檔案，在此環境中，我們要求 TravisCI 準備一個 golang 1.8 版本的環境，同時對於 install 以及 script 這兩個階段我們都執行非常簡單的指令。 language: go go: - &quot;1.8&quot; before_install: - echo &quot;before_install&quot; install: - echo &quot;install&quot; - before_script: - echo &quot;before_script&quot; script: - echo &quot;script&quot;  上述的運行結果如下圖，該圖示我們可以觀察到 golang 版本 1.8四個步驟的結果依序輸出  ","version":"Next","tagName":"h2"},{"title":"Solution​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#solution","content":"有了關於 TravisCI 建置週期的概念後，回過頭來探討一些下最初的需求 進行 單元測試/整合測試 等各種確保程式碼正常運作的測試建置 Docker 相關映像檔並且更新到相關的容器倉庫將最新的程式碼部屬到相關的環境上 首先，這三個要求是有依賴性的，前面的失敗，後面的就不需要運行。 這邊沒有一個標準答案，有非常多的實現方式，譬如說 將所有的步驟都放在 script 這個步驟去依序執行使用 script, after_script 甚至是其他 deploy 等不同的步驟來依序完成這些事情 譬如下面範例 (單純舉例) language: go go: - &quot;1.8&quot; before_install: - echo &quot;before_install&quot; install: - echo &quot;install&quot; before_script: - echo &quot;before_script&quot; script: - go test -v ./... after_script: - sudo docker build -t .... - sudo docker push .... deploy: - ./deploy.sh  可以到官方網頁這邊學到更多不同的建置步驟以及彼此之間的先後關係https://docs.travis-ci.com/user/customizing-the-build/ 用下列這張圖來幫這個章節做一個總結  Job ","version":"Next","tagName":"h2"},{"title":"Definition​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#definition","content":"瞭解了基本的用法後，我們要來看看一些關於 TravisCI 的進階用法，看看透過這些進階用法我們能夠完成什麼樣更豐富的 CI 流程。 首先，我們先定義什麼叫做 Job, Job 就是一個歷經 TravisCI 生命週期所有步驟的基本單位。 所以一個Job 簡單來說會經歷過 Environment, before_install, install, before_script, script 以及 after_script 所有步驟 這邊列舉的步驟並不是TravisCI的所有步驟，只是舉出幾個常見的步驟 ","version":"Next","tagName":"h2"},{"title":"Multiple Job​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#multiple-job","content":"有了 Job 的基本概念後，我們就可以往下思考一些更進階的用法。 假設專案本身是透過 golang 程式語言撰寫而成的，我們現在希望測試該專案在不同 golang 版本下是否都能夠正常運行。 舉例來說，我希望使用 golang 1.8 以及 golang 1.9 這兩個版本來進行專案的測試。 問題來了，這種情況下，我們要如何透過TravisCI來完成呢? 最直覺的就是我們什麼都硬幹，自己在 TravisCI 內去安裝各式各樣的環境，然後撰寫腳本去分開各式各樣的測試，將所有的需求都在一個Job 內完成。 當然這種情況下整個環境準備/測試等相關的邏輯就會複雜且不好維護 為了讓整個測試的架構乾淨與明瞭，我們可以透過 Travis 平行的運行多個Job 來滿足我們的需求。 在此架構下， Travis 會併行的去運行這些 Job, 且每個 Job 都有自己的建置週期，所有的 Job 都要都要成功該次測試才算成功。 使用下列圖示再次說明 Multiple Job 的概念 上方描述的是一個簡單的 Jobs 概念，涵蓋了本文提及的基本建置週期。 下面則是為了滿足特別需求，希望多個golang 版本同時測試，此時我們就可以一次運行多個 Job, 其中只有 Environemnt 的部份是完全獨立，其餘則是都會採用相同的設定。 在 TravisCI 的設定檔案 .travis.yml 裡面，我們可以用下列的方式完成這個需求 language: go go: - &quot;1.8&quot; - &quot;1.9&quot; before_install: - echo &quot;before_install&quot; install: - echo &quot;install&quot; before_script: - echo &quot;before_script&quot; script: - echo &quot;script&quot;  最後運行的結果如下, 可以看到該次的測試同時運行了兩個 Job, 這兩個 Job 分別是不同的 Golang 版本。 ","version":"Next","tagName":"h2"},{"title":"Custom Job​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#custom-job","content":"上述我們利用個 go 這個由 TravisCI 所定義的語法來完成產生 MultipleJob 的功能。 這時候腦筋一轉，install,script 這些建置步驟是否也都可以有類似的概念呢? 舉例來說，我希望對我的專案進行不同的測試，譬如 Unit Test, Integration test。 而這些測試除了測試的方式不同之外，環境的準備也不同 此外，同時運行這些測試也能夠減少測試的時間，並且將測試結果更清楚的標示出是哪種測試出問題。 將上述的需求轉換成 TravisCI 的概念的話 就是需要同時運行多個 Job, 每個 Job 裡面對於每個建置步驟都有自己客製化的需求。 這個需求我們透過下圖視覺化的方式來重新檢視一次  我們的需求很簡單，希望同時運行多個 Job 且這些 Job 針對不同的運行階段能夠選擇是否要使用預設的規則或是客製化自身的需求。 TravisCI 就有提供了這樣的功能供各位去使用，在其 .travis.yml 這個 yml 的檔案中，我們要透過 jobs:include 的概念去撰寫 language: go go: - &quot;1.8&quot; before_install: - echo &quot;before_install&quot; install: - echo &quot;install&quot; before_script: - echo &quot;before_script&quot; script: - echo &quot;script&quot; jobs: include: - stage: Custom Testing name: Unit-Testing go: &quot;1.8&quot; script: echo &quot;unit script&quot; - name: Integration-Testing before_install: &quot;Integration-Testing_before_install&quot; go: &quot;1.9&quot; script: &quot;Integration-Testing_script&quot;  其運行結果如下 我們可以觀察到我們的確運行了兩個 Job 而這兩個 Job 都有明確的名稱，這邊就沒有點進去看各自 Job 的運行結果，有興趣的人可以自行嘗試看看。 這邊先不討論語法，等到所有的概念都講述完畢後，再來討論語法的撰寫。 Build Stage 有了上述的 Multiple Job 的概念後，我們重新審視一下最初的需求 進行 單元測試/整合測試 等各種確保程式碼正常運作的測試建置 Docker 相關映像檔並且更新到相關的容器倉庫將最新的程式碼部屬到相關的環境上 首先，不同的測試本身可以透過同時運行多個 Job 來滿足，這邊好處理。 那建置/更新 Docker Image 這件事情，我們要讓誰來處理? 上述的測試選一個 Job, 客製化其某些建置步驟來處理額外產生一個 Job 來專門處理這類的需求 只是採用第一種方式可能會有一個問題 假設我們需要 所有 測試都通過才能進行 Docker 相關的處理，那我們就沒有辦法在任意一個測試 Job 內去處理這個邏輯。 為了解決這個問題，除了將所有的工作重新集中回一個Job處理外，就只能在開啟第三個 Job 來處理。 但是這個 Job 本身有相依性的問題，它必須要確認前述相關測試的所有 Job 都完成才能夠繼續往下運行。 為了解決這個問題，我們要在這邊介紹 Stage 這個概念。 Stage 的特色以及概念如下 由一群 Job 組成只要有一個 Job 失敗，該 Stage 就會被視為失敗只有當該前 Stage 是成功的狀態，才會執行下一個 Stage 有了 Stage 的概念，我們可以把上述的需求重新整理歸納成三個 Stage Testing Stage Unit Testing JobIntegration Testing Job Docker Build StageDeploy Stage 將這個概念用下圖再次檢視一次  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#example-1","content":"每個 Stage 之間彼此有依賴性，只要當其中一個 Stage 失敗，就不會往下執行 下圖是每個 Stage 都順利執行的成果 下圖則是當第一個 Testing Stage 有任何失敗的結果  language: go go: - &quot;1.8&quot; before_install: - echo &quot;before_install&quot; install: - echo &quot;install&quot; before_script: - echo &quot;before_script&quot; script: - echo &quot;script&quot; jobs: include: - stage: Custom Testing name: Unit-Testing script: echo &quot;unit script&quot; - name: Integration-Testing before_install: echo &quot;Integration-Testing_before_install&quot; - stage: Build Docker Image script: echo &quot;docker build&quot; - stage: Deploy script: echo &quot;release&quot;  首先，我們要先定義 Stage, 在 Stage 裡面可以定義多個 Job, 而每個 Job 內又可以自定義每個建置階段，若沒有特別設定的，都會採用最上層的全域設定 定義 Stage 則採用 stage 這個關鍵字來幫建立，並且命名，針對每個 job 可以透過 name 的方式把對應的名稱替換掉讓整個測試報告更有閱讀性，然後接下來就可以去描述每個建置步驟，如 script, install 等各式各樣的建置週期步驟。 更詳細的設定可以直接參考官網的說明 Summary 本文跟大家分享了關於 TravisCI 的使用心得，從基本的使用方法到進階的 Multiple Job 以及 Stage 透過這些概念的組合，我們能夠將 CI/CD 的流程拆的更細緻，讓整個架構與流程更加清楚，同時透過平行運行的方式加快整體流程的速度 (這部份不一定，完全是看每個專案的流程). ","version":"Next","tagName":"h2"},{"title":"Reference​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2018/travisci-step-job-stage#reference","content":"https://github.com/travis-ci/travis-ci/issues/1392https://docs.travis-ci.com/user/customizing-the-build/https://docs.travis-ci.com/user/build-stages ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/aks-cni-i","content":"","keywords":"azure cni","version":"Next"},{"title":"Azure-VNET​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/aks-cni-i#azure-vnet","content":"Azure-VNET CNI 的詳細原始碼都在 Github-Azure-Container-Networking, 有興趣的讀者可以自行閱讀來學習。 基本上 Azure-VNET 跟常見的 L2 Linux Bridge 非常相似，其運作流程如下 創建一個 Linux Bridge Azure0 (若存在就不創造)創造一條 Veth 的 Linux Logical Link將該 Veth 的一端放到 Pause Container 內，並且命名為 eth0.將該 Veth 的另一端綁到 Azure0 上，該 Veth 的名稱都是會 azvxxxxxxx呼叫 Azure-VNET-IPAM 去取得可用的 IP/Subnet 並且設定到 Pause Container 內的 eth0 介面 這部分會在下個章節解釋其運作 有興趣的讀者也可以參閱 AKS SSH to Node 這篇文章的方式連接到 AKS 內部的節點來實際看看這些資訊 我們可以使用 brctl 這個工具來觀察 Linux Bridge 的關係 azureuser@aks-agentpool-15026905-1:~$ brctl show bridge name bridge id STP enabled interfaces azure0 8000.000d3a51cdbb no azv1be2dcd6c83 azv2705efbd6d8 azv499967b4ec4 azv4d966079c93 azv5a6822c76a3 azv93928864e55 azva985f33c456 azvaa217c54e39 azvbc5198bad97 azvc1ab312d517 azvc92e6588502 eth0 docker0 8000.024220b0b010 no  基本上整個節點中的狀況如下圖，每個 Pod 都會透過 Veth 與 Azure0 這個 Linux Bridge相連 基本上該節點上面有多少個沒有設定 HostNetwork=true 的 Pod, 就會有多少條對應的 azvxxxx veth link. 此外，我個人對於 Azure-VNET CNI 覺得很好也喜愛的地方就是留有大量地資訊在節點上，這部分不論是對於研究或是除錯都非常的好用。 該資訊被放置於 /var/run/azure-vnet.json, 當 azure-vnet CNI 每次被呼叫來執行對應工作的時候，都會詳細的紀錄這次的資訊，包含 ContainerID, PodName, IP, Route等各式各樣的設定資訊。 { &quot;Network&quot;: { &quot;Version&quot;: &quot;v1.0.17&quot;, &quot;TimeStamp&quot;: &quot;2019-03-22T14:30:27.934886517Z&quot;, &quot;ExternalInterfaces&quot;: { &quot;eth0&quot;: { &quot;Name&quot;: &quot;eth0&quot;, &quot;Networks&quot;: { &quot;azure&quot;: { &quot;Id&quot;: &quot;azure&quot;, &quot;Mode&quot;: &quot;bridge&quot;, &quot;VlanId&quot;: 0, &quot;Subnets&quot;: [ { &quot;Family&quot;: 2, &quot;Prefix&quot;: { &quot;IP&quot;: &quot;10.240.0.0&quot;, &quot;Mask&quot;: &quot;//8AAA==&quot; }, &quot;Gateway&quot;: &quot;10.240.0.1&quot; } ], &quot;Endpoints&quot;: { &quot;1bd38ad8-eth0&quot;: { &quot;Id&quot;: &quot;1bd38ad8-eth0&quot;, &quot;SandboxKey&quot;: &quot;&quot;, &quot;IfName&quot;: &quot;eth0&quot;, &quot;HostIfName&quot;: &quot;azv5a6822c76a3&quot;, &quot;MacAddress&quot;: &quot;nkfKrRbG&quot;, &quot;InfraVnetIP&quot;: { &quot;IP&quot;: &quot;&quot;, &quot;Mask&quot;: null }, &quot;IPAddresses&quot;: [ { &quot;IP&quot;: &quot;10.240.0.94&quot;, &quot;Mask&quot;: &quot;//8AAA==&quot; } ], &quot;Gateways&quot;: [ &quot;10.240.0.1&quot; ], &quot;DNS&quot;: { &quot;Suffix&quot;: &quot;&quot;, &quot;Servers&quot;: [ &quot;168.63.129.16&quot; ] }, &quot;Routes&quot;: [ { &quot;Dst&quot;: { &quot;IP&quot;: &quot;0.0.0.0&quot;, &quot;Mask&quot;: &quot;AAAAAA==&quot; }, &quot;Src&quot;: &quot;&quot;, &quot;Gw&quot;: &quot;10.240.0.1&quot;, &quot;Protocol&quot;: 0, &quot;DevName&quot;: &quot;&quot;, &quot;Scope&quot;: &quot;0&quot;} ], &quot;VlanID&quot;: 0, &quot;EnableSnatOnHost&quot;: false, &quot;EnableInfraVnet&quot;: false, &quot;EnableMultitenancy&quot;: false, &quot;NetworkNameSpace&quot;: &quot;/proc/5336/ns/net&quot;, &quot;ContainerID&quot;: &quot;1bd38ad8d840dd1f84597d4343b3bd116188cd1e4a797cc31bdc1aa3dc654a5b&quot;, &quot;PODName&quot;: &quot;addon-http-application-routing-external-dns-74db4f974b-8w4wz&quot;, &quot;PODNameSpace&quot;: &quot;kube-system&quot; }, ..................................  上述 Endpoints 裡面的每個物件都會描述到一個 Pause Container 的網路環境與設定，不過對於 CNI 來說本身不在意是不是 Pause Container. 這部分是 Kubelet 自行實現的邏輯，所以基本上不會再 CNI 這邊看到 Pause Container 相關的文字。 ","version":"Next","tagName":"h2"},{"title":"Azure-VNET-IPAM​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/aks-cni-i#azure-vnet-ipam","content":"接下來要探討的是要如何分配 IP/Subnet 這件事情，基本上任意兩個 Pod 都不應該使用相同的 IP/Subnet。但是對於 CNI 這種非中央極權管理的執行程式來說，要做到不衝突就必須要有一些機制了。 各式各樣的 CNI 都有自己的機制來處理，不論是透過 ectd 或是自行實現集中式管理機制，只要能夠避免分配重複 IP/Subnet 即可 前面有提過，三大公有雲的 Kubernetes Service 相較於自建來說有更多的優勢與特色就是因為可以將 KUbernetes 與公有雲內的設施與狀態結合來提供更多的功能 Azure-VNET-IPAM 這個 IP/Subnet 管理機制基本上就是與 Azure 的環境有高度整合，接下來我們來看一下其運作的原理。 每台機器上的 Azure-VNET-IPAM 這個 CNI 都會執行相同的運作原理，最後卻要取得不同的 IP/Subnet, 這整個運作原理如下 每個 Azure-VNET-IPAM CNI 都會透過 HTTP 去詢問叢集 API, 來確認當前節點在 VNET 內可以擁有的 IP 數量從可用的 IP 數量內隨機挑選一個 IP 並返回最後 Azure-VNET 就會取得 Azure-VNET-IPAM 得到的 IP/Subnet 並且設定到對應的 Pause Container 裡面 ","version":"Next","tagName":"h2"},{"title":"API​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/aks-cni-i#api","content":"從原始碼中可以觀察到下列 URL 的設定 const ( // Host URL to query. azureQueryUrl = &quot;http://168.63.129.16/machine/plugins?comp=nmagent&amp;type=getinterfaceinfov1&quot; // Minimum time interval between consecutive queries. azureQueryInterval = 10 * time.Second )  然而目前實際上使用沒有通，原因在於先前的 Commit 有修改過該 URL 的數值，而且該 Commit 距離這篇文章不到一個月前，所以我認為 Azure 上面還沒有採用新的版本，因此實際上操作時還是使用舊有的 URL。 Update host machine ip (#300) * Limiting the size of our buffered payload to ~2MB * Changing IPs for calls to host machines from 169.254.169.254 to 168.63.129.16.  根據上述的原始碼，我們可以直接在 AKS 節點中直接透過 curl 的方式去詢問，結果如下 azureuser@aks-agentpool-15026905-0:~$ curl &quot;http://169.254.169.254/machine/plugins?comp=nmagent&amp;type=getinterfaceinfov1&quot; &lt;Interfaces&gt;&lt;Interface MacAddress=&quot;000D3A51C490&quot; IsPrimary=&quot;true&quot;&gt;&lt;IPSubnet Prefix=&quot;10.240.0.0/16&quot;&gt; &lt;IPAddress Address=&quot;10.240.0.35&quot; IsPrimary=&quot;true&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.36&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.37&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.38&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.39&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.40&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.41&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.42&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.43&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.44&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.45&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.46&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.47&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.48&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.49&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.50&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.51&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.52&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.53&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.54&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.55&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.56&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.57&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.58&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.59&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.60&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.61&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.62&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.63&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.64&quot; IsPrimary=&quot;false&quot;/&gt; &lt;IPAddress Address=&quot;10.240.0.65&quot; IsPrimary=&quot;false&quot;/&gt; &lt;/IPSubnet&gt;&lt;/Interface&gt;&lt;/Interfaces&gt;  有趣的是，不同的 AKS 節點問到的結果會是不同的，所以每台節點上得 AKS-VNET-IPAM 都可以透過這個方式來取得該台節點上所擁有能夠使用的 IP 數量以及對應的網段。 此外 AKS-VNET-IPAM 也會在本機端記錄相對應的資訊, 檔案位置位於 /var/run/azure-vnet-ipam.json 其內容主要會紀錄本機端可以使用的所有 IP 位置 { &quot;IPAM&quot;: { &quot;Version&quot;: &quot;v1.0.17&quot;, &quot;TimeStamp&quot;: &quot;2019-03-18T21:48:15.199533623Z&quot;, &quot;AddressSpaces&quot;: { &quot;local&quot;: { &quot;Id&quot;: &quot;local&quot;, &quot;Scope&quot;: 0, &quot;Pools&quot;: { &quot;10.240.0.0/16&quot;: { &quot;Id&quot;: &quot;10.240.0.0/16&quot;, &quot;IfName&quot;: &quot;eth0&quot;, &quot;Subnet&quot;: { &quot;IP&quot;: &quot;10.240.0.0&quot;, &quot;Mask&quot;: &quot;//8AAA==&quot; }, &quot;Gateway&quot;: &quot;10.240.0.1&quot;, &quot;Addresses&quot;: { &quot;10.240.0.36&quot;: { &quot;ID&quot;: &quot;&quot;, &quot;Addr&quot;: &quot;10.240.0.36&quot;, &quot;InUse&quot;: true }, &quot;10.240.0.37&quot;: { &quot;ID&quot;: &quot;&quot;, &quot;Addr&quot;: &quot;10.240.0.37&quot;, &quot;InUse&quot;: true }, &quot;10.240.0.38&quot;: { &quot;ID&quot;: &quot;&quot;, &quot;Addr&quot;: &quot;10.240.0.38&quot;, &quot;InUse&quot;: true }, &quot;10.240.0.39&quot;: { &quot;ID&quot;: &quot;&quot;, &quot;Addr&quot;: &quot;10.240.0.39&quot;, &quot;InUse&quot;: false }, .............. }}  基本上這些 IP 的資訊就跟 Azure Portal 裡面 VNet (Virtual Networks) 顯示的數量是一致的，因此我們可以透過 Portal 的方式就知道每台節點上運行的 Pod 的 IP 範圍。 ","version":"Next","tagName":"h2"},{"title":"Capacity​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/aks-cni-i#capacity","content":"一個很有趣的問題這時候就會浮現了，因為每個節點上能夠使用的 IP 數量跟 Virtual Networks 裡面的每台節點有關，那如果我部署大量的 Pod 到該節點上是否會發生 IP 不足夠的問題? 針對這個問題，我嘗試部署了超過 IP 數量的 Pod 到節點上，結果最後看到的是滿滿的 Pending hwchiu-utils-785f896cc5-cgjsq 0/1 Pending 0 4m hwchiu-utils-785f896cc5-4gmwv 0/1 Pending 0 4m hwchiu-utils-785f896cc5-brnl6 0/1 Pending 0 4m hwchiu-utils-785f896cc5-gwgcz 0/1 Pending 0 4m hwchiu-utils-785f896cc5-v5s45 0/1 Pending 0 4m hwchiu-utils-785f896cc5-knz2r 0/1 Pending 0 4m hwchiu-utils-785f896cc5-26pn2 0/1 Pending 0 4m hwchiu-utils-785f896cc5-xcmft 0/1 Pending 0 4m hwchiu-utils-785f896cc5-wlckh 0/1 Pending 0 4m hwchiu-utils-785f896cc5-dm49b 0/1 Pending 0 4m hwchiu-utils-785f896cc5-nkcgw 0/1 Pending 0 4m hwchiu-utils-785f896cc5-cwkbl 0/1 Pending 0 4m  這意味者這些 Pod 在 kubernetes scheduler/kubelet 這階段就被阻止了，根本沒有機會讓 CNI 繼續往下執行，這時候我們可以研究一下該 kubernetes node 的設定, 可以發現到有趣的資訊 Capacity: attachable-volumes-azure-disk: 8 cpu: 2 ephemeral-storage: 30428648Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 7137108Ki pods: 30 Allocatable: attachable-volumes-azure-disk: 8 cpu: 1931m ephemeral-storage: 28043041951 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 5357396Ki pods: 30  這兩者的資訊可以參閱 reserve-compute-resources 其中可以注意到 Pod 的數量被設定成 30，這個數字跟我們先前透過 API 去問到的 IP 數目幾乎是一致的 API 問到的 IP 還包括節點本身，所以是 30 + 1 = 31 所以這邊可以看到針對 IP 用光的問題， AKS 處理的方式除了 CNI 本身顯示錯誤訊息之外，最上層還先透過 Kubernetes Node Capacity 的方式進行了第一層的阻擋，理論上 CNI 本身不應該遇到 IP 用光的情形，因為根本不應該有超過數量的 Pod 被嘗試部署到該節點上。 Summary 本文中我們研究了 AKS 中節點的 CNI 運作流程，包含了簡單的網路設定 (L2 Bridge)，同時也探討一下了 IPAM 的運作邏輯與流程，發現到該 IPAM 與 Azure 的架構本身有強烈的整合，透過 Azure VNET 的設定來控管每個節點上的 Pod 能夠使用的 IP 範圍與數量。 最後我們用一張流程圖來幫本文的 CNI 做一個總結 下一篇文章會著重在此基礎上，當上述的 L2 Bridge 與 IPAM 都處理完畢後，這些 Pod 彼此中間是怎麼溝通的，不論是同節點或是跨節點的傳輸，特別是這些節點實際上都是 VM 的情況下，是要如何做到跨節點傳輸的 Reference Connect with SSH to Azure Kubernetes Service (AKS) cluster nodesGithub-Azure-Container-Networkingreserve-compute-resources ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/aws-profile","content":"","keywords":"","version":"Next"},{"title":"Usage​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/aws-profile#usage-1","content":"打開該 aws 套件後，使用上大概會是如下 &lt;aws:Account1&gt;╭─hwchiu@hwchius-MBP ~ ╰─$  使用指令 asp 搭配 tab 可以列出目前所有設定過的 aws profile &lt;aws:LFCORD&gt;╭─hwchiu@hwchius-MBP ~ ╰─$ asp Account1 Account2 Account3 Account4  同時該 aws plugin 也會幫忙設定安裝 aws aws_zsh_completer, 意味你可以透過 tab 的方式來更方便的使用 aws cli ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/aws-profile#configuration","content":"基本上參考 on-my-zsh 的設定方式，先到 $HONE/.zshrc 中將 plugin 的選項加入 aws 即可，譬如 ... # Which plugins would you like to load? # Standard plugins can be found in ~/.oh-my-zsh/plugins/* # Custom plugins may be added to ~/.oh-my-zsh/custom/plugins/ # Example format: plugins=(rails git textmate ruby lighthouse) # Add wisely, as too many plugins slow down shell startup. plugins=(git aws) source $ZSH/oh-my-zsh.sh ...  這邊有一個要注意的是，預設的 aws plugin 使用的是 RPROMPT 而非 PROMPT, 這意味他的顯示會是在畫面最右邊，而非左邊。 此外也要注意妳選用的 theme 會不會幫你把 PROMPT 給蓋掉，導致功能失效。 譬如我選擇的主題是 bira，所以 PROMPT 以及 RPROMPT 都會被覆蓋掉導致 aws plugin 不會成功。 因此我最後還是自己修改 $HOME/.zsrhc 來處理 ... # alias zshconfig=&quot;mate ~/.zshrc&quot; # alias ohmyzsh=&quot;mate ~/.oh-my-zsh&quot; PROMPT='$(aws_prompt_info)'&quot;$PROMPT&quot;  詳細的更多實作可以直接參閱 $HOME/.on-my-zsh/plugin/aws 裡面的介紹 藉由這個套件的幫助，我平常就會使用 asp 來切換不同的 AWS Account, 同時透過 shell PROMPT 來知道當前使用的 PROFILE，避免在不同的 Account 執行錯誤的動作 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/b4-after","content":"","keywords":"","version":"Next"},{"title":"Flat topology scales poorly and hurts availability​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#flat-topology-scales-poorly-and-hurts-availability","content":"請搭配 Slides 的 p.16 - 18 一併觀看 ","version":"Next","tagName":"h2"},{"title":"Saturn​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#saturn","content":"在原先的 B4 架構中(p.17)，每個 site topology 都成為 Saturn, 每個 Saturn 中都分成上下兩個部分，下半部分總共有四個 Saturn chassis, 而上半部分則是 兩個 或是 四個 的 Saturn chassis，其中上下 Saturn Chassis 中間溝通的頻寬是 2.56/5.12 Tbps. 為了解決 Scale 的問題，Google 只能繼續打造更多的 Datacenter Site 並且緊鄰原先欲解決 Scale 問題的 Site. 如圖 18. 然而這種愈來愈多的相同地理位置的 Datacenter Site 解決方案卻產生三個問題 愈來愈多的 Datacenter Site 會增加 TE (Traffic Engineering) 計算的成本，導致計算路徑需要的時間更長。 原先 TE 的設計就是基於 Site-Level 的設計，因此 Site 的數量愈多， 計算路徑所需要的時間也相對愈多。原文使用了 super-linearly 來描述彼此的關係。 同時 TE 計算的時間增加，也會導致當有任何 data plane 發生問題時修復所需要的時間隨者 Site 數量的增加，實際上也對底下的交換機產生了更大的壓力，因為交換機內部的傳輸規則表大小是有上限的。最重要的問題就是，因為相鄰地區的site大幅度的增加Capacity的計算與規劃，對於應用程式開發者來說也產生的很大的困惑。基於 Site-Level 的設計下，卻有很多的 Site 是要服務相同地區的使用者，但是彼此又非常接近。 在原先的 B4 設計中， 所謂的 Capacity 計算與規劃主要是用來處理 site to site 之間的 WAN 頻寬計算 為了解決 Scale 的問題，同時又要能夠處理這些延伸出來的挑戰，Google 最後提出了全新的 B4 架構 Jumpgate (p.19 - 20). ","version":"Next","tagName":"h3"},{"title":"Jumpgate​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#jumpgate","content":"Jumpgate 的整體網路架構不在如同之前 Saturn 的扁平，而是採用了階層式的架構. 首先 site 是由一個叫做 supernode 的基本單位所組成的 (p.19). 每個 site 內會包含多組 supoernode, 同時這些 supernode 會互相連結組出一個 full mesh 的網路拓墣來支撐整個 site. Supernode 本身則是一個 兩階層的 Clos 網路拓墣，這部分你可以在(p.19)看到更詳細的圖文說明。 Google 說明在基於這種階層式架構下的實驗顯示了其有三個優點 Scalability, 可以透過水平擴增的方式增加 supernode 即可以增加該 Site 節點的能力，而不用繼續增加 site 的數量，可以避免讓 TE 產生更多的計算問題。Availability, 透過垂直擴增的方式去逐步的更新 supernode 就可以避免當前的傳輸服務中斷而被影響。 ","version":"Next","tagName":"h3"},{"title":"Solving capacity asymmetry problem in hierarchical topology​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#solving-capacity-asymmetry-problem-in-hierarchical-topology","content":"雖然採用了 階層式架構 帶來了不少好處，但是經過 Google 觀察到 階層式架構 對 TE 的計算實際上也帶來了不少的問題。 最簡單的範例就是當整個網路實體架構中有任何因為 設備維護,操作 等導致了當前有任何 data plane 裝置不穩定，這些會影響整體連結的頻寬，連帶影響最終的 Capacity 的計算，最後會影響整體網路流量的分配。 這個情形就稱為 Capacity Asymmetry, 流量的不對等。 接下來透過 (p22-24) 來解釋這個問題到底實際上會產生什麼樣的影響。 在觀看(p22,p23) 以前要先強化一個觀念， TE 是基於 Site to Site 的流量去進行計算的. 首先，先看一下最完美的模型(p22)在這個模型之中我們總共有 三個 site, 其中每個 site 都各自擁有四個 supernode.每個 site 跟 site 之間的 supernode 也都各自擁有四條連線，其中每個 link 的權重都是 1在此模型下，我們可以計算出每個 site 之間的 capacity 是 16(4x4), 有四個 supernode, 且每個node有四條連線.根據 capacity 的計算，可以知道這些 site 彼此間的最大流量是 16。根據上述的資訊, TE 最後分配流量的時後，會分配最多 16 的流量到這些 site 來傳輸。 接下來考慮到假設遇到一些硬體設備發生故障時的情形。假設第一個 Site 裡面的第四個 Supernode 發生了一些問題，導致對外連線從本來的 4 條變成了 2 條。 因此在計算 capacity 容量時，會因為這個節點擁有最小的頻寬，也就是所謂的頻頸點。 所以最後計算整體的 site to site 之間的頻寬就不會是 16(4x4), 而是 8(4x2). 因此最後透過 TE 分配流量時最多只會分配 8 單位的流量到第一個 site. 實際上當前總共有 14(4x3 + 2) 單位的頻寬可以用，但是因為 TE 算法的關係最後只能傳輸 8 單位，因此就會有所謂的頻寬浪費(43%) 為了解決這個問題， Google 最後引入了兩個新的概念來，分別是 Sidelink 以及 Supernode-Level TE. ","version":"Next","tagName":"h2"},{"title":"Sidelink​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#sidelink","content":"sidelink 的含義就是在同一個 site 內的 supernode 之間加上一條額外的 link. 想法很單純，如果有任何一個 supernode 本身對外的頻寬流量是瓶頸的話，那就將該多出來的流量導向其他的 supernode 去幫忙轉發處理。 所以根據 (p.26) 的圖表，我們可以這樣解讀，現在該 site 中的 supernode 可以互相轉發流量，所以我們的 TE 最後還是可以傳送高達 14 單位的流量到第一個 site 裡面。 但是如果有超過 2 以上的單位流量傳輸到第四個 supernode 的話，因為該 supernode 本身只有 2 的對外流量，因此透過 sidelink 將多出來得流量都轉發到其他的 supernode 去處理，就能夠盡可能地利用所有 site to site 之間的傳輸頻寬。 上述的敘述中有一個沒有描述清楚的東西就是 TE 現在要如何去計算 sidelink 的容量? 因此 google 就提出了不同於之前的 site-level TE 的新算法,supernode-level TE. ","version":"Next","tagName":"h3"},{"title":"Supernode-Level TE​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#supernode-level-te","content":"為了在階層式架構中 能夠正確的處理 Capacity Asymmetry，勢必要想出一套支援 Supernode-level 的負載平衡演算法來達到最大的 Site-Level 頻寬使用率。 此外， Google 希望這個新的算法/機制要能夠有下列的特性 當網路架構發生問題狀況時，能夠更快收斂避免當前網路狀況不通的空窗期基於有限的硬體交換機傳輸規則表大小下能夠更有效率且更高速的去轉發封包。 當 super node 過多的時候，數量就會暴增，這意味整個路徑的計算就會更加的困難與麻煩 最重要的是這個新的機制與算法最多只能使用到一層的封包封裝 太多層的封裝會導致封包處理效率不佳 首先 Google 嘗試過基於完全的 supernode-level 去進行 TE 的計算，結果遇到了一些問題 基於 supernode-level 的傳輸，由於跨site,因此本身也需要導入 IP-in-IP 的封裝.這個方法有高效能，但是卻沒有辦法有好的收斂與計算時間，整體的時間卻是之前的 188倍， 同時這個方法也需要更好的硬體交換機支援(更大的傳輸規則表大小) 在論文中，google 提到了第二個採用的方式，就是 site-level TE 加上 supernode-level 最短路徑的結合，文中提到這個方式帶來的特性 只需要一次的封包封裝就可以完成 site to site 的傳輸可達成 scalability, 畢竟 shortest path 的計算成本比較低但是 shortest path routing 也會帶來不少問題，譬如 sidelink 的權重設計可能就會導致該 sidelink 完全不會被用到，最後就回到最原始的 Capacity Asymmetry 問題。 ","version":"Next","tagName":"h3"},{"title":"Hierachical TE​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#hierachical-te","content":"為了解決這一系列的問題，最後產生了 階層式 的 TE 架構。 在這個架構中分成了 Flow Group (FG)Tunnel Group (TG)Tunnel Split Group (TSG)Switch Split Group (SSG) 這部分的最後架構以及算法實在有點多，有興趣的人請自行參閱 B4 and after: managing hierarchy, partitioning, and asymmetry for availability and scale in google's software-defined WAN 中的 section 4.2 開始學習全貌。 這部分之後我會再額外開一篇文章來介紹其算法與實現。 ","version":"Next","tagName":"h2"},{"title":"Efficient switch rule management​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#efficient-switch-rule-management","content":"由於每個硬體交換機本身所支持的轉發規則表格大小是有限度的，因此要如何透過這些有限度的資源來滿足之前提供對應的轉發能力來滿足一切需求就是一大挑戰 由於先前的 TE 架構沒有完全說明清除，因此此章節在探討規則的時候會有部分沒有辦法釐清其原理，不過就算如此，我們還是可以學習 Google 針對這類型問題時思考問題的脈絡與方向 根據前述的討論，透過了 階層式 的 TE 設計雖然能夠解決不少的問題，但是其需要的轉發規則數量也急速上升，超過了硬體交換機當前的限制。 為了解決這個問題， Google 提出了兩個方式來解決問題 將 flow matching 相關的規則拆給給兩個 switch pipeline table 來處理，藉由繼續這種階層式的兩階段比對規則，google發現到可以支援的 site 數量提升到 60倍，意味還有很多的空間供擴充。接下來將 path 的比對概念也拆開，在一個 Clos fabric 的網路架構中，將 Path 路由相關的概念分別實作於 Edge Switch 以及 Backend Switch 兩個階段處理就能夠完全的處理之前所設計 階層式 TE 所需要的一切路由規則。 採用這個解法雖然彈性且強大，但是實際上會降低整體的流量大概 6%, google 認為是一個可接受的損失，利大於弊。ㄌ 接下來針對 p39 - p44 進行一個比較細緻的解說 首先 p39 描述的是最基本原始的狀態，每個交換機內部的 pipeline table 包含了三個狀態，分別是 ACL, ECMP 以及 Encp. 用來比對相關的規則，尋找路由規則最後進行封裝。 ","version":"Next","tagName":"h2"},{"title":"ACL​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#acl","content":"但是此種做法並沒有辦法支撐新架構 TE 所帶來的大量規則。 因此在 p40 這邊描述了第一個問題，就是到底 ACL 路由表的規則數量到底有多少 Size(ACL) ≥ (#Sites ✕ #PrefixesPerSite ✕ #ServiceClasses) 根據 p40 的統計資料顯示， ACL 的規則大概是 3k 也就是三千條左右，然而總共需要的比對規則數量則是 Site x PrefixesPerSite x ServiceClasses. 目前有 33 個節點根據論文顯示，平均的數量大概是16 左右目前有 6 種類別 SC0 - SC5在這個情況下，三個數值得到的結果大概就是 3k, 這意味者已經到達了極限，幾乎沒有辦法進行更大的擴充，不論是 site, IP prefix 或是 Service 的種類。 如同上述所說，一旦將 flow table(ACL)拆成兩個部分，分別是 VFP 以及 Per-VRF LPM 兩個規則進行處理，最後達到的效果就是可以提升將近 60倍 數量的規則數.(意味者在IP跟服務種類不變的情況下，可以擴充到1900多個節點左右) 概念如下 將 cluster prefix 符合相關的規則全部移動到 LPM (Longest Prefex Match) 表格中。 在交換機中，不同表格的大小不一定，根據論文顯示， LPM 表格的數量上限遠遠比 ACL 還要來得高。由於 LPM 表格中沒有辦法針對 DSCP 的比對(DSCP用來代表不同的服務), 但是 LPM 可以比對 VRF (Virtual Routing Forwarding) 的標籤，因此決定透過 VRF 與 DSCP 進行一個比對的關係。一開始則使用 VFP(Virtual Forwarding Plane) 表格來進行比對，在這個表格中則會透過 DSCP 的比對並且設定特定的 VRF 標籤，供後續的 LPM 表格識別其為特定的應用服務 ","version":"Next","tagName":"h3"},{"title":"Traffic Hashing​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/b4-after#traffic-hashing","content":"接下來的 p42 要描述的則是 ECMP 表格比對的大小問題。 相關的數字該投影片中都已經描述了，結論來說就是按照目前的設計，需要大概 198K 的大小，但是實際上只有 16K 可以使用。 這龐大的差距最後的解決概念就是如同 p43 所描述，將不同概念的比對放到 Clos Fabric 網路中不同階層去處理。 這邊的概念必須要完整理解 TSG/GS/SSG/TG 等各種不同階層的 TG 才更好理解。 Summary 整篇論文大概 13 左右，搭配該投影片來閱讀大概可以理解其遇到的問題以及大概上解決的想法。 然而一些更細部的技術探討，主要是 TG 整個演算法的處理以及決策順序都必須要反覆的閱讀該論文並且理解其 Pseudo Code 才能領悟。 此外論文中也有一些數據的比較，這部分在投影片中也只有稍微的一些比對，並沒有如論文裡面的詳細。 我認為有時間的話，還是值得將該篇論文好好地仔細咀嚼去學習一下整體背後的脈絡 最後直接引用投影片中的比較表來一個總結 Before\tAfterCopy network with 99% availability\tHigh-available network with 99.99% availability Inter-DC WAN with moderate number of sites\t100x more traffic, 60x more tunnels Saturn: flat site topology &amp; per-site domain TE controller\tJumpgate: hierarchical topology &amp; granular TE control domain Site-level tunneling\tSite-level tunneling in conjunction with supernode-level TE (“Tunnel Split Group”) Tunnel splits implemented at ingress switches\tMulti-stage hashing across switches in Clos supernode Reference B4 and after: managing hierarchy, partitioning, and asymmetry for availability and scale in google's software-defined WANSlides ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/circleci","content":"","keywords":"","version":"Next"},{"title":"SSH Debug​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/circleci#ssh-debug","content":"過往使用一些工具時，常常遇到設定出錯然後需要通靈的情況，因為也許該指令在自己的測試環境都可以正常運行，但是跑到 SaaS 服務上則不通，可能因為系統版本差異，軟體版本差異甚至是系統有些相關的 Service 本身沒開啟。 這種情況下要除錯都非常麻煩，幸好 CircleCI 提供了 SSH Server 的概念，讓開發者可以直接進去 CI 運行的環境進行操作，不但可以進行偵錯，也可以嘗試在該環境中將 CI 想要運行的指令都先行測試一遍，這樣要撰寫後續的設定檔就會相對輕鬆 於 Workflow 右上方可以重新運行該 Workflow, 並且開啟 SSH 的功能 接者可以觀察到原先的 Jobs 中被加入了一個新的 Enable SSH，點開該 Job 即可得到連接資訊 我本身的 Github 帳戶有設定 SSH Key 的綁定，因此將 CircleCI 與 Github 綁定整合後，創建的 SSH 環境會只能接受 CircleCI 綁定的私鑰進行登入 實際透過 ssh指令 登入到該環境後即可看到相關專案的內容，此時就可以在這個環境下去進行偵錯與測試。 ","version":"Next","tagName":"h2"},{"title":"Executors​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/circleci#executors","content":"CircleCI 的運行環境中大部分都是基於 Container 來運行的，譬如下述的範例 jobs: build: docker: - image: circleci/golang:1.12 steps: - checkout - run: name: Build Code command: | sleep 10 echo &quot;Build Done&quot; deploy: docker: - image: circleci/golang:1.12  這個範例內我會針對不同的 Job 去設定該 Job 要運行的環境，範例中都使用了官方提供的 golang:1.12 容器。 就如同寫程式一樣，相同的部分如果可以抽象一層對於後續的維護跟修改會相對的輕鬆一點，這一點上 CircleCI 則提供了 Executor 的語法讓開發者管理其設定檔案 executors: golang-ci: working_directory: ~/repo docker: - image: circleci/golang:1.12 jobs: build: executor: golang-ci steps: - checkout - run: name: Build Code command: | sleep 10 echo &quot;Build Done&quot; deploy: executor: golang-ci  這個範例中我們透過 Executor 的方式創造了一個名為 golang-ci 的運行環境，該環境中描述其使用的容器是 circleci/golang:1.12. 之後只要這些不同的 Job 會使用的環境都一致的話，可以繼續專注維護 executors 的部分即可。 此外由於所有的運行環境都是基於容器，所以其實可以自行客製化自行的運行環境，將必定會使用的工具與環境事先設定完畢，譬如透過 pip3, apt-get, yarn, apk, dnf 等各式各樣的工具來預先安裝需要的套件與工具，最後將這個環境打包成新的容器映像檔案。 最後再將該環境套用到自行的 CircleCI 運行環境中，就可以減少一些不必要的安裝動作，藉此減少每次運行所需要的時間。 Commands 雖然 Yaml 本身已經有 Anchors 這種機制來避免重複撰寫設定的方式，然後 Anchors 裡面會用到大量的 &amp;,* 這類型的符號，其實看了不是很順眼，同時 Anchors 是針對完全相同的設定來處理，若本身有參數的差異時就沒有辦法透過這個機制來處理。 CircleCI 提供了 Commands 的概念，可讓開發者將 Jobs 的描述方式撰寫成類似 Function 的概念，可設定呼叫所需要的參數，以及這些參數的型態與預設值。 下列的 yaml 則是一個簡單的範例，創建了兩個 Commands, 分別叫做 setup_env 以及 deploy_env. 這兩個 Commands 都有設定相關的參數以及對應的型態與預設值 最後可以透過 CircleCI 規範的使用方式 &lt;&lt; parameters.$NAME &gt;&gt; 的方式來存取。 commands: setup_env: parameters: environment: type: string default: &quot;dev&quot; steps: - run: name: &quot;Setup the environment &lt;&lt; parameters.environment &gt;&gt;&quot; command: | echo &lt;&lt; parameters.environment &gt;&gt; deploy_env: parameters: environment: type: string default: &quot;dev&quot; seconds: type: string default: &quot;5&quot; steps: - run: name: &quot;Deploy to environment &lt;&lt; parameters.environment &gt;&gt;&quot; command: | sleep &lt;&lt; parameters.seconds &gt;&gt; echo &lt;&lt; parameters.environment &gt;&gt;  接下每個 Jobs 中就可以針對上述的 Commands 名稱直接使用，針對需要的部分可以傳遞參數覆蓋預設值，或是直接採用預設值即可  deploy: executor: golang-ci steps: - checkout - setup_env - deploy_env deploy-staging: docker: - image: circleci/golang:1.12 steps: - checkout - setup_env: environment: staging - deploy_env: environment: staging seconds: &quot;10&quot;  藉由 Commands 的方式能夠讓你好好的思緒有沒有辦法將你的 Pipepline 流程給設計的更模組化，將相同的部分抽出，透過不同變數的方式來減少重複撰寫的手續，同時也提高維護的方便性。 Orbs 當有了 Commands 的概念後，就可以將滿多常見的功能給模組化，模組化的下一個步驟就是分享。這個概念其實與其他工具的 Plugin 雷同，只是在 CircleCI 中使用 Orbs 這個詞來表示。 可以到 Orbs Registry 看看目前提供的 Orbs 這邊我們以 Slack Orbs 為範例，有自行撰寫過 Slack 相關通知的開發者就會知道在 Bash 中要透過 Curl 等方式去描述 Json 物件有多煩瑣。 我們可以看一下 slack orbs 裡面怎處理的  - run: command: | # Provide error if no webhook is set and error. Otherwise continue if [ -z &quot;&lt;&lt; parameters.webhook &gt;&gt;&quot; ]; then echo &quot;NO SLACK WEBHOOK SET&quot; echo &quot;Please input your SLACK_WEBHOOK value either in the settings for this project, or as a parameter for this orb.&quot; exit 1 else # Webhook properly set. echo Notifying Slack Channel #Create Members string if [ -n &quot;&lt;&lt; parameters.mentions &gt;&gt;&quot; ]; then IFS=&quot;,&quot; read -ra SLACK_MEMBERS \\&lt;&lt;&lt; &quot;&lt;&lt; parameters.mentions &gt;&gt;&quot; for i in &quot;${SLACK_MEMBERS[@]}&quot;; do if [ $(echo ${i} | head -c 1) == &quot;S&quot; ]; then SLACK_MENTIONS=&quot;${SLACK_MENTIONS}&lt;!subteam^${i}&gt; &quot; elif echo ${i} | grep -E &quot;^(here|channel|everyone)$&quot; &gt; /dev/null; then SLACK_MENTIONS=&quot;${SLACK_MENTIONS}&lt;!${i}&gt; &quot; else SLACK_MENTIONS=&quot;${SLACK_MENTIONS}&lt;@${i}&gt; &quot; fi done fi curl -X POST -H 'Content-type: application/json' \\ --data \\ &quot;{ \\ \\&quot;attachments\\&quot;: [ \\ { \\ \\&quot;fallback\\&quot;: \\&quot;&lt;&lt; parameters.message &gt;&gt; - $CIRCLE_BUILD_URL\\&quot;, \\ \\&quot;text\\&quot;: \\&quot;&lt;&lt; parameters.message &gt;&gt; $SLACK_MENTIONS\\&quot;, \\ \\&quot;author_name\\&quot;: \\&quot;&lt;&lt; parameters.author_name &gt;&gt;\\&quot;, \\ \\&quot;author_link\\&quot;: \\&quot;&lt;&lt; parameters.author_link &gt;&gt;\\&quot;, \\ \\&quot;title\\&quot;: \\&quot;&lt;&lt; parameters.title &gt;&gt;\\&quot;, \\ \\&quot;title_link\\&quot;: \\&quot;&lt;&lt; parameters.title_link &gt;&gt;\\&quot;, \\ \\&quot;footer\\&quot;: \\&quot;&lt;&lt; parameters.footer &gt;&gt;\\&quot;, \\ \\&quot;ts\\&quot;: \\&quot;&lt;&lt; parameters.ts &gt;&gt;\\&quot;, \\ \\&quot;fields\\&quot;: [ \\ &lt;&lt;# parameters.include_project_field &gt;&gt; { \\ \\&quot;title\\&quot;: \\&quot;Project\\&quot;, \\ \\&quot;value\\&quot;: \\&quot;$CIRCLE_PROJECT_REPONAME\\&quot;, \\ \\&quot;short\\&quot;: true \\ }, \\ &lt;&lt;/ parameters.include_project_field &gt;&gt; &lt;&lt;# parameters.include_job_number_field &gt;&gt; { \\ \\&quot;title\\&quot;: \\&quot;Job Number\\&quot;, \\ \\&quot;value\\&quot;: \\&quot;$CIRCLE_BUILD_NUM\\&quot;, \\ \\&quot;short\\&quot;: true \\ } \\ &lt;&lt;/ parameters.include_job_number_field &gt;&gt; ], \\ \\&quot;actions\\&quot;: [ \\ &lt;&lt;# parameters.include_visit_job_action &gt;&gt; { \\ \\&quot;type\\&quot;: \\&quot;button\\&quot;, \\ \\&quot;text\\&quot;: \\&quot;Visit Job\\&quot;, \\ \\&quot;url\\&quot;: \\&quot;$CIRCLE_BUILD_URL\\&quot; \\ } \\ &lt;&lt;/ parameters.include_visit_job_action &gt;&gt; ], \\ \\&quot;color\\&quot;: \\&quot;&lt;&lt; parameters.color &gt;&gt;\\&quot; \\ } \\ ] \\ }&quot; &lt;&lt; parameters.webhook &gt;&gt; fi name: Slack Notification shell: /bin/bash  可以看到滿滿的跳脫字元處理，整個使用與除錯想到就麻煩。 但是透過 orbs 的設計，我們可以重複使用上列的功能而不需要去管實作細節，整個使用範例如下 orbs: slack: circleci/slack@2.4.0 deploy: executor: golang-ci environment: SLACK_WEBHOOK: https://hooks.slack.com/services/xxxxxxxxxxxxxxxxxxxxxxxx steps: - checkout - setup_env - deploy_env - slack/notify: message: Deploy dev done  根據 Slack Orbs 的文件解說，我們最需要的就是 SLACK_WEBHOOK 的設定，剩下都可以依據預設值來處理，這邊我針對訊息的部分進行了覆蓋。 透過這些語法與生態系，我們能夠讓整個設定檔更加的精簡與組織。 CircleCI 這部分發展的比較晚，目前提供的 Orbs 只有數十多個，為數不多，且目前沒有提供 Private Orbs 的使用。 這意味如果你有多個 Project 想要共用這些模組，你只能公開這些模組不然就是在每個 Project 的設定內去撰寫相同的 Commands。 Cache CircleCI 本身有提供 Cache 的機制，可以讓你在不同的 Workflow 中傳遞檔案與資料夾。 透過這個機制我們可以將一些中間產物給保存下來，譬如 go build, npm build 等建置後產生的檔案給共享，藉此降低下次執行相同 Workflow 所需要的時間。 使用上分成兩個概念 save_to_cacherestore_cahce  - restore_cache: keys: - go-mod-v4-{{ checksum &quot;go.sum&quot; }} - run: make - save_cache: key: go-mod-v4-{{ checksum &quot;go.sum&quot; }} paths: - &quot;/go/pkg/mod&quot;  通常的使用邏輯都是產生這些中間產物後，透過 save_cache 的方式將特定路徑下的檔案給保存起來。同時我們需要針對這份 cache 給一個名稱。 使用上都會搭配 checksum 的方式來幫這份 cache 上一個跟其內容有關的名稱。 這樣下次運行的時候，透過 restore_cache 搭配特定的 key 就可以從眾多的 cache 中取出該資料並且覆蓋回去該系統中。 上述的範例可以這樣解讀，只要 go.sum 這個描述中間產物來源的檔案本身沒有跟動，那我們就可以放心的繼續採用相同的 cache. 但是一旦本次的更動有改變到 go.sum，不論是新的 library 或是版本的更新，都會導致 go.sum checksum 的改變，所以就會要到一個不存在的 cache. 最後就會迫使 workflow 去產生一份全新的 cache 供未來使用。 Summary 除了上述的功能外，還有一些滿多內建功能可以使用，譬如 store_artifacts, store_test_results 等，這部分就是有需要的時候再到官方文件查詢即可。 再次重申，沒有最完美的工具能夠適合各種情境，只有想清楚自己的需求與情境，在針對需求去選擇適合自己的工具。此外需求確定的情況下來尋找合適的工具也可以避免踩入一套工具最後卻很難抽身的情況。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/ingress-1","content":"","keywords":"kubernetes ingress","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#architecture","content":"Kubernetes 的Ingress 裡面我們可以設定一些如何轉發封包的選項，範例如下 apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: name: nginx-demo namespace: default spec: rules: - host: nginx.example.com http: paths: - path: /v1 backend: serviceName: nginx servicePort: http - path: /v2 backend: serviceName: nginx-v2 servicePort: http - host: note.example.com http: paths: - path: / backend: serviceName: jupyter servicePort: http  Ingress 內我們可以針對 Host 或是 Path 不同的選項來決定該封包要怎麼轉發。以上述範例來說，我們希望達到的是 如果看到的是 note.example.com 可以送給特定的 service如果看到的是 nginx.example.com ，則根據後面的 path (v1/v2) 來決定最後要怎麼轉發。 這邊要特別注意的是，對於 Kubernetes 來說， Ingress 物件本身只有描述的功用，實際上並不會真的把使用者所描述與敘述的功能給實現完畢，這部分需要依賴剩下的元件來補足。 接下來我們使用下列這張圖示來解釋一下一個完整 Ingress 的架構。  圖中標示為 Ingress Resource 的元件就是使用者們透過 Ingress Yaml 去描述預期行為的設定檔，也就是上圖的部分。 綠色的 Backend server 則是後端不同類型的服務器，使用者會預期 Ingress 可以根據 Host/Path 等不同的規則將對應的封包轉發到後端真正服務的 Backend Server. 接下來真正重要的就是 Ingress-Controller 以及 Ingress-Server.Ingress-Server 普遍上來說，就是一個能夠接受 HTTP/HTTPS 連線的網路伺服器，以本篇文章來說就是 Nginx. 過往的使用經驗上，我們的確可以透過 nginx.conf 的方式來設定 nginx server. 來達到根據不同的情況來決定不同的封包轉發等行為。 但是在 kubernetes ingress 的架構下，使用者並不一定熟悉 nginx.conf 的格式與撰寫，而熟悉的只有 Kubernetes Ingress 的格式。 在此條件下，我們需要一個轉換者，該轉換者能夠將 Ingress Resource 的設定轉換成 Ingress-Server (Nginx) 所能夠處理的格式。 這個角色也就是所謂的 Ingress-Controller. 將上述的設定與使用流程以順序來看 使用者透過 yaml 部署 ingress 設定到 kubernetes 裡面Ingress-Controller 偵測到 Ingress Resource 的更動，讀取該設定後產生對應的 Nginx.conf 供 Ingress-Server 使用外部使用者嘗試存取服務，該封包會先到達 Ingress-Server(Nginx).Ingrss-Server(Nginx) 根據 nginx.conf 的設定決定將該封包轉發到後段的服務器 backend server. 從上述的概念來說，我們可以簡單歸納一下 Ingress 的架構 Kubernetes 本身只提供一個統一的 Ingress 介面，本身不參與該介面的實作服務提供者本身必須要實現 Ingress-Server 以及 Ingress-Controller 這兩個元件將使用者描述的抽象概念轉換成實際上可以運作的設定 所以可以看到目前現實上有非常多的 Ingress 實作，不論是 Traefik , Kong, Nginx 甚至是各個公有雲(GKE/AKS/EKS)都有跟自行架構更加整合的實現。 關於 Ingress 更多的概念可以參考官方文件 ","version":"Next","tagName":"h2"},{"title":"Annotation​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#annotation","content":"有實際上使用過 Ingress 的玩家會發現在 Yaml 內有各式各樣的 annotation 要使用，譬如  annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /  首先，根據上述的架構概念，我們可以知道 Ingress-Controller 會去讀取 Ingress 的設定，然後來進行後續的動作。 假想一個情況，系統內同時有多個 Ingress-Controller 的實現，使用者要如何指派該 Ingress 要使用哪一個 Ingress-Controller 來使用? 再者個情況下，我們可以透過 annotations 的方式來加註一些額外的資訊，當然這些資訊不是標準的，反而是各個 Ingress-Controller 自行決定看到什麼樣的資訊來進行什麼處理。 此外，不同的 Ingress-Server 提供的功能與使用方式都不同，基本上 Ingress Resource 很難有一個完美的介面來滿足所有的實現，因此大部分的情況下，不同的 Ingress-Controller/Ingress-Server 都會要求使用者在 Annotation 的部分使用特定的字眼來描述額外的功能，譬如  nginx.ingress.kubernetes.io/rewrite-target: /  所以在選擇 Ingess 的使用上，遇到任何問題的時候，如果是 Ingress Resource 的介面問題，則可以尋求 Kubernetes 官方文件的幫助，如果是更細緻的需求，則該查詢 Ingress-Controller 的說明文件，看看自己的需求與相對應的設定是否有辦法完成。 ","version":"Next","tagName":"h3"},{"title":"Compare​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#compare","content":"跟 Kubernetes Service 比較起來，兩者都在提供便捷的網路存取服務Server 針對的單位主要是 Pod(Container), 提供一個更方便的方式讓用戶端可以不用在意後端 Pod(Container) 的真實 IP 地址。 而 Ingress 目前的使用上更偏向是 HTTP/HTTPS 的應用，在上述的 Service 上搭建一層更方便的服務，可以根據 Host(NameBasd Virtaul Hosting) 或是 Path(Fanout) 來決定後續真正轉發的對象，而該對象則是不同後端服務所搭建起來的 Service. 因此在使用上，這兩者並沒有誰取代誰的問題，反而是根據需求來使用，大部分情況下都是互相整合來提供更方便與好用的功能。 Example 上述已經介紹完關於 Ingress 的基本概念，接下來要使用 Nginx 作為 Ingress-Server 來實際搭建一個 Ingress 的範例。 接下來的所有範例文件都可以在 KubeDemo 內找到對應的檔案。 該範例的架構圖如下 我們會在 Kubernetes 裡面進行下列部署 搭建一個基於 Nginx 的 Ingress-Controller/Ingress-Server，透過 Deployment 部署三套不同的後端服務，分別是 Jupyter Notebook 以及兩個有者不同 Index.html 的 Nginx Server.透過三個不同的 kubernetes service 將上述的 Deployment 包裝起來提供更方便的存取功能部署對應的 Ingress, 希望可以完成 存取 note.example.com 會存取到 jupyter notebook存取 nginx.example.com/v1 會存取到 nginx存取 nginx.example.com/v2 會存取到 nginx-v2 接下來我們就要依據上述的概念進行相關檔案的部署。 我的測試環境是基於 MAC 上透過 Vagrant 創建一個 UbuntuOS 並且使用 Kubeadm 實現的一個小型 Kubernetes 叢集。 ","version":"Next","tagName":"h2"},{"title":"Nginx Controller​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#nginx-controller","content":"首先我們要先部署 Nginx Controller 到 kubernetes 叢集內，詳細的安裝方式可以參閱其官網 首先我們要先安裝相關的資源，譬如 RBAC 以及相關的 Deployment. kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml  接下來為了讓外界的服務可以存取到該 Nginx Server (Ingress-Server)，這邊會根據你的機器環境而有所不同。 以我 Baremetal 的環境，我需要部署下列的資源，透過 Kubernetes Service NodePort 的方式讓我的 Nginx Server 可以被外界存取 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml  上述的部屬都會安裝到 ingress-nginx 的 kubernetes namespace 上，所以透過下列的指令觀察安裝的情形 $ kubectl -n ingress-nginx get all NAME READY STATUS RESTARTS AGE pod/nginx-ingress-controller-d88dbf49c-9b6td 1/1 Running 0 23h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx NodePort 10.111.134.97 &lt;none&gt; 80:32663/TCP,443:31309/TCP 1d NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ingress-controller 1 1 1 1 1d NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ingress-controller-d88dbf49c 1 1 1 1d  ","version":"Next","tagName":"h2"},{"title":"Outside Internet Access​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#outside-internet-access","content":"為了從外部MAC的瀏覽器進行測試，我對機器上的 /etc/hosts 進行了相關的修改, 加上了下列的資料，讓我可以透過相關的設定來存取該 NodePort 的 Nginx(Ingress-Server). 172.17.8.101 nginx.example.com note.example.com  172.17.8.101 是我VM(Ubuntu)的Virtaul IP address. ","version":"Next","tagName":"h2"},{"title":"Backend Servers​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#backend-servers","content":"","version":"Next","tagName":"h2"},{"title":"Jupyter​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#jupyter","content":"基本上需要的就是 Deployment 配上一個對應的 Service 即可 詳細的請參閱 KubeDemo kubectl apply -f https://raw.githubusercontent.com/hwchiu/kubeDemo/master/ingress/jupyter.yml  ","version":"Next","tagName":"h3"},{"title":"Nginx​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#nginx","content":"類似上述 Jupyter 的安裝流程，但是為了客製化 index.html 的內容，會額外部署一個 configMap 來產生不同的內容 kubectl apply -f https://raw.githubusercontent.com/hwchiu/kubeDemo/master/ingress/nginx.yaml kubectl apply -f https://raw.githubusercontent.com/hwchiu/kubeDemo/master/ingress/nginx2.yaml  這些應用程式都會部署到 default 這個 namespace，所以可以用下列指令確保部署正確 kubectl -n default get all NAME READY STATUS RESTARTS AGE pod/jupyter 1/1 Running 0 15h pod/nginx-7dd9f89db4-tvfkk 1/1 Running 0 15h pod/nginx-v2-5c45597f57-746p8 1/1 Running 0 15h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/jupyter LoadBalancer 10.106.190.88 &lt;pending&gt; 80:32444/TCP 15h service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 89d service/nginx ClusterIP 10.110.237.87 &lt;none&gt; 80/TCP 15h service/nginx-v2 ClusterIP 10.103.43.44 &lt;none&gt; 80/TCP 15h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 1 1 1 1 15h deployment.apps/nginx-v2 1 1 1 1 15h NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-7dd9f89db4 1 1 1 15h replicaset.apps/nginx-v2-5c45597f57 1 1 1 15h  上述服務完畢後，先用 curl 針對三個 service 的 ClusterIP 去確認服務有正常起來 $ curl 10.110.237.87 Nginx V1 $ curl 10.103.43.44 Nginx V2 $ curl 10.106.190.88/tree ....  ","version":"Next","tagName":"h3"},{"title":"Deploy Ingress​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#deploy-ingress","content":"上述相關的服務都部署完畢後，接下來就要部署 Ingress 物件進去，我們期望的行為是 Nginx Controller 能夠讀取這個 Ingress 的物件並且產生對應的 nginx.conf 供 Nginx Server(Ingress-Server) 使用. 所以先來看一下對應的 Ingress Resource  apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / name: nginx-demo namespace: default spec: rules: - host: nginx.example.com http: paths: - path: /v1 backend: serviceName: nginx servicePort: http - path: /v2 backend: serviceName: nginx-v2 servicePort: http - host: note.example.com http: paths: - path: / backend: serviceName: jupyter servicePort: http  該物件部署完畢後，透過下列指令觀察部署結果 $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx-demo nginx.example.com,note.example.com 80 15h  ","version":"Next","tagName":"h3"},{"title":"Check Nginx​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#check-nginx","content":"接下來我們將直接進入到 Nginx Controller 去觀察一下是否有對應的 nginx.conf 被產生 $ kubectl -n ingress-nginx exec -it $(kubectl -n ingress-nginx get pods -l app.kubernetes.io/name=ingress-nginx -o jsonpath=&quot;{.items[0].metadata.name}&quot;) bash www-data@nginx-ingress-controller-d88dbf49c-9b6td:/etc/nginx$ www-data@nginx-ingress-controller-d88dbf49c-9b6td:/etc/nginx$ grep &quot;example.com&quot; nginx.conf ## start server nginx.example.com server_name nginx.example.com ; ## end server nginx.example.com ## start server note.example.com server_name note.example.com ; ## end server note.example.com www-data@nginx-ingress-controller-d88dbf49c-9b6td:/etc/nginx$ grep &quot;v1&quot; nginx.conf ssl_protocols TLSv1.2; location ~* &quot;^/v1\\/?(?&lt;baseuri&gt;.*)&quot; { set $location_path &quot;/v1&quot;; rewrite &quot;(?i)/v1/(.*)&quot; /$1 break; rewrite &quot;(?i)/v1$&quot; / break; www-data@nginx-ingress-controller-d88dbf49c-9b6td:/etc/nginx$ grep &quot;v2&quot; nginx.conf location ~* &quot;^/v2\\/?(?&lt;baseuri&gt;.*)&quot; { set $service_name &quot;nginx-v2&quot;; set $location_path &quot;/v2&quot;; set $proxy_upstream_name &quot;default-nginx-v2-http&quot;; rewrite &quot;(?i)/v2/(.*)&quot; /$1 break; rewrite &quot;(?i)/v2$&quot; / break;  到這邊為止，基本上的一切都順利設定完畢了，接下來就可以開啟瀏覽器嘗試去瀏覽看看. ","version":"Next","tagName":"h3"},{"title":"Access the Nginx Server​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/ingress-1#access-the-nginx-server","content":"因為我們的 Nginx Server 是基於 NorePort 的方式來供對外存取，所以我們要先確認一下開啟的NodePort資訊是什麼 $ kubectl -n ingress-nginx get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.111.134.97 &lt;none&gt; 80:32663/TCP,443:31309/TCP 1d  在我的範例環境中，預設 HTTP 所使用的的連接埠是 32663, 因此等下需要使用這個資訊來進行測試連線 首先，我們先針對 nginx.example.com 進行測試，結果如下圖 結果如預期般，可以透過不同的 PATH 來導向不同的服務後端 接下來測試 node.example.com, 看看是否能夠針對 host 來導向不同的後端  的確也能夠正常運作，意味者我們的 Ingress 測試滿順利的，都能夠如預期般的運作 Summary 本篇文章旨在透過簡單的介紹讓大家知道 Kubernetes Ingress 的基本架構與介紹，最後透過一個常用的 Nginx Ingress 實作來實際使用看看Ingress 的架構。 不同的 Ingress 的後端實現功能的方法都不盡相同，同時能夠支援的功能也都會有些許的差異，這部分就仰賴各位在選擇對應的解決方案時的研究與先行測試。 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iptables-masquerade","content":"","keywords":"iptables nat","version":"Next"},{"title":"UserSpace​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/iptables-masquerade#userspace","content":"原始碼的部分請參閱此檔案 iptables v.1.6.1 User Space 的部分是一個依賴於 iptables 框架的模組，主要功能就是擴充 iptables 這個指令，能夠支援不同的參數與用法並且將該規則輸入的參數順利地記住下來，之後會傳入到 kernel space 中交給真正負責處理封包的邏輯區塊去處理。 接下來簡單的看一下這個檔案的內容，快速的看一下即可 static struct xtables_target masquerade_tg_reg = { .name = &quot;MASQUERADE&quot;, .version = XTABLES_VERSION, .family = NFPROTO_IPV4, .size = XT_ALIGN(sizeof(struct nf_nat_ipv4_multi_range_compat)), .userspacesize = XT_ALIGN(sizeof(struct nf_nat_ipv4_multi_range_compat)), .help = MASQUERADE_help, .init = MASQUERADE_init, .x6_parse = MASQUERADE_parse, .print = MASQUERADE_print, .save = MASQUERADE_save, .x6_options = MASQUERADE_opts, .xlate = MASQUERADE_xlate, }; void _init(void) { xtables_register_target(&amp;masquerade_tg_reg); }  對於每個 iptable module(xtables) 來說都必須要實現一個相關的結構，並且針對每個 function pointer 設定相關的處理函式。 並且透過一個框架所提供的函式去註冊這個結構，只要這邊成功處理，就可以透過 iptables 的指令來操作這個擴充功能 MASQUERADE。 這裡面有滿多的函式，就針對比較重要的來看，也就是針對參數處理的部分 x6_parse(MASQUERADE_parse) 來看。 static const struct xt_option_entry MASQUERADE_opts[] = { {.name = &quot;to-ports&quot;, .id = O_TO_PORTS, .type = XTTYPE_STRING}, {.name = &quot;random&quot;, .id = O_RANDOM, .type = XTTYPE_NONE}, XTOPT_TABLEEND, };  對於每個 iptables 的擴充模組來說，都可以透過定義 xt_option_entry 類別來定義要使用的參數以及型態，所以其實 MASQUERADE 本身還有兩個參數可以使用，分別是 to-ports 以及 random，至於真正的含義以及用途就要看 help 或是直接看底層的實現來了解。 static void MASQUERADE_parse(struct xt_option_call *cb) { const struct ipt_entry *entry = cb-&gt;xt_entry; int portok; struct nf_nat_ipv4_multi_range_compat *mr = cb-&gt;data; if (entry-&gt;ip.proto == IPPROTO_TCP || entry-&gt;ip.proto == IPPROTO_UDP || entry-&gt;ip.proto == IPPROTO_SCTP || entry-&gt;ip.proto == IPPROTO_DCCP || entry-&gt;ip.proto == IPPROTO_ICMP) portok = 1; else portok = 0; xtables_option_parse(cb); switch (cb-&gt;entry-&gt;id) { case O_TO_PORTS: if (!portok) xtables_error(PARAMETER_PROBLEM, &quot;Need TCP, UDP, SCTP or DCCP with port specification&quot;); parse_ports(cb-&gt;arg, mr); break; case O_RANDOM: mr-&gt;range[0].flags |= NF_NAT_RANGE_PROTO_RANDOM; break; } }  上面的內容細節不太需要知道，只要知道其運作流程就是解析規則的參數並且設定相關的資料結構 nf_nat_ipv4_multi_range_compat，其中最重要的就是 nf_nat_ipv4_multi_range_compat 這個結構會需要定義兩份，一份給 iptables 的指令用，另外一份給 kernel-space 使用。 下面是該結構的定義，基本上對於 MASQUERADE 來說其兩個參數最主要設定的內容就是如何選擇 source port，當封包進行 source NAT 轉換後連接埠該怎麼選擇。 #ifndef _NETFILTER_NF_NAT_H #define _NETFILTER_NF_NAT_H #include &lt;linux/netfilter.h&gt; #include &lt;linux/netfilter/nf_conntrack_tuple_common.h&gt; #define NF_NAT_RANGE_MAP_IPS (1 &lt;&lt; 0) #define NF_NAT_RANGE_PROTO_SPECIFIED (1 &lt;&lt; 1) #define NF_NAT_RANGE_PROTO_RANDOM (1 &lt;&lt; 2) #define NF_NAT_RANGE_PERSISTENT (1 &lt;&lt; 3) #define NF_NAT_RANGE_PROTO_RANDOM_FULLY (1 &lt;&lt; 4) #define NF_NAT_RANGE_PROTO_RANDOM_ALL \\ (NF_NAT_RANGE_PROTO_RANDOM | NF_NAT_RANGE_PROTO_RANDOM_FULLY) struct nf_nat_ipv4_range { unsigned int flags; __be32 min_ip; __be32 max_ip; union nf_conntrack_man_proto min; union nf_conntrack_man_proto max; }; struct nf_nat_ipv4_multi_range_compat { unsigned int rangesize; struct nf_nat_ipv4_range range[1]; }; struct nf_nat_range { unsigned int flags; union nf_inet_addr min_addr; union nf_inet_addr max_addr; union nf_conntrack_man_proto min_proto; union nf_conntrack_man_proto max_proto; }; #endif /* _NETFILTER_NF_NAT_H */  ","version":"Next","tagName":"h2"},{"title":"Kernel Space​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/iptables-masquerade#kernel-space","content":"原始碼的部分請參閱此連結 Linux Kernel 4.15.18 kernel space 則是整個封包邏輯運作的核心，包含了 怎麼根據使用輸入的參數來挑選 source port如何動態選擇網路卡介面上的 IP 地址，如果有多個會怎麼選擇 我們的進入點是這個檔案 ipt_MASQUERADE.c ","version":"Next","tagName":"h2"},{"title":"多重 IP 選擇​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/iptables-masquerade#多重-ip-選擇","content":"... static unsigned int masquerade_tg(struct sk_buff *skb, const struct xt_action_param *par) { struct nf_nat_range range; const struct nf_nat_ipv4_multi_range_compat *mr; mr = par-&gt;targinfo; range.flags = mr-&gt;range[0].flags; range.min_proto = mr-&gt;range[0].min; range.max_proto = mr-&gt;range[0].max; return nf_nat_masquerade_ipv4(skb, xt_hooknum(par), &amp;range, xt_out(par)); } ...  概念也是非常簡單，從 xt_action_param 這個變數取出使用者對應的參數，並且往後呼叫其他參數去處理。 這邊可以看到其背後也是將 xt_action_param 裡面的void* 參數轉向 nf_nat_ipv4_multi_range_compat 這個物件來處理。 因為這些程式碼都是屬於 kernel 內的一部份，因此 kernel 也有一份跟上述非常相似的結構定義檔案，如下。 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */ #ifndef _NETFILTER_NF_NAT_H #define _NETFILTER_NF_NAT_H #include &lt;linux/netfilter.h&gt; #include &lt;linux/netfilter/nf_conntrack_tuple_common.h&gt; #define NF_NAT_RANGE_MAP_IPS (1 &lt;&lt; 0) #define NF_NAT_RANGE_PROTO_SPECIFIED (1 &lt;&lt; 1) #define NF_NAT_RANGE_PROTO_RANDOM (1 &lt;&lt; 2) #define NF_NAT_RANGE_PERSISTENT (1 &lt;&lt; 3) #define NF_NAT_RANGE_PROTO_RANDOM_FULLY (1 &lt;&lt; 4) #define NF_NAT_RANGE_PROTO_RANDOM_ALL \\ (NF_NAT_RANGE_PROTO_RANDOM | NF_NAT_RANGE_PROTO_RANDOM_FULLY) #define NF_NAT_RANGE_MASK \\ (NF_NAT_RANGE_MAP_IPS | NF_NAT_RANGE_PROTO_SPECIFIED | \\ NF_NAT_RANGE_PROTO_RANDOM | NF_NAT_RANGE_PERSISTENT | \\ NF_NAT_RANGE_PROTO_RANDOM_FULLY) struct nf_nat_ipv4_range { unsigned int flags; __be32 min_ip; __be32 max_ip; union nf_conntrack_man_proto min; union nf_conntrack_man_proto max; }; struct nf_nat_ipv4_multi_range_compat { unsigned int rangesize; struct nf_nat_ipv4_range range[1]; }; struct nf_nat_range { unsigned int flags; union nf_inet_addr min_addr; union nf_inet_addr max_addr; union nf_conntrack_man_proto min_proto; union nf_conntrack_man_proto max_proto; }; #endif /* _NETFILTER_NF_NAT_H */  接下來看看真正的邏輯處理函式 nf_nat_masquerade_ipv4 unsigned int nf_nat_masquerade_ipv4(struct sk_buff *skb, unsigned int hooknum, const struct nf_nat_range *range, const struct net_device *out) { ... rt = skb_rtable(skb); nh = rt_nexthop(rt, ip_hdr(skb)-&gt;daddr); newsrc = inet_select_addr(out, nh, RT_SCOPE_UNIVERSE); if (!newsrc) { pr_info(&quot;%s ate my IP address\\n&quot;, out-&gt;name); return NF_DROP; } nat = nf_ct_nat_ext_add(ct); if (nat) nat-&gt;masq_index = out-&gt;ifindex; /* Transfer from original range. */ memset(&amp;newrange.min_addr, 0, sizeof(newrange.min_addr)); memset(&amp;newrange.max_addr, 0, sizeof(newrange.max_addr)); newrange.flags = range-&gt;flags | NF_NAT_RANGE_MAP_IPS; newrange.min_addr.ip = newsrc; newrange.max_addr.ip = newsrc; newrange.min_proto = range-&gt;min_proto; newrange.max_proto = range-&gt;max_proto; /* Hand modified range to generic setup. */ return nf_nat_setup_info(ct, &amp;newrange, NF_NAT_MANIP_SRC); }  前面三個函式就是我們要關注的第一個重點，選出一個可用的地址作為封包之後的來源 IP 地址  rt = skb_rtable(skb); nh = rt_nexthop(rt, ip_hdr(skb)-&gt;daddr); newsrc = inet_select_addr(out, nh, RT_SCOPE_UNIVERSE);  從 skb 取出該封包對應的 routing entry從該 routing entry 中根據封包的目的位置選出下一個節點next_hop接下來根據兩個參數 下一個節點的IP地址, 已經決定好的輸出網路介面 來決定最後使用的 IP __be32 inet_select_addr(const struct net_device *dev, __be32 dst, int scope) { __be32 addr = 0; struct in_device *in_dev; struct net *net = dev_net(dev); int master_idx; rcu_read_lock(); in_dev = __in_dev_get_rcu(dev); if (!in_dev) goto no_in_dev; for_primary_ifa(in_dev) { if (ifa-&gt;ifa_scope &gt; scope) continue; if (!dst || inet_ifa_match(dst, ifa)) { addr = ifa-&gt;ifa_local; break; } if (!addr) addr = ifa-&gt;ifa_local; } endfor_ifa(in_dev); if (addr) goto out_unlock; no_in_dev: master_idx = l3mdev_master_ifindex_rcu(dev); ....  仔細觀察上述的流程之前，我們先來看一個擁有多個 IP 地址的範例 vagrant@linux-study:~/linux$ ip addr show dev docker0 4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:5a:4d:94:32 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global docker0 valid_lft forever preferred_lft forever inet 172.19.1.3/16 scope global docker0 valid_lft forever preferred_lft forever inet 172.18.0.2/16 scope global secondary docker0 valid_lft forever preferred_lft forever inet 172.18.0.3/16 scope global secondary docker0 valid_lft forever preferred_lft forever inet 172.18.1.3/16 scope global secondary docker0 valid_lft forever preferred_lft forever  這個範例中，我們針對 docker0 這個網卡設定了五組 IP 地址，其中根據網段分成兩大類 172.18.0.0/16 172.18.0.1/16172.18.0.2/16172.18.0.3/16172.18.1.3/16 172.19.0.0/16 172.19.1.3 第一個網段中總共有四個 IP 地址，上面的順序就是加入到系統的順序。可以觀察到這四個 IP 地址實際上系統中顯示的內容有些許不同  inet 172.18.0.1/16 brd 172.18.255.255 scope global docker0 valid_lft forever preferred_lft forever inet 172.18.0.2/16 scope global secondary docker0 valid_lft forever preferred_lft forever inet 172.18.0.3/16 scope global secondary docker0 valid_lft forever preferred_lft forever inet 172.18.1.3/16 scope global secondary docker0 valid_lft forever preferred_lft forever  可以觀察到除了第一個 IP 地址外，都被加上了 secondary 的描述詞，這邊可以猜想對於每個同網段的 IP 地址來說，由於都同網段，所以只會有一個主要的 IP 地址使用，而其他的都是作為 secondary。 第二個網段由於只有一個 IP 地址，本身也自然就沒有 secondary 的描述。 此外每個 IP 地址後面都有一個有趣的欄位 scope，這個欄位目前有四個值，分別如下，等等會再解釋這個值的用途 global - the address is globally valid. site - (IPv6 only, deprecated) the address is site local, i.e. it is valid inside this site. link - the address is link local, i.e. it is valid only on this device. host - the address is valid only inside this host. 接下來我們再次認真的回歸到程式碼的層級去看一下邏輯 ... #define for_primary_ifa(in_dev) { struct in_ifaddr *ifa; \\ for (ifa = (in_dev)-&gt;ifa_list; ifa &amp;&amp; !(ifa-&gt;ifa_flags&amp;IFA_F_SECONDARY); ifa = ifa-&gt;ifa_next) #define endfor_ifa(in_dev) } .... for_primary_ifa(in_dev) { if (ifa-&gt;ifa_scope &gt; scope) continue; if (!dst || inet_ifa_match(dst, ifa)) { addr = ifa-&gt;ifa_local; break; } if (!addr) addr = ifa-&gt;ifa_local; } endfor_ifa(in_dev); ...  把上述的 MACRO 給展開後會得到下列的迴圈內容(排版過) { struct in_ifaddr *ifa; for (ifa = (in_dev)-&gt;ifa_list; ifa &amp;&amp; !(ifa-&gt;ifa_flags&amp;IFA_F_SECONDARY); ifa = ifa-&gt;ifa_next) { if (ifa-&gt;ifa_scope &gt; scope) continue; if (!dst || inet_ifa_match(dst, ifa)) { addr = ifa-&gt;ifa_local; break; } if (!addr) addr = ifa-&gt;ifa_local; } }  首先可以看到，該邏輯會透過迴圈的方式去取出 (in_dev)-&gt;ifa_list 內的所有清單，其實就是所有的 IP 地址，而終止條件條件很間單，遇到最後一個或是遇到第一個被標為 secondary 的 IP 地址就會停下來 接下來會開始比較每張網卡的 scope 是否跟參數的輸入值有關，這個範例中使用到的是 global(RT_SCOPE_UNIVERSE=0)，而上述的範例中全部都是 global，所以基本上可以忽略這個選項 接下來最重要的比較邏輯是 如果知道目標位置，則會確認該目標位置與該目標IP的所屬網段是否符合，是的話就可以用這個 IP 地址當作來源 IP 送出去。 static __inline__ bool inet_ifa_match(__be32 addr, struct in_ifaddr *ifa) { return !((addr^ifa-&gt;ifa_address)&amp;ifa-&gt;ifa_mask); }  這個比較的概念非常簡單，首先全部的 IP 地址都適用 32位元的方式表達，為了判斷一個 IP 地址是不是屬於一個 IP 網段內的合法 IP 地址，其思考邏輯如下 假設地址 A = a.b.c.d, 網段則是 x.y.z.v/n 只要判斷 n bit 以前是否一樣即可，舉例來說 A = 172.17.8.23 , B = 172.17.12.53/16 最簡單的方式就是將 IP 的部分都用 32 位元表示，接者比較前 n 個位元是否完全一致即可 A = 0xAC110817 =&gt; 10101100 00010001 00001000 00010111B = 0xAC110C35 =&gt; 10101100 00010001 00001100 00110101N = 16 =&gt; 11111111 11111111 00000000 00000000 所以先針對 A/B 兩個 IP 地址使用 XOR 的方式來找出差異點 A^B =&gt; 00000000 00000000 00000100 00100010 接者針對前 N(16) 個位元比較 (A^B)&amp;N =&gt; 00000000 00000000 00000000 00000000 所以只要出來的結果是0 就代表前 (N) 個位元都一致，也可以表示 A IP 屬於 B 網段。 所以上面的 IP挑選邏輯可以簡單整理成 針對所有非 SECONDARY 的 IP 地址依序檢查確認 scope 是否符合 3. 確認該 IP 的網段是否包含 目標IP地址, 此範例中的 目標IP地址 則是根據路由表的 next hop 地址。 ","version":"Next","tagName":"h3"},{"title":"來源連接埠的選擇​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/iptables-masquerade#來源連接埠的選擇","content":"接下來要看看到底怎麼選擇 來源連接埠 (Source Port)，這部分的選擇牽扯到太多的相關背景，譬如 connection track，甚至是 DNAT 的資料共用，所以在探索上會著重於 SNAT 相關的邏輯。 我們回到最初的函式 nf_nat_masquerade_ipv4 來看一下後面的處理  /* Transfer from original range. */ memset(&amp;newrange.min_addr, 0, sizeof(newrange.min_addr)); memset(&amp;newrange.max_addr, 0, sizeof(newrange.max_addr)); newrange.flags = range-&gt;flags | NF_NAT_RANGE_MAP_IPS; newrange.min_addr.ip = newsrc; newrange.max_addr.ip = newsrc; newrange.min_proto = range-&gt;min_proto; newrange.max_proto = range-&gt;max_proto; /* Hand modified range to generic setup. */ return nf_nat_setup_info(ct, &amp;newrange, NF_NAT_MANIP_SRC);  這邊有三個欄位值得注意，分別是 newrange.flagsnewrange.min_protonewrange.max_proto 如果今天使用者在操作 MASQUERADE 的時候有設定其他參數 --random, --to-ports 的話，上述的欄位會有不同的數值，接下來的邏輯處理則會根據這些數值來處理。 --random flags = NF_NAT_RANGE_PROTO_RANDOM --to-ports min_proto:max_proto =&gt; 使用者設定的 Port Number 區間flags = NF_NAT_RANGE_PROTO_SPECIFIED 接下來我們就來看看 nf_nat_setup_info 這個函式裡面會怎麼做 unsigned int nf_nat_setup_info(struct nf_conn *ct, const struct nf_nat_range *range, enum nf_nat_manip_type maniptype) { struct net *net = nf_ct_net(ct); struct nf_conntrack_tuple curr_tuple, new_tuple; ... get_unique_tuple(&amp;new_tuple, &amp;curr_tuple, range, ct, maniptype); .... }  對於 linux kernel 來說，進行一個有效的 NAT 除了尋找到一個合法且唯一的連接埠/IP 地址 之外，如何讓這個連線能夠更有效的處理也是一個議題，總不可能每次該連線中的封包都要重新檢查，尋找並且轉換，這部分就會仰賴 Conntrack (Connection Tracking) 的整合與運作，能夠提供更快速的運作同時也可以避免 NAT 相關的規則每個封包都要執行一次。 因此上述的原始碼中會有非常大量的部分都跟 Conntrack 有關，這邊就不詳細談這概念，主要專注於 NAT 連接埠的選擇。 這邊我們專注於 get_unique_tuple 這個函式，這邊有五個參數，其中有兩個需要注意，分別是 range 以及 mainiptype。 其中 range 則是 nf_nat_masquerade_ipv4 根據使用者設定產生並傳入的，而 mainiptype 則是 nf_nat_masquerade_ipv4 傳入的 NF_NAT_MANIP_SRC. /* Manipulate the tuple into the range given. For NF_INET_POST_ROUTING, * we change the source to map into the range. For NF_INET_PRE_ROUTING * and NF_INET_LOCAL_OUT, we change the destination to map into the * range. It might not be possible to get a unique tuple, but we try. * At worst (or if we race), we will end up with a final duplicate in * __ip_conntrack_confirm and drop the packet. */ static void get_unique_tuple(struct nf_conntrack_tuple *tuple, const struct nf_conntrack_tuple *orig_tuple, const struct nf_nat_range *range, struct nf_conn *ct, enum nf_nat_manip_type maniptype) { ... rcu_read_lock(); l3proto = __nf_nat_l3proto_find(orig_tuple-&gt;src.l3num); l4proto = __nf_nat_l4proto_find(orig_tuple-&gt;src.l3num, orig_tuple-&gt;dst.protonum); ... /* Last change: get protocol to try to obtain unique tuple. */ l4proto-&gt;unique_tuple(l3proto, tuple, range, maniptype, ct); }  從註解可以看到該函式的目的，因為 MASQUERADE 本身必須是 POST-ROUTING 位置才可以執行的 TARGET，所以也可以看到註解中有特別提到 NF_INTER_POST_ROUTING 的情況下，會做的就是改變 source 來源端相關的數值。 NAT 本身還有一個很有趣的概念就是針對不同的 Layer4 協定會有不同的選擇方式跟處理方式，目前總共有九種不同的實作，分別是 greicmpicmpv6dccpsctptcpudpliteudpunknown 而我們的範例中專注於 TCP 本身的處理，所以接下來我們來看看 TCP 裡面的 unique_tuple 會怎麼處理 const struct nf_nat_l4proto nf_nat_l4proto_tcp = { .l4proto = IPPROTO_TCP, .manip_pkt = tcp_manip_pkt, .in_range = nf_nat_l4proto_in_range, .unique_tuple = tcp_unique_tuple, #if IS_ENABLED(CONFIG_NF_CT_NETLINK) .nlattr_to_range = nf_nat_l4proto_nlattr_to_range, #endif };  上述只是一個 TCP 結構的表達，實際上會有九種相關的物件都使用 nf_nat_l4proto 來註冊。 static void tcp_unique_tuple(const struct nf_nat_l3proto *l3proto, struct nf_conntrack_tuple *tuple, const struct nf_nat_range *range, enum nf_nat_manip_type maniptype, const struct nf_conn *ct) { nf_nat_l4proto_unique_tuple(l3proto, tuple, range, maniptype, ct, &amp;tcp_port_rover); } void nf_nat_l4proto_unique_tuple(const struct nf_nat_l3proto *l3proto, struct nf_conntrack_tuple *tuple, const struct nf_nat_range *range, enum nf_nat_manip_type maniptype, const struct nf_conn *ct, u16 *rover) { unsigned int range_size, min, i; __be16 *portptr; u_int16_t off; if (maniptype == NF_NAT_MANIP_SRC) portptr = &amp;tuple-&gt;src.u.all; else portptr = &amp;tuple-&gt;dst.u.all; /* If no range specified... */ if (!(range-&gt;flags &amp; NF_NAT_RANGE_PROTO_SPECIFIED)) { /* If it's dst rewrite, can't change port */ if (maniptype == NF_NAT_MANIP_DST) return; if (ntohs(*portptr) &lt; 1024) { /* Loose convention: &gt;&gt; 512 is credential passing */ if (ntohs(*portptr) &lt; 512) { min = 1; range_size = 511 - min + 1; } else { min = 600; range_size = 1023 - min + 1; } } else { min = 1024; range_size = 65535 - 1024 + 1; } } else { min = ntohs(range-&gt;min_proto.all); range_size = ntohs(range-&gt;max_proto.all) - min + 1; } if (range-&gt;flags &amp; NF_NAT_RANGE_PROTO_RANDOM) { off = l3proto-&gt;secure_port(tuple, maniptype == NF_NAT_MANIP_SRC ? tuple-&gt;dst.u.all : tuple-&gt;src.u.all); } else if (range-&gt;flags &amp; NF_NAT_RANGE_PROTO_RANDOM_FULLY) { off = prandom_u32(); } else { off = *rover; } for (i = 0; ; ++off) { *portptr = htons(min + off % range_size); if (++i != range_size &amp;&amp; nf_nat_used_tuple(tuple, ct)) continue; if (!(range-&gt;flags &amp; NF_NAT_RANGE_PROTO_RANDOM_ALL)) *rover = off; return; } }  找 Port 連接埠的概念非常簡單 先擬定一個範圍區間接下來於這個區間內找到一個起始位置，於這個位置嘗試，如果該 port(連接埠) 與 source IP 產生的 tuple 是唯一的，就可以使用，否則就遞加該 port(連接埠) (++).  for (i = 0; ; ++off) { *portptr = htons(min + off % range_size); if (++i != range_size &amp;&amp; nf_nat_used_tuple(tuple, ct)) continue; if (!(range-&gt;flags &amp; NF_NAT_RANGE_PROTO_RANDOM_ALL)) *rover = off; return; }  上述的概念用程式碼表示就是，透過 off 絕對一個 起始位置 的偏移量 透過 min 以及 range_size 決定範圍區間 最後透過 ++i 以及 nf_nat_used_tuple 來遞加選擇的 port 並且確認是否為一。 看完了選擇 port(連接埠) 的邏輯後，我們就可以很清楚地猜到，使用者輸入的那些參數 --to-ports, --random 其實就是控制上面那些變量的初始值。 ... if (!(range-&gt;flags &amp; NF_NAT_RANGE_PROTO_SPECIFIED)) { .... } else { min = 1024; range_size = 65535 - 1024 + 1; } } else { min = ntohs(range-&gt;min_proto.all); range_size = ntohs(range-&gt;max_proto.all) - min + 1; } ...  可以看到如果有設定 NF_NAT_RANGE_PROTO_SPECIFIED 這個參數的話，就會透過之前設定的 min_proto/max_proto 來決定 min/range_size 的大小。 否則一般情況下就是使用 min=1024, range_size=65535-1024+1.  if (range-&gt;flags &amp; NF_NAT_RANGE_PROTO_RANDOM) { off = l3proto-&gt;secure_port(tuple, maniptype == NF_NAT_MANIP_SRC ? tuple-&gt;dst.u.all : tuple-&gt;src.u.all); } else if (range-&gt;flags &amp; NF_NAT_RANGE_PROTO_RANDOM_FULLY) { off = prandom_u32(); } else { off = *rover; }  針對 RANDOM 的話，根據不同的用法有兩種方式去產生 off，如果都沒有特別指定的話就會使用 rover 這個全域變數來決定。 特別注意的是 rover 是一個 static 的變數，所以可以想成當前系統內的所有 TCP 相關的NAT 會透過一個共同的變數來決定當前要決定的起始區間。 ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/iptables-masquerade#summary","content":"本篇文章透過閱讀原始碼的方式來學習 MASQUERADE 的運作方式，主要著重於兩個部分 如何挑選 IP 地址，如果本身網路卡有多個 IP 地址的話，會怎麼挑選如何挑選 source port(連接埠), 不同的參數會怎麼影響 source port 的選擇。 針對第一個問題，我們可以知道 Linux Kernel 本身對於相同網段的多重 IP 地址會有特別處理，除了第一個被設定的 IP 之外，其餘都會被設定為 SECONDARY，而選擇 IP 地址時則會依序詢問每個 非 SECONDARY 的 IP 地址，並且確認目標 IP 地址是否屬於選到的 IP 地址。 針對第二個問題，我們觀察到 MASQUERADE 有提供額外兩個參數，分別是 --to-ports 以及 --random。這兩個變數最後會影響怎麼挑選 port 的可用區間起始 port 的偏移量 下篇文章我們會嘗試透過直接修改 source code 的方式來觀察整個問題並驗證上述的觀察結果。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/cni","content":"","keywords":"","version":"Next"},{"title":"Specification​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni#specification","content":"接下來跟大家探討一下 CNI 的標準介面以及是如何運作的，不同於 CRI 是定義 gRPC 介面， CNI 的標準要求 CNI 解決方案需要提供一個執行檔(為了方便我接下來都用 binary 稱呼），該 binary 要能夠接收不同的參數來處理不同的網路生命週期，譬如創建網路，回收網路。 當 kubernetes 創建或刪除 Pod 時，就會準備好一系列的參數與設定檔案，然後呼叫該執行擋來幫忙提供網路能力。 舉例來說，假設今天 kubelet 決定要創建一個 Pod， 透過 CRI 已經準備好相關資源，如 network namespace 後，就會呼叫該 binary，同時帶入下列參數 Container ID 當前要被提供網路能力的 Container ID，基本上就是一個唯一不重複的數值，有些 CNI 解決方案可能會用此 ID 作為一些資料索引之類的需求。Network namespace path. 這個是最重要的參數，就是目標 container 其真正 network namespace 的路徑，之前提到過這些 namespace 都是透過 linux kernel 的方式來達成的，所以其實大家可以到 host 上找到這些 network namespace 並且對其操作。 因此大部分的 CNI 的 binary 就是會根據這個位置，然後找到該 network namespace 最後進入到該空間去進行一些網路設定Network configuration 這個部分非常長，等等再來說明Extra arguments 一個彈性的部分，以 Container 為單位的不同參數，主要是要看上面的呼叫者怎麼處理，因為 CNI 其實不是只有單純 kubernetes 有支援，所以最上層的應用可以根據自己的需求傳入這些額外的參數，同時只要你選擇的 CNI 解決方案會去處理這些參數即可Name of the interface inside the container 最後這個就是會在該 Container 內被創建的 network interface 名稱，常見的基本上都是 eth0, enp0s8 這種變化 假設今天有一個名為 mycni 的 CNI binary，且假設 Network Configuration 的內容存放一個名為 config 的檔案。 那執行一個 CNI 的過程就類似如下 其中 CNI_COMMAND 要告訴該 binary 目前要執行 ADD 的功能，剩下的 CONTAINERID, NETNS, IFNAME 就如同上面所述。 $ sudo ip netns add ns1 $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=`pwd` ./mycni &lt; config  ","version":"Next","tagName":"h2"},{"title":"Sandbox Container​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni#sandbox-container","content":"這邊有一個觀點要先大家討論一下，就是 network namespace 共用的議題，每個 contaienr 都會有一個 network namespace 是個常態，但是並非絕對。實際上可以多個 container 共用一個相同的 network namespace，這樣這些 container 就會看到相同的網路介面，譬如 network interface name 以及 IP 地址。 看到這邊有沒有想到關於 kubernetes POD 的定義? The applications in a Pod all use the same network namespace (same IP and port space), and can thus “find” each other and communicate using localhost. Because of this, applications in a Pod must coordinate their usage of ports. Each Pod has an IP address in a flat shared networking space that has full communication with other physical computers and Pods across the network. 如果你手邊能夠操作 docker 的話，其實我們也可以透過 docker run 中的 --net=container:xxx 這個參數來指定目標的 container id，希望他的網路與特定 container 是綁定再一起的。 $ sudo docker run -d --name c1 hwchiu/netutils $ sudo docker exec c1 ifconfig eth0 Link encap:Ethernet HWaddr 02:42:ac:12:00:02 inet addr:172.18.0.2 Bcast:172.18.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:13 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1046 (1.0 KB) TX bytes:0 (0.0 B) ... $ c1_id=$(sudo docker ps | grep hwchiu/netutils | awk '{print $1}') $ sudo docker run -d --net=container:${c1_id} --name c2 hwchiu/netutils $ sudo docker exec c2 ifconfig eth0 Link encap:Ethernet HWaddr 02:42:ac:12:00:02 inet addr:172.18.0.2 Bcast:172.18.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:14 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1116 (1.1 KB) TX bytes:0 (0.0 B) ...  在上述的測試中，我們先創建了 c1 container，並且讓 c2 跟 c1 共用同樣一個 network namespace. 試想一個情況，這時候如果 c1 container 終止了，整個情況會變成怎樣? $ sudo docker exec c2 ifconfig eth0 Link encap:Ethernet HWaddr 02:42:ac:12:00:02 inet addr:172.18.0.2 Bcast:172.18.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:15 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1242 (1.2 KB) TX bytes:0 (0.0 B) ... $ sudo docker stop c1 $ sudo docker exec c2 ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  可以觀察到當 c1 結束後， c2 的 container 其實還在運作，但是最重要的 eth0 這個網卡卻消失了，這是因為 c1 離開使得其network namespace 的 owner 一併消失。如果 c2 這時候有一些對外的網路服務，就會沒有辦法存取與使用。 在這種狀況下，我們可以思考一下假如我想要有一個類似 Pod 的服務，裡面可以同時運行多個 Container 且可以共用同的 network namespace，到底該怎麼設計才可以滿足這個需求又要避免上述的問題。 通常的一個作法是於背後創造一個非常簡單 container，本身可能就是睡眠等待信號中斷來結束自己的 sandbox container，接下來所有使用者請求的 container 全部都掛到這個 sandbox container 上，這樣使用者請求的任何一個 container 出現問題終止，甚至重啟都不會影響到其他 container 的網路環境。 整個運作流程可以參考下圖，當 contaienrtd 創造好整個 Pod 所需要的一切資源後，最上層的呼叫者 cri-contaienrd 就會去呼叫 CNI 並且傳入上述所描述的參數。 上圖節錄自Containerd Brings More Container Runtime Options for Kubernetes ","version":"Next","tagName":"h3"},{"title":"Network Configuration​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni#network-configuration","content":"前面沒有講完的 Network Configuration，這個本質是上一個 json 格式的檔案內容，由於是 json 的格式，所以其內容格式是由兩種規格所組成 CNI 標準所定義各自 CNI 解決方案所定義 所以未來去看每個 CNI 解決方案所使用的設定檔案時就會覺得看起來都很像，但是又看起來不像，原因就在於每個解決方案都有自己需要的資訊。 根據 CNI Spec，目前的 Network Configuration 有下列欄位 cniVersion(Required)​ 設定檔案希望對應到的 CNI 版本，如果不符合的話 CNI Plugin binary 要回報錯誤 name(Required)​ 一個單純用來標示的名稱，本身沒有額外作用，唯一即可 type(Required)​ 這個非常重要，他對應的就是 CNI Plugin binary 的名稱。上層工具會先解析這個名稱，接者去找到對應的 binary 來執行，因此只要知道這個名稱就可以知道會被呼叫的 binary 是哪隻，反之依然這個名稱錯打錯，那整個 CNI 就不會順利執行，網路也就不會順利建立。 args​ 一些額外的參數，主要是由上層呼叫 CNI的應用程式填入的，譬如上述的 cri-containerd 如果有一些額外資訊也可以這邊提供。 ipMasq​ 標明當前這個 CNI Plugin 會不會幫忙做 SNAT 的動作，實作都還在各自 CNI 解決方案裡面，本身只是個欄位 dns​ 如果希望該 CNI 幫忙設定 DNS 相關資訊的話，可以在這邊去設定 DNS 譬如 name serverssearchdomainoptions 這樣要特別注意的是雖然 CNI 有這個欄位，但是 kubernetes 本身並不希望 CNI 去設定 DNS，而是透過 Pod Yaml 裡面的欄位去處理，因此 kubernetes 處理這邊得時候會忽略這個值。 ipam​ IP Address Management Plugin(IPAM), 專門負責管理 IP 的分配，這部分目前常見的有 host-local 以及 dhcp 這邊這個欄位的意思是讓 CNI binary 知道當前設定的 IPAM 是哪個應用程式，請呼叫該 ipam 去取得可用的 IP address 並且套用到相關的 network namespace 上。 之後會有一篇文章詳細的介紹 IPAM 的運作原理。 下面是兩個範例，可以觀察到範例中有一些欄位是上述標準沒有定義的，則是該 CNI Plugin 自己需要的欄位。 使用名為 bridge 的 CNI 解決方案，其中希望 IPAM 透過 host-local 去處理，並且分發的網段是 10.1.0.0/16。DNS 的部份希望可以設定成 10.1.0.1。自定義參數有 bridge。 { &quot;cniVersion&quot;: &quot;0.4.0&quot;, &quot;name&quot;: &quot;dbnet&quot;, &quot;type&quot;: &quot;bridge&quot;, // type (plugin) specific &quot;bridge&quot;: &quot;cni0&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, // ipam specific &quot;subnet&quot;: &quot;10.1.0.0/16&quot;, &quot;gateway&quot;: &quot;10.1.0.1&quot; }, &quot;dns&quot;: { &quot;nameservers&quot;: [ &quot;10.1.0.1&quot; ] } }  下面這個範例則是 使用名為 ovs 的 CNI binary ipam 的部份希望採用 dhcp，並且希望最後可以加入一些 routes 相關的資訊。 自定義參數有 bridge, bxlanID 等 { &quot;cniVersion&quot;: &quot;0.4.0&quot;, &quot;name&quot;: &quot;pci&quot;, &quot;type&quot;: &quot;ovs&quot;, // type (plugin) specific &quot;bridge&quot;: &quot;ovs0&quot;, &quot;vxlanID&quot;: 42, &quot;ipam&quot;: { &quot;type&quot;: &quot;dhcp&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;10.3.0.0/16&quot; }, { &quot;dst&quot;: &quot;10.4.0.0/16&quot; } ] }, // args may be ignored by plugins &quot;args&quot;: { &quot;labels&quot; : { &quot;appVersion&quot; : &quot;1.0&quot; } } }  最後最後要談的是 Network Configuration Lists， CNI 標準中還提供了類似 Plugin List 的功能，允許對於相同 network namespace 運行多次 CNI 來創建多次網路，其中有個特別的點在於上層的 CNI 呼叫者需要把當前 CNI binary 的輸出結果當作下一個 CNI binary 的輸入一併傳入。 這種情況下會使用名為 plugins 的方式創造一個 Network Configuration Array，範例如下。 { &quot;name&quot;: &quot;cbr0&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ] }  對於上層呼叫 CNI 的應用程式，每個處理的邏輯跟流程並不一定相同，這個完全是看最上層的應用程式想要怎麼使用 CNI，這邊以 dockershim 為範例 尋找 CNI 放置設定檔案的資料夾，並且排序所有檔案(.conf conflist *json)如果是 *.conflist 的檔案，就用 Network Configuration List的格式去讀取解析, 否則就用 Network Configuration 的格式去讀取來解析找到第一個合法的 CNI 設定檔案後，就根據裡面的 type 去執行對應的 CNI binary。 func getDefaultCNINetwork(confDir string, binDirs []string) (*cniNetwork, error) { files, err := libcni.ConfFiles(confDir, []string{&quot;.conf&quot;, &quot;.conflist&quot;, &quot;.json&quot;}) switch { case err != nil: return nil, err case len(files) == 0: return nil, fmt.Errorf(&quot;no networks found in %s&quot;, confDir) } cniConfig := &amp;libcni.CNIConfig{Path: binDirs} sort.Strings(files) for _, confFile := range files { var confList *libcni.NetworkConfigList if strings.HasSuffix(confFile, &quot;.conflist&quot;) { confList, err = libcni.ConfListFromFile(confFile) if err != nil { klog.Warningf(&quot;Error loading CNI config list file %s: %v&quot;, confFile, err) continue } } else { conf, err := libcni.ConfFromFile(confFile) if err != nil { klog.Warningf(&quot;Error loading CNI config file %s: %v&quot;, confFile, err) continue } // Ensure the config has a &quot;type&quot; so we know what plugin to run. // Also catches the case where somebody put a conflist into a conf file. if conf.Network.Type == &quot;&quot; { klog.Warningf(&quot;Error loading CNI config file %s: no 'type'; perhaps this is a .conflist?&quot;, confFile) continue } confList, err = libcni.ConfListFromConf(conf) if err != nil { klog.Warningf(&quot;Error converting CNI config file %s to list: %v&quot;, confFile, err) continue } } if len(confList.Plugins) == 0 { klog.Warningf(&quot;CNI config list %s has no networks, skipping&quot;, confFile) continue } // Before using this CNI config, we have to validate it to make sure that // all plugins of this config exist on disk caps, err := cniConfig.ValidateNetworkList(context.TODO(), confList) if err != nil { klog.Warningf(&quot;Error validating CNI config %v: %v&quot;, confList, err) continue } klog.V(4).Infof(&quot;Using CNI configuration file %s&quot;, confFile) return &amp;cniNetwork{ name: confList.Name, NetworkConfig: confList, CNIConfig: cniConfig, Capabilities: caps, }, nil } return nil, fmt.Errorf(&quot;no valid networks found in %s&quot;, confDir) }  Summary 本篇文章開始跟大家討論什麼是 Container Network Interface (CNI)， 由於 CNI 牽扯到的內容實在太多，同時網路這個詞就是個概念，能夠做的事情實在太多，一時之間沒有辦法再一篇文章內講述並消化所有的事情。 因此本篇文章主要先有一個基本概念就是 CNI 本身的標準長什麼樣，會有什麼樣的設定 接下來會先探討 CNI 與 Kubernetes 的整合，包含了 kubelet 的設定檔案，以及相關 sandbox container 的架構。 再 CNI 的世界裡面，一切都是透過 binary 來呼叫對方，同時透過 STDOUT/STDIN 來傳輸資料，因此幾乎所有的 CNI 解決方案都是由不少個 Binary 組成的 因此我會特別介紹 IPAM(HostLocal) 以及 Linux Bridge (CNI) 這兩個最常被使用的工具，其被廣泛地使用到各個 CNI 解決方案內，原因就是他們提供的功能基本上很基本，但是也很好用。 參考 https://github.com/containernetworking/cnihttps://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/https://kubernetes.io/docs/concepts/workloads/pods/pod/https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/dockershim/network/cni/cni.go ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/cni-experience","content":"","keywords":"","version":"Next"},{"title":"3rd party plugins​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-experience#3rd-party-plugins","content":"Project Calico - a layer 3 virtual networkWeave - a multi-host Docker networkContiv Networking - policy networking for various use casesSR-IOVCilium - BPF &amp; XDP for containersInfoblox - enterprise IP address management for containersMultus - a Multi pluginRomana - Layer 3 CNI plugin supporting network policy for KubernetesCNI-Genie - generic CNI network pluginNuage CNI - Nuage Networks SDN plugin for network policy kubernetes support Silk - a CNI plugin designed for Cloud FoundryLinen - a CNI plugin designed for overlay networks with Open vSwitch and fit in SDN/OpenFlow network environmentVhostuser - a Dataplane network plugin - Supports OVS-DPDK &amp; VPPAmazon ECS CNI Plugins - a collection of CNI Plugins to configure containers with Amazon EC2 elastic network interfaces (ENIs)Bonding CNI - a Link aggregating plugin to address failover and high availability networkovn-kubernetes - an container network plugin built on Open vSwitch (OVS) and Open Virtual Networking (OVN) with support for both Linux and WindowsJuniper Contrail / TungstenFabric - Provides overlay SDN solution, delivering multicloud networking, hybrid cloud networking, simultaneous overlay-underlay support, network policy enforcement, network isolation, service chaining and flexible load balancingKnitter - a CNI plugin supporting multiple networking for KubernetesDANM - a CNI-compliant networking solution for TelCo workloads running on KubernetesVMware NSX – a CNI plugin that enables automated NSX L2/L3 networking and L4/L7 Load Balancing; network isolation at the pod, node, and cluster level; and zero-trust security policy for your Kubernetes cluster.cni-route-override - a meta CNI plugin that override route informationTerway - a collection of CNI Plugins based on alibaba cloud VPC/ECS network product 可以看到滿滿的解決方案，這時候其實沒有每個仔細研究的話，根本不知道彼此的差異性，同時如果自己沒有辦法評估或是描述自己的需求，最後就會變成一個不知道需要什麼解決方案的人再一堆不知道做什麼的解決方案中打轉尋找。 我覺得 CNI 有個最有趣的現象就是網路架構太不專一性，每個系統解決方案都有其搭配的網路架構，最後產生出來的解決方案都會彼此不同，所以其實可以看到上面不少 CNI 上面都伴隨者服務商的名稱，譬如 AmazonVMWareNuageJuniperTerway (Alibaba 阿里巴巴)... 等 當然也有一些 CNI 沒有被收錄進來，譬如 Azure 扣除掉服務商之後還是有為數眾多的 CNI 解決方案，這時候還是很令人困惑到底該怎麼選擇，目前最多人安裝大概就是 calico 以及 flannel，我想原因就是因為他們提供了基本的解決方法，已經可以滿足大部分人的需求，同時安裝簡單，一鍵部署輕鬆處理。 如同 CRI 有針對安全性提供的解決方案， CNI 這邊也有解決方案想要提高封包的安全性，譬如 cilium。 針對與 OpenvSwith 整合則有 OVN，想要使用 multicast 可能會想要採用 weave net，對於 Link Aggregation也可以考慮使用 bonding-cni。 幾乎每個 CNI 都有自己要解決的問題，而這些問題到底你的環境有沒有也只有你自己有能力去評估跟評斷。 為了能夠更有能力去處理這類型的需求，我認為加強網路基本概念，對於封包轉發，路由，防火牆甚至是 Linux Kernel Network Stack 等各式各樣的領域都會有所幫助，只要目前都還是基於 TCP/IP 網路模型來傳輸的話，掌握幾個基本大方向，我認為對於大部分的問題都會有所幫助。 所以基本上我不會推薦一定要用什麼 CNI，畢竟不瞭解每個人的需求，不瞭解每個系統的瓶頸，就沒有辦法根據資訊去評斷出一個可行的方案。 此外，我先前也有寫過一篇文章介紹常見的 CNI 解決方案，有興趣的人可以閱讀一下掌握一下基本概念。常見 CNI (Container Network Interface) Plugin 介紹 ","version":"Next","tagName":"h2"},{"title":"開發 CNI​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-experience#開發-cni","content":"之前因為一些需求，自己也有嘗試開發 SDN 相關的 CNI 以及一個跨節點同網段的 IPAM 分配，開發的過程中其實遇到很多問題，這些問題仔細思量後發現 CNI 是個不歸路，這邊來跟大家分享一些不歸路的經驗。 通常講到系統效能最佳化或是提升的時候，都必須要先進行分析與測試找出系統中真正的瓶頸處，有些可能是系統資源(CPU/Memory)不足，導致處理速度不快，有些可能是儲存系統讀寫太慢，導致所有的處理都卡在IO，也有些可能是網路延遲太高或是頻寬過低，導致封包傳輸變成呈整體的系統瓶頸。 就一般來說網路通常不太會是個瓶頸，況且使用公有雲服務的 kubernetes service，使用者/管理者又真的有辦法去動到這些底層網路架構? 所以大部分情況下會比較少看到人在討論 kubernetes 內關於網路效能這一塊，比較多的都是網路帶來的功能，譬如 service discovery，service mesh 等各式各樣堆疊起來的服務。 但是，人生就是有個但是 隨者 kubernetes 的火紅與熱門，有些非常在意網路延遲與頻寬的使用場景都在思考是否能夠引入 kubernetes 來試試看，甚至是進行應用程式容器化 講白一點，NFV(網路功能虛擬化)，電信商應用 等相關使用場景的基礎建設，只要談到 kubernetes， 網路 這一塊就會被拿出來探討該怎麼使用，譬如 容器要有多張網卡容器想要低延遲的傳輸容器的網路傳輸可以多快容器是否能銜接原先的網路架構 上述的這些問題其實目前於 kubernetes都有相關的解決方案可以解決，譬如 多張網卡可以透過 Multus, Genie, Knitter 等相關 CNI 去呼叫不同的 CNI來創建多張網卡高速網路目前也有各式各樣的方式可以做，不論是 DPDK, SR-IOV, RDMA, InfiniBand, SmartNIC 等各種不同的網路架構能否銜接網路就要看本來的網路架構，用什麼樣的路由規則，用什麼樣的方式串接，用什麼樣的方式管理 其中最讓人頭痛且崩潰的就是第二點，高速網路能夠輕鬆取得大家都想要，但是一旦使用後就會發現 kubernetes 帶來的優點幾乎少一半。 Kubernetes Service/IngressConfiguration/Deployment ","version":"Next","tagName":"h2"},{"title":"Service/Ingress​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-experience#serviceingress","content":"我先前曾經寫過四篇文章 [Kubernetes] What Is Service?[Kubernetes] How to Implement Kubernetes Service - ClusterIP[Kubernetes] How to Implement Kubernetes Service - NodePort[Kubernetes] How to Implement Kubernetes Service - SessionAffinity 來探討 kubernetes service 本身的實作，預設情況下是如何透過 Linux Kernel Netfilter 來完成這些功能，就換切換成 IPVS 這種選項，也依然是透過 Linux kernel 來滿足的。 Ingress 不用說，後面也是依賴 Service 來完成後端的轉發。 所以 kernel 尤其重要，幾乎是整個 Service 功能的核心，但是上面提到那些高速網路解決方案，不是直接跳過 Linux Kernel Network Stack 不然就是他根本不是 IP網路。 這情況下，整個 kubernetes service 完全起不了作用，所謂的 DNS 帶來的輕鬆存取功能根本完全消失。 的確必非所有的應用情境都會需要這個功能，但是如果一但需要這個功能的話，就是一個額外的問題要去思考，該怎麼處理跟解決。 從 CNI 的角度來看，要解決這個問題還真的很煩，我覺得有些可以解，但是就是很煩，必須要寫程式碼去跟 K8S API 做同步，一旦 Kube-Proxy 有需求要增加任何規則的時候，該 CNI 要有其他的方式去做到一模一樣的功能來滿足這個需求，想到就是覺得很麻煩，整個 CNI 的功能就變成完全跟 kubernetes 跑，當初希望藉由 interface 來降低黏著性結果現在又反其道而行。 ","version":"Next","tagName":"h3"},{"title":"Misc​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-experience#misc","content":"除此之外，還有很多很有趣的需求，有些應用程式本身的設計是要固定 IP 的，造成該容器每次重啟或是 Pod 轉移後都需要固定 IP，這對於目前的架構來說是個挑戰，但是要解決還是有辦法，重新開發 IPAM 根據 containerID 來決定配置的 IP，或是用上述的 CNI 串接起來組合出一個很噁心的用法(Multus + Static IPAM + CRD + Pod Annotation) 也有一些情境是該容器本身的傳輸協定導致其幾乎不能做 scale out，永遠都只能有一個 Pod 運行，有的甚至連封包送出去的 Port 的號碼都被限制，意味單純的 SNAT 之類的方式就不能滿足需求，這時候工程師又要開始思考可以怎麼解決這一連串煩悶的問題，而這些問題最後帶來的大多數都只有一個結果，就是網路通了。 這也是我認為為什麼網路這個議題這麼普遍令人枯燥的原因，千辛萬苦只為求得一個 Ping 通... ","version":"Next","tagName":"h3"},{"title":"Configuration/Deployment​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-experience#configurationdeployment","content":"再來談談設定檔案的部分為什麼會讓人煩悶，前面我們已經知道可以透過 daemonset 的方式來自動安裝 CNI 相關的檔案，但是舉一些不同的 CNI 設定為範例 .... &quot;delegates&quot;: [ { &quot;type&quot;: &quot;sriov&quot;, &quot;if0&quot;: &quot;ens786f1&quot;, &quot;if0name&quot;: &quot;net0&quot;, &quot;dpdk&quot;: { &quot;kernel_driver&quot;: &quot;ixgbevf&quot;, &quot;dpdk_driver&quot;: &quot;igb_uio&quot;, &quot;dpdk_tool&quot;: &quot;/path/to/dpdk/tools/dpdk-devbind.py&quot; } }, .... { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;sriov-dpdk&quot;, &quot;type&quot;: &quot;sriov&quot;, &quot;deviceID&quot;: &quot;0000:03:02.0&quot;, &quot;vlan&quot;: 1000, &quot;max_tx_rate&quot;: 100, &quot;spoofchk&quot;: &quot;off&quot;, &quot;trust&quot;: &quot;on&quot; }  可以看到上述的設定檔案裡面包含了一些 device資訊，譬如 0000:03:02.0, ens786f1 等跟硬體有關的資訊，其實都會造成部署麻煩，沒有一個統一的 CNI 設定檔案可以安裝到所有節點，變成是這些檔案可能還要透過一些運算邏輯去產生，或是所有節點的增減都要人工介入去設定一切資訊。同時硬體資源還要考慮有限制，不能全部的 Pod 都使用這些資源，勢必又要有其他的機制譬如 annotation 來指名該 Pod 想要使用什麼樣的網路。 Summary 網路的問題百百種，範圍與領域也幾乎沒有邊界可言，所以每次看到有人問網路該怎麼下手學習的時候，其實有時候反而不知道怎麼回答，感覺不論怎麼做都會先嚇跑一些人。 譬如上篇講到的 overlay network, 除了 vxlan 之外，還有各式各樣的實作，譬如 GRE, Geneve, NVGRE 等不同的東西，有些技術可能使用的廠商解決方案或是你的環境根本不需要，所以也不會有什麼機會去操作來實際了解，這些都造成了網路學習上的困難。 網路還有一些令人討厭的地方不一定所有環節都是可被觀察跟操控的，譬如你使用公有雲的服務，其實底下的節點怎麼互通的，每家廠商的解決方法都不同，有時候單純從一台機器的設定去看還看不出來到底怎麼實作的。 作為網路系列文的最後一篇，碎碎念了一些各式各樣的網路問題，接下來就要探討到儲存篇章了，歷經了 OCI, CRI, CNI 後，要來踏入 Container Storage Interface (CSI) 的範圍，儲存本身也是個坑，不同的裝置·不同的備援方式，異地備援。本地備援，快照，分散式儲存，多重讀寫等都是不同的議題，之後再來好好的討論儲存方面的各種有趣議題。 參考 https://github.com/containernetworking/cni/blob/master/README.mdhttps://github.com/intel/sriov-cni#using-dpdk-drivers ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i","content":"","keywords":"","version":"Next"},{"title":"安裝過程理解​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i#安裝過程理解","content":"官方的 安裝yaml 非常的長，這裡面包含了六種 kubernetes 的資源，分別如下。 PodSecurityPolicyClusterRoleClusterRoleBindingServiceAccountConfigMapDaemonSet 雖然這邊有六個資源，但是基於使用類別來看，我個人認為其實就是分成兩大類別，前面四個類別是相互依賴一起運作的，目的就是創造一個可控管且權利被限制的 kubernetes service account，而後面兩個則算是整個邏輯處理的核心，所有上述的問題都在這兩個資源內處理。 ","version":"Next","tagName":"h2"},{"title":"PodSecurityPolicy​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i#podsecuritypolicy","content":"用來限制每個被創造 Pod 的定義，一旦 Pod 本身資源的定義沒有滿足 PodSecurityPolicy 的規範，該 Pod 就不會被拒絕建立。 這些規範與限制都是希望能夠加強 Pod 本身的安全性，基於沒有用到的部分就不要打開，針對有需求的部分才去使用。 這個功能目前預設沒有開啟，若要開啟的話需要對 admission controller 設定參數，同時要注意的是一但開啟這個功能，但是卻沒有任何相關的 PodSecuirtyPolicy 設定的話，預設情況下所有的 Pod 都不能被創造，算是一個白名單的機制 Pod security policy control is implemented as an optional (but recommended) admission controller. PodSecurityPolicies are enforced by enabling the admission controller, but doing so without authorizing any policies will prevent any pods from being created in the cluster. 此功能沒有辦法單獨使用，需要搭配 Service Account 一併使用，所以接下來的 RBAC 等都是串連再一起使用的。 對於 flannel 來說，其規範了下列安全設定來確保其創建的 Pod 的能力是被限制的 privilegedvolumesallowedHostPaths 這邊可以看到一個跟 cni 相關的地址，代表 flannel 的 pod 勢必會對該資料夾進行一些手腳Users/GroupsCapabilities 這邊特別允許了只能給 NET_ADMIN 這個選項，這個意味 flannel 會想要針對 Pod 上的網路部分做些設定，這些設定跟其網路連接有很大的關係。Host namespaces 之前 CNI 章節忘了提到的就是所謂的 hostnetwork 的設定，如果 pod 裡面有設定 hostnetwork:true 就意味該 pod 所屬的 infrastructure container: Pause 其實不會創建新的 network namespace，而是會跟 host 也就是 kubernetes node 共用相同的網路空間。 這個功能相對危險，只要該 Pod 也有其他的能力，譬如 NET_ADMIN，其實該 Pod 是有能力摧毀整個 kubernetes cluster 的連線能力的，譬如亂修改 route table, ip address 等 更多關於這個資源的介紹可以參考 PodSecurityPolicy。 apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: &quot;/etc/cni/net.d&quot; - pathPrefix: &quot;/etc/kube-flannel&quot; - pathPrefix: &quot;/run/flannel&quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unsed in CaaSP rule: 'RunAsAny' ---  這邊定義了 ClusterRole 的資源，其中會透過 use 這個動作與上述的 PodSecurityPolicy 給綁定，此外也可以看到其他相關的能力，譬如 對 pod 的取得對 node 本身要可以 list 以及 watch對 nodes/status 執行 patch 有 Patch 就可以期待 flannel 會對 node 本身添加什麼樣的資訊，之後會再討論。 ","version":"Next","tagName":"h3"},{"title":"ClusterRole​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i#clusterrole","content":"kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - &quot;&quot; resources: - pods verbs: - get - apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch  ","version":"Next","tagName":"h3"},{"title":"ClusterRoleBinding/Service Account​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i#clusterrolebindingservice-account","content":"最後就是透過 Service Account 以及 ClusterRoleBinding 這兩個資源將上述的資源全部整合起來，創造出一個名為 flannel 的 service account。 可以預期之後看到 daemonset 的時候會使用這個 service account 來作為創建的使用者。 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system ---- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system  $ kubectl -n kube-system get sa,clusterrole,clusterrolebinding,psp | grep flannel serviceaccount/flannel 1 14h clusterrole.rbac.authorization.k8s.io/flannel 14h clusterrolebinding.rbac.authorization.k8s.io/flannel 14h podsecuritypolicy.extensions/psp.flannel.unprivileged false NET_ADMIN RunAsAny RunAsAny RunAsAny RunAsAny false configMap,secret,emptyDir,hostPath  ","version":"Next","tagName":"h3"},{"title":"ConfigMap​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i#configmap","content":"下一個要研究的資源就是牽扯到檔案的 configmap， flannel 這邊設定了兩個檔案，分別是 cni-conf.json 以及 net-conf.json。 kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { &quot;name&quot;: &quot;cbr0&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ] } net-conf.json: | { &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } }  cni-conf 如其名稱，就是給 CNI 使用的設定檔案，我們可以觀察到其使用了 plugins 的關鍵字，其格式則是 Network Configuration List，而裡面包含了兩個 CNI，分別是 flannel 以及 portmap。 Portmap CNI​ portmap 這個 CNI 則是官方維護的，其功能是類似提供 docker -p host_port:container_port 的功用，能夠幫忙在 host 也提供一個路口幫忙轉發封包到 container 裡面。 基本上我覺得有使用 kubernetes service 的話就不需要這個功能了。 若要開啟這個功能除了 CNI 有要支援之外，也必須要在 pod 裡面去描述 hostPort，這樣 CRI 創建的時候就會把這些資訊包裝起來一併傳給 CNI 去處理。 以下是一個範例，創建該資源後可以在有部署該 Pod 的節點上發現一些由 CNI 創建的 iptables 規則，這種情況下可以達到類似 NodePort 的效果。 $ cat server.yaml apiVersion: apps/v1 kind: Deployment metadata: name: k8s-udpserver spec: selector: matchLabels: run: k8s-udpserver replicas: 6 template: metadata: labels: run: k8s-udpserver spec: containers: - name: k8s-udpserver imagePullPolicy: IfNotPresent image: hwchiu/pythontest ports: - containerPort: 20001 hostPort: 20001 protocol: UDP $ sudo iptables-save -t nat | grep 20001 -A CNI-DN-eb9116f984fef9374c9e2 -s 10.244.0.6/32 -p udp -m udp --dport 20001 -j CNI-HOSTPORT-SETMARK -A CNI-DN-eb9116f984fef9374c9e2 -s 127.0.0.1/32 -p udp -m udp --dport 20001 -j CNI-HOSTPORT-SETMARK -A CNI-DN-eb9116f984fef9374c9e2 -p udp -m udp --dport 20001 -j DNAT --to-destination 10.244.0.6:20001 -A CNI-HOSTPORT-DNAT -p udp -m comment --comment &quot;dnat name: \\&quot;cbr0\\&quot; id: \\&quot;c55a611c4b8e8a4bef86ac05a3328258a858165b3bf4a3982997f9662bd82916\\&quot;&quot; -m mult iport --dports 20001 -j CNI-DN-eb9116f984fef9374c9e2 vagrant@k8s-dev:~$  但是使用這個最大的問題是 host port 是獨佔的，所以如果今天 pod 的數量超過 node 的數量，就會發生很多 pod 創建不起來，譬如下圖 $ kubectl get pods NAME READY STATUS RESTARTS AGE k8s-udpserver-5b989865bf-8jlwv 1/1 Running 0 15m k8s-udpserver-5b989865bf-jmljc 0/1 Pending 0 15m k8s-udpserver-5b989865bf-l25f4 1/1 Running 0 15m k8s-udpserver-5b989865bf-ps2wx 1/1 Running 0 15m k8s-udpserver-5b989865bf-t9glv 0/1 Pending 0 15m k8s-udpserver-5b989865bf-zvw6r 0/1 Pending 0 15m $ kubectl describe pod k8s-udpserver-5b989865bf-jmljc ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 74s (x16 over 15m) default-scheduler 0/3 nodes are available: 3 node(s) didn't have free ports for the requested pod ports.  CNI這邊除了 portmapping 之外還有各式各樣的組合，譬如可以限速的 bandwidth，調整 MAC 地址的，一時之間難以講完，就有遇到再來分享吧 除了 CNI 設定外，另外一個檔案 net-conf.json 則是給 flannel 程式使用的設定檔案。 這邊會設定 flannel 的相關資訊，特別要注意的是如果使用的是 kubeadm 來安裝 kubernetes 的話，要確認 net-conf.json 裡面關於 network 的資訊需要與 kubeadm init --pod-network-cidr=xxxx 一致。 如果沒有使用 kubeadm 的話，該參數則會被用來分配 ip 地址給所有的 Pod。 根據官方文件 A ConfigMap containing both a CNI configuration and a flannel configuration. The network in the flannel configuration should match the pod network CIDR. The choice of backend is also made here and defaults to VXLAN. ","version":"Next","tagName":"h3"},{"title":"DaemonSet​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i#daemonset","content":"接下來就要來看最大的重點 DaemonSet，首先 flannel 準備了非常多的 daemonset 檔案，分別針對不同的系統架構，譬如 amd64, arm, ppc 之類的，由於內容大同小異，差別於 node selector 而已，因此這邊我們就針對 amd64 這個範例來看 ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-i#configuration","content":"前述文章都有探討到， CNI 本身是一個以節點為單位的設定檔案，每個節點都需要一份獨立的設定檔案，這意味者所有新加入的節點也都要有該設定檔案，不單純只是現在有的節點需要維護而已。 這個情況下最常用的方式就是透過 daemonset 的方式，讓每一台機器上都跑一個特定的 Pod，該 Pod 會透過 configmap 的方式安裝相關設定檔案到 kubernetes 中啟動一個 Pod，並且將相關資料夾掛載到 Pod 裡面透過 cp 的方式將檔案安裝到各節點中 flannel 就是採用這樣的方式，並且把這個動作放到 init container 去執行，因為這類型的指令其實不太算是 daemon，比較類似 job，執行完就結束離開的應用程式，放到 Container 這邊則會使得該 Container 必須要一直利用 pause/sleep 等方式來運作得像一個 daemon，反而搞得複雜。 此外也可以觀察到該 DaemonSet 有特別指定 serviceAccountName: flannel， 算是把上述的資源跟這邊給接起來。更重要的是基於安全性方面的設定完全與上面的 PodSecutityPolicy 一致。 securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;]  apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg  接下來看看主要的 Container，這個 Container 是一個額外的應用程式，會幫忙處理 CNI 當下不方便處理的事情。 目前官方預設的安裝檔案內只有設定兩個變數，分別是 --ip-masq 會透過 masquerade 的功能幫往外送出的封包進行 SNAT，譬如下列這些規則就是 flannel 幫忙下的 -A POSTROUTING -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN -A POSTROUTING -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE -A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN -A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE  --kube-subnet-mgr 這個主要是用來告訴 flannel 如何處理 IP Subnet，目前有兩種模式，如果有特別開啟 kube-subnet-mgr 的話就會使用 kubernetes API 來處理，同時相關的設定檔案也會從 net-conf.json 來讀取，反之就全部都從 etcd 來儲存與處理。 此外我們還可以看一下相關的 Volume 到底有哪些，除了上述的 configMap 之外，我們發現該 Pod 也掛載了下列的位置 /run/flannel 接下來我們就實際看一下這個位置的檔案 $ sudo ls /run/flannel/ subnet.env $ sudo cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true  看到跟 IP 有關的設定，決定看一下另外幾台機器 $ sudo cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.2.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true  可以看到不同機器上面的 SUBNET 欄位不同，同時 Pod 得到的 IP 去觀察  $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES k8s-udpserver-6576555bcb-7h8jh 1/1 Running 0 13m 10.244.0.8 k8s-dev &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-c52rk 1/1 Running 0 13m 10.244.1.7 k8s-dev-1 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-dxm8h 1/1 Running 0 13m 10.244.1.6 k8s-dev-1 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-f49m4 1/1 Running 0 13m 10.244.2.7 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-hfhw2 1/1 Running 0 13m 10.244.2.8 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-hswhn 1/1 Running 0 13m 10.244.2.6 k8s-dev-2 &lt;none&gt; &lt;none&gt;  很巧的是每個 Node 上面該檔案的 SUBNET 都與運行 POD 的 IP 網段符合，看起來這個檔案勢必有動了一些手腳。 對於整體 IP 分配的過程我們將到下篇文章再來分析 Summary 本篇文章探討了的 Flannel 的安裝過程，從官方提供的 yaml 過程中來一一探討每個資源的用途，同時也觀察到了其利用 DaemonSet 配上 init container 來幫每個節點安裝 CNI 以及本身運行的設定檔案，確保未來任何新加入的節點都能夠順利的擁有這些檔案並正常運作。 用下圖幫本章節做個總結 https://github.com/coreos/flannel/blob/443d773037ac0f3b8a996a6de018b903b6a58c62/Documentation/kubernetes.mdhttps://github.com/kubernetes/kuberneteshttps://kubernetes.io/docs/concepts/policy/pod-security-policy/https://github.com/coreos/flannel/blob/ba49cd4c1e49d566da4a08b370384ce8ced0c0e3/Documentation/troubleshooting.md? ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-ii","content":"","keywords":"","version":"Next"},{"title":"Workflow​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-ii#workflow","content":"直接先講結論，從結論講起再來講流程會比較清楚。 kubernetes 會針對每個 node 去標示一個名為 PodCIDR 的值，代表該 Node 可以使用的網段是什麼，flannel 的 Pod 會去讀取該資訊，並且將該資訊寫道 /run/flannel/subnet.env 的這個檔案中flannel CNI 收到任何創建 Pod 的請求時，會去讀取 /run/flannel/subnet.env 的資訊，並且將其內容轉換最後呼叫 host-local 這隻 IPAM CNI，來取得可以用的 IP 並且設定到 POD 身上 相關檔案驗證 $ kubectl describe nodes | grep PodCIDR PodCIDR: 10.244.0.0/24 PodCIDR: 10.244.1.0/24 PodCIDR: 10.244.2.0/24 $ sudo cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true $ sudo ls /var/lib/cni/networks/cbr0 10.244.0.2 10.244.0.3 10.244.0.8 last_reserved_ip.0 lock $ sudo cat /var/lib/cni/networks/cbr0/10.244.0.8 2d39d5afb81e56314a7fd6bdd57c9ccf6d02c32b556273cfb6b9bb8a248c851b $ sudo docker ps --no-trunc | grep $(sudo cat /var/lib/cni/networks/cbr0/10.244.0.8) 2d39d5afb81e56314a7fd6bdd57c9ccf6d02c32b556273cfb6b9bb8a248c851b k8s.gcr.io/pause:3.1 &quot;/pause&quot; Up 4 hours k8s_POD_k8s-udpserver-6576555bcb-7h8jh_default_87196597-ccda-4643-ac5d-85343a3b6c90_0  先根據上面的指令解釋一下每個的含義，接下來再來研究其流程 透過 kubectl describe node 可以觀察到每個節點上都有一個 PodCIDR 的欄位，代表的是該節點可以使用的網段由於我的節點是對應到的 PodCIDR 是 10.244.0.0/24，接下來去觀察 /run/flannel/subnet.env，確認裡面的數值一致。接下來由於我的系統上有跑過一些 Pod，這些 Pod 形成的過程中會呼叫 flannel CNI 來處理，而該 CNI 最後會再輾轉呼叫 host-local IPAM CNI 來處理，所以就會在這邊看到有 host-local 的產物由於前篇介紹 IPAM 的文章有介紹過 host-local 的運作，該檔案的內容則是對應的 CONTAINER_ID，因此這邊得到的也是 CONTAINER_ID最後則是透過 docker 指令去尋該 CONTAINER_ID，最後就看到對應到的不是真正運行的 Pod，而是先前介紹過的 Infrastructure Container: Pause 接下來就是細談上述的流程 ","version":"Next","tagName":"h2"},{"title":"kubeadm​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-ii#kubeadm-1","content":"首先是 kubeadm 與 controller-manager 兩者的關係，當我們透過 --pod-network-cidr 去初始化 kubeadm 後，其創造出來的 controller-manager 就會自帶三個參數 root 20459 0.8 2.4 217504 100076 ? Ssl 05:22 0:36 kube-controller-manager --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --node-cidr-mask-size=24 --allocate-node-cidrs=true --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --use-service-account-credentials=true  裡面的參數過多，直接挑出重點就是 --cluster-cidr=10.244.0.0/16--allocate-node-cidrs=true--node-cidr-mask-size=24 這邊就標明的整個 cluster network 會使用的網段，除了 cidr 大網段之外還透過 node-cide--mask 去標示寫網段，所以根據上述的範例，這個節點的數量不能超過255台節點，不然就沒有足夠的 可用網段去分配了。 此外很有趣的一點是，這邊的運作邏輯再 controller-manager 內被稱為 nodeipam，也就是今天 kubernetes 自己跳下來幫忙做 IPAM 的工作，幫忙分配 IP/Subnet，只是單位是以 Node 為基準，不是以 Pod。 根據 GitHub Controler 可以看到當 Controller Manager 物件被創造的時候會根據上述的參數去產生一個名為 cidrset 的物件 .... set, err := cidrset.NewCIDRSet(clusterCIDR, nodeCIDRMaskSize) if err != nil { return nil, err } ...  而 CIDRSet 的結構如下 type CidrSet struct { sync.Mutex clusterCIDR *net.IPNet clusterIP net.IP clusterMaskSize int maxCIDRs int nextCandidate int used big.Int subNetMaskSize int }  基本上就是定義了 subnet 相關的所有變數，接下來裡面有一個函式叫做 allocateRange，顧名思義就是要出一塊可以用的網段  func (op *updateOp) allocateRange(ctx context.Context, sync *NodeSync, node *v1.Node) error { if sync.mode != SyncFromCluster { sync.kubeAPI.EmitNodeWarningEvent(node.Name, InvalidModeEvent, &quot;Cannot allocate CIDRs in mode %q&quot;, sync.mode) return fmt.Errorf(&quot;controller cannot allocate CIDRS in mode %q&quot;, sync.mode) } cidrRange, err := sync.set.AllocateNext() if err != nil { return err } // If addAlias returns a hard error, cidrRange will be leaked as there // is no durable record of the range. The missing space will be // recovered on the next restart of the controller. if err := sync.cloudAlias.AddAlias(ctx, node.Name, cidrRange); err != nil { klog.Errorf(&quot;Could not add alias %v for node %q: %v&quot;, cidrRange, node.Name, err) return err } if err := sync.kubeAPI.UpdateNodePodCIDR(ctx, node, cidrRange); err != nil { klog.Errorf(&quot;Could not update node %q PodCIDR to %v: %v&quot;, node.Name, cidrRange, err) return err } if err := sync.kubeAPI.UpdateNodeNetworkUnavailable(node.Name, false); err != nil { klog.Errorf(&quot;Could not update node NetworkUnavailable status to false: %v&quot;, err) return err } klog.V(2).Infof(&quot;Allocated PodCIDR %v for node %q&quot;, cidrRange, node.Name) return nil }  裡面最重要的就是呼叫 UpdateNodePodCIDR 這個函式來進行最後的更新 根據其原始碼 func (a *adapter) UpdateNodePodCIDR(ctx context.Context, node *v1.Node, cidrRange *net.IPNet) error { patch := map[string]interface{}{ &quot;apiVersion&quot;: node.APIVersion, &quot;kind&quot;: node.Kind, &quot;metadata&quot;: map[string]interface{}{&quot;name&quot;: node.Name}, &quot;spec&quot;: map[string]interface{}{&quot;podCIDR&quot;: cidrRange.String()}, } bytes, err := json.Marshal(patch) if err != nil { return err } _, err = a.k8s.CoreV1().Nodes().Patch(node.Name, types.StrategicMergePatchType, bytes) return err }  可以看到最後會在 spec下面產生一個名稱為 podCIDR 的內容，且其數值就是分配後的網段(cidrRange.String())。 這部分可以透過 kubectl get nodes xxxx -o yaml 來驗證 $ kubectl get nodes k8s-dev -o yaml apiVersion: v1 kind: Node metadata: annotations: flannel.alpha.coreos.com/backend-data: '{&quot;VtepMAC&quot;:&quot;3e:94:52:9b:7e:d9&quot;}' flannel.alpha.coreos.com/backend-type: vxlan flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot; flannel.alpha.coreos.com/public-ip: 10.0.2.15 kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: &quot;0&quot; volumes.kubernetes.io/controller-managed-attach-detach: &quot;true&quot; creationTimestamp: &quot;2019-09-23T05:21:46Z&quot; labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: k8s-dev kubernetes.io/os: linux node-role.kubernetes.io/master: &quot;&quot; name: k8s-dev resourceVersion: &quot;57899&quot; selfLink: /api/v1/nodes/k8s-dev uid: cd8fadc0-e58c-4509-9056-3a06bdb8440f spec: podCIDR: 10.244.0.0/24 ...  ","version":"Next","tagName":"h2"},{"title":"Pod Flannel​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-ii#pod-flannel","content":"鏡頭一轉，我們來看當 flannel 部署的 Pod 運行起來後會做什麼事情。 前文有提過，預設的安裝設定檔案中會使得 flannel 使用 kubernetes API 來存取資訊，這同時也意味其 subnet manager 會使用 kubernetes API 來完成，這部分的程式碼都在這 其中要特別注意的一個函式AcquireLease可以看到裡面嘗試針對 node 底下的 sped.PodCIDR 去存取，並且透過 net.ParseCIDR 的方式去解讀。 ... if n.Spec.PodCIDR == &quot;&quot; { return nil, fmt.Errorf(&quot;node %q pod cidr not assigned&quot;, ksm.nodeName) } bd, err := attrs.BackendData.MarshalJSON() if err != nil { return nil, err } _, cidr, err := net.ParseCIDR(n.Spec.PodCIDR) if err != nil { return nil, err } ...  接下來於主要的 main.go 這邊會在呼叫 WriteSubnetFile 把相關的結果寫到檔案內，最後大家就可以到 /run/flannel/subnet.env 去得到相關資訊。 func WriteSubnetFile(path string, nw ip.IP4Net, ipMasq bool, bn backend.Network) error { dir, name := filepath.Split(path) os.MkdirAll(dir, 0755) tempFile := filepath.Join(dir, &quot;.&quot;+name) f, err := os.Create(tempFile) if err != nil { return err } // Write out the first usable IP by incrementing // sn.IP by one sn := bn.Lease().Subnet sn.IP += 1 fmt.Fprintf(f, &quot;FLANNEL_NETWORK=%s\\n&quot;, nw) fmt.Fprintf(f, &quot;FLANNEL_SUBNET=%s\\n&quot;, sn) fmt.Fprintf(f, &quot;FLANNEL_MTU=%d\\n&quot;, bn.MTU()) _, err = fmt.Fprintf(f, &quot;FLANNEL_IPMASQ=%v\\n&quot;, ipMasq) f.Close() if err != nil { return err } // rename(2) the temporary file to the desired location so that it becomes // atomically visible with the contents return os.Rename(tempFile, path) //TODO - is this safe? What if it's not on the same FS? }  ","version":"Next","tagName":"h2"},{"title":"CNI Flannel​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-ii#cni-flannel","content":"話題一轉，我們來看最後一個步驟，當 CRI 決定創建 POD 並且準備好相關環參數呼叫 CNI 後的運作。 這邊要額外提醒， flannel 的程式碼分兩的地方存放 CoreOS - PodContainetNetworking - CNI 同時這也可以解釋為什麼一開始安裝好 kubernetes 後，系統內就有 flannel CNI 的執行檔案了，因為被放在官方的 repo 裡面。 我們先來看創建 POD 的時候 Flannel CNI 會做的事情 const ( defaultSubnetFile = &quot;/run/flannel/subnet.env&quot; defaultDataDir = &quot;/var/lib/cni/flannel&quot; ) ... func cmdAdd(args *skel.CmdArgs) error { n, err := loadFlannelNetConf(args.StdinData) if err != nil { return err } fenv, err := loadFlannelSubnetEnv(n.SubnetFile) if err != nil { return err } if n.Delegate == nil { n.Delegate = make(map[string]interface{}) } else { if hasKey(n.Delegate, &quot;type&quot;) &amp;&amp; !isString(n.Delegate[&quot;type&quot;]) { return fmt.Errorf(&quot;'delegate' dictionary, if present, must have (string) 'type' field&quot;) } if hasKey(n.Delegate, &quot;name&quot;) { return fmt.Errorf(&quot;'delegate' dictionary must not have 'name' field, it'll be set by flannel&quot;) } if hasKey(n.Delegate, &quot;ipam&quot;) { return fmt.Errorf(&quot;'delegate' dictionary must not have 'ipam' field, it'll be set by flannel&quot;) } } if n.RuntimeConfig != nil { n.Delegate[&quot;runtimeConfig&quot;] = n.RuntimeConfig } return doCmdAdd(args, n, fenv) }  有個常見且習慣的名稱 cmdAdd，裡面可以看到呼叫了 loadFlannelSubnetEnv，其中若使用者沒有特別設定的話，預設的 SubnetFile 就是 defaultSubnetFile，如上面示，其值為 /run/flannel/subnet.env。 接者該函式 func loadFlannelSubnetEnv(fn string) (*subnetEnv, error) { f, err := os.Open(fn) if err != nil { return nil, err } defer f.Close() se := &amp;subnetEnv{} s := bufio.NewScanner(f) for s.Scan() { parts := strings.SplitN(s.Text(), &quot;=&quot;, 2) switch parts[0] { case &quot;FLANNEL_NETWORK&quot;: _, se.nw, err = net.ParseCIDR(parts[1]) if err != nil { return nil, err } case &quot;FLANNEL_SUBNET&quot;: _, se.sn, err = net.ParseCIDR(parts[1]) if err != nil { return nil, err } case &quot;FLANNEL_MTU&quot;: mtu, err := strconv.ParseUint(parts[1], 10, 32) if err != nil { return nil, err } se.mtu = new(uint) *se.mtu = uint(mtu) case &quot;FLANNEL_IPMASQ&quot;: ipmasq := parts[1] == &quot;true&quot; se.ipmasq = &amp;ipmasq } } if err := s.Err(); err != nil { return nil, err } if m := se.missing(); m != &quot;&quot; { return nil, fmt.Errorf(&quot;%v is missing %v&quot;, fn, m) } return se, nil }  就會去讀取該檔案，並且整理成一個 subnetEnv 的物件格式，一切都處理完畢後，就會透過 CNI 內建的函式去呼叫其他的 CNI 來處理 可以再doCmdAdd 這個函式看到最後塞了一個 ipam 的字典資訊進去，然後裡面設定了 host-local 會用到的所有參數。 func doCmdAdd(args *skel.CmdArgs, n *NetConf, fenv *subnetEnv) error { n.Delegate[&quot;name&quot;] = n.Name if !hasKey(n.Delegate, &quot;type&quot;) { n.Delegate[&quot;type&quot;] = &quot;bridge&quot; } if !hasKey(n.Delegate, &quot;ipMasq&quot;) { // if flannel is not doing ipmasq, we should ipmasq := !*fenv.ipmasq n.Delegate[&quot;ipMasq&quot;] = ipmasq } if !hasKey(n.Delegate, &quot;mtu&quot;) { mtu := fenv.mtu n.Delegate[&quot;mtu&quot;] = mtu } if n.Delegate[&quot;type&quot;].(string) == &quot;bridge&quot; { if !hasKey(n.Delegate, &quot;isGateway&quot;) { n.Delegate[&quot;isGateway&quot;] = true } } if n.CNIVersion != &quot;&quot; { n.Delegate[&quot;cniVersion&quot;] = n.CNIVersion } n.Delegate[&quot;ipam&quot;] = map[string]interface{}{ &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: fenv.sn.String(), &quot;routes&quot;: []types.Route{ { Dst: *fenv.nw, }, }, } return delegateAdd(args.ContainerID, n.DataDir, n.Delegate) }  這個檔案其實也無形透露了， flannel 最後其實是產生一個使用 bridge 作為主體 CNI 且 IPAM 使用 host-local 的設定檔案。 這也是我之前所說的這些由官方維護的基本功能解決方案，不論是基於提供網路功能的，或是 IPAM 相關的套件都會給受到其他的套件反覆使用而組合出更強大的功能。 一旦當 host-local 處理結束後，就會再 /var/run/cni/cbr0/networks 看到一系列由 host-local 所維護的正在使用 IP 清單。 Summary flannel 本身並不處理 Linux Bridge 的設定以及 IPAM 相關的設定，反而是透過更上層的辦法去處理設定檔案的問題，確保每一台機器上面 host-local 看到的網段都不同，而 host-local 則專注於對每個網段都能夠不停的產生出唯一不被使用的 IP 地址。 這種分工合作的辦法也是現在軟體開發與整合的模式，隨者效能與功能愈來愈強大，很難有一個軟體可以涵括所有領域的功能，適度的合作與整合才有辦法打造出更好的解決方案。 本篇我們大概理解了 flannel 是如何處理 IP 分配的問題，透過 kubernetes nodeIPAM 的設計，以及 CNI Host-local IPAM 的處理來完成。 最後使用下圖來作為一個總結  參考 https://github.com/coreos/flannel/blob/443d773037ac0f3b8a996a6de018b903b6a58c62/Documentation/kubernetes.mdhttps://github.com/coreos/flannel ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-iii","content":"","keywords":"","version":"Next"},{"title":"IP 分配​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-iii#ip-分配","content":"IP 分配問題的話，有些情況會希望所有的 Pod 跟外面的節點與現存服務使用相同的網段，有些情況則覺得沒有關係，分配一些私有網段即可。 舉一個範例，如果今天系統中已經運行了大量的服務，這時候希望導入 kubernetes 作為部署工具，但是基於現實考量，譬如測試，容器化等因素，並不是所有的服務都可以一次就直上 kubernetes。 同時本來的系統架構中，會有類似防火牆等安全機制，會根據 來源IP/目的IP 進行一些檢查與過濾，這種情況下，使用者可能就會希望 kubernetes 內的 pod 可以跟本來環境中的網路使用相同網段的 IP 甚至是使用 dhcp 等方式來獲取 IP。 但是對於部分的使用者來說，其實不太需要在乎這一塊議題，主要是在意的點只有 能不能方便存取, kubernetes service 能不能滿足需求，你裡面的 Pod 到底是什麼 IP 很多時候根本不重要。 如果是公有雲的解決方案，有些也會希望能夠同網段，這樣可以跟公有雲其他的資源進行整合，不論是 IP的發放，防火牆的設定等都希望能夠只用公有雲一套規則滿足全部。 這部分我之前有一篇文章在介紹 Azure - AKS是怎麼實現其 CNI 來達到上述需求的。有興趣的讀者可以再額外閱讀。 ","version":"Next","tagName":"h2"},{"title":"Overlay Network​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-iii#overlay-network","content":"網路串連的方法百百種，每種方法都有其價值以及使用場景，這次要來討論的則是 VXLAN，這個 flannel 預設的網路實現方式。 以下圖為一個基本範例，根據前篇文章我們知道每個節點都會分配到不同網段，這個案例中分別是 10.244.0.0/2410.244.1.0/2410.244.2.0/24 所以這三個節點所創造的 Pod 都會基於上述所描述的網段 # kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES k8s-udpserver-6576555bcb-4dc77 1/1 Running 0 2m11s 10.244.0.3 k8s-dev &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-4wl9n 1/1 Running 0 2m11s 10.244.1.8 k8s-dev-1 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-7rvnj 1/1 Running 0 24h 10.244.2.2 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-949wp 1/1 Running 0 2m11s 10.244.2.4 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-9mwcv 1/1 Running 0 2m11s 10.244.0.5 k8s-dev &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-b7nbx 1/1 Running 0 2m11s 10.244.2.6 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-bt94h 1/1 Running 0 2m11s 10.244.2.5 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-c9v9w 1/1 Running 0 24h 10.244.1.4 k8s-dev-1 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-d6lqp 1/1 Running 0 2m11s 10.244.2.3 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-dhmw9 1/1 Running 0 2m11s 10.244.1.5 k8s-dev-1 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-jlc45 1/1 Running 0 2m11s 10.244.1.6 k8s-dev-1 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-nwfbl 1/1 Running 0 24h 10.244.0.2 k8s-dev &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-rtrq9 1/1 Running 0 2m11s 10.244.2.7 k8s-dev-2 &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-v9cwz 1/1 Running 0 2m11s 10.244.0.4 k8s-dev &lt;none&gt; &lt;none&gt; k8s-udpserver-6576555bcb-xwdbv 1/1 Running 0 2m11s 10.244.1.7 k8s-dev-1 &lt;none&gt; &lt;none&gt;  這個情況下，我們希望讓 10.244.0.0/24 網段可以與 10.244.1.0/24 網段溝通，這時候flannel 會怎麼做。 先假設一個情境, 10.244.0.5 想要與 10.244.2.7 溝通，由於牽扯到跨網段的問題，所以會有 gateway 的涉入，更麻煩的是這些網段都是私有網段，一旦封包從該節點出去後，要怎麼確保外面的所有網路架構都知道該如何轉送這些封包？ 舉例來說，假設 10.244.0.5 的封包很順利的從節點出去了，接下來整個外網的網路要怎麼知道原來目的 10.244.2.7 是屬於 172.17.8.103 這台機器上的? 只要外部網路裡面有一個環節不通，整個封包就送不到 172.17.8.103，就沒有辦法送進去到 Pod，更不要提假如今天外部網路已經有一個一模一樣的 IP 或是網段，是不是會造成 IP 衝突的問題，導致網路存取出問題？ 此外這些外部的網路機器交換機大部分 kubernetes cluster 也不會去操控去管理，所以就會變成根本沒有一條合適的路由規則來轉發封包。 為了解決這個問題，我們決定使用基於 overlay network 的技術 VXLAN 來解決這個問題。 ","version":"Next","tagName":"h2"},{"title":"Overlay Network​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-iii#overlay-network-1","content":"overlay 顧名思義就是基於原先 underlay network 上在疊一層封包，flannel 採用的是基於 VXLAN 的封包協定來實現 overlay network。 一個簡單的概念，VXLAN 會在本來的傳輸封包上再重新開放疊加 Layer 2/3/4 總共三層全新的封包，我們先定義最原始傳輸的目標叫做 Original Packet， 而新疊加的則叫做 Outer Packet。 所以上述的封包傳輸 10.244.0.5 -&gt; 10.244.2.7 為範例 10.244.0.5 -&gt; 10.244.2.7 傳輸的封包就是所謂的 original packet。 因為 10.244.0.5 位於機器 172.17.8.101，10.244.2.7 位於機器 172.17.8.103。 因此會再額外包一層172.17.8.101 與 172.17.8.103 的傳輸的封包就是所謂的 outer packet。 下面來看一下 VXLAN 的封包格式該圖截取自Configuration Guide - VXLAN 所以其實基於 VXLAN 傳輸的封包再網路上傳送的時候，其內部其實會有兩層傳送的資料，一層是透過 underlay 搞定的，也就是 outer packet，而另外一層則是真正要傳送的，也就是 original packet。 搭配上圖的格式圖可以發現， VXLAN 本身也會額外再 Original 與 Outer 中間塞一個 VXLAN Header 去標注一些相關功能，其中的 VNI 是做到類似 VLAN 相同的效果，相同 VNI 的兩端點才有能力溝通。 眼尖的人會發現上述 Outer Packet 裡面有 UDP Packet 裡面有一個 DestPort(VXLAN) Port，這就意味者其實收端(172.17.8.103)會有一個網路程式聽在該 Port上面去幫忙處理這些封包。 當該程式看到這些封包後，會將這些的外皮剝掉然後看到最裡面的 Original Packet，最後幫忙轉發。 ","version":"Next","tagName":"h3"},{"title":"流程​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-iii#流程","content":"重新整理一下整個流程，如何透過 VXLAN 來解決 10.244.0.5 -&gt; 10.244.2.7 的傳輸問題 10.244.0.5 往 10.244.2.7 發送封包這些封包再離開節點之前會先被本地上的 VXLAN 應用程式處理，首先加上 VXLAN Header接者填補 UDP Header接者根據某些方式知道 10.244.2.7 位於 172.17.8.103，所以補上一個 IP Header，其中將來源設定成本機172.17.8.101，目的設定成 172.17.8.103封包送出去外面的所有機器都是看到 172.17.8.101 -&gt; 172.17.8.103 (這個傳輸本來就應該要可以運作，不然 kubernetes cluster 沒辦法建立)當封包到達 172.17.8.103 收到封包後，一路拆解最後被 VXLAN 應用程式收到封包，並且看到裡面是 10.244.0.5 往 10.244.2.7 送的封包，於是將該封包往下轉發最後 10.244.2.7 就可以順利地收到 10.244.0.5 的封包。 上面的流程看似簡單合理，但是其中隱藏些許問題 VXLAN 的應用程式是什麼上述的應用程式怎麼知道 10.244.2.7 位於 172.17.8.103，反之亦然，目標端的要知到 10.244.0.5 位於 172.17.8.101 ","version":"Next","tagName":"h3"},{"title":"實作​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-flannel-iii#實作","content":"接下來我們來探討上述的兩個問題是怎麼處理的，先說結論 Linux Kernel + flannel pod + Kubernetes API server 一起合力完成上述的所有流程。 先來看一個有趣的東西，仔細再看一次每個 node 上面的 annotation，會發現一些有趣的東西 $ kubectl get node k8s-dev -o=json | jq -r .metadata.annotations { &quot;flannel.alpha.coreos.com/backend-data&quot;: &quot;{\\&quot;VtepMAC\\&quot;:\\&quot;0a:72:64:c9:50:f4\\&quot;}&quot;, &quot;flannel.alpha.coreos.com/backend-type&quot;: &quot;vxlan&quot;, &quot;flannel.alpha.coreos.com/kube-subnet-manager&quot;: &quot;true&quot;, &quot;flannel.alpha.coreos.com/public-ip&quot;: &quot;172.17.8.101&quot;, &quot;kubeadm.alpha.kubernetes.io/cri-socket&quot;: &quot;/var/run/dockershim.sock&quot;, &quot;node.alpha.kubernetes.io/ttl&quot;: &quot;0&quot;, &quot;volumes.kubernetes.io/controller-managed-attach-detach&quot;: &quot;true&quot; } $ kubectl get node k8s-dev -o=json | jq -r .spec { &quot;podCIDR&quot;: &quot;10.244.0.0/24&quot; }  backend-type: vxlan， 代表其用到的類型backend-data: 有 VtepMac, 還有一些 MAC Address。public-ip: &quot;172.17.8.101&quot;podCIDR: &quot;10.244.0.0/24&quot; 是不是只要把(3)跟(4)的資訊給合併起來，就完全解決上述的問題(2)？ 這其實也反應到為什麼最上面的環境建置的時候，要透別加入 --iface eth1 的參數到 flannel 的環境中，因為需要讓 flannel 知道真正的對外 IP 是使用哪張網卡，他才有辦法擷取到相關的 IP 並且寫入到 Node 之中。 由於這些資訊是每個節點都要知道每個節點的，所以其實 flanneld 本身有實作 List/Watch Node 相關的流程，一旦 Node 本身的資訊有更動，就會去抓取這些資訊來更新當前節點的知識 如果還記得前兩天的文章，是否還記得 Flannel 創建的 RBAC 裡面會特別允許 Node List/Watch ，目的就是為了這個。 所以現在的流程是 flannel pod 會知道本地端用的各種資訊，譬如 subnet，publicIP，接者把這些資訊都會打到 Node 裡面所有節點的 flannel pod 都會去監聽相關事件，聽取到之後就會將該資訊存放在本地端的記憶體內，知道每個網段對應的節點資訊。 接下來我們來看一下最後幾個步驟，這幾個步驟就不會描述太多，細節比較複雜，稍微帶過相關的資訊，知道所有的資訊在哪邊即可。 首先上述提到 Linux Kernel 現在都有支援 VXLAN 的實作，這意味我們可以透過 Kernel 內建的功能來幫忙處理 VXLAN 的應用程式VXLAN 封包的封裝與解封裝 我們先來觀察到底實際上系統被加料了什麼來處理這些資訊 $ ip -d addr show dev flannel.1 5: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 0a:72:64:c9:50:f4 brd ff:ff:ff:ff:ff:ff promiscuity 0 vxlan id 1 local 172.17.8.101 dev eth1 srcport 0 0 dstport 8472 nolearning ttl inherit ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 10.244.0.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::872:64ff:fec9:50f4/64 scope link valid_lft forever preferred_lft forever  透過 ip link 指令可以看到系統上被創建了一個新的網卡，這個網卡有一些資訊很重要 flannel.1 這個 .1 是有意思的，代表其 VNI 是 1vxlan id 1 (該介面是屬於特殊型態 VXLAN)nolearning (本身不會主動去學習該怎麼轉送，代表要有人教)local 172.17.8.101 dev: 對應到本地的對外介面10.244.0.0/32: flannel.1 的 IP 地址，這意味所有送到 10.244.0.0 的封包都會送到給 flannel.1 處理 接下來我們來看一下會怎麼轉送 $ route -n | grep flannel 10.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 $ arp -na | grep 10.244 ? (10.244.2.0) at ee:8a:1f:f7:96:c7 [ether] PERM on flannel.1 ? (10.244.1.0) at 8e:79:a7:a7:bd:1c [ether] PERM on flannel.1 $ bridge fdb show | grep 172 8e:79:a7:a7:bd:1c dev flannel.1 dst 172.17.8.102 self permanent ee:8a:1f:f7:96:c7 dev flannel.1 dst 172.17.8.103 self permanent  路由表中表示，如果今天封包要送給 10.244.2.0/24，則把 gateway 設定成 10.244.2.0，並且透過 flannel.1 這張網卡傳輸由於封包中的目標 MAC 是 next hop 的地址，所以此情況需要填入 10.244.2.0 的 MAC 地址，該地址可以在 kubernetes node 上找到。最後會被 flannel pod 透過 arp 的方式寫死在系統內，可以由上述的 arp -n 看到相關資料有了上述資料後，我們已經可以把 Original Packet填寫完畢，接下來就剩下 outer pakcet 要填寫最後透過 bridge forward database 去查看，對於 flannel.1來說，看到裡面封包的目標地址是 ee:8a:1f:f7:96:c7 的，請於 outer packet 轉發到 172.17.8.103。 透過上述流程就可以組合出一個合法的 VXLAN 封包格式，並且送到不同節點去。 另外每台機器上面都會被創造一個 flannel.1 的介面，該介面其實就會作為每個節點的 VXLAN 處理程式，封包收到相關的封包後，會透過 vni 的方式與 mac 比對的規則找到對應的介面去進行處理，然後解封裝後再次轉發。 最後補上一個流程，這些 flannel.1 的介面都是由 flannel pod (flanneld) 這個應用程式創造的，同時當該應用程式從 kubernetes API server 學習到不同的節點資訊的時候，就會把上述看到的 route, arp, beidge fdb 等資訊都寫一份到 kernel 內，藉此打通所有的傳送可能性。 Summary 最後就用一張圖來解釋上述的所有流程。 參考 https://vincent.bernat.ch/en/blog/2017-vxlan-linuxhttps://www.slideshare.net/Ciscodatacenter/vxlan-introductionhttps://support.huawei.com/enterprise/en/doc/EDOC1100004365/f95c6e68/vxlan-packet-format ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/cni-golnag","content":"","keywords":"","version":"Next"},{"title":"Step 1​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-golnag#step-1","content":"為了快速使用 golang 開發 CNI 的應用程式，我們可以借助官方提供的函示庫來幫助我們快速建立整個 CNI 的框架 package main import ( &quot;fmt&quot; &quot;github.com/containernetworking/cni/pkg/skel&quot; &quot;github.com/containernetworking/cni/pkg/version&quot; ) func cmdAdd(args *skel.CmdArgs) error { fmt.Printf(&quot;interfance Name: %s\\n&quot;, args.IfName) fmt.Printf(&quot;netns path: %s\\n&quot;, args.Netns) fmt.Printf(&quot;the config data: %s\\n&quot;, args.StdinData) return nil } func cmdDel(args *skel.CmdArgs) error { return nil } func main() { skel.PluginMain(cmdAdd, cmdDel, version.All) }  這個範例中，我們建立的兩個 function, 分別要用來處理 ADD/DEL 兩個事件，對應到 Container 被創立以及 Container 被刪除 接者透過 skel 這個函式庫將這兩個 function 與 ADD/DEL 事件給關聯起來 其中要注意的是這些 function 的參數都必須是 skel.CmdArgs，其結構如下 type CmdArgs struct { ContainerID string Netns string IfName string Args string Path string StdinData []byte }  有沒有覺得這些欄位與之前介紹的 CNI 標準內定義的欄位很相似？ 這個結構就是用來幫助處理相關參數的，該 skel 的函式庫會幫忙把相關參數收集完畢後塞到這個結構內，並且傳入到 ADD/DEL 對應的 function. 此外這邊的 StdinData 其實就是所謂的 Network Configuration 的 json檔案，而這個範例中我們希望透過一個 config 去描述 bridge 的名稱以及 network namespace 會用到的 IP 地址。 因此我們先設計一個簡單的 Config 內容，並且存放到名為 config 的檔案內 { &quot;name&quot;: &quot;mynet&quot;, &quot;BridgeName&quot;: &quot;test&quot;, &quot;IP&quot;: &quot;192.0.2.12/24&quot; }  假設上述的 golang 程式編譯完成後名為 mycni，則我們可以這樣進行測試 $ sudo ip netns add ns1 $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=`pwd` ./mycni &lt; config interfance Name: eth10 netns path: /var/run/netns/ns1 the config data: { &quot;name&quot;: &quot;mynet&quot;, &quot;BridgeName&quot;: &quot;test&quot;, &quot;IP&quot;: &quot;192.0.2.12/24&quot; }  其中用到的環境變數 CNI_XXX 由 CNI SPEC 所定義，分別有 CNI_COMMANDCNI_CONYAINERIDCNI_IFNAMECNI_ARGSCNI_PATH 完成了這一步就意味我們的程式已經可以處理 CNI 相關的資訊了，只要把上述的設定檔案與執行檔放入到 kubernetes cluster 內，依照 --cni-bin-dri 以及 --cni-conf-dir 的設定的位置下，就可以順利地被執行然後印出相關資訊。 不過由於目前的程式什麼都沒有做，所以執行起來的 Pod 會變成沒有對外連接上網的能力，但是整個流程算是已經打通了，下一步就是如何透過這些資訊來操作 Linux 以及 Network Namespace。 ","version":"Next","tagName":"h2"},{"title":"Step 2​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-golnag#step-2","content":"接下來我們要做的事情就是在系統內創建一個 Linux Bridge，這部分會使用到 netlink 相關的函示庫進行操作，主要是透過 netlink 這個IPC的機制直接告訴 kernel 幫忙操作。 此外，我們在上一個步驟定義了簡單的 config 內容，因此這次也要在程式內定義相關的結構來讀取這些資料。 { &quot;name&quot;: &quot;mynet&quot;, &quot;BridgeName&quot;: &quot;test&quot;, &quot;IP&quot;: &quot;192.0.2.12/24&quot; }  import ( &quot;encoding/json&quot; &quot;fmt&quot; &quot;syscall&quot; &quot;github.com/containernetworking/cni/pkg/skel&quot; &quot;github.com/containernetworking/cni/pkg/version&quot; &quot;github.com/vishvananda/netlink&quot; ) type SimpleBridge struct { BridgeName string `json:&quot;bridgeName&quot;` IP string `json:&quot;ip&quot;` } func cmdAdd(args *skel.CmdArgs) error { sb := SimpleBridge{} if err := json.Unmarshal(args.StdinData, &amp;sb); err != nil { return err } fmt.Println(sb) br := &amp;netlink.Bridge{ LinkAttrs: netlink.LinkAttrs{ Name: sb.BridgeName, MTU: 1500, // Let kernel use default txqueuelen; leaving it unset // means 0, and a zero-length TX queue messes up FIFO // traffic shapers which use TX queue length as the // default packet limit TxQLen: -1, }, } err := netlink.LinkAdd(br) if err != nil &amp;&amp; err != syscall.EEXIST { return err } if err := netlink.LinkSetUp(br); err != nil { return err } return nil }  定義一個簡單的結構，用來讀取該 json 檔案該 config 會放到 args.StdinData ，嘗試從這邊讀取內容接下來我們要使用 netlink 的函示庫操作 Linux Bridge 分成三個步驟 創建 Bridge 的物件告知 Kernel 幫忙創建 Bridge將該 Bridge 啟動 (類似 ifconfig br0 up) 由於這個範例中我們還沒有真的去操控到 namespace，所以不需要真的創建 namespace 也是可以運行的 $ brctl show bridge name bridge id STP enabled interfaces $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=`pwd` ./mycni &lt; config {test 192.0.2.12/24} $ brctl show bridge name bridge id STP enabled interfaces test 8000.000000000000 no  ","version":"Next","tagName":"h2"},{"title":"Step 3​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-golnag#step-3","content":"再來重新檢視一下我們的目標圖 第三步驟我們要滿足上圖的(2)的功能，建立一對 veth 並且分別連接到 Linux Bridge 以及預先創立好的 network namespace 上，同時該名稱必須是我們透過參數傳進去的。 由於接下來要直接針對 network namespace (netns) 進行操作，同時也會用到一些相關的介面，因此我們要引用更多官方提供的函示庫 import ( &quot;encoding/json&quot; &quot;fmt&quot; &quot;syscall&quot; &quot;github.com/containernetworking/cni/pkg/skel&quot; &quot;github.com/containernetworking/cni/pkg/types/current&quot; &quot;github.com/containernetworking/cni/pkg/version&quot; &quot;github.com/containernetworking/plugins/pkg/ip&quot; &quot;github.com/containernetworking/plugins/pkg/ns&quot; &quot;github.com/vishvananda/netlink&quot; )  延續 Step 2的程式，我們創建完畢 Linux Bridge 之後，接下來我們要開始處理 network namespace，運作流程如下 根據參數 CNI_NETNS 給的路徑取得相關 network namespace(netns) 的物件於該 netns 內創建一對 veth ，需要三個參數，分別是 interface name, 也就是 CNI_INFNAMEmtu, 範例測試使用 1500 即可另外一端的 netns 物件，由於我們是在目標的 netns 內創造，所以這個變數則是要給 host 本身的 netns 創建完畢後透過 veth 的回傳變數取得創建於 host 上的 interface 名稱，通常是 vethxxxxxxx 這種格式根據上述的名稱再次透過 netlink 去取得該網路介面的物件最後透過 netlink 的方式把該介面接上已經創建好的 Linux Bridfge 接下來一個步驟一個步驟試試看 首先透過 官方函式庫 提供的功能來取得 netns 的物件，其參數就是我們在執行時傳入的 /var/run/netns/ns1  netns, err := ns.GetNS(args.Netns) if err != nil { return err }  接者我們可以透過 netns.Do 的方式於該 netns 內執行任意 function. 所以先定義一個 function (handler)，該 function 必須要能夠創建一對 veth 並且收集到創建後的另外一端名稱 vethxxxx  hostIface := &amp;current.Interface{} var handler = func(hostNS ns.NetNS) error { hostVeth, _, err := ip.SetupVeth(args.IfName, 1500, hostNS) if err != nil { return err } hostIface.Name = hostVeth.Name return nil } if err := netns.Do(handler); err != nil { return err }  對 netns.Do 有興趣的可以觀看其原始碼，該實作內會取得當前 host 的 netns 並且傳入到參數的函式中。 上述流程其實可以把 current.Interface 物件單純換成字串就好，因為我們這個範例中只有要收集 interface name, 沒有其他的網卡資訊。 接下來就是透過 netlink 將該 interface name 轉換成相關的物件，以利後面的 LinkSetMaster操作  hostVeth, err := netlink.LinkByName(hostIface.Name) if err != nil { return err } if err := netlink.LinkSetMaster(hostVeth, br); err != nil { return err }  最後依序執行下列的步驟，先清除先前創立過的所有資源，然後手動創建一個 netns。 要注意 netns 的名稱 ns1 必須要與參數 CNI_NETNS 後面的名稱一致。 # Teardown all resoureces $ sudo ip netns del ns1 $ sudo ifconfig test down $ sudo brctl delbr test # Create network namespace $ sudo ip netns add ns1 $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=`pwd` ./mycni &lt; config $ sudo brctl show test bridge name bridge id STP enabled interfaces test 8000.6a5cc34310be no veth99b22b47 $ sudo ip netns exec ns1 ifconfig -a eth10 Link encap:Ethernet HWaddr 96:7c:33:2b:f3:42 inet6 addr: fe80::947c:33ff:fe2b:f342/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1 errors:0 dropped:0 overruns:0 frame:0 TX packets:1 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:90 (90.0 B) TX bytes:90 (90.0 B) lo Link encap:Local Loopback LOOPBACK MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  跑完這個範例我們就已經順利的建立好相關的橋樑，將 host 與 network namespace 透過虛擬連結 veth 給打通了。 最後一件事情就是設立該 network namespace 裡面使用的 IP 地址 ","version":"Next","tagName":"h2"},{"title":"Step 4​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-golnag#step-4","content":"由於先前的 config 以及相關的結構已經有將 IP 的欄位給設定好了，因此接下來我們只要針對 設定IP 這個步驟進行探討 創建 veth 後我們還需要額外取得當前 eth10 的 netlink 物件，這樣才可以透過 netlink 對該物件進行 IP 的設定netlink 設定 IP 的方式是透過物件 ip.IPNet, 這邊要怎麼創造這個物件方法百百種，也跟你怎麼設計自己的 config 有關。 跟剛剛上述不同，這次創建 veth 配對的時候，我們第二個物件也要一併收集 containerVeth，代表的就是 eth10 這張網卡。  hostVeth, containerVeth, err := ip.SetupVeth(args.IfName, 1500, hostNS) if err != nil { return err } hostIface.Name = hostVeth.Name  因為 config 內目前的設計是 192.168.1.12/24 這種 CIDR 的格式，所以我直接採用 net.ParseCIDR 的方式來解讀該格式，並且可以直接取得 ip.IPNet 的物件。 由於 ParseCIDR 產生後的 IPNet 物件，放置的是網段內容並非 IP 資訊，我們需要將 IP 的部分重新覆蓋 假如我們傳進去的參數是 192.168.2.12/24, 則創建出來的 ip.IPNet 會長  IPNet{ 192.0.2.0/24 }  但是要傳入給 netlink 的物件，我們希望是 IPNet{ 192.0.2.12/24 }  因此需要把 IP欄位重新設定  ipv4Addr, ipv4Net, err := net.ParseCIDR(sb.IP) if err != nil { return err } ipv4Net.IP = ipv4Addr  最後透過 netlink 的方式先把 eth10 的物件找出來，接者使用 netlink 的方式去設定 IP 地址。  link, err := netlink.LinkByName(containerVeth.Name) if err != nil { return err } addr := &amp;netlink.Addr{IPNet: ipv4Net, Label: &quot;&quot;} if err = netlink.AddrAdd(link, addr); err != nil { return err }  $ cat config { &quot;name&quot;: &quot;mynet&quot;, &quot;BridgeName&quot;: &quot;test&quot;, &quot;IP&quot;: &quot;192.0.2.12/24&quot; } # Teardown all resoureces $ sudo ip netns del ns1 $ sudo ifconfig test down $ sudo brctl delbr test # Create network namespace $ sudo ip netns add ns1 $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=`pwd` ./mycni &lt; config $ sudo brctl show test bridge name bridge id STP enabled interfaces test 8000.6a5cc34310be no veth99b22b47 $ sudo ip netns exec ns1 ifconfig -a eth10 Link encap:Ethernet HWaddr 9a:f9:1c:98:9b:7c inet addr:192.0.2.12 Bcast:192.0.2.255 Mask:255.255.255.0 inet6 addr: fe80::98f9:1cff:fe98:9b7c/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1 errors:0 dropped:0 overruns:0 frame:0 TX packets:1 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:90 (90.0 B) TX bytes:90 (90.0 B) lo Link encap:Local Loopback LOOPBACK MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  這時候你如果嘗試使用 ping 去測試剛剛創建好的 IP，你會發現完全打不通，主要問題有兩個 系統上沒有配置適當的 routing跨網段連接沒有對應的 gateway 幫忙轉發 最簡單的辦法就是幫 Linux Bridge 設定一個 IP，譬如 192.0.2.1 即可。 $ ping 192.0.2.15 PING 192.0.2.15 (192.0.2.15) 56(84) bytes of data. ^C --- 192.0.2.15 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms $ sudo ifconfig test 192.0.2.1/24 $ ping 192.0.2.15 PING 192.0.2.15 (192.0.2.15) 56(84) bytes of data. 64 bytes from 192.0.2.15: icmp_seq=1 ttl=64 time=0.038 ms 64 bytes from 192.0.2.15: icmp_seq=2 ttl=64 time=0.023 ms  Summay 經過了四個簡單的範例我們成功的撰寫了一個基於 CNI 標準的解決方案，內容非常簡單就是將 host 與 network namespace 連接起來並且設定IP。 目前的做法還有很多問題需要改善 相關的 Routing 沒有設定，封包出不去也進不來沒有設定相關的 SNAT, network namespace 內的封包可能出不去IP 完全寫死，這意味如果針對第二個 network namespace 去執行就會發生 IP 相同且衝突的問題 所以為了完成一個堪用的 CNI，背後要做的事情其實滿多的，為了讓網路可以於各式各樣的環境內都可以正常使用，這部分需要做很多的處理與判斷。 下文章我們要來探討 IP 分配的問題，看看目前官方維護的三套 IPAM 分別是哪些以及如何運作才可以避免各種 IP 衝突且寫死的問題。 參考 https://github.com/hwchiu/CNI_Tutorial_2018https://github.com/containernetworking/pluginshttps://github.com/containernetworking/cni ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/cni-ipam","content":"","keywords":"","version":"Next"},{"title":"DHCP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-ipam#dhcp","content":"首先介紹的第一個 IPAM 就是 DHCP，這個 DHCP IPAM 我自己只有拿來做一些 POC 測試的時候玩過，其使用限制不少，並不太容易直接整合到大部分的使用情境中。 一個 DHCP 的服務流程需要有兩個元件分別是 dhcp clientdhcp server 而這個 DHCP IPAM 扮演的角色就是 DHCP Client，而 dhcp client 按照慣例的運作模式就是 發送 DHCP Request等待 DHCP Reply設定 IP 到目標網路介面定期 Renew 而 DHCP IPAM 實實在在的扮演上面四個角色，而這邊就有一個問題了，這個 IPAM 會幫你設定 IP，因為 DHCP 會需要定期 renew，同時有更換 IP 的話就會自動幫你替換掉，這個是正常的行為。 由這邊可以知道不同的 IPAM 的運作行為不同，所以使用前一定要確認其使用方法與情境。 另外一個問題就是 DHCP 的封包，在預設的情況下是 Layer2 的封包，沒有任何的 dhcp relay 的幫忙的話，你的 DHCP Request 很難送到外面的 dhcp server 來取得一個 IP，所以這個 DHCP IPAM 的官方文件有特別說明 With dhcp plugin the containers can get an IP allocated by a DHCP server already running on your network. This can be especially useful with plugin types such as macvlan. 一種使用情境是直接透過 macvlan 的方式把 host 上的網路介面與 network namespace 共用，這樣從 network namespace 出去的封包就會直接從該網路介面出去。 接下來來探討一下整個 DHCP IPAM 的運作模式，該專案本身提供個兩種運作模式，一種是單純的 CNI 模式，一種則是一個不停運行的 daemon 模式。 daemon 模式的功用很簡單，接受所有來自 CNI 模式的請求，然後切換到目標的 network namespace 裡面去根據目標的網路介面發送一個 DHCP 請求封包。 所以運行這個 DHCP IPAM 之前，要先在系統上跑一隻 daemon，然後會透過 unix socket 的方式等待 DHCP IPAM CNI 發送命令過來，當然該命令會包含 目標的 network namespace目標的 網路卡名稱 整個運作流程可以歸納為下圖 首先當該 DHCP CNI被呼叫後，會先透過 unix socket 的方式通知 daemondaemon 接者潛入到該 netns 之中，確認該 Interface 存在後，就開始發送 DHCP 請求這邊我用一個 magic 的意思代表沒有限定外表要怎麼實作，總之你的 DHCP 封包要有辦法出去就好最後外面的(甚至同一台機器)上面的 DHCP Server 可以看到 Request 並且回覆 Reply最後當 DHCP Daemon 發送 DHCP 請求的那隻 thread 接收到 DHCP 回覆後，就會幫目標網卡設定 IP 地址。 最後，這個 IPAM 沒有這麼好用，光是那個 magic 的部分就不是一般使用者習慣的用法，我認為可以當作研究並增廣見聞即可。 ","version":"Next","tagName":"h2"},{"title":"Static​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-ipam#static","content":"這個 IPAM 其實沒有什麼好說，就是一個測試用的 IPAM，根據其 GitHub 上面的介紹 Overview static IPAM is very simple IPAM plugin that assigns IPv4 and IPv6 addresses statically to container. This will be useful in debugging purpose and in case of assign same IP address in different vlan/vxlan to containers. 就是一個除錯使用的 IPAM，我覺得唯一可以看的就是格式內容，完全可以補足我們前篇文章所設計的用法，將其擴大到更完整。 首先裡面分成三大塊，分別是 IP 地址，包含了 ipv4/ipv6Route 路由表DNS 設定 如果你對於上述三個概念都熟悉的話，其實下面的設定檔案不太需要講，大概看過就知道代表什麼意思。 另外要注意一下的是這個 IPAM 的運作模式就比較上述講述的，只專心在分 IP 地址，本身沒有任何設定的功能。 所以呼叫者最後要根據回傳的資訊自己去決定要怎麼設定。 { &quot;name&quot;: &quot;test&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;static&quot;, &quot;addresses&quot;: [ { &quot;address&quot;: &quot;10.10.0.1/24&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot; }, { &quot;address&quot;: &quot;3ffe:ffff:0:01ff::1/64&quot;, &quot;gateway&quot;: &quot;3ffe:ffff:0::1&quot; } ], &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }, { &quot;dst&quot;: &quot;192.168.0.0/16&quot;, &quot;gw&quot;: &quot;10.10.5.1&quot; }, { &quot;dst&quot;: &quot;3ffe:ffff:0:01ff::1/64&quot; } ], &quot;dns&quot;: { &quot;nameservers&quot; : [&quot;8.8.8.8&quot;], &quot;domain&quot;: &quot;example.com&quot;, &quot;search&quot;: [ &quot;example.com&quot; ] } } }  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-ipam#example","content":"kubeadm 本身沒有內建這個 CNI 執行檔，需要的要自行去官方下載或是自行編譯安裝。 假設有這個檔案後，我們可以直接使用之前執行 CNI 的方式來執行該檔案，先把上述的設定存成一個名為 static 的檔案。 最後可以觀察其輸出結果，這些結果理論上是呼叫他的 CNI 去解讀，然後根據需求去設定 IP, Route, DNS 這些資源。 $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=/opt/cni/bin/ /opt/cni/bin/static &lt; static { &quot;cniVersion&quot;: &quot;0.2.0&quot;, &quot;ip4&quot;: { &quot;ip&quot;: &quot;10.10.0.1/24&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }, { &quot;dst&quot;: &quot;192.168.0.0/16&quot;, &quot;gw&quot;: &quot;10.10.5.1&quot; } ] }, &quot;ip6&quot;: { &quot;ip&quot;: &quot;3ffe:ffff:0:1ff::1/64&quot;, &quot;gateway&quot;: &quot;3ffe:ffff::1&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;3ffe:ffff:0:1ff::1/64&quot; } ] }, &quot;dns&quot;: { &quot;nameservers&quot;: [ &quot;8.8.8.8&quot; ], &quot;domain&quot;: &quot;example.com&quot;, &quot;search&quot;: [ &quot;example.com&quot; ] } }  ","version":"Next","tagName":"h3"},{"title":"Host-Local​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-ipam#host-local","content":"最後終於要講最重要的 IPAM 了，其使用率也是頗高的，滿多的 CNI 會使用這個 IPAM 作為基底去處理 IP 分配的問題，因此這邊來好好的研究一下這個 IPAM。 ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-ipam#example-1","content":"開始研究其特色之前，我們直接先直接運行一個簡單的範例 準備一個 config 給 host-local呼叫 host-local cni，觀察其結果呼叫 host-local cni，觀察其結果呼叫 host-local cni，觀察其結果 上面是認真的要呼叫三次，來觀察呼叫三次會有什麼不一樣的結果 首先我們先觀察一下其設定檔案，裡面相對於 static 來說，裡面最大的不一樣是出現了 range, subnet 之類的字眼 $ cat config { &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;ranges&quot;: [ [ { &quot;subnet&quot;: &quot;10.10.0.0/16&quot;, &quot;rangeStart&quot;: &quot;10.10.1.20&quot;, &quot;rangeEnd&quot;: &quot;10.10.3.50&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot; }, { &quot;subnet&quot;: &quot;172.16.5.0/24&quot; } ] ] } }  接者我們就運行該 host-local CNI 三次，看看三次的結果如何 $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=/opt/cni/bin/ /opt/cni/bin/host-local &lt; config { &quot;cniVersion&quot;: &quot;0.2.0&quot;, &quot;ip4&quot;: { &quot;ip&quot;: &quot;10.10.1.20/16&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot; }, &quot;dns&quot;: {} } $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=/opt/cni/bin/ /opt/cni/bin/host-local &lt;config { &quot;cniVersion&quot;: &quot;0.2.0&quot;, &quot;ip4&quot;: { &quot;ip&quot;: &quot;10.10.1.21/16&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot; }, &quot;dns&quot;: {} } $ sudo CNI_COMMAND=ADD CNI_CONTAINERID=ns1 CNI_NETNS=/var/run/netns/ns1 CNI_IFNAME=eth10 CNI_PATH=/opt/cni/bin/ /opt/cni/bin/host-local &lt;config { &quot;cniVersion&quot;: &quot;0.2.0&quot;, &quot;ip4&quot;: { &quot;ip&quot;: &quot;10.10.1.22/16&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot; }, &quot;dns&quot;: {} }  輸出的結果非常的有趣，每次的輸出內容幾乎都一樣，除了 ip4.ip 這個欄位之外有些許差別，分別是10.10.1.20/16, 10.10.1.21/16, 10.10.1.22/16 同時我們在複習一下剛剛設定裡面的 range.subnet 相關設定 &quot;subnet&quot;: &quot;10.10.0.0/16&quot;, &quot;rangeStart&quot;: &quot;10.10.1.20&quot;, &quot;rangeEnd&quot;: &quot;10.10.3.50&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot;  看到這邊應該心裡已經有個譜了， host-local 會根據參數給予的 IP 範圍，依序回傳一個沒有被使用過的 IP， 這個運作原理非常的符合我們真正的需求，每次有 POD 產生的時候都可以得到一個沒有被使用過的 IP 地址，避免重複同時又能夠使用。 接下來我們來正式的研究這個 IPAM CNI，看看其設計上還有什麼樣的特色與注意事項 ","version":"Next","tagName":"h3"},{"title":"Introduction​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/cni-ipam#introduction","content":"如慣例一樣，我們先看看官方 GitHub 怎麼描述這個專案 host-local IPAM plugin allocates ip addresses out of a set of address ranges. It stores the state locally on the host filesystem, therefore ensuring uniqueness of IP addresses on a single host. The allocator can allocate multiple ranges, and supports sets of multiple (disjoint) subnets. The allocation strategy is loosely round-robin within each range set. 擷取幾個重點 從 address ranges 中分配 IP將分配的結果存在本地機器，所以這也是為什麼叫做 host-local 其中(2)算是一個手段，用來滿足(1)，畢竟如果沒有地方進行紀錄來進行比較，就沒有辦法每次都回傳一個沒有被用過的 IP 地址。 接下來看一個比較完整的設定檔案 { &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;ranges&quot;: [ [ { &quot;subnet&quot;: &quot;10.10.0.0/16&quot;, &quot;rangeStart&quot;: &quot;10.10.1.20&quot;, &quot;rangeEnd&quot;: &quot;10.10.3.50&quot;, &quot;gateway&quot;: &quot;10.10.0.254&quot; }, { &quot;subnet&quot;: &quot;172.16.5.0/24&quot; } ], [ { &quot;subnet&quot;: &quot;3ffe:ffff:0:01ff::/64&quot;, &quot;rangeStart&quot;: &quot;3ffe:ffff:0:01ff::0010&quot;, &quot;rangeEnd&quot;: &quot;3ffe:ffff:0:01ff::0020&quot; } ] ], &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }, { &quot;dst&quot;: &quot;192.168.0.0/16&quot;, &quot;gw&quot;: &quot;10.10.5.1&quot; }, { &quot;dst&quot;: &quot;3ffe:ffff:0:01ff::1/64&quot; } ], &quot;dataDir&quot;: &quot;/run/my-orchestrator/container-ipam-state&quot; } }  這裡面我認為相對有趣的事情有 支援 ipv6, 其支援 ipv6 的速度遠早於 kubernetes 1.16，這意味之前其實就可以透過 host-local 的方式去分配 ipv6 address， 只是 kubernetes 內部的所有功能都還是基於 ipv4，變成使用上沒有整合很不方便dataDir 的變數會指定要用哪個資料夾作為 host-local 記錄用過的資訊，預設值是 /var/lib/cni/networks/. 根據上述簡單的範例，因為我沒有特別指定 dataDir，所以所有的檔案都會存放在 /var/lib/cni/networks/ 裡面 $ sudo find /var/lib/cni/networks/ -type f /var/lib/cni/networks/last_reserved_ip.0 /var/lib/cni/networks/10.10.1.20 /var/lib/cni/networks/10.10.1.22 /var/lib/cni/networks/10.10.1.21 $ sudo find /var/lib/cni/networks/ -type f | xargs -I % sh -c 'echo -n &quot;%: -&gt;&quot;; cat %; echo &quot;&quot;;' /var/lib/cni/networks/last_reserved_ip.0: -&gt;10.10.1.22 /var/lib/cni/networks/10.10.1.20: -&gt;ns1 /var/lib/cni/networks/10.10.1.22: -&gt;ns1 /var/lib/cni/networks/10.10.1.21: -&gt;ns1  我們可以觀察到，每個被用過的 IP 都會產生一個以該 IP 為名的檔案，該檔案中的內容非常簡單，就是使用的 container ID，由於我目前的範例非常簡單，所以資訊不夠豐富，等之後我們探討 kubernetes 的使用情境後，就可以再次觀察這個欄位。 此外，還可以觀察到一個名為 last_reserved_ip 的檔案，該檔案用來記住每個 range 目前分配的最後一個 IP 是哪個。 目前 host-local 分配的演算法是 round-robin，對演算法有興趣的可以參考下方的原始碼 // GetIter encapsulates the strategy for this allocator. // We use a round-robin strategy, attempting to evenly use the whole set. // More specifically, a crash-looping container will not see the same IP until // the entire range has been run through. // We may wish to consider avoiding recently-released IPs in the future. func (a *IPAllocator) GetIter() (*RangeIter, error) { ....  最後我們再來思考一個問題，今天我們可以使用 range.subnet 這類的設定檔案讓 host-local來幫我們分配 IP 地址，避免重複的問題。 但是前述有提過， CNI 本身是每個節點都要配置的，所以如果今天每個節點都使用一樣的設定檔，會發生什麼事情? 基於 round-robin 的演算法下，就會發生不同節點上的 Pod 使用到相同的 IP 地址，這樣問題還是沒有解決。 為了解決這個問題，唯一的辦法就是每台節點上面都要部署不同內容的設定檔案，譬如第一個節點使用 10.0.1.0/24，第二個使用 10.0.2.0/24，諸如此類的方式。 這樣的使用雖然可以解決問題，但是對於安裝與部署來說又產生其他的困擾，如果今天有舊的節點要移除，新的節點要進來，也要確保設定檔案沒有重複，不然 IP 問題就會繼續浮上來。 只能說這種分散式的東西本身在處理與使用上就要格外小心，沒有集中控制的管理就容易導致群龍無首然後各自為王。 當然要解決這個問題也是有其他的辦法，下一篇會來探討 ｆlannel 的基本安裝過程，並且探討一下 flannel 是如何解決對所有節點上的 Pod 都能夠分配一個不重複的 IP 地址。 Summary 本篇文章介紹三種不同官方提供的 IPAM 解決方案，這些解決方案也都基於 CNI 的標準去設計，所以相容彼此的參數傳遞以及結果回傳。這使得這些 IPAM 能夠與其他的 CNI 更好整合，藉由分層的概念讓 IPAM 專心處理 IP 管理分配的問題，而其他的 CNI 則是專注於如何建立網路資源，確保目標 network namespace 可以獲得想要的上網能力。 到這一邊我們已經對 CNI 有一些基本概念了，接下來我們要實際演練一台具有三個節點的 kubernetes cluster 與 Flannel CNI 是如何運作的，包含了安裝過程，設定檔案內容，到最後封包的轉發是如何做到跨節點存取的。 參考 https://github.com/containernetworking/pluginshttps://github.com/containernetworking/cnihttps://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/container-design-i","content":"","keywords":"","version":"Next"},{"title":"Runtime Spec​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-i#runtime-spec","content":"Runetime Spec 的文件可以在 Github 上參閱， Runtime Spec 規範的範疇有三，分別是 設定檔案格式 該內容基本上會根據不同平台而有不同的規範，同時也明確表示創建該平台上 container 所需要的一切參數。 執行環境一致 確保 container 運行期間能夠有一致的運行環境。 Container的生命週期 必須支持下列的操作行為，這邊不再贅述，可以到此閱覽 Query StateCreateStartDeleteKillHooks Configuration​ 講到設定檔案的部分，我們可以稍微看一下到底在 Linux 上創建一個 Container 會需要哪些設定。 首先是資源隔離，這部分是仰賴 Linux Kernel 內的 namespace 來完成的，藉由資源隔離可以達到讓 namespace 內部與 host 本身互相使用該資源卻不衝突的優點。 根據上述的規範，會需要用到的 namespace 如下。 pid process 相關的隔離。不知道大家是否有觀察到在 docker container 的世界內，用 ps auxw 能夠看到的 process 比想像中的少? network 精準來說是 Network Stack 的隔離，體現出來簡單的範例就是 網卡,iptables 規則 等諸多與網路有關的資源。 ipc Inter Process Communication 意味者 namespace 內的 process 只能跟同 namespace 內的其他 process 透過 IPC 的機制溝通 mount 常常使用 mount 指令的人就可以發現到 docker container 內與外面看到的結果不同也是因為該 namespace 將資源區隔了 uts UNIX Timesharing System 主要負責的就是 hostname, 包含給 NIS 使用的 domain name user 使用者與群組清單，透過 id 指令等對應到的 UID/GID 都是獨立的 cgroup Control Group，主要是跟資源存取限制有關，譬如 CPU，Memory 等資源的存取上限。再 docker container 的使用中就可以透過該機制設定一些 soft, hard/limit memory 等設定來避免 OOM 或是減少 CPU 存取。 每個 namespace 都有自己的設定檔案，因此 Configuration 在設定的時候則必須要仔細的設定每個 namespace 的資訊。 譬如  { &quot;type&quot;: &quot;pid&quot;, &quot;path&quot;: &quot;/proc/1234/ns/pid&quot; }, { &quot;type&quot;: &quot;network&quot;, &quot;path&quot;: &quot;/var/run/netns/neta&quot; }, { &quot;type&quot;: &quot;mount&quot; }, { &quot;type&quot;: &quot;ipc&quot; }, { &quot;type&quot;: &quot;uts&quot; }, { &quot;type&quot;: &quot;user&quot; }, { &quot;type&quot;: &quot;cgroup&quot; }  除了 namespace 之外，container 內部也還有許多的資源需要設定，譬如 Devices, Sysctl, Seccomp(SECure COMPuting)，有興趣的請記得參閱官方文件。 到這邊對於 Runtime Spec 已經有一些基本的認知，用來定義不同平台上如何管理 Container，為了管理這些 Container 必須要有一些相關的設定來描述該 Container 的資訊，而這些設定本身並非跨平台，需要針對每個平台去獨立設計來描述如何產生一個符合需求的 Container. ","version":"Next","tagName":"h2"},{"title":"Image Spec​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-i#image-spec","content":"接下來來看一下到底所謂的 Image 是什麼，我們常用的 Docker pull/push/build/images 所產生的 image 檔案是怎麼被定義的。 下圖片清楚明瞭的說明到底 Image 要處理什麼事情(圖片擷取自：GitHub opencontainers/image-spec) 一個簡單的 Java 應用程式包裝成 Container 後，整個 Image Layer 處理了下列事情 Layer: 相關的檔案系統配置，檔案的位置/內容/權限Image Index: 用來描述該 Image 的檔案Configuration: 應用程式相關的設定檔案，包含了使用的參數，用到的環境變數等 如果仔細去研究的話，其實總共會有7大相關的類別被標準化，分別是 Image Manifest - a document describing the components that make up a container imageImage Index - an annotated index of image manifestsImage Layout - a filesystem layout representing the contents of an imageFilesystem Layer - a changeset that describes a container's filesystemImage Configuration - a document determining layer ordering and configuration of the image suitable for translation into a runtime bundleConversion - a document describing how this translation should occurDescriptor - a reference that describes the type, metadata and content address of referenced content 如果以前有稍微留意過 Docker Images 的話就會觀察到有很多個 Layer 的產生，每個 Layer 都有獨立的 Image ID，這邊有興趣的可以參閱 Filesystem Layer 來了解更多，到底這些 Layer 代表甚麼，以及底層到底怎麼實現。 每個類別在官方文件中都由獨立的檔案去描述，有興趣的可以自行參閱來理解每個部分實際上的關係。 ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-i#summary","content":"Open Container Initiative (OCI) 定義了 Runtime 以及 Image 兩大標準來規範 Container 的介面，本文中跟各位探討了 Runtime/Image 這兩標準大致上想要完成什麼，以及要如何完成。 下篇文章將會跟大家分享如果今天想要開發一個符合 OCI 標準的 Container 解決方案，到底有什麼工具可以直接使用而避免重造輪子，同時也能夠透過這些工具來理解到底 Docker 目前是怎麼與 OCI 標準銜接的。 Reference https://www.slideshare.net/jeevachelladhurai/runc-open-container-initiativehttps://github.com/opencontainers/runtime-spec/blob/master/config-linux.mdhttps://github.com/opencontainers/image-spec/blob/master/spec.md ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii","content":"","keywords":"","version":"Next"},{"title":"Runtime​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii#runtime","content":"OCI 官方基於 Runtime 規範 實現了一個解決方案，稱為 RunC, 根據其官方文件的說明 runc is a CLI tool for spawning and running containers according to the OCI specification. 這意味者藉由這套工具，並且搭配適宜的設定，就可以輕鬆的創建出一個符合OCI 標準的 Container 運行。 但是單純的 CLI 工具並不一定適合所有的開發者，部分的開發者可能只希望擁有一套能夠符合 OCI 標準的相關函示庫可以使用，這時候要可以使用 libcontainer這套由官方維護並且以 golang 撰寫的函示庫，根據其說明文件 Libcontainer provides a native Go implementation for creating containers with namespaces, cgroups, capabilities, and filesystem access controls. It allows you to manage the lifecycle of the container performing additional operations after the container is created. 透過這個函式庫，開發者可以輕鬆的撰寫出滿足整個 container 的生命週期，同時也能夠創建相關的 namespace/cgroups 等的程式碼，並且將心力專注在更上層的服務提供。 除了 runC 這套實現方案之外，官方GitHub 可以看到目前官方收錄的所有 Runtime Spec 的實現方案，這些方案有些由 OCI 組織本身維護，有些由其他組織維護。 其中特別有趣的就是這些實現方案目前分為兩大類，分別是 Runtime (Container) 以及 Runtime (Virtual Machine). 其中 Container 就是我們一直在探討的 Container 而 Virtual Machine 這類型則是透過 Virtual Machine 相關的技術去完成虛擬化的環境，但是同時又符合 OCI 的標準。 這意味者使用者可以創建 Contaienr 來使用，但是其底層是以 Virtual Machine 的技術創建出來的。 這相關的概念其實不難想像，畢竟 Container 一直以來被認為不夠安全，畢竟其部分功能都是依賴 Host 上的 Kernel 來實現，其隔離能力沒有 Virtual Machine 這麼明確。 所以如何打造一個速度又快，安全度又高的虛擬化環境一直以來都是一個探討的議題。 該清單中的 google/gvisor 以及 kata-containers 都算是滿知名的專案 有興趣的讀者可以自行研究這些技術底層並看看各大專案是希望如何實現高效能,高安全 的虛擬化環境。 ","version":"Next","tagName":"h2"},{"title":"Image​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii#image","content":"Image 的部分也有相對應的工具可以使用，一樣由官方 GitHub 進行維護,該文件中會介紹如何搭配 skopeo 等工具來完成一個關於 Image 相關的案例。 此外，也有其他的專案如 buildah 也針對 OCI Image 的部分提供一些解決方案 Buildah - a tool that facilitates building Open Container Initiative (OCI) container images Docker 對於 Open Container Initiative (OCI) 有基本概念之後，接下來就要探討作為 OCI 重大貢獻者的 docker (libcontainer, image spec...etc)，是如何在其架構中透過何種方式跟來創建基於 OCI 介面的 Container. 下圖是一個滿棒的架構圖，當有了 OCI 的概念後再來看這張圖會覺得親切許多。 (圖片擷取自：blod.docker.com - docker-engine-1-11-runc) 這張圖片的右半部分標出了四個不同層級的概念，分別是 Docker UI/CommandsDocker EngineContainerdRunc ","version":"Next","tagName":"h2"},{"title":"Docker UI/Commands​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii#docker-uicommands","content":"大家最為熟悉的 docker 指令其實在整個 Docker 的架構中扮演了所謂了 client 的角色，負責將使用者的需求(指令)打包，並且與後方的 server 溝通 這邊除了常用的 docker run/build/image/exec/attach...etc 等直接使用的 CLI 工具外，也是有相關的函式庫可以供開發者使用，將自己的應用程式直接與 Docker Server 連動來溝通。 在預設的情況下，docker 指令都會透過 unix socket 與本地的 docker engine 溝通，這個部分可以透過環境變數來描述，譬如 export DOCKER_HOST=tcp://192.168.0.123:2376 docker run  ","version":"Next","tagName":"h2"},{"title":"Docker Engine​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii#docker-engine","content":"當系統內安裝 Docker 後，你可以透過系統指令 ps 觀察到系統上會有一個名為 dockerd 的程序 root 2487 0.6 2.2 694888 90000 ? Ssl 22:26 0:11 dockerd -G docker --exec-root=/var/snap/docker/384/run/docker --data-root=/var/snap/docker/common/var-lib-docker --pidfile=/var/snap/docker/384/run/docker.pid --config-file=/var/snap/docker/384/config/daemon.json --debug  這個 server 就是所謂的 docker engine, 所有的 docker client 都會將指令送到這個 engine 進行相關整理。這一層級相對於 OCI 的層級還是算高，偏向上層的應用，所以特色還是以 Docker 自己的特色為主。 ","version":"Next","tagName":"h2"},{"title":"Containerd​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii#containerd","content":"當 Docker Engine 收到指令後就會將指令往後傳送到 containerd 進行處理。 相對於 Docker Engine, containerd 則更面向 OCI 標準，向上提供 gRPC 接口供 Docker Engine 使用，向下則是根據需求創建符合 OCI 標準的 Container. 就如同昨天所述， Runtime spec 目前有眾多的實現方案可以選擇，而最知名且由 OCI 組織維護的就是 runc. 所以 Containerd 本身也會透過這些現有的解決方案來創建符合 OCI 標準的 Container. root 2571 0.6 0.8 558432 35808 ? Ssl Sep12 0:39 docker-containerd --config /var/snap/docker/384/run/docker/containerd/containerd.toml  ","version":"Next","tagName":"h2"},{"title":"Containerd-Shim​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii#containerd-shim","content":"此外，為了滿足一些軟體設計上的需求，containerd 並沒有直接呼叫 runc，反而是中間會在填補一層所謂的 containerd-shim, containerd 會創建一個獨立的 process containerd-shim 並由其呼叫 runc 來真正創建 container. 根據下列 dockercon-2016 相關的演講，我們可以歸納出下列為什麼需要 containerd-shim 的理由 daemonless 將 container 運行與 docker 分開，這意味者 docker 升級的過程中這些運行的 container 並不會被影響，可以繼續使用。 因為 docker engine/containerd 目前都是屬於 docker 套件的程式。 re-parenting 當 runc 創建出 container 後可以直接讓 runc 離開，並且把其程序的 process 交由更上層的祖父去管理，這個情況中我們就可以讓 containerd-shim 去管理。此外假設當 containerd 意外重啟後，則新的 containerd-shim 可以交由 init 去管理，藉此做到系統更新而不影響現存的 container tty/stdin 為了處理 container 本身的輸入問題，則會用 FIFO 這種 IPC的方式再 parent &amp; child process 中溝通。所以我們將 parent 的重責大任就交給了 containerd-shim 上 關於 re-parenting 的演變可以直接參閱該份投影片，如下(圖片擷取自：dockercon-2016)(圖片擷取自：dockercon-2016) 由上面的概念可以知道，每個 containerd-shim 都會對應到一個 container, 因此當透過 docker run 的方式來運行容器後，系統就會產收一個 container-shim 相關的應用程式. 可以使用以下範例創建多個容器，然後觀察相關的 containerd-shim 的狀態 sudo docker run -d hwchiu/netutils sudo docker run -d hwchiu/netutils sudo docker run -d hwchiu/netutils sudo docker run -d hwchiu/netutils ps auxw | grep docker-containerd-shim | wc -l ps auxw | grep docker-containerd-shim  root 11732 0.0 0.1 7380 4420 ? Sl 18:17 0:00 docker-containerd-shim -namespace moby -workdir /var/snap/docker/common/var-lib-docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/a12e5594d0d666759c51b2420db0e361649a39b43aa6b5e928382c69381be0a0 -address /var/snap/docker/384/run/docker/containerd/docker-containerd.sock -containerd-binary /snap/docker/384/bin/docker-containerd -runtime-root /var/snap/docker/384/run/docker/runtime-runc -debug  ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-ii#summary","content":"此外，上述所有的元件在最後於 docker 的環境中都有重新命名，包含了 containerd -&gt; docker-containerdcontainerd-shim -&gt; docker-containerd-shim 用下列架構圖來重新說明一次 Docker 內部的構造以及是如何創建出符合 OCI 標準的容器 Reference https://blog.docker.com/2017/08/what-is-containerd-runtime/http://alexander.holbreich.org/docker-components-explained/https://github.com/crosbymichael/dockercon-2016/blob/master/Creating%20Containerd.pdfhttps://ops.tips/blog/run-docker-with-forked-runc/#forking-runchttps://medium.com/tiffanyfay/docker-1-11-et-plus-engine-is-now-built-on-runc-and-containerd-a6d06d7e80ef ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/container-design-iii","content":"","keywords":"","version":"Next"},{"title":"Namespace​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-design-iii#namespace","content":"Network namespace 本身隔離了 Network Stack, 這意味包含了 interface, ip address, iptagbles, route 等各式各樣跟網路有關的資源都被隔離。 接下來我們可以做一個簡單的操作來看看，再操作上我們都會使用 ip netns 的指令來使用 #create network namespace ns1 ip netns add ns1 ##exec in ns1 ip netns exec ns1 bash #check interface ifconfig -a  這時候你應該會看到類似下面的畫面。 lo: flags=8&lt;LOOPBACK&gt; mtu 65536 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0  因為該新創建的 network namespace 是完全空的，所以裡面除了最基本的 loopback 之外不會有任何其他的網卡。 此外，這時候前往 /var/run/netns 你會觀察到有一個名為 ns1 的檔案，上述ip netns 相關的指令則會根據這個檔案進行處理。 docker 接下來嘗試創建一個 docker container, 並且觀察看看是否有辦法透過 ip netns 的方式來觀察該 container. sudo docker run -d hwchiu/netutils  這時候按照上述的方法去觀察 sudo ip netns ls sudo ls /var/run/netns  會發現完全沒有看到其他的資訊，依然只有先前創立的 ns1，原因是docker 創建 network namespace 後會將該檔案從 /var/run/netns/ 中移除，所以導致沒有辦法用 ip netns 相關的指令去檢視。 但是其實這些檔案一直都在系統之中，畢竟系統要運行，資訊也必須存在，所以我們可以透過一些方法把該檔案重新找回來，最後重新放回 /var/run/netns 中，最後就可以透過 ip netns 的方式來操作。 先取得待觀察之 container 的 containerID先取得該 Container Process 於 Host 上的 PID前往該 PID 於 /proc/xxxx/ns 底下找到所有的 namespace將上述發現的 namespace 建立連結到 /var/run/ns可以使用 ip netns 等指令來操作 sudo docker ps  hwchiu@k8s-dev:/var/run$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2be547d81b69 hwchiu/netutils &quot;/bin/bash ./entrypo…&quot; 6 minutes ago Up 6 minutes priceless_cray  container_id=2be547d81b69 pid=$(sudo docker inspect -f '{{.State.Pid}}' ${container_id}) sudo ln -sfT /proc/$pid/ns/net /var/run/netns/${container_id} sudo ls /proc/19265/ns sudo ls /proc/19265/ns/ sudo ip netns ls sudo ip netns exec ${container_id} ifconfig  這時候你應該會看到類似下面的輸出 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.18.0.2 netmask 255.255.0.0 broadcast 172.18.255.255 ether 02:42:ac:12:00:02 txqueuelen 0 (Ethernet) RX packets 14 bytes 1116 (1.1 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0  這時候可以嘗試使用 docker 系列的指令來觀察看到的資訊是否一致 sudo docker exec -it ${container_id} bash ifconfig  理論上我們先前透過 ip netns 操作的對象就是該 container 的 network namespace, 所以看到的資訊必須要一致且一樣的。 除了這個基本概念之外，在 docker 與 kubernetes 裡面都有一個網路選項是 net=hostnetwork, 這個的意思就是請不要創建額外的 network namespace,請使用與 host 相同的 network namespace. 這個情物下，你就可以在 container 內外都看到相同的網路資源 NIC, Route, IP, IPtables..etc Storage 常常使用 docker 的人一定對於 volume mount 這個概念不陌生，不論是 docker volume 更上層的抽象化或是單純運行時期掛載上去的 docker run -v xxx:xxx 等都能夠用來解決部分的 Container 內的需要的儲存問題 於 linux 底下，通常我們都會使用 mount 來處理檔案的掛載問題 首先我們先啟動一個簡單的 Container 來掛載一個外部的資料夾到 Container 內使用 sudo docker run -d -v /home/:/outside-home hwchiu/netutils  這時候透過本機的指令去檢查 host mount namespace 會完全看不到跟 /home 有關的任何資料 mount | grep home sudo cat /proc/self/mountinfo | grep home  這是因為該容器的 mount 相關資訊也都被 mount namespace 隔離了，就如同 networking 一樣，我們其實也可以在該 container process 的相關檔案中找到該資訊 #change the id to your container id container_id=b9428568d3ff pid=$(sudo docker inspect -f '{{.State.Pid}}' ${container_id}) sudo cat /proc/$pid/mountinfo | grep home sudo docker exec $container_id cat /proc/self/mountinfo | grep home  這時候就會看到相關的資訊，譬如 478 459 8:1 /home /outside-home rw,relatime - ext4 /dev/sda1 rw,data=ordered  反過來說，如果今天你知道目標的 ContainerID，你就可以透過類似的方式找到當初創建該 Container 時設定的相關 Mount 資訊 Mount 相關的概念非常龐大也非常複雜，我非常推薦有興趣的可以把這篇文章看完。 除了基本的 mount 的使用方法外，其實在 kubernetes 裡面還有一個 mount propagation 的設定可以使用，但是這個設定其實本身背後的概念並不簡單，一般的使用者基本上都不會碰到這個設定，但是一旦遇到的時候就會需要了。 此外對於 Container 來說，我們也可以觀察到其實 Contianer 本身不太去管到底怎麼跟外界的 Storage 串連的， 一切就是依賴 Mount Namespace 將這些儲存空間掛進去，至於你要採用什麼檔案系統，背後有什麼備援機制，都是 host 本身去管理， Container 本身不處理。 Summary 今天透過一些基本的 linux 工具帶大家稍微過了一下 docker container 底下關於 networking 以及 storage 的一些冷知識，跟大家分享平常在使用 docker container 時到底背後有哪些機制撐起了這複雜的 container 系統，同時藉由理解這些資訊，未來想要做更進一步的除錯也都可以有其他的工作來幫忙輔助ㄡ 除了 networking 以及 mount 外，還有其他的如 user, uts 等不同的 namespace 幫忙隔離其餘的系統資源以完成所謂的 container 虛擬化。 有興趣的人都可以針對其他的資源去研究看看要如何再 host 端存取相關的資訊並且學習更多底層的實作。 Reference http://man7.org/linux/man-pages/man5/proc.5.html ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-security-container","content":"","keywords":"","version":"Next"},{"title":"Machine-Based​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-security-container#machine-based","content":"我們先看一下大家熟悉的 Virtual Machine 通常怎麼做到安全隔離，這邊我們使用 KVM 的範例來看 圖片節錄自Architecture Guide KVM 本身基於 host 的主機上再創造一個全新的虛擬化環境，該環境中有一個全新的 Kernel, 所以名詞上就會有所謂的 Host Kernel 以及 Guest Kernel. 當然資訊安全本就沒有絕對，所有的服務都基於 kvm 的設計包含 kvm kernel module, 這些可能互動的過程中要是有問題也是會造成安全性漏洞。 不過今天討論的範疇在於虛擬化環境與 Host 本身是否會互相影響，所以暫時就先忽略其他的因素。 基於這種完全不同 Kernel 的架構， Guest 有發生任何事情，也都只會影響在 Guest Kernel 構築的環境中，不會影響到 Host Kernel 上。 ","version":"Next","tagName":"h2"},{"title":"Rule-Based​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-security-container#rule-based","content":"另外一種安全性的解決方案是則是透過 system call 的管理，譬如 SECure COMPuting (SECCOM) 這種技術，可以限制目標應用程式能夠呼叫到的 sysytem call，藉此確保該應用程式(Application/Container)本身沒有辦法去呼叫其不應該呼叫的 system call, 算是一種基於規則的安全性方案。 圖片節錄自Architecture Guide 這邊有一個其規則的範例檔案 ... { &quot;names&quot;: [ &quot;get_mempolicy&quot;, &quot;mbind&quot;, &quot;set_mempolicy&quot; ], &quot;action&quot;: &quot;SCMP_ACT_ALLOW&quot;, &quot;args&quot;: [], &quot;comment&quot;: &quot;&quot;, &quot;includes&quot;: { &quot;caps&quot;: [ &quot;CAP_SYS_NICE&quot; ] }, &quot;excludes&quot;: {} }, ...  然而因其設定複雜，實務上很難可以很準確的產生出各種設定檔案來套用到各式各樣的應用程式。 接下來就來看一下 gVisor 以及 kata Container 怎麼做出不同於以往的安全性架構 gVisor gVisor 作為相容於 OCI Runtime 的解決方案，理論上只要能夠支援 OCI 的上層服務應該都要可以直接使用 gVisor，所以之前提到的 containerd 以及 cri-o 都要可以切換 runtime 從基本的 runc 切換過來。 從其 官方 Github 的介紹 gVisor is a user-space kernel, written in Go, that implements a substantial portion of the Linux system surface. It includes an Open Container Initiative (OCI) runtime called runsc that provides an isolation boundary between the application and the host kernel. The runsc runtime integrates with Docker and Kubernetes, making it simple to run sandboxed containers. gVisor 最底層的 OCI Runtime 叫做 runsc，而其達到安全性隔離的手段則是透過所謂的 user-space kernel 的手段，接下來將透過介紹到底 gVisor 是怎麼 實現高安全性的 Container。 如果說 SECCOMP 是透過限制的方式禁止應用程式存取特別的 systel call, 那 gVisor 就是極端的把所有的 system call 完全都修改掉，讓你看起來有使用 system call，但是其實你使用的 system call 根本不是跟真正的 host kernel 溝通，而是跟 gVisor 所重新打造的 user-space kernel 溝通。 這也是為什麼其稱為 user-space kernel, 在 user-space 重新打造一個仿 kernel 的環境，架構如下。 圖片節錄自Architecture Guide 所有送到 gVisor 的 system call 都會被二次處理，接者才會送到真正的 Host Kernel 去取得需要的資訊。 官方文章特別表明這些不同的做法沒有明顯的優劣，各有擅長的領域，整句話如下，值得好好思考 Each of the above approaches may excel in distinct scenarios. For example, machine-level virtualization will face challenges achieving high density, while gVisor may provide poor performance for system call heavy workloads. Kata Container 與 gVisor 一樣，都是基於 OCI Runtime 的解決方案，這時候就會覺得有個 OCI 的標準真的是讓世界稍微美好了一些，各式各樣的解決方案都能夠專注於自己的開發，就可以很輕鬆地與其他的應用程式結合而不需要各種客製化。 從其 官網 的介紹 Kata Containers is an open source container runtime, building lightweight virtual machines that seamlessly plug into the containers ecosystem. 其開宗明義表明建置一個輕量化的 Virtual Machine 同時能夠銜接到 Container 的系統，接下來我們可以從 Kata Containers Architecture 看到更多關於其設計的架構 首先先看一下簡單的使用架構，左邊是基於完全 docker + runc 的場景，此架構已經在 Container &amp; Open Container Initiative 中跟大家介紹過，而最下面的 runc 就是所謂的 OCI Runtime，因此 kata container 中的 OCI Runtime kata-runtime 就可以無縫的替換掉 runc。 對使用者來說可以繼續使用習慣的 docker run/exec/attach 等指令而不會發現底層其實已經完全不同了。 圖片節錄自Kata Containers Architecture kata-container 的作法完全不同於 gVisor, 其就如同過往的 Virtual Machine 一樣，真正的去創造一個 VM 來隔離所謂的 Host Kernel 以及 Guest Kernel. 接者會在這個 VM 上面去運行使用者所請求的 Container，藉此達到一個外表看似 Container 實際上是一個運行在 VM 上的 Container. 相關的架構可以參考下圖圖片節錄自About Kata Containers 在此架構下，我們可以想像到如果今天有任何 container CLI 想要直接操控該 Container的話，問題就在於從 OCI Runtime 送出的指令要如何操控到該 VM 裡面的 Container. 於是你可以看到架構中有所謂的 Proxy 以及 Agent這兩個角色就是負責幫忙進行指令交換的，讓整個運作環境操作起來跟熟悉的 Container 一致，也符合最上面的專案介紹 seamlessly plug into the containers ecosystem. ","version":"Next","tagName":"h2"},{"title":"Network​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-security-container#network","content":"當基本的 Container 運行起來後，接下來就要思考網路的問題，目前有兩套網路解決方案·分別是 Docker 提出的 Container Network Management (CNM) 以及 kubernetes 開發後提出的 Container Network Interface (CNI)。 更多的細節會到之後 CNI 的章節再來仔細探討這些，這邊先用一個簡單的流程說明到底 Kata Container 遇到什麼問題以及怎麼解決。 大部分的 CNI 會透過 Linux Kernel 提供的 veth 這個功能來串聯不同的 Network Namespace，藉此讓封包可以在不同的隔離空間中傳輸Kata Container 本身包了一個 Virtual Machine 在最外層，使得上述的方法不可行為了相容所有的 CNM 以及 CNI, 勢必要找一個 network namespace 供這些介面去使用於是決定先創造一個 network namespace(只有單純的網路功能隔離),CNM/CNI 會對這個 network namespace 進行設定，將相關的網路功能設定好，之後透過 Linux Bridge 配合 Tap 的方式將該 network namespace 上的網路介面與 VM 裡面的網路介面給串接起來。這邊實際上是透過 MACVTAP 這個方式來串接, 有興趣的可以自行閱讀 圖片節錄自Kata Containers Architecture 圖片節錄自Kata Containers Architecture Summary 本章節跟大家分享並討論了一下基於安全性考量所發展的 OCI Runtime 專案,可以清楚地看到 gVisor 與 kata container 採取了兩種截然不同的方式來發展。 這兩個解決方案都相容於 OCI Runtime，所以只要上層的服務也支援 OCI Runtime，那就可以很輕鬆的轉移測試，譬如 containerd 或是 cri-o。 由於最後底層都是 Container, 所以使用者部屬的任何服務理論上都不需要修改，可以繼續使用各式各樣的 Container Image 以及相關的工具來處理。 下一篇就是 CRI 系列文章的最後一篇，到時候將跟大家如何將 Kubernetes CRI 與 Virtual Machine 給串接起來，這種情況下已經不需要 Container Image 了，而是要採用真正的 VM Image 並且透過 Kubernetes 來管理這些支援 CRI 操作但是實際上是完全跟 Container 無關的 Virtual Machine 解決方案。 參考 https://github.com/kata-containers/runtimehttps://thenewstack.io/how-to-implement-secure-containers-using-googles-gvisor/https://github.com/kata-containers/runtimehttps://github.com/google/gvisorhttps://github.com/moby/moby/blob/master/profiles/seccomp/default.jsonhttps://katacontainers.io/ ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-vm","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-vm#introduction","content":"就跟前述的慣例一下，先看一下 官方 GitHub 是如何描述自己這個專案的 Virtlet is a Kubernetes runtime server which allows you to run VM workloads, based on QCOW2 images. 這邊值得注意的是其用的詞是 Kubernetes runtime server, 這邊所指的就是 Container Runtime Interface，該專案本身額外實現了一個全新的應用程式，該應用程式本身支援 CRI 的 gRPC 介面，但是底下實現這些功能時全部都使用基於 QCOW2 Images 格式的 Virtual Machine。 ","version":"Next","tagName":"h2"},{"title":"Design​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-vm#design","content":"virtlet 開發的初衷並不是要用 VM 取代所有的 Container, 而是希望能夠提供另外一種選擇。為了達成這個目的，則 CRI 的部分勢必要重新撰寫，不能使用原生的 containerd 或是 cri-o。 同時這個全新設計的 CRI處理程式也要能夠根據情況決定使用 Virtual Machine 或是 Container 來創建對應的運算資源。 於是乎， CRI Proxy 這個專案就因應這個需求而生 ","version":"Next","tagName":"h2"},{"title":"CRI Proxy​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-vm#cri-proxy","content":"CRI Proxy Server 的功能分成簡單，就是根據條件轉發 CRI 請求到不同的後端，針對 container 的部分，目前支援 dockershim 或是 containerd ，而針對 Virtual Machine 的部分則是 virtlet server 。 本圖節錄自GitHub CRIProxy CRI Proxy 要用什麼條件來判斷到底該怎麼處理這個請求，這部分就使用上了 kubernetes 本身針對其資源內部提供的標記欄位，也就是所謂的 annotation 對於每個創建的 Pod，只要於 metadata.annotations.kubernetes.io/target-runtime 設定為 virtlet.cloud, 則 CRI Proxy 就會認得這個 Pod 要走 VM 去處理，而非傳統的 Container。 apiVersion: v1 kind: Pod metadata: name: cirros-vm annotations: kubernetes.io/target-runtime: virtlet.cloud ...  透過這種標記的方式與架構，可以讓使用者方便的去根據需求來決定要使用 VM 還是 Container。 此外 CRI Proxy 會被 kubelet 呼叫，所以本身也是每個節點上都要存在，因此一開始會先用 systemd 的方式在每台節點上都運行安裝，這樣基本的 kubelet 才可以啟動。 接者所有沒有設定的 Pod 就會走 kubelet -&gt; CRI Proxy -&gt; dockershim/containerd 的方式以 Container 被創建出來。 ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-vm#architecture","content":"除了上述的 CRI Proxy 之外，我們接下來看一下其完整的運作架構。 本圖節錄自 Architecture 當 CRI Proxy 收到創建 VM 的請求後，就會將該 CRI 的請求轉發到後端處理，這個處理的角色就是 Virtlet Container，也是俗稱的 virtlet Manager。 當整個 kubernetes 系統起來後，會透過 daemonset 的方式去部署 virtlet Manager vagrant@k8s-dev:~$ sudo kubectl -n kube-system get daemonset NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-proxy 3 3 3 3 3 &lt;none&gt; 8h virtlet 1 1 1 1 1 &lt;none&gt; 8h vagrant@k8s-dev:~$ sudo kubectl get pods --all-namespaces -l runtime=virtlet NAMESPACE NAME READY STATUS RESTARTS AGE kube-system virtlet-gghd4 3/3 Running 0 8h  然而該 DaemonSet 本身其實也有設定節點的選擇條件，並非所有的節點都會部署，畢竟該節點要有能力產生 VM，目前使用的規則是該節點必須要含有個標籤 extraRuntime: virtlet 即可，值得注意的是其使用的條件是 In, 所以只要含有 virtlet 這個字的節點都會被部署 virtlet Manager。 ... spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: extraRuntime operator: In values: - virtlet ...  當 virtlet Manager 收到指令後，會透過 libvirt API 的方式進行後續的處理，叫起 vmwrapper 來產生對應的 VM 環境 vmrapper is run by libvirt and wraps the emulator (QEMU/KVM). It requests tap file descriptor from Virtlet, adds command line arguments needed by the emulator to use the tap device and then execs the emulator. 其完整架構非常複雜，其中自行設計了不少元件來處理資源的處理，譬如使用 tapmanager 來處理整個 CNI，這部分幾乎沒有文件，只能依賴閱讀原始碼的方式來理解其實作方法。 vm-pod-lifecycle 這邊描述了關於 Pod 創造與刪除時整體處理流程，非常的長，有興趣的可以自行閱讀。 ","version":"Next","tagName":"h3"},{"title":"Installation​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-vm#installation","content":"官方文件 中有提供兩種安裝方式，一種是使用 kubernetes-dind-cluster 去安裝整個測試環境，另外一種則是按部就班的描述要安裝的所有元件，只是單純測試跟研究的話，我認為選擇第一種會比較方便 我自己的電腦環境是MAC Pro，平常都會透過 Vagrant + VirtualBox 產生一個 Linux 環境來測試，這次就基於這個 Linux 的環境使用 kubernetes-dind-cluster 安裝 kubernetes 並且在裡面使用 virtlet 產生 VM。 架構如下，非常的有趣 先疊一層 VM裡面創三個 Container, 以這三個 Container 組成一個 Kubernetes ClusterKubernetes 使用 Virtlet 作為其 CRI 解決方案，最後在裡面產生一個基於 Virtual Machine 的 Pod ","version":"Next","tagName":"h2"},{"title":"Steps​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/container-runtime-vm#steps","content":"準備好 Ubuntu 環境，下載demo.sh執行安裝，我自己是沒有遇到任何問題最後會幫你創建好 VM 並且要你透過 ssh 登入到該 VM, 密碼是 gocubsgo.  可以看到創建出來的 pod, 有特別標注一個 annotations，這樣 CRIProxy 就會根據需求使用 containerd 或是 VM 來創建服務 這時候透過 kubectl get pods -o wide 取得該 Pod 運行的節點位置，並且透過 docker exec -it $name bash 到節點裡面進行觀察 透過觀察真的發現該節點內透過 qemu 創建了一個 VM 除了上述最簡單的範例之外，GitHub 這邊還有提供其他不同的 Yaml, 其中我覺得非常有趣的就是 k8s.yml 可以讓你在 kubernetes 裡面透過 VM 產生另外一個 kubernetes cluster， 我暫時想不到應用情境，但是就是一個很有趣的架構。 節錄一下裡面的內容，可以看到該 VM 起來後會透過 kubeadm 的方式去初始化一個 cluster.  - path: /usr/local/bin/provision.sh permissions: &quot;0755&quot; owner: root content: | #!/bin/bash set -u -e set -o pipefail curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - apt-get update apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni sed -i 's/--cluster-dns=10\\.96\\.0\\.10/--cluster-dns=10.97.0.10/' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf systemctl daemon-reload if [[ $(hostname) =~ -0$ ]]; then # master node kubeadm init --token adcb82.4eae29627dc4c5a6 --pod-network-cidr=10.200.0.0/16 --service-cidr=10.97.0.0/16 --apiserver-cert-extra-sans=127.0.0.1,localhost export KUBECONFIG=/etc/kubernetes/admin.conf export kubever=$(kubectl version | base64 | tr -d '\\n') kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$kubever&quot; while ! kubectl get pods -n kube-system -l k8s-app=kube-dns|grep ' 1/1'; do sleep 1 done mkdir -p /root/.kube chmod 700 /root/.kube cp &quot;${KUBECONFIG}&quot; /root/.kube/config echo &quot;Master setup complete.&quot; &gt;&amp;2 else # worker node kubeadm join --token adcb82.4eae29627dc4c5a6 --discovery-token-unsafe-skip-ca-verification k8s-0.k8s:6443 echo &quot;Node setup complete.&quot; &gt;&amp;2 fi  Summary Container Runtime Interface(CRI) 的文章到這邊告了一個段落，我個人對於 CRI 這種介面的設計是滿喜歡的，透過介面將實作與主體抽離，能夠讓社群開發者自己開發想要的功能，同時又能夠簡單且順利的與 kubernetes 整合。 也正是因為如此才可以看到各式各樣針對不同議題而努力的專案，每個專案都有自己的特色與優劣，所以對於一個管理者來說，如果能夠理解這些不同的解決方案的優劣之處，不論是基於 CRI 標準的方案，或是更底下相容於 OCI Runtime 的實作，對於未來遇到任何不同的使用情境與問題時，腦中就可以很快的反射出是不是有相關的議題與資源可以去研究，而不會只用一套 docker 打天下。 接下來將針對網路的部分，從 Container Network Interface(CNI) 為出發點介紹其概念與架構，也包含了 ipam 的介紹，讓大家知道到底 IP 是怎麼被分配與指派的，接者也會探討常見的 flannel 其實作概念與運作流程。 參考 https://github.com/Mirantis/criproxyhttps://github.com/Mirantis/virtlet ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/csi","content":"","keywords":"","version":"Next"},{"title":"Before Kubernetes​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi#before-kubernetes","content":"各位其實可以回想一下過往所有的經驗中，遇過儲存什麼樣的問題以及議題? 光檔案系統本身就是一個戰局，BTRFS, Ext4, ZFS, CephFS, GLusterFS，各自的特色與優劣該如何選擇快照的支援，以及快照後容量是否加倍？LLVM/RAID/RAID2.0 等相關議題的討論，能夠容錯多少硬碟，能夠多快修復?RWO 讀寫的限制，可否同時多重讀寫或是只能單一處理?介面的選擇，是更底層的 Block Device 還是上層已經包裝可以使用的檔案系統路徑?異地備援? 本地備援?儲存服務本身有沒有HA的機制，有沒有SLA的保障?...等 過往方面就有洋洋灑灑的議題要處理，很多情況甚至都是尋找獨立的儲存廠商進來與現存系統整合，提供一個儲存解決方案，將這些問題都責任歸屬來處理，讓儲存伺服器本身來負責這些議題，而自己的服務則專注於處理獨特的商業邏輯。 如果過往操作與維運上有這些經驗與概念，今天要將服務全面導向 Kubernetes，也一定要有一樣的概念來處理，因為 kubernetes 本身沒有任何進階的儲存功能，上面提到的概念與技術全部都沒有，一切都是要仰賴額外的儲存設備與技術來提供這些功能，所以不要抱持太大的夢想 kubernetes 能夠提供一步到位解決所有事情。 ","version":"Next","tagName":"h2"},{"title":"Kubernetes​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi#kubernetes","content":"kubernetes 針對儲存部分，使用者可以使用的方式有很多種，雖然看似多種，其實背後的邏輯脈絡是一致的。 宣告／請求 儲存空間Pod 去請求使用以創立的儲存空間來使用Container 裡面描述如何使用 Pod 請求來的儲存空間 最簡單的使用方法就是將上述所有邏輯全部都描述在同一個 Pod 的資源中，統一管理統一維護，但是這種方法一旦該儲存空間是需要跨 Pod 使用時就會帶來維護不見。 所以可以透過 PersistemVolume 以及 PersistemVolumeClaim 等不同層級的儲存空間概念來維護，作為整個 kubernetes cluster 內部資源的話對於共用，管理方面也都有相當好的控管性。 然而上述的資源調度有時候又太過於靜態，缺乏彈性，因此後來又衍生出 StorageClass 這種動態請求的資源，對於使用者來說可以減少更多設定，整體使用起來會更加順手。 關於上述三種資源的彼此關係，概念，可以參考這篇文章 kubernetes storage 除了這三個類別資源外，其實還有兩個常用的資源也與儲存息息相關，ConfigMap 以及 Secret， 這兩個資源設定的介面與上述提到的些許不同，但是最後都會以檔案或其他的形式出現於 Container 供應用程式使用。 只要能夠讓 Container 有辦法存取到外部的存取空間，這過程都會牽扯到 Container 的創造，甚至是 Linux Mount Namespace 的涉入與處理。 對於大部分的使用者來說， Storage 的介面(上述概念)用起來都不會有太多的問題，選定好自己要使用的儲存後端，參考文件如何設定，接下來到 Pod 層級時使用就相對簡單，不會有太多設定上的困擾。 那到底 Container Storage Interface 於整個過程中是扮演什麼角色? 這個問題就是接下來的幾篇文章會探討的，並且說明為什麼要引入 CSI 以及其可以帶來什麼樣的幫助。 ","version":"Next","tagName":"h2"},{"title":"Why Container Storage Interface​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi#why-container-storage-interface","content":"如同前述探討 CRI 以及 CNI 時都有討論過為什麼要使用 Interface 的理由，藉由將模組與主程式抽離，讓各自的專案都有自己的開發週期，彼此不會互相被影響而導致開發或使用受阻。 這一篇官方部落格的文章 Container Storage Interface (CSI) for Kubernetes GA 有特別描述到為什麼需要 Container Storage Interface。 Although prior to CSI Kubernetes provided a powerful volume plugin system, it was challenging to add support for new volume plugins to Kubernetes: volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries—vendors wanting to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. 長期以來所有儲存的解決方案的整合端都是直接實作於 Kubernetes 的程式碼內，也是所謂的 in-tree 所描述的概念，這導致對於這些儲存應用服務的提供者很難及時的增加修復任何問題，因為全部的功能都跟 kubernetes 本身綁再一起，若 kubernetes 本身沒有更新，則使用者也都享受不到修復或是新功能。 更重要的是這些儲存相關程式碼本身的安全性程度以及穩定性都會變成額外的隱憂，是否會對 kubernetes 本身帶來各種負面的都是不能掌握的，同時這些程式碼的維護對於 kubernetes 維護者來說也是不好維護及掌握的。 CSI was developed as a standard for exposing arbitrary block and file storage storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes. With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Using CSI, third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This gives Kubernetes users more options for storage and makes the system more secure and reliable. 為了解決這個問題於是提出了 Container Storage Interface 的概念，希望能夠將儲存方面的程式碼都搬出去 kubernetes 本身，如同 CRI/CNI 一樣，能夠讓 kubernetes 專心維護與介面供通的整合，而其餘的儲存解決方案提供商專注於 CSI 介面的開發，最後就可以透過參數等方式來間接使用與整合。 那說了這麼多，今天 CSI 全面引進後，對於使用者到底會有什麼差異? 有什麼部分需要修改以符合新的架構? 我們先來看一下沒有使用 CSI 的架構，會怎麼使用 Network File System (NFS)。 kind: Pod apiVersion: v1 metadata: name: nfs-in-a-pod spec: containers: - name: app image: alpine volumeMounts: - name: nfs-volume mountPath: /var/nfs # Please change the destination you like the share to be mounted too command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;sleep 500000&quot;] volumes: - name: nfs-volume nfs: server: nfs.example.com # Please change this to your NFS server path: /share1 # Please change this to the relevant share  從上述的 yaml 中可以看到直接描述使用 NFS 的結構，並且因為 NFS 需要的參數有兩個，因此也需要於 yaml 去描述這兩個參數。 這個用法非常的綁死，實際上這些 yaml 的解讀都是依賴 kubernetes 本身去處理，所以其本身關於 volume 的資料結構中就包含了 NFS 的欄位，以及相關的參數，這種情況對於 NFS 來說有任何修改增減，都必須要修改 kubernetes 原始碼，也是所謂的 in-tree 架構造成的困境。 如果改善成 CSI 之後，整個應用會變成怎麼樣？ apiVersion: v1 kind: PersistentVolume metadata: name: data-nfsplugin labels: name: data-nfsplugin spec: accessModes: - ReadWriteMany capacity: storage: 100Gi csi: driver: csi-nfsplugin volumeHandle: data-id volumeAttributes: server: 127.0.0.1 share: /export --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: data-nfsplugin spec: accessModes: - ReadWriteMany resources: requests: storage: 100Gi selector: matchExpressions: - key: name operator: In values: [&quot;data-nfsplugin&quot;] --- apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: maersk/nginx imagePullPolicy: Always name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/www name: data-nfsplugin volumes: - name: data-nfsplugin persistentVolumeClaim: claimName: data-nfsplugin  可以看到這個範例中，不再使用 NFS 的關鍵字，而是採用了 CSI 這個關鍵字，並且於其中描述了幾個資訊 driver: 類似 CNI 設定檔案中的 type，描述要用哪個對應的 driver 來處理這個儲存需求volumeHandle: 一組重複使用的 ID，之後會再介紹volumeAttributes: server: 127.0.0.1share: /export 客製化的參數，根據不同的 Driver 傳入不同的參數。 根據目前官方文件 裡面的描述，現在 CSI 使用的參數如戲ㄚ drivervolumeHandlereadOnlyfsTypevolumeAttributescontrollerPublishSecretRefnodeStageSecretRefnodePublishSecretRef 這邊的參數與 CSI 的標準以及運作流程有關，因此等到介紹 CSI 標準後會再來重新看這些參數。 套用 CSI 的架構後，最大的差異使用就是之後所有的儲存連接都要使用 CSI 這個選項來描述，而非以前直接去描述目標的儲存解決方案。 Summary 本文討論了 Kubernetes 與儲存的一些基本關係，並且帶出了 Container Storage Interface 與 Kubernetes 的使用方式。 接下來會開始探討 CSI 相關的架構，並且以一些已經實現的 CSI 解決方案來討論該怎麼使用。 參考 https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/https://github.com/kubernetes-csi/drivers/blob/master/pkg/nfs/examples/kubernetes/nginx.yamlhttps://docs.docker.com/ee/ucp/kubernetes/storage/use-nfs-volumes/ ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/csi-ii","content":"","keywords":"","version":"Next"},{"title":"組成架構​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-ii#組成架構","content":"接下來看一下 CSI 的架構中會有什麼樣的角色，譬如 CRI 中規定要有一個伺服器實現 CRI 標準即可，而 CNI 則是要有一個支援 CNI 標準的執行檔案即可。 CSI 相對複雜，其組成至少要有兩個元件，分別是 Controller 以及 Node 這兩種不同 API 的實作。 根據 官方說明 Node Plugin: A gRPC endpoint serving CSI RPCs that MUST be run on the Node whereupon an SP-provisioned volume will be published. Controller Plugin: A gRPC endpoint serving CSI RPCs that MAY be run anywhere. 所有要使用該儲存解決方案的節點都必須要有一個對應的應用程式來提供 Node 的服務，而控管整個儲存解決方案的管理者 Controller 本身並沒有限定要運行在哪個節點。 從剛剛上述的 CSI protobuf 的檔案也可以看到有兩個明顯的 service 需要實作，如下 service Controller { rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {} rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {} rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {} rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {} rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest) returns (ValidateVolumeCapabilitiesResponse) {} rpc ListVolumes (ListVolumesRequest) returns (ListVolumesResponse) {} rpc GetCapacity (GetCapacityRequest) returns (GetCapacityResponse) {} rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest) returns (ControllerGetCapabilitiesResponse) {} rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {} rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {} rpc ListSnapshots (ListSnapshotsRequest) returns (ListSnapshotsResponse) {} rpc ControllerExpandVolume (ControllerExpandVolumeRequest) returns (ControllerExpandVolumeResponse) {} }  service Node { rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {} rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {} rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {} rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {} rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {} rpc NodeExpandVolume(NodeExpandVolumeRequest) returns (NodeExpandVolumeResponse) {} rpc NodeGetCapabilities (NodeGetCapabilitiesRequest) returns (NodeGetCapabilitiesResponse) {} rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {} }  此外 Node 以及 Controller 都必須要實現另外一個名為 Identity 的 Service 來表明自己的身份與能力。 service Identity { rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {} rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {} rpc Probe (ProbeRequest) returns (ProbeResponse) {} }  所以基於這種情況下，我們可以想像一個解決方案可能會有一些不同的架構 不同的應用程式，分別各自實現 Controller 以及 Node 的服務使用相同的應用程式，內部同時實現兩個服務。 此外官方也提供了四種參考的部署方式，主要還是會依據不同儲存方案本身的特性去設計。 下圖的 master 以及 node 兩種不同的節點身份可對應到 kubernetes 內的 master 以及 worker 節點。 第一種是中央集權管理的部署方式，於 Master 去部署 Controller 服務，而剩下所有的工作節點都要部署 Node 服務。 第二種則是不管 Master 節點了，把 Controller 服務部署到其中一個 worker 節點即可。 第三種則是將兩個服務整合，透過一個應用程式去實現 Controller 以及 Node 的介面，這種情況下部署就是將該應用程式部署到所有的 worker 節點即可 第四種則是一個非常稀少，少到官方也沒有說明什麼類型會這樣部署，就是沒有 Controller 的解決方案，單純依賴 Node 的服務來處理所有儲存相關資院的創建與釋放。 基於這些內容的討論，至少可以確認對於 CSI 的儲存方案來說，可以預想到部署時應該會採用 DaemonSet 的方式來部署 Noode 服務，而對於 Controller 服務來說則是不一定。 ","version":"Next","tagName":"h2"},{"title":"Lifecycle​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-ii#lifecycle","content":"接下來看一下最重要的生命週期，看看到底 Container Orchestration, Controller, Node 這三者到底會怎麼合作來提供儲存空間。 這部分也是有多種流程，主要取決於儲存方案本身的能力。 開始前先來定義一些相關名詞 ","version":"Next","tagName":"h2"},{"title":"Controller​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-ii#controller","content":"Create Volume 此呼叫是請求儲存方案根據需求去創建一個可用的儲存空間，但是就只是創造出來該空間而已，還沒有辦法被使用。以 AWS 來說可能就是創造一個 VolumeController Publish Volume 此呼叫是請求儲存空間將之前創造的 volume 與特定的節點進行連動，譬如該節點有能力去存取該創造出來的 Volume。 以 AWS 來說就將創造好的 Volume 掛載到特定的運算資源上(VM) ","version":"Next","tagName":"h3"},{"title":"Node​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-ii#node","content":"Stage Volume 當透過上述 Controller 的相關操作將 Volume 給掛載到節點後，接下來可以對 Node 進行 stage 的動作，將該 Volume 給掛到一個暫時的位置，接下來相關的工作資源Pod 就可以使用，甚至可以多個 Pod 共享。 此外這個步驟也需要確保該 Volume 有被格式化過Publish Volume 這是最後一個步驟，透過類似 bind mount 的方式將欲使用的儲存空間給投入到 Pod 裡面去使用 上面的敘述沒有非常精準的描述一切行為，因為對於 Block Device 以及 Mountable Volume 來說，兩者的使用方法不太一樣，因此執行的行為也會有點差異。而上述的行為描述比較偏向將一個 (block device) 提供給多個 Pod 去使用。 The main difference between block volumes and mount volumes is the expected result of the NodePublish(). For mount volumes, the CO expects the result to be a mounted directory, at TargetPath. For block volumes, the CO expects there to be a device file at TargetPath. The device file can by a bind-mounted device from the hosts /dev file system, or it can be a device node created at that location using mknod() 接下來看一下官方分享的幾個可能的運作流程  CreateVolume +------------+ DeleteVolume +-------------&gt;| CREATED +--------------+ | +---+----+---+ | | Controller | | Controller v +++ Publish | | Unpublish +++ |X| Volume | | Volume | | +-+ +---v----+---+ +-+ | NODE_READY | +---+----^---+ Node | | Node Publish | | Unpublish Volume | | Volume +---v----+---+ | PUBLISHED | +------------+ Figure 5: The lifecycle of a dynamically provisioned volume, from creation to destruction.  可以看到這邊描述的是 Dynamically Provisioned Volume，對應到 kubernetes 就是所謂的 StorageClass。 此範例中就是呼叫 Controller 去創建空間，接者透過 Publish 使其與 Node 互動，最後直接透過 Publish Volume 掛到對應的 Container 中。這範例中就沒有去使用 Stage 的概念，因為創造出來的空間就是直接可存取的 Mount Volume，譬如 NFS  CreateVolume +------------+ DeleteVolume +-------------&gt;| CREATED +--------------+ | +---+----+---+ | | Controller | | Controller v +++ Publish | | Unpublish +++ |X| Volume | | Volume | | +-+ +---v----+---+ +-+ | NODE_READY | +---+----^---+ Node | | Node Stage | | Unstage Volume | | Volume +---v----+---+ | VOL_READY | +------------+ Node | | Node Publish | | Unpublish Volume | | Volume +---v----+---+ | PUBLISHED | +------------+ Figure 6: The lifecycle of a dynamically provisioned volume, from creation to destruction, when the Node Plugin advertises the STAGE_UNSTAGE_VOLUME capability.  與上述行為雷同，不過面對的是 block device 的類別，所以還需要經過 stage 階段進行一次處理，才可以讓 block device 能夠被多次存取。  Controller Controller Publish Unpublish Volume +------------+ Volume +-------------&gt;+ NODE_READY +--------------+ | +---+----^---+ | | Node | | Node v +++ Publish | | Unpublish +++ |X| &lt;-+ Volume | | Volume | | +++ | +---v----+---+ +-+ | | | PUBLISHED | | | +------------+ +----+ Validate Volume Capabilities Figure 7: The lifecycle of a pre-provisioned volume that requires controller to publish to a node (`ControllerPublishVolume`) prior to publishing on the node (`NodePublishVolume`).  最後一個則是 pre-provisioned 的類別，所以事先創立好的空間已經先準備好相關的資源，所以就透過兩次的 Push 把該空間給掛載到 Pod 裡面使用。 除了這些基本流程之外，整個 CSI 裡面的規範還有非常多的細節，譬如 snapshot的處理，各式各樣能力需要怎麼處理請求與回應，這邊我想就是針對儲存空間有興趣的人可以自行研究，並且嘗試開發一個簡易的 CSI 套件看看。 Summary 綜觀下來， CSI 其運作的流程與之前使用 StorageClass, PVC ,PV 非常雷同，此外若本身有在使用公有雲的服務，裡面提供的儲存服務用法也大概如此。 先創建，接者連接，最後存取，而這次只是把存取的角色限定為 Container 而連接的部分都是所謂的系統節點罷了。 說罷了系統設計本一家，看似嶄新的發展其實背後也是用到了很多過往的經驗與技術，譬如所謂的 block device/mountable volume 或是 mount, bind mount，這些底子的技術若平常就有精進累積，來看這些相關的知識與架構就會覺得沒那麼陌生，甚至覺得上手不會太困難。 參考 https://kubernetes.io/docs/concepts/storage/volumes/#csihttps://github.com/container-storage-interface/spechttps://medium.com/searce/glusterfs-dynamic-provisioning-using-heketi-as-external-storage-with-gke-bd9af17434e5https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/ ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/csi-iii","content":"","keywords":"","version":"Next"},{"title":"Release​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-iii#release","content":"從下列的一次次的釋出版本對應表中可以看到自從 v1.13 後， CSI 可算是正式支援，而非先前的 Alpha/Beta 等測試行為。 Kubernetes\tCSI Spec Compatibility\tStatusv1.9\tv0.1.0\tAlpha v1.10\tv0.2.0\tBeta v1.11\tv0.3.0\tBtea v1.13\tv0.3.0, v1.0.0\tGA 從 官方文件 來看，過往存放眾多儲存解決方案介紹的頁面，現在最下面也新增了 CSI 的選項，接下來儲存這條路線應該都是往 CSI 邁進，而 in-tree 整合的程式碼也不會主要維護的對象，而是讓這些解決方案提供者自行透過 CSI 的架構來維護。 因此能的話會建議接下來的部署都往 CSI 邁進，不要再使用各種 in-tree 的發展，除非你使用的儲存方案目前還沒有對應的 CSI 整合。 此外 CSI 使用上也有一些要注意的點，譬如官方文件有提到 Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users may use the csi volume type to attach, mount, etc. the volumes exposed by the CSI driver. The csi volume type does not support direct reference from Pod and may only be referenced in a Pod via a PersistentVolumeClaim object. 使用上一定要注意相關的版本，這其實非常的麻煩，現在同時擁有 CRI, CNI 以及 CSI 三個標準在手，每個標準都有自己的發行版本號，同時 kubernetes 本身也有自己的版本號。所以升級 kubernetes 的時候更要注意這些相容性的問題。 此外一個使用上的改變就是若採用 CSI，則不能直接於 Pod 的資源內直接描寫對應的解決方案，而一定要透過 PVC 的方式來採用，至於最底層是 PV 或是 StorageClass 都可以。 ","version":"Next","tagName":"h2"},{"title":"Migration​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-iii#migration","content":"開發者也知道要從 in-tree 切換到 CSI 是一個轉換期，因此目前 kubernetes 也內建了 migrating-to-csi-drivers-from-in-tree-plugins 相關的功能，但是要注意的是這個功能並不是全部的解決方案都支援，必須要仔細看每個解決方案本身是否有支援，如下方所列 awsElasticBlockStore CSI Migration FEATURE STATE: Kubernetes v1.14 alpha The CSI Migration feature for awsElasticBlockStore, when enabled, shims all plugin operations from the existing in-tree plugin to the ebs.csi.aws.com Container Storage Interface (CSI) Driver. In order to use this feature, the AWS EBS CSI Driver must be installed on the cluster and the CSIMigration and CSIMigrationAWS Alpha features must be enabled. azureDisk CSI Migration FEATURE STATE: Kubernetes v1.15 alpha The CSI Migration feature for azureDisk, when enabled, shims all plugin operations from the existing in-tree plugin to the disk.csi.azure.com Container Storage Interface (CSI) Driver. In order to use this feature, the Azure Disk CSI Driver must be installed on the cluster and the CSIMigration and CSIMigrationAzureDisk Alpha features must be enabled. 不同的解決方案都有於不同版本的 kubernetes 實作了 CSI Migration 的方式，預期中管理者是可以不需要重新修改 StorageClass, PVs, PVCs 等相關設定檔案，而是會在背後自動地將操作一個一個的全部導向 CSI 的架構，而非 In-tree 的運作邏輯。 ","version":"Next","tagName":"h2"},{"title":"CSI Implementation​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-iii#csi-implementation","content":"現在到底有多少個解決方案支援 CSI 架構，就如同 CRI, CSI 一樣官方都有去紀錄目前有自己提交支援 CSI 的解決方案，根據 drivers 的列表，目前支援的解決儲存方案數量高達數十種(請點選連結自行查看) 根據文章中的表格可以看到公有雲支援的不少，譬如 Google，Azure，AWS，Digital Ocean 甚至是 Alicloud。 此外除了公有雲之外，也可以看不少儲存設備廠商的支援，譬如 Pure Storage，NetApp，Nutanix，此外不少的開源儲存解決方案， OpenEBS, Ceph, GlusterFS, MooseFS 等都在支持清單中。 這麼多的儲存方案到底要怎麼選，我認為這個問題非常困難，絕對不是一句聽說xxxx好像很厲害，我們就導入來使用 就樣不負責任就可以輕鬆選擇的議題。 儲存設備如同第一篇所述，牽扯過多的議題要選擇 有沒有想過自己的使用情境是 random access 還是 sequencial access?本身公司是否已經有相關的儲存設備，這些設備有提供 CSI 的解決方案嗎?需要的會是 BlockDevice 還是 Mountable Volume?本身要考慮多重讀寫嗎?備份本身的議題有研究過嗎 真的很在意儲存功能的團隊，建議還是要找對儲存功能瞭解的人來評估，千萬不要搖擺不定聽別人一句話就一頭栽下去，到時候要換資料也麻煩。 ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-iii#architecture","content":"如果你嘗試去搜尋關於 Kubernetes &amp; CSI 的架構，看到的圖片大致上都跟下面類似本圖節錄自supercharging-kubernetes-storage-with-csi 這張圖片裡面有幾個東西要特別觀察 分成 DaemonSet Pod 以及 StatefulSet Pod每個 Pod 裡面都有多個 Container, 其中分成兩個顏色，綠色跟橘色到處都有 gRPC 的溝通 接下來我們來仔細探討這些重點，因為基本上都大部分的 CSI 解決方案架構都長這樣，這也是為了 CSI 開發者所設計的架構，能夠最簡化一切的需求。 ","version":"Next","tagName":"h2"},{"title":"DaemonSet/StatefulSet​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-iii#daemonsetstatefulset","content":"之前有提過 CSI 裡面會有兩個服務角色，分別是 Controller 以及 Node，首先 Node 服務本身是一個所有欲使用儲存方案的節點都要運行的服務，因此會透過 DaemonSet來部署。 相對來說，Controller 除非本身有自行額外的多重服務架構，不然基本上都是會希望只有一個副本的服務在運行，採用 StatefulSet 可以確保都只會有一個服務在運行，特別是如果今天有重起的時候，會特別確認前一個狀態已經 Terminated 後才去創建下一個，避免任何時刻有兩個以上的 Pod 運行。 ","version":"Next","tagName":"h3"},{"title":"Sidecar Containers​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-iii#sidecar-containers","content":"綠色的部分就是儲存方案提供者自行撰寫的部分，對應到 Controller 以及 Node 兩個角色。 而橘色的部分是 kubernetes 提供給開發者輔助，降低開發門檻同時把責權抽離的好幫手。 由於 kubernetes 內部會有 storageclass, pvc, pv 等資源的創立，要如何讓 CSI Controller/Node之後有這些事件發生，一種常見做法就是這些服務本身內部也使用 kubernetes API 去監聽相關的事件，並且執行對應的事情。 但是這種情況下其實跟 CSI 的 gRPC 介面好像也沒有什麼關聯，就一個 kubernetes controller 就可以完成的架構。 因此上述的橘色內的各式各樣的容器就被發展出來，這些容器專心於 kubernetes 溝通，監聽各種相關事件，一旦聽到任何需要處理的資訊的時候，就會經由 unix socket 並且透過 gRPC 叫同Pod裡面的 Driver 去處理。 這種情況下， CSI 解決方案提供者只需要專注於解決方案跟介面的實現即可，不需要去在意 kubernetes 本身的事件。 不同的 sidecar 都有不同的用途，可以參考官方開發指南 詳細介紹每種 Container 的用途。 ","version":"Next","tagName":"h3"},{"title":"gRPC​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-iii#grpc","content":"就如同 CRI,CNI 一樣， kubelet 本身也有實現部分 CSI 處理的邏輯，特別是有些跟 Node 有關的操作就會直接透過 kubelet 去呼叫，譬如 CSI NodeGetInfo, NodeStageVolume, 以及 NodePublishVolume 這種情況下，kubelet 會使用 unix socket 配上 gRPC 與 Node服務聯繫來直接觸發相關操作。 詳細的溝通過程可以參考 Communication Channels Summary 本文簡述了一下 CSI 與 Kubernetes 的相對關係，以及相關架構，接下來我們會嘗試部署一個基於 CSI 的儲存設備來直接嘗試看看新架構下系統又多出了什麼資源，以及有什麼資訊可以觀察 參考 https://kubernetes-csi.github.io/docs/introduction.htmlhttps://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/https://blogs.vmware.com/cloudnative/2019/04/18/supercharging-kubernetes-storage-with-csi/https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs","content":"","keywords":"","version":"Next"},{"title":"安裝內容​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#安裝內容","content":"接下來我們來看一下剛剛安裝的過程到底裝了哪些資源到系統中，這邊我們就忽略其他資源，專注於跟儲存有關的資源上 ","version":"Next","tagName":"h2"},{"title":"Controller​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#controller","content":"首先我們先看一下所謂的 CSI Controller 相關的設定檔案，如之前所述，這種情況通常會使用 StatefulSet 來設定，除非你的 Controller 本身有額外實現多實例的架構，可確保同時只有會一個副本正在運行。 kind: StatefulSet apiVersion: apps/v1beta1 metadata: name: csi-attacher-nfsplugin spec: serviceName: &quot;csi-attacher&quot; replicas: 1 template: metadata: labels: app: csi-attacher-nfsplugin spec: serviceAccount: csi-attacher containers: - name: csi-attacher image: quay.io/k8scsi/csi-attacher:v1.0.1 args: - &quot;--v=5&quot; - &quot;--csi-address=$(ADDRESS)&quot; env: - name: ADDRESS value: /csi/csi.sock imagePullPolicy: &quot;IfNotPresent&quot; volumeMounts: - name: socket-dir mountPath: /csi - name: nfs image: quay.io/k8scsi/nfsplugin:canary args : - &quot;--nodeid=$(NODE_ID)&quot; - &quot;--endpoint=$(CSI_ENDPOINT)&quot; env: - name: NODE_ID valueFrom: fieldRef: fieldPath: spec.nodeName - name: CSI_ENDPOINT value: unix://plugin/csi.sock imagePullPolicy: &quot;IfNotPresent&quot; volumeMounts: - name: socket-dir mountPath: /plugin volumes: - name: socket-dir emptyDir:  該 Pod 中運行了兩個 Container，分別是 quay.io/k8scsi/nfsplugin:canaryquay.io/k8scsi/csi-attacher:v1.0.1 第一個則是關於基於 NFS 所實現相容於 CSI Controller 的解決方案，而第二個則是由官方推出便於發開者的 sidecar container，用來監聽相關的 kubernetes 事件並且透過 gRPC 的方式告知第一個容器。 這邊同時可以觀察到 系統上透過 emptyDir 的方式創建了一個空間，並且讓兩個 container 都共同使用，而該檔案則是其實背後就是創建出來的 unix://plugin/csi.sock， 因此這兩個 container 就會透過這種方式來進行 IPC 的交談。 ","version":"Next","tagName":"h3"},{"title":"Node​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#node","content":"接下來觀察一下另外一個部署，使用 DaemonSet 來部署，對應的就是之前提到的 CSI Node 服務，是必須每台需要使用儲存方案的節點都要部署的。 kind: DaemonSet apiVersion: apps/v1beta2 metadata: name: csi-nodeplugin-nfsplugin spec: selector: matchLabels: app: csi-nodeplugin-nfsplugin template: metadata: labels: app: csi-nodeplugin-nfsplugin spec: serviceAccount: csi-nodeplugin hostNetwork: true containers: - name: node-driver-registrar image: quay.io/k8scsi/csi-node-driver-registrar:v1.0.2 lifecycle: preStop: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;rm -rf /registration/csi-nfsplugin /registration/csi-nfsplugin-reg.sock&quot;] args: - --v=5 - --csi-address=/plugin/csi.sock - --kubelet-registration-path=/var/lib/kubelet/plugins/csi-nfsplugin/csi.sock env: - name: KUBE_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumeMounts: - name: plugin-dir mountPath: /plugin - name: registration-dir mountPath: /registration - name: nfs securityContext: privileged: true capabilities: add: [&quot;SYS_ADMIN&quot;] allowPrivilegeEscalation: true image: quay.io/k8scsi/nfsplugin:canary args : - &quot;--nodeid=$(NODE_ID)&quot; - &quot;--endpoint=$(CSI_ENDPOINT)&quot; env: - name: NODE_ID valueFrom: fieldRef: fieldPath: spec.nodeName - name: - name: CSI_ENDPOINT value: unix://plugin/csi.sock imagePullPolicy: &quot;IfNotPresent&quot; volumeMounts: - name: plugin-dir mountPath: /plugin - name: pods-mount-dir mountPath: /var/lib/kubelet/pods mountPropagation: &quot;Bidirectional&quot; volumes: - name: plugin-dir hostPath: path: /var/lib/kubelet/plugins/csi-nfsplugin type: DirectoryOrCreate - name: pods-mount-dir hostPath: path: /var/lib/kubelet/pods type: Directory - hostPath: path: /var/lib/kubelet/plugins_registry type: Directory name: registration-dir  該 Pod 中也是部署了兩個 Container，分別是 quay.io/k8scsi/csi-node-driver-registrar:v1.0.2quay.io/k8scsi/nfsplugin:canary 跟 Controller 的部署一樣，其中一個是自行解決方案的設計，另外一個則是官方提供的好用輔助容器。 首先看一下Volume 的部分，這邊有三個資料夾並且都是透過 hostpath 的方式來使用 /var/lib/kubelet/plugins/csi-nfsplugin/var/lib/kubelet/pods/var/lib/kubelet/plugins_registry 第一個 /var/lib/kubelet/plugins/csi-nfsplugin 根據觀察可以發現最後其實是用來指定相關的 unix socket。 vagrant@k8s-dev:~$ sudo ls /var/lib/kubelet/plugins/csi-nfsplugin csi.sock vagrant@k8s-dev:~$ sudo file /var/lib/kubelet/plugins/csi-nfsplugin/csi.sock /var/lib/kubelet/plugins/csi-nfsplugin/csi.sock: socket  第二個 /var/lib/kubelet/pods 比較特別，這個資料夾是 kubelet 用來存放跟 pod 相關的資訊，其中該目錄底下都是基於 pod ID 來區隔的。 vagrant@k8s-dev:~$ sudo ls /var/lib/kubelet/pods/ 16010371-22a4-4152-af23-712def2a764d 7a5a5fa4-ea62-4106-8e2e-69f3a0abf48d 8bf65cf7-a02a-4829-9fab-784355fe5ba5 af50052304877bc1d4cd4d3409c6be5a c3aedceb2d751faeb02f85ebcec869a6 db52b7d4-c828-49d8-b899-e52be600f0b5 28f76958a7cfb29c8091821d6746ddea 7d5d3c0a6786e517a8973fa06754cb75 9f7d3f8c-63a8-46b9-8d69-d70656caf0b7 bd4f816d-01f0-4217-a66c-27bd4893c130 c9da1474-797c-4386-acb1-f7c74cc30dbd vagrant@k8s-dev:~$ sudo docker ps | grep nginx 2107ec8d04e1 maersk/nginx &quot;nginx&quot; 10 hours ago Up 10 hours k8s_nginx_nginx_default_8bf65cf7-a02a-4829-9fab-784355fe5ba5_0 dad41423889e k8s.gcr.io/pause:3.1 &quot;/pause&quot; 10 hours ago Up 10 hours k8s_POD_nginx_default_8bf65cf7-a02a-4829-9fab-784355fe5ba5_0 vagrant@k8s-dev:~$ sudo ls /var/lib/kubelet/pods/8bf65cf7-a02a-4829-9fab-784355fe5ba5 containers etc-hosts plugins volumes vagrant@k8s-dev:~$  可以看到基於測試的 nginx pod 底下有個 volumes 的資料夾，接下來往下去看裡面有什麼資訊 vagrant@k8s-dev:~$ sudo tree /var/lib/kubelet/pods/8bf65cf7-a02a-4829-9fab-784355fe5ba5/volumes /var/lib/kubelet/pods/8bf65cf7-a02a-4829-9fab-784355fe5ba5/volumes ├── kubernetes.io~csi │ └── data-nfsplugin | ├── mount │ │ ├── influxdb │ │ ├── mongodb │ │ ├── test1 │ │ └── user │ └── vol_data.json └── kubernetes.io~secret └── default-token-6bcvc ├── ca.crt -&gt; ..data/ca.crt ├── namespace -&gt; ..data/namespace └── token -&gt; ..data/token vagrant@k8s-dev:~$ sudo cat /var/lib/kubelet/pods/8bf65cf7-a02a-4829-9fab-784355fe5ba5/volumes//kubernetes.io~csi/data-nfsplugin/vol_data.json | jq { &quot;attachmentID&quot;: &quot;csi-f642ae48557f63a1f4377a265c43d6afe2e8d859837925fef2331b9e541e31a3&quot;, &quot;driverMode&quot;: &quot;persistent&quot;, &quot;driverName&quot;: &quot;csi-nfsplugin&quot;, &quot;nodeName&quot;: &quot;k8s-dev&quot;, &quot;specVolID&quot;: &quot;data-nfsplugin&quot;, &quot;volumeHandle&quot;: &quot;data-id&quot; }  同時也可以看到上述的 mount 資料夾內有之前設定的 NFS 相關的分享資料夾內容，這部分引起我的好奇心，所以透過 mount 再次觀察。 vagrant@k8s-dev:~$ mount | grep nfsshare 127.0.0.1:/nfsshare on /var/lib/kubelet/pods/8bf65cf7-a02a-4829-9fab-784355fe5ba5/volumes/kubernetes.io~csi/data-nfsplugin/mount type nfs4 (rw,relatime,vers=4.1,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=127.0.0.1,local_lock=none,addr=127.0.0.1)  大概可以瞭解整個運作原理了，先透過 CSI 的方式把目標的解決方案掛載到相關節點上，而掛載的地點必須是 /var/lib/kubelet/pods/${pod_id}/volumes/kubernetes.io~csi/${csi_name} 這個位置，當這邊處理完畢後。一旦 Pod 開始啟用後，就會透過 mount namespace 的方式再度的把這個空間給掛載到 Pod 裡面去使用。 最後的 /var/lib/kubelet/plugins_registry 看名稱就跟註冊有關，的確也是專門給 Register 這個額外的容器使用。 根據該專案說明 Registration socket: Registers the driver with kubelet. Created by the node-driver-registrar. Exposed on a Kubernetes node via hostpath in the Kubelet plugin registry. (typically /var/lib/kubelet/plugins_registry/$drivername.example.com-reg.sock). The hostpath volume must be mounted at /registration. 所以可以看到這算是一個該容器的標準用法，如果要將該CSI解決方案註冊到 Node 上，就直接透過這個容器加上 Unix socket 與 kubelet 溝通，就算是完成註冊相關的功能了。 vagrant@k8s-dev:~$ sudo ls /var/lib/kubelet/plugins_registry csi-nfsplugin-reg.sock vagrant@k8s-dev:~$ sudo file /var/lib/kubelet/plugins_registry/csi-nfsplugin-reg.sock /var/lib/kubelet/plugins_registry/csi-nfsplugin-reg.sock: socket  根據官方提供的參考部署架構圖該圖節錄自Recommended Mechanism for Deploying CSI Drivers on Kubernetes 可以對應到上述所描述的部署方式以及相關的 volume 操作，同時對於整體的運作邏輯有更清晰的表達。 ","version":"Next","tagName":"h3"},{"title":"Nginx​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#nginx","content":"作為一個使用者 Pod 來說，這邊採用 PV/PVC 這種預先配置的方式來使用前述部署完畢的 CSI 環境，其中只有 PV 的部分使用方式跟過往不同，剩下的 PVC/Pod 都沒有任何變化。 可以看到 PV 之中必須要採用 csi 的架構，並且透過 volumeAttrivutes 傳遞每個解決方案需要的參數過去，以 NFS 為範例，就是目標伺服器的 IP 地址以及欲分享的資料夾。 apiVersion: v1 kind: PersistentVolume metadata: name: data-nfsplugin labels: name: data-nfsplugin spec: accessModes: - ReadWriteMany capacity: storage: 100Gi csi: driver: csi-nfsplugin volumeHandle: data-id volumeAttributes: server: 127.0.0.1 share: /nfsshare --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: data-nfsplugin spec: accessModes: - ReadWriteMany resources: requests: storage: 100Gi selector: matchExpressions: - key: name operator: In values: [&quot;data-nfsplugin&quot;] --- apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: maersk/nginx imagePullPolicy: Always name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/www name: data-nfsplugin volumes: - name: data-nfsplugin persistentVolumeClaim: claimName: data-nfsplugin  ","version":"Next","tagName":"h3"},{"title":"觀察​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#觀察","content":"環境都架設完畢之後，第一步先觀察 NFS 是否如預期般的運作 vagrant@k8s-dev:~$ kubectl exec nginx ls /var/www influxdb mongodb user vagrant@k8s-dev:~$ ls /nfsshare/ influxdb mongodb user vagrant@k8s-dev:~$ sudo touch /nfsshare/test1 vagrant@k8s-dev:~$ kubectl exec nginx ls /var/www influxdb mongodb test1 user  看起來非常順利，基本的 NFS 掛載已經成功，接下來我們就要來觀察基於 CSI 的環境有什麼不同 vagrant@k8s-dev:~$ kubectl api-resources | grep -i storage csidrivers storage.k8s.io false CSIDriver csinodes storage.k8s.io false CSINode storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment  我們透過 kubectl api-resources 去觀察系統上目前有哪些跟 storage 有關的資源，發現除了過往的 storageclasses 之外，還多出了三個資源，分別是 csidrivers, csinodes 以及 volumeattachmenets。 ","version":"Next","tagName":"h2"},{"title":"CSIDriver​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#csidriver","content":"非常不幸的，我們的範例中並沒有創建任何資源於這個物件類別下 vagrant@k8s-dev:~$ kubectl get csidrivers No resources found.  根據官方開發指南，裡面對於 CSIDriver 的描述是 The CSIDriver Kubernetes API object serves two purposes: Simplify driver discovery If a CSI driver creates a CSIDriver object, Kubernetes users can easily discover the CSI Drivers installed on their cluster (simply by issuing kubectl get CSIDriver) Customizing Kubernetes behavior Kubernetes has a default set of behaviors when dealing with CSI Drivers (for example, it calls the Attach/Detach operations by default). This object allows CSI drivers to specify how Kubernetes should interact with it. 簡單來說這個資源不一定會有，主要取決於 CSI 解決方案有沒有要創建，創建的話提供兩個好處 使用者更容易的觀察到系統上安裝了哪些 CSI 解決方案 我認為這滿方便的，但是能夠強制要求創建該物件就更棒了讓 CSI解決方案能夠有機會對 kubelet 進行控制，去管理整個 CSI 的運作邏輯。 一個合法的內容可能如下 apiVersion: storage.k8s.io/v1beta1 kind: CSIDriver metadata: name: mycsidriver.example.com spec: attachRequired: true podInfoOnMount: true volumeLifecycleModes: # added in Kubernetes 1.16 - Persistent - Ephemeral  其中 volumeLifycecleModes 甚至是 kubernetes 1.16 所加入的，有興趣的可以自行參閱開發指南 去瞭解每個欄位的定義。 ","version":"Next","tagName":"h3"},{"title":"CSINodes​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#csinodes","content":"透過 kubectl describe 我們可以看到更多關於 CSINodes 的資料。 vagrant@k8s-dev:~$ kubectl get csinodes NAME CREATED AT k8s-dev 2019-09-29T07:19:27Z vagrant@k8s-dev:~$ kubectl describe csinodes k8s-dev Name: k8s-dev Namespace: Labels: &lt;none&gt; Annotations: &lt;none&gt; API Version: storage.k8s.io/v1beta1 Kind: CSINode Metadata: Creation Timestamp: 2019-09-29T07:19:27Z Owner References: API Version: v1 Kind: Node Name: k8s-dev UID: 8ed067fe-dbe7-4297-8177-c8a9da227962 Resource Version: 540 Self Link: /apis/storage.k8s.io/v1beta1/csinodes/k8s-dev UID: 1fd550dd-194a-4f20-8d41-fd1f42fbe16a Spec: Drivers: Name: csi-nfsplugin Node ID: k8s-dev Topology Keys: &lt;nil&gt; Events: &lt;none&gt;  其中最重要的就是 spec.drivers 內的資料，包含了 該解決方案的名稱，定義於該解決方案內的程式碼。該節點的名稱，因為 CSI 裡面有很多的介面都會需要 NodeID 來進行一些處理，特別是 Controller 端的介面。 根據 官方開發指南 內的描述， CSINode 有下列的事項要注意 CSI 解決方案本身不需要自行創造，只要透過上篇介紹過的開發小幫手 - sidecar containers 中的 node-device-register 就會自動創造該物件該物件創立的目的是希望提供下列資訊 將 kubernetes node 的名稱轉換至 CSI node name透過相關物件判斷特定節點上是否有註冊相關的 CSI 解決方案 ","version":"Next","tagName":"h3"},{"title":"VolumeAttachments​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/csi-nfs#volumeattachments","content":"這個物件是用來描述 CSI 使用過程中去掛載 Volume 的相關資訊。 vagrant@k8s-dev:~$ kubectl get volumeattachments NAME ATTACHER PV NODE ATTACHED AGE csi-f642ae48557f63a1f4377a265c43d6afe2e8d859837925fef2331b9e541e31a3 csi-nfsplugin data-nfsplugin k8s-dev true 20m vagrant@k8s-dev:~$ kubectl describe volumeattachments Name: csi-f642ae48557f63a1f4377a265c43d6afe2e8d859837925fef2331b9e541e31a3 Namespace: Labels: &lt;none&gt; Annotations: &lt;none&gt; API Version: storage.k8s.io/v1 Kind: VolumeAttachment Metadata: Creation Timestamp: 2019-09-29T07:19:49Z Resource Version: 577 Self Link: /apis/storage.k8s.io/v1/volumeattachments/csi-f642ae48557f63a1f4377a265c43d6afe2e8d859837925fef2331b9e541e31a3 UID: 991d3c32-0d4f-43e8-bcdc-c5a5d038cd38 Spec: Attacher: csi-nfsplugin Node Name: k8s-dev Source: Persistent Volume Name: data-nfsplugin Status: Attached: true Events: &lt;none&gt;  譬如從 spec 可以看到有一個掛載的行為是 透過 Attacher csi-nfsplugin 於 node k8s-dev 上掛載一個 volume，而該 volume 的來源是名為 data-nfsplugin 的 PVC 所定義的。 這些資訊都可以幫忙釐清與確認當前是否 CSI 的儲存功能有正常運作，我覺得相對於之前單純的 PV/PVC 有來得清楚一些。 Summary 本文基於 CSI 架構下部署了一個 NFS 的儲存方案，並且用一個 Pod 作為掛載的範例，來實際部署的流程與架構，同時觀察整個 kubernetes 本身是否有新增加的資源與內容。 參考 https://github.com/kubernetes-csi/csi-driver-nfshttps://kubernetes-csi.github.io/docshttps://github.com/kubernetes-csi/node-driver-registrarkubernetes.io/blog/2019/01/15/container-storage-interface-ga/ ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/csi-other","content":"前言 作為 Container Storage Interface 的最後一篇，如同慣例這篇文章就不會探討太多深入的議題，分享一些我自己目前對儲存這方面的一些經驗與想法。 就 CRI/CNI/CSI 三個面向的議題來說，我認為 CSI 是最困難的，對於 CRI 來說，一般使用根本不會在意底層到底是走哪套實現來提供容器的功能，更何況很多公有雲提供的 kubernetes service 是根本沒有機會讓你去修改 kubelet 的參數，讓你來替換不同的 CRI 解決方案。 而 CNI 的選擇相對於 CRI 又多了一些，不過滿多的使用情境都是基於網路能通即可，這種情況下反而 CNI 的選擇也不會太大的問題。不過相較於 CRI來說，地端部署的 kubernetes 有時候會有滿多網路的限制需要處理，甚至是一些 load-balancer 等相關功能的引進，這時候也需要對 CNI 稍微看一下是否能完全支援所需。 此外就如同 CRI 一樣，公有雲上面的 kubernetes service 也是不方便讓你去修改 CNI 的設定，通常都會與該公有雲本身的基礎建設整合一起。 最後的 CSI 則不一樣了，不同於 CRI/CNI 為一個堪用容器的最低限度，很多時候甚至沒有額外的儲存設備整個 kubernetes cluster 還是可以運作得很好 而且 CSI 本身又因為透過 StorageClass, PV, PVC 等不同層次架構的抽象化，比較不會有公有雲跟自架的區別，你可以隨時安裝相關的套件到需要使用的節點上去，而且也可以隨時抽換，這相對起來選擇性非常多。也因為選擇性眾多，才會導致選擇性障礙而難以選擇一個適合自己的儲存解決方案。 儲存選擇 儲存方案的互相比較一直都沒有停止，不同的儲存方案都有各自的特色與使用場景，就如同第一篇文章所說的，很多時候都要先從使用情境下手，去估算 空間要多大，會增長嗎存取速度要多快，是讀重要，還是寫重要讀寫的資料是隨機讀寫，還是依序讀寫希望面向的是 Block Device 還是 Mountable Volume?希望可以多重讀寫？本身能否有快照? 快照大概要多久快照能否復原，復原大概要多久儲存設備本身要地端還是雲端?本身能不能有備份的效果，硬碟壞掉能夠承受多少顆？ 相關的議題列也列不完，針對每個答案都會有不同適合的儲存方案可以使用，而每個儲存方案又分成 開源軟體，自己架設自己維護購買商業解決方案，從硬體到軟體一次搞定 這時候又要考量到預算的多寡，到底是找人維護方便還是購買解決方案方便? 這個問題也是沒有標準答案，就看每個單位自己的處理方式 選擇玩解決方案後，如果今天要導入到不同的管理平台，又要開始思考這些解決方案跟 kubernetes 是否整合良好？ 整合的專案是否屬於活躍維護的狀態，相關的議題跟開發熱度夠不夠，甚至說是不是廠商自己有提供的相關的整合 無論如何，我堅決反對基於 別人怎麼說，所以我就想要 的這種心態去選擇儲存方案，沒有事先研究與評估，這種通常到後來都只會帶來各種災難。 Mount 接下來探討一下關於 Mount Propagation 的概念，這篇功能於 kubernetes v1.8 之後推出，再一些比較複雜的系統設定中都會看到需要設定 Mount propagation，但是這個功能到底會做造成什麼變化與影響 詳細的過程非常複雜，牽扯到 Linux Kernel 內對於檔案系統的實作以及變化，欲知詳情可以參閱這篇文章 Kubernetes Mount Propagation。 一個最簡單會影響的功能就是對已經 Mount 過後的資料夾內，再次透過 Mount 掛載資料夾進去會發生什麼事情 vagrant@k8s-dev:~$ sudo docker run -d -v /home/vagrant/kubeDemo/:/kubeDemo --name false hwchiu/netutils 288ddbb01a1b0e020bc227f1e9dfb58e9aba8885256bd394ebe2c74fbb6f05ad 首先我們透過 docker 創建一個測試的容器，命名為 false，接者將本地的 /home/vagrant/kubeDemo 資料夾掛載進去到容器裡面的 /kubeDemo vagrant@k8s-dev:~$ sudo docker exec false ls /kubeDemo Vagrantfile cert-manager dns docker ingress services vagrant@k8s-dev:~$ ls /home/vagrant/kubeDemo/ cert-manager dns docker ingress services Vagrantfile 接者先觀察 mount 的資訊是否正確，兩邊看到的資料夾與內容一致 vagrant@k8s-dev:~$ mkdir kubeDemo/k8s vagrant@k8s-dev:~$ sudo mount --bind k8s-course/ kubeDemo/k8s/ vagrant@k8s-dev:~$ ls -ls kubeDemo/ total 28 4 drwxrwxr-x 3 vagrant vagrant 4096 Sep 29 07:17 cert-manager 4 drwxrwxr-x 7 vagrant vagrant 4096 Sep 29 07:17 dns 4 drwxrwxr-x 2 vagrant vagrant 4096 Sep 29 07:17 docker 4 drwxrwxr-x 2 vagrant vagrant 4096 Sep 29 07:17 ingress 4 drwxrwxr-x 12 vagrant vagrant 4096 Sep 29 07:17 k8s 4 drwxrwxr-x 5 vagrant vagrant 4096 Sep 29 07:17 services 4 -rw-rw-r-- 1 vagrant vagrant 2483 Sep 29 07:17 Vagrantfile vagrant@k8s-dev:~$ ls -ls kubeDemo/k8s/ total 40 4 drwxrwxr-x 6 vagrant vagrant 4096 Sep 29 07:17 addons 4 drwxrwxr-x 3 vagrant vagrant 4096 Sep 29 07:17 docker 4 drwxrwxr-x 3 vagrant vagrant 4096 Sep 29 07:17 harbor 4 drwxrwxr-x 5 vagrant vagrant 4096 Sep 29 07:17 kubeflow 4 drwxrwxr-x 4 vagrant vagrant 4096 Sep 29 07:17 load-balancing 4 drwxrwxr-x 5 vagrant vagrant 4096 Sep 29 07:17 manual-installation 4 drwxrwxr-x 4 vagrant vagrant 4096 Sep 29 07:17 minikube-lab 4 drwxrwxr-x 4 vagrant vagrant 4096 Sep 29 07:17 multi-cluster 4 drwxrwxr-x 4 vagrant vagrant 4096 Sep 29 07:17 practical-k8s 4 -rw-rw-r-- 1 vagrant vagrant 250 Sep 29 07:17 README.md 接下來嘗試透過 mount 指令，把本機端的另外一個資料夾 k8s-course 給掛載到 kubeDemo/k8s ，這時候本機上面透過 ls 的指令可以順利觀察到於 kubeDemoo/k8s 中有完全跟 k8s-course 一樣的內容。 到這個步驟都如同預料的一樣。 vagrant@k8s-dev:~$ sudo docker exec false ls /kubeDemo Vagrantfile cert-manager dns docker ingress k8s services vagrant@k8s-dev:~$ sudo docker exec false ls -l /kubeDemo/k8s total 0 下一步則是我們嘗試觀察容器中的資料，就會發現完全資料，只有看到一個完全空的 k8s 資料夾，代表外面主機上面的 mount 並沒有真實的反映於容器內。 這個問題就是因為 mount propagation 並沒有一路傳遞進去，所以對於之後才新增加的 mount point 就沒有辦法被容器內看到，反之亦然。 為了解決這個問題我們可以開啟 mount propagatioon: shard，將上述的環境清除後來重新測試 vagrant@k8s-dev:~/kubeDemo$ sudo docker run -d --mount type=bind,src=/home/vagrant/kubeDemo/,dst=/kubeDemo,bind-propagation=shared --name true hwchiu/netutils bd70f4044d468fa9794e798c64e313057cf14d4ee12d3cdac748a4206fce3249 vagrant@k8s-dev:~/kubeDemo$ sudo docker exec true ls /kubeDemo Vagrantfile cert-manager dns docker ingress services 一樣創建容器，只是這次我們採用更複雜的指令來設定 mount 相關的選項，type=bind,src=/home/vagrant/kubeDemo/,dst=/kubeDemo,bind-propagation=shared， 透過該指令我們可以做到跟之前一樣的對照關係，同時特別設定 mount propagation 為 shared。 vagrant@k8s-dev:~$ mkdir kubeDemo/k8s vagrant@k8s-dev:~$ sudo mount --bind k8s-course/ kubeDemo/k8s/ vagrant@k8s-dev:~$ sudo docker exec true ls -l /kubeDemo/k8s total 40 -rw-rw-r-- 1 1000 1000 250 Sep 29 07:17 README.md drwxrwxr-x 6 1000 1000 4096 Sep 29 07:17 addons drwxrwxr-x 3 1000 1000 4096 Sep 29 07:17 docker drwxrwxr-x 3 1000 1000 4096 Sep 29 07:17 harbor drwxrwxr-x 5 1000 1000 4096 Sep 29 07:17 kubeflow drwxrwxr-x 4 1000 1000 4096 Sep 29 07:17 load-balancing drwxrwxr-x 5 1000 1000 4096 Sep 29 07:17 manual-installation drwxrwxr-x 4 1000 1000 4096 Sep 29 07:17 minikube-lab drwxrwxr-x 4 1000 1000 4096 Sep 29 07:17 multi-cluster drwxrwxr-x 4 1000 1000 4096 Sep 29 07:17 practical-k8s 反覆上面的動作，接下來直接去容器內觀察，就會發現這時候可以看到相關的資料夾了，這樣就解決了沒辦法關看到後續新增的 mount point 的問題。 前一篇講到 CSI 解決方案的範例中，都會把大量的 /var/lib/kubelet 相關的資料夾掛載到 Pod 之中使用，對於 CSI Node 來說已經牽扯到真正與容器有關的空間掛載，所以這時候就會有需要再 CSI Node 容器內處理相關問題的需求，而這些更動也需要讓外面主機可以觀察到，所以都會看到對於 mount propagation 都需要設定。 Summary 作為 CSI 的最後一篇文章，差不多把 CSI 相關的資訊都討論了一遍，最後就是基於自己的需求去選擇一個需要的解決方案，並且確認該解決方案是否已經有良好相容的 CSI 實作。 參考 https://medium.com/kokster/kubernetes-mount-propagation-5306c36a4a2d","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-cni","content":"","keywords":"","version":"Next"},{"title":"Vagrant​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cni#vagrant","content":"這邊先跟大家分享一下自己平常使用的 Vagrant 腳本，我習慣在 MAC 上面透過 Vagrant 的方式去創建一個單機的 Kubernetes Cluster 來進行測試，主要是快速且方便，且該腳本中還會透過 kubeadm 的方式自動建立 kubernetes cluster ，安裝 Flannel CNI 以及透過 taint 的方式允與 master node 運行 Pod。 # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;bento/ubuntu-18.04&quot; config.vm.hostname = 'k8s-dev' config.vm.define vm_name = 'k8s' config.vm.provision &quot;shell&quot;, privileged: false, inline: &lt;&lt;-SHELL set -e -x -u export DEBIAN_FRONTEND=noninteractive #change the source.list sudo apt-get update sudo apt-get install -y vim git cmake build-essential tcpdump tig jq # Install ntp sudo apt-get install -y ntp # Install Docker # kubernetes official max validated version: 17.03.2~ce-0~ubuntu-xenial export DOCKER_VERSION=&quot;18.06.3~ce~3-0~ubuntu&quot; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; sudo apt-get update sudo apt-get install -y docker-ce=${DOCKER_VERSION} # Install Kubernetes export KUBE_VERSION=&quot;1.13.5&quot; export NET_IF_NAME=&quot;enp0s8&quot; curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee --append /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubeadm=${KUBE_VERSION}-00 kubelet=${KUBE_VERSION}-00 kubectl=${KUBE_VERSION}-00 kubernetes-cni=0.7.5-00 # Disable swap sudo swapoff -a &amp;&amp; sudo sysctl -w vm.swappiness=0 sudo sed '/swap.img/d' -i /etc/fstab sudo kubeadm init --kubernetes-version v${KUBE_VERSION} --apiserver-advertise-address=172.17.8.101 --pod-network-cidr=10.244.0.0/16 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml kubectl taint nodes --all node-role.kubernetes.io/master- SHELL config.vm.network :private_network, ip: &quot;172.17.8.101&quot; config.vm.provider :virtualbox do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, 2] v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, 4096] v.customize ['modifyvm', :id, '--nicpromisc1', 'allow-all'] end end  接下來我的操作環境都會基於這個範例去操作，唯一要注意的就是如果要修改 kubernetes 版本的話，要注意 flannel 的 yaml 是否支援當前版本。 譬如 kubernetes 1.16 中 daemonset 就類型就要修改成 apps/v1。 ","version":"Next","tagName":"h2"},{"title":"kubelet​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cni#kubelet","content":"kubernetes 目前在網路方面提供兩者設定，再啟動 kubelet 的時候可以透過參數 --network-plugin 的方式來決定要如何設定該 kubernetes cluster 的網路解決方案 目前支援兩種設定 cnikubenet 接下來針對這兩種選項討論一下彼此的用法與架構 ","version":"Next","tagName":"h2"},{"title":"CNI​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cni#cni","content":"CNI 顧名思義就是使用各式各樣的 CNI 解決方案來為 Kubernetes cluster 提供網路能力， 前述文章討論 CNI 的時候，有提到兩個跟檔案有關的概念，一個是 CNI 解決方案的 binary，另外一個則是基於 json 格式的 CNI 設定檔案。 有這些檔案就會有檔案存取的問題需要處理，因此 kubelet 這邊就必須要針對上述兩種類型的檔案設定其系統位置。 該兩個參數分別是 --cni-bin-dir 預設情況是在 /opt/cni/bin，用來存放各式各樣的 CNI 解決方案的執行檔--cni-conf-dir 預設情況是在 /etc/cni/net.d/，用來存放當前要提供給 CNI 使用的設定檔案，檔案格式就如同前篇討論過的 Network Configuration 或是 Network Configuration List。 我們先來觀察一下當前 kubelet 的設定 vagrant@k8s-dev:~$ ps axuw | grep kubelet | grep cni root 2433 2.0 2.2 1346404 90016 ? Ssl 00:37 4:21 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1 --resolv-conf=/run/systemd/resolve/resolv.conf  可以觀察到預設情況下， kubeadm 會使用 cni 的方式作為 network-plugin 的選項，此時我們來看看上述提到的兩個資料夾到底放了什麼檔案 vagrant@k8s-dev:~$ sudo ls /opt/cni/bin/ bridge dhcp flannel host-device host-local ipvlan loopback macvlan portmap ptp sample tuning vlan vagrant@k8s-dev:~$ file /opt/cni/bin/* /opt/cni/bin/bridge: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/dhcp: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/flannel: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/host-device: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/host-local: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/ipvlan: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/loopback: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/macvlan: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/portmap: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/ptp: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/sample: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/tuning: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped /opt/cni/bin/vlan: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped  首先我們先觀察 --cni-bin-dir 設定的資料夾，裡面放置了各式各樣的執行檔，每個執行檔案的名稱必須與 Network Configuration 裡面的 type 一致。 上述所有的執行檔除了 flannel 外都是內建的，其他的執行檔都是 ContainerNetworking GitHub 官方維護的，算是提供一些最基本的 CNI 解決方案給其他的開發者重複利用。 所以如果今天需要自行開發一套 CNI，則必須要將該執行檔也放入到這個資料夾中才可以被使用。 vagrant@k8s-dev:~$ sudo ls /etc/cni/net.d/ 10-flannel.conflist vagrant@k8s-dev:~$ sudo cat /etc/cni/net.d/10-flannel.conflist { &quot;name&quot;: &quot;cbr0&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ] } vagrant@k8s-dev:~$  首先我們先觀察 --cni-conf-dir 設定的資料夾，裡面只有放置一個檔案 10-flannel.conflist， 這個檔案是安裝 flannel 時一併安裝進來的。預設情況下該資料夾是完全空的，不會有任何設定。 所以這個運作流程是欲使用的 CNI 解決方案必須要 安裝欲使用的 binary 檔案放到 --cni-bin-dir 的位置將要使用的 CNI 設定檔案(json) 放到 --cni-conf-dir 的位置 同時這邊要注意的是 CNI 是伴隨 kubelet 去運行的，因此 kubernetes cluster 內的所有節點都要獨立進行上述的設定，所以其實也是可以做到每一台節點使用不同的 CNI 解決方案或是相同解決方案但是採用不同的設定檔案。 另外上述 flannel 安裝的檔案 10-flannel.conflist 是一個 Network Configuration List 的格式 其裡面描述了兩個要依序執行的 CNI binary,分別是 flannel 以及 portmap，所以也要確認這兩個檔案都有在 --cni-bin-dir 的位置內。  &quot;plugins&quot;: [ { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ]  由這邊可以觀察到 flannel 本身並不是單純只依賴自己的 binary 去完成所需功能的，實際上背後用到的 binary 數量更多，之後會有篇文章詳細介紹 flannel 的運作原理與安裝過程。 所以用一個流程來描述一下當前環境內的 CNI 運作流程 使用者發送創建 Pod 的請求，最後送到 KubeletKubelet 透過 dockershim + docker-containerd 的方式創建出需要的 container當 Pod 創建完畢後， kubelet 準備透過 CNI 的流程來設定該 Pod 內的網路 a. 去搜尋 --cni-conf-dir 資料夾下的所有檔案，依照字母排序的方式找到第一個合法的設定檔案，這個範例就是 10-flannel.conflistb. 解讀該設定檔案的內容 Network Configuration List， 接者針對每個該陣列內的內容去解讀，讀取到第一個項目的 type 是 flannelc. 將所有需要的參數準備，呼叫 --cni-bin-dir 底下的 flannel binary d. 執行完畢將相關的 STDOUT 準備好，然後再次呼叫 --cni-bin-dir 底下的 portmap binaryCNI 相關程序執行完畢 ","version":"Next","tagName":"h3"},{"title":"Kubenet​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cni#kubenet","content":"剛剛前述談了這麼多，接下來看一下另外一個 network-plugin 的選項 kubenet kubenet 是個非常簡單的 L2 Bridge 實現，背後的實現也還是透過基本的 CNI 解決方案來建立一個非常簡單的環境，這個環境簡單到根本不能實現節點對節點溝通，只能用在單一節點或是你有其他的方式去設定節點間的網路路由問題。 使用這個 Kubenet 選項的話，也是要確認系統上 --cni-bin-conf 裡面有下列的 binary bridgelohost-local 所以基本上這個選項就是拿來測試用，不會拿來商用現在也不是預設選項，有興趣的瞭解更多細節的細節可以參考官網說明 ","version":"Next","tagName":"h3"},{"title":"Pause Container​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cni#pause-container","content":"上篇文章有討論過所有的 Sandbox Container 的概念，作為一個創世 container，讓使用者的 container 都跟其共享 network namespace，藉此滿足多個 container 可以共用 網卡名稱IP 地址IPtables 規則路由規則...等各種網路資源 上圖節錄自Containerd Brings More Container Runtime Options for Kubernetes 而 kubernetes 之中也是用了相同概念的方式去實現所謂的 Pod，而這個 Sandbox Contaienr 也被稱為 Infrastructure Container。 如果仔細觀察 kubelet 的設定參數就會看到一個相關的設定 vagrant@k8s-dev:~$ ps axuw | grep kubelet | grep cni root 2433 2.0 2.2 1346404 90016 ? Ssl 00:37 4:21 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1 --resolv-conf=/run/systemd/resolve/resolv.conf  上述的設定中將 --pod-infra-container-image 設定成 k8s.gcr.io/pause:3.1, 這個是其 container image 的位置，代表 kubernetes 會透過這個 image 創立一個 sandbox container 當做基底，接下來才把使用者規劃的 container 都與其共享 network namespace 這時候如果透過 docekr ps 的指令去觀察，可以觀察到有非常多的 pause container 被創立，而其 container name 都是一個一個的 Pod vagrant@k8s-dev:~$ sudo docker ps | grep pause 10a6fcd1692d k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_coredns-86c58d9df4-srncw_kube-system_a448c5f9-dc93-11e9-9d35-080027c2be11_3 104bb6a0c83e k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_coredns-86c58d9df4-v8f5d_kube-system_a449ee4f-dc93-11e9-9d35-080027c2be11_4 4581173892a6 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_kube-proxy-dl4bm_kube-system_a46016b1-dc93-11e9-9d35-080027c2be11_1 209b3af9308c k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_kube-flannel-ds-amd64-5xmgf_kube-system_a4626d19-dc93-11e9-9d35-080027c2be11_1 1710678a5a02 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_kube-scheduler-k8s-dev_kube-system_15c129447b0aa0f760fe2d7ba217ecd4_1 c63c097c20c5 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_kube-controller-manager-k8s-dev_kube-system_8a8f55dd50b2821b309adc83a00139ff_1 3bee914719b2 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_kube-apiserver-k8s-dev_kube-system_d834af7c4c9483e2d999ded255dd7798_1 82aa2e03cec9 k8s.gcr.io/pause:3.1 &quot;/pause&quot; 6 hours ago Up 6 hours k8s_POD_etcd-k8s-dev_kube-system_71e763946160f2ea04d6946ece43e176_1  Pause container 的原始內容都可以在 GitHub 這邊找到，其實作非常簡單，就是一個攔截相關系統訊號後透過 system call 進行 pause 的應用程式而已，相關內容如下。 /* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ #include &lt;signal.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;sys/types.h&gt; #include &lt;sys/wait.h&gt; #include &lt;unistd.h&gt; #define STRINGIFY(x) #x #define VERSION_STRING(x) STRINGIFY(x) #ifndef VERSION #define VERSION HEAD #endif static void sigdown(int signo) { psignal(signo, &quot;Shutting down, got signal&quot;); exit(0); } static void sigreap(int signo) { while (waitpid(-1, NULL, WNOHANG) &gt; 0) ; } int main(int argc, char **argv) { int i; for (i = 1; i &lt; argc; ++i) { if (!strcasecmp(argv[i], &quot;-v&quot;)) { printf(&quot;pause.c %s\\n&quot;, VERSION_STRING(VERSION)); return 0; } } if (getpid() != 1) /* Not an error because pause sees use outside of infra containers. */ fprintf(stderr, &quot;Warning: pause should be the first process\\n&quot;); if (sigaction(SIGINT, &amp;(struct sigaction){.sa_handler = sigdown}, NULL) &lt; 0) return 1; if (sigaction(SIGTERM, &amp;(struct sigaction){.sa_handler = sigdown}, NULL) &lt; 0) return 2; if (sigaction(SIGCHLD, &amp;(struct sigaction){.sa_handler = sigreap, .sa_flags = SA_NOCLDSTOP}, NULL) &lt; 0) return 3; for (;;) pause(); fprintf(stderr, &quot;Error: infinite loop terminated\\n&quot;); return 42; }  ","version":"Next","tagName":"h2"},{"title":"CRI Impact​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cni#cri-impact","content":"最後來分享的是 CRI 對 CNI 的關係 CNI 作為完成整體 Container 創建的一部份，所以都是由 CRI 解決方案那邊去處理相關流程，因此不同的 CRI 解決方案對於 CNI 的處理流程也就不一定相似。 譬如上述有提到於 dockershime 的環境下，會將 --cni-conf-dir 內的設定檔案進行排序，並且使用第一個合法的設定檔案。 相關的原始碼可以參考 GitHub 而如果採用的是基於 containerd 或是 cri-o 這種解決方案的話，最後都會透過 ocicni 這個工具去處理，而同樣的流程其運作過程就不太一樣，反而會是一次讀取全部的資料進來，之後再慢慢處理 相關的原始碼可以參考 GitHub Summary 本篇文章跟大家討論了下 Kubernets 與 CNI 相關的設定，只要對這些資料夾有基本的理解，以後看到任何 kubernetes cluster 都能夠有辦法粗略的看一下這個 cluster 是用哪套 CNI 來提供服務的。 特別的是如果你今天使用的是公有雲的服務，譬如 GKE, AKS, EKS 等服務，你也可以嘗試去看看這些公有雲到底是用哪套 CNI, 其設定檔案又是如何，接者可以搭配他們的文件或是原始碼去理解其其運作原理。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-i","content":"","keywords":"","version":"Next"},{"title":"Docker & Kubernetes​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-i#docker--kubernetes","content":"解釋 Docker 與 kubernetes 的最好方法就是閱讀官方部落格的文章 kubernetes-containerd-integration-goes-ga。 為了節省讀者的時間，接下來就幫大家導讀一下這篇文章，以下的圖片都來自於上述的官方部落格 上面提到 CRI 本身是個溝通介面，這代表溝通的兩方都需要根據這個界面去實現與滿足。 對於 kubernetes 來說，kubelet 自己維護與開發的，要支援 CRI 本身就不是什麼困難的事情。 但是另外一端如果要使用 docker 的話，那到底要怎麼辦?docker 背後也是有公司再經營，也不是說改就改，這種情況下到底要如何將 docker 給整合進來？ 最直觀的想法就是如果沒有辦法使得 docker 本身支援 CRI 的標準，那就額外撰寫一個轉接器，其運作在 kubelet 與 Docker，該應用程式上承 CRI 與 kubernetes 溝通，下承 Docker API 與 Docker Engine 溝通 早期的 kubernetes 採取了這種做法，kuberlet 內建相關了 dockershim 的程式碼來處理這段邏輯。這種做法可行，但是其實效能大大則扣，同時也把整體架構帶到了更複雜的境界，引進愈來愈多的元件會對開發與除錯帶來更大的成本。 可以參考下圖中的上半部份，而圖中的下半部分則是後來的改變之處 (圖片擷取自：kubernetes blog kubernetes-containerd-integration-goes-ga) 反正最後都是透過 containerd 進行操作，而本身也不太需要 docker 自己的功能，那是否就直接將 dockershim 溝通的角色從 docker engine 轉移到 containerd 即可。 因此後來又有所謂的 CRI-Containerd 的出現。 到這個階段，已經減少了一個溝通的 Daemon, 也就是 docker engine。 但是這樣並不能滿足想要最佳化的心情。 伴隨者 Containerd 本身的功能開發，提供了 Plugin 這種外掛功能的特色後，將 CRI-Containerd 的功能直接整合到該 Plugin 內就可以直接再次減少一個元件，直接讓 kubelet 與 containerd 直接溝通來創建相關的 container. 相關的演進可以參考下圖(圖片擷取自：kubernetes blog kubernetes-containerd-integration-goes-ga) 同時根據該篇文章內關於效能的評比，可以看到目前這個整合的版本不論是 CPU 或是 Memory 等系統資源的消耗都遠比過往還來得少。 (圖片擷取自：kubernetes blog kubernetes-containerd-integration-goes-ga) (圖片擷取自：kubernetes blog kubernetes-containerd-integration-goes-ga) 這種架構下，使用者可以在一台伺服器中同時安裝 kubernetes 與 docker, 同時彼此會共用 containerd 來管理自己所需要的 container. 架構如下圖，有趣的一點在於這種情況下要如何確保 docker 的指令不會看到 kubernetes 所要求創建的 container, 反之亦然。 兩者都是透過 containerd 來創建 Container, 幸好有鑒於 containerd 本身提供的 namespace 的功能，可以確保不同的客戶端 docekrd, CRI-plugin 都可以有自己的 namespace，所以用 docker ps 就不會互相影響到彼此的運作。(圖片擷取自：kubernetes blog kubernetes-containerd-integration-goes-ga) 不過上述的假設是 啟用 containerd 於 kubernetes cluster 內才會有這個效果。 根據 這篇官方文章 install-kubeadm，目前預設的情況下都還是採用第一種方案, dockershim 的方式來使用，若需要使用 containerd 則必須要先安裝 containerd 到系統之中並且安裝 kubernetes 時設定特定的參數來切換過去。 下一篇文章就會跟大家分享要如何透過創建一個可以使用 containerd 的 local kubernetes cluster, 並且使用 CRI 的工具 crictl 來操作相關的容器創建。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii","content":"","keywords":"","version":"Next"},{"title":"環境需求​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii#環境需求","content":"OS: Ubuntu 16.04 18.04 會因為 kubernetes deb 相關的問題導致套件更新失敗，建議先基於 16.04 測試即可 Python: 2.7+Ansible: 2.4+ 有沒有使用 ansible 都無所謂，因為我們會拆解裡面的步驟，去看看全部的安裝步驟 ","version":"Next","tagName":"h2"},{"title":"環境安裝​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii#環境安裝","content":"環境的部分只有包括 containerd 的相依套件，並不包含 kubernetes 叢集本身，但是有包含 kubeadm, kubectl, kubelet 相關套件。 ","version":"Next","tagName":"h2"},{"title":"Ansible 版本​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii#ansible-版本","content":"基本上官方文件已經將相關的步驟都建置完畢，本身只需要準備 hosts 的檔案即可，該 ansible playbook 支援 Ubuntu 以及 CentOS 兩套發行版本。 首先在 ansible host 執行下列指令抓取相關的 ansible-playbook git clone https://github.com/containerd/cri cd ./cri/contrib/ansible  接下來根據需求產生對應的 hosts 檔案，裡面放置要被安裝的機器，或是直接採用 localhost 的方式再本機運行該 playbook 即可。 這部分比較屬於 ansible 的方式，不熟悉的讀者可以自行尋找資源看看如何運行，或是放棄採用 ansible, 而是逐條逐條的自行安裝 ansible-playbook -i hosts cri-containerd.yaml  順利跑完應該會看到類似下圖的結果 ... TASK [Create a directory for cni binary] ***********************************************************************************************************************************$ changed: [localhost] TASK [Create a directory for cni config files] *****************************************************************************************************************************$ changed: [localhost] TASK [Create a directory for containerd config] ****************************************************************************************************************************$ changed: [localhost] TASK [Start Containerd] ****************************************************************************************************************************************************$changed: [localhost] TASK [Load br_netfilter kernel module] *************************************************************************************************************************************$ changed: [localhost] TASK [Set bridge-nf-call-iptables] *****************************************************************************************************************************************$ changed: [localhost] TASK [Set ip_forward] ******************************************************************************************************************************************************$ changed: [localhost] TASK [Check kubelet args in kubelet config] ********************************************************************************************************************************$ changed: [localhost] TASK [Add runtime args in kubelet conf] ************************************************************************************************************************************$ skipping: [localhost] TASK [Start Kubelet] *******************************************************************************************************************************************************$ changed: [localhost] TASK [Pre-pull pause container image] **************************************************************************************************************************************$ changed: [localhost] PLAY RECAP ****************************************************************************************************************************************************************** ...  ","version":"Next","tagName":"h3"},{"title":"手動安裝版本​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii#手動安裝版本","content":"如果不想透過 ansible 一鍵安裝完畢，接下來透過拆解該 ansible playbook 就可以知道實際上要做哪些步驟來建置 containerd 的環境。 讀取下列變數 var/vars.yaml - containerd_release_version: 1.1.0-rc.0 - cni_bin_dir: /opt/cni/bin/ - cni_conf_dir: /etc/cni/net.d/ 第一個指明 containerd 的版本後，後續兩個變數則是 CNI (Container Network Interface) 會用到的路徑，之後的文章會細部探討 CNI 的設計與使用。根據發行版本讀取不同的檔案來安裝相關套件 (Ubuntu 為範例) unziptarapt-transport-httpsbtrfs-toolslibseccomp2socatutil-linux 安裝 kubernetes 會用到的相關工具 kubeletkubeadmkubectl 安裝 containerd 會用到的相關工具 cri-containerd來源網址是 https://storage.googleapis.com/cri-containerd-release, 點進去可以看到各種不同版本的 cri-containerd.解壓縮該安裝檔案後可以看到有下列內容，包含了 systemd 的設定，相關的執行檔案。 ./ ./opt/ ./opt/containerd/ ./opt/containerd/cluster/ ./opt/containerd/cluster/gce/ ./opt/containerd/cluster/gce/cloud-init/ ./opt/containerd/cluster/gce/cloud-init/node.yaml ./opt/containerd/cluster/gce/cloud-init/master.yaml ./opt/containerd/cluster/gce/configure.sh ./opt/containerd/cluster/gce/env ./opt/containerd/cluster/version ./opt/containerd/cluster/health-monitor.sh ./usr/ ./usr/local/ ./usr/local/sbin/ ./usr/local/sbin/runc ./usr/local/bin/ ./usr/local/bin/crictl ./usr/local/bin/containerd ./usr/local/bin/containerd-stress ./usr/local/bin/critest ./usr/local/bin/containerd-release ./usr/local/bin/containerd-shim ./usr/local/bin/ctr ./etc/ ./etc/systemd/ ./etc/systemd/system/ ./etc/systemd/system/containerd.service ./etc/crictl.yaml  透過 systemd 啟動 containerd設定 netfilter 相關設定，讓 kernel 啟動相關功能 - br_netfilter -&gt; bridge 層級啟動 netfilter (ebtables) - bridge-nf-call-iptables -&gt; bridge 層級也會去呼叫 iptables 處理 - ip_forward -&gt; 轉發 ip 封包 這些功能基本上以前安裝 docker 的時候都會幫忙處理，所以基本上都不會特別注意到接下來就是重頭戲了，如何讓 kubernetes 知道要使用 containerd 作為其 Container Runtime 而非使用 dockershim 來銜接 docker. 根據 官方文件， kubelet 裡面有下列的參數可以設定 --container-runtime string The container runtime to use. Possible values: 'docker', 'remote', 'rkt(deprecated)'. (default &quot;docker&quot;) --container-runtime-endpoint string[Experimental] The endpoint of remote runtime service. Currently unix socket is supported on Linux, and tcp is supported on windows. 其中 container-runtime 可以使用三種來處理，分別是內建的 docker, rkt 以及客製化的 remote. 當上述選擇了 remote 後，也必須要一起設定 container-runtime-endpoint 告訴 kubelet 這時候要怎麼跟非內建的 container runtime 溝通。 因此安裝過程中，該 ansible playbook 就會嘗試修改 kubelet 的啟動參數，這部分因為整個安裝過程都是透過 kubeadm 去架設 kubernetes cluster, 所以最後是修改 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 這個檔案. 關於 kubeadm 與 kubelet 彼此之間的設定過程可以參考下列文章 kubeadm/kubelet-integration vagrant@k8s-dev:~$ sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=&quot;KUBELET_EXTRA_ARGS= --runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot; Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot; Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot; # This is a file that &quot;kubeadm init&quot; and &quot;kubeadm join&quot; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS  當我們實際觀察這個檔案，可以發現裡面關於環境變數的部分加入了下列選項，這時候我們就明瞭到底 kubernetes 之後是如何跟 containerd 溝通的--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot; 最後就是啟動 kubelet 將基本的服務起來 ","version":"Next","tagName":"h3"},{"title":"Kubernetes Cluster​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii#kubernetes-cluster","content":"一切都準備就緒後，接下來非常簡單的透過 kubeadm 的方式來創建 kubernetes cluster. 這個範例中我採用 flannel 作為 CNI, 之後也會有文章詳細介紹 Flannel 的運作原理。 依序執行下列指令將 kubernetes cluster 給建立起來，並安裝 CNI 同時也打開taint 讓 master node 也可以部署 Pod. sudo swapoff -a &amp;&amp; sudo sysctl -w vm.swappiness=0 sudo kubeadm init --pod-network-cidr=10.244.0.0/16 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml kubectl taint nodes --all node-role.kubernetes.io/master-  這時候使用簡單的工具確認 kubernetes 有正常起來即可 vagrant@k8s-dev:~/cri/contrib/ansible$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5c98db65d4-ghzpl 0/1 Running 0 26s kube-system coredns-5c98db65d4-thsdq 0/1 Running 0 26s kube-system kube-flannel-ds-amd64-ztqgs 1/1 Running 0 14s kube-system kube-proxy-jfs66 1/1 Running 0 26s  觀察 ","version":"Next","tagName":"h2"},{"title":"Containerd​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii#containerd","content":"接下來我們要透過不同的工具來觀察當使用 containerd 作為部署時有什麼不同 由於此時我們不是透過 docker 來管理整個 kubernetes 底層需要的 container，而是透過 kubelet &amp; CRI &amp; containerd 來管理。 所以這時候就不能透過 docker ps 或是 docker images 等指令來看相關的 container 資源，取而代之的是 crictl 這個在前面步驟伴隨 containerd 一起安裝到系統內的工具。 vagrant@k8s-dev:~/cri/contrib/ansible$ crictl NAME: crictl - client for CRI USAGE: crictl [global options] command [command options] [arguments...] VERSION: 1.0.0-beta.0 COMMANDS: attach Attach to a running container create Create a new container exec Run a command in a running container version Display runtime version information images List images inspect Display the status of one or more containers inspecti Return the status of one ore more images inspectp Display the status of one or more pod sandboxes logs Fetch the logs of a container port-forward Forward local port to a pod sandbox ps List containers pull Pull an image from a registry runp Run a new pod sandbox rm Remove one or more containers rmi Remove one or more images rmp Remove one or more pod sandboxes pods List pod sandboxes start Start one or more created containers info Display information of the container runtime stop Stop one or more running containers stopp Stop one or more running pod sandboxes update Update one or more running containers config Get and set crictl options stats List container(s) resource usage statistics completion Output bash shell completion code help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --config value, -c value Location of the client config file (default: &quot;/etc/crictl.yaml&quot;) [$CRI_CONFIG_FILE] --debug, -D Enable debug mode --image-endpoint value, -i value Endpoint of CRI image manager service [$IMAGE_SERVICE_ENDPOINT] --runtime-endpoint value, -r value Endpoint of CRI container runtime service (default: &quot;unix:///var/run/dockershim.sock&quot;) [$CONTAINER_RUNTIME_ENDPOINT] --timeout value, -t value Timeout of connecting to the server (default: 10s) --help, -h show help --version, -v print the version  說明非常簡單，就是 CRI 命令列，其預設的設定檔案位於 /etc/crictl.yaml vagrant@k8s-dev:~/cri/contrib/ansible$ cat /etc/crictl.yaml runtime-endpoint: /run/containerd/containerd.sock  所以接下來呼叫的任何 crictl 指令都會跟 containerd 互動來取得回應，我們就根據上面的提示來試試看各種指令 基於 kubernetes POD 概念的資源顯示 vagrant@k8s-dev:~/cri/contrib/ansible$ sudo crictl pods PODSANDBOX ID CREATED STATE NAME NAMESPACE ATTEMPT 4ed054a456e96 17 minutes ago SANDBOX_READY coredns-5c98db65d4-ghzpl kube-system 0 c5209a01e0936 17 minutes ago SANDBOX_READY coredns-5c98db65d4-thsdq kube-system 0 0a771e9912232 18 minutes ago SANDBOX_READY kube-flannel-ds-amd64-ztqgs kube-system 0 0642c904298c8 18 minutes ago SANDBOX_READY kube-proxy-jfs66 kube-system 0 46039553a0fcc 18 minutes ago SANDBOX_READY kube-scheduler-k8s-dev kube-system 0 ec6d7cde198cc 18 minutes ago SANDBOX_READY kube-controller-manager-k8s-dev kube-system 0 be0bc17da244d 18 minutes ago SANDBOX_READY etcd-k8s-dev kube-system 0 490d8b240d8ca 18 minutes ago SANDBOX_READY kube-apiserver-k8s-dev kube-system 0  相關的 container images, 這些 images 再之前都是透過 docker pull 等方式安裝，如今我們系統內完全不裝 docker 也是可以使用，這一切都是仰賴 OCI 標準的制定外加 containerd 的幫忙。 vagrant@k8s-dev:~/cri/contrib/ansible$ sudo crictl images IMAGE TAG IMAGE ID SIZE k8s.gcr.io/coredns 1.3.1 eb516548c180f 12.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4ff 76.2MB k8s.gcr.io/kube-apiserver v1.15.3 5eb2d3fc7a44e 49.3MB k8s.gcr.io/kube-controller-manager v1.15.3 e77c31de55475 47.8MB k8s.gcr.io/kube-proxy v1.15.3 232b5c7931462 30.1MB k8s.gcr.io/kube-scheduler v1.15.3 703f9c69a5d57 29.9MB k8s.gcr.io/pause 3.1 da86e6ba6ca19 317kB quay.io/coreos/flannel v0.11.0-amd64 8a9c4ced3ff92 16.9MB vagrant@k8s-dev:~/cri/contrib/ansible$ sudo crictl ps  接下來看看運行的 container, 其結果跟 docker ps 非常相似，使用起來幾乎沒有任何違和感，我覺得設定 alias docker=crictl 都不會有什麼錯覺? vagrant@k8s-dev:~/cri/contrib/ansible$ sudo crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT 1cf00f473d03a sha256:eb516548c180f8a6e0235034ccee2428027896af16a509786da13022fe95fe8c 19 minutes ago CONTAINER_RUNNING coredns 0 221b815ca1f56 sha256:eb516548c180f8a6e0235034ccee2428027896af16a509786da13022fe95fe8c 19 minutes ago CONTAINER_RUNNING coredns 0 f35552b98abc1 sha256:8a9c4ced3ff92c63c446d55c2353c42410c78b497a0483d4048e8d30ebe37058 19 minutes ago CONTAINER_RUNNING kube-flannel 0 8eafd7980b1d3 sha256:232b5c79314628fbc788319e2dff191f4d08e38962e42ebd31b55b52fecd70ec 19 minutes ago CONTAINER_RUNNING kube-proxy 0 feb2ed1caa423 sha256:2c4adeb21b4ff8ed3309d0e42b6b4ae39872399f7b37e0856e673b13c4aba13d 20 minutes ago CONTAINER_RUNNING etcd 0 00be90f2a7ac0 sha256:703f9c69a5d578378a022dc75d0c242d599422a1b7cc9cf5279b49d39dc7ca08 20 minutes ago CONTAINER_RUNNING kube-scheduler 0 e16c5a24ff09d sha256:e77c31de554758f3f03e78e124a914b7d86de7d7cf3d677c9b720efb90a833f9 20 minutes ago CONTAINER_RUNNING kube-controller-m anager 0 fbe374eb60712 sha256:5eb2d3fc7a44e3a8399256d4b60153a1a59165e70334a1c290fcbd33a9a9d8a7 20 minutes ago CONTAINER_RUNNING kube-apiserver 0  除了 container 本身的管理外，我們來看一下系統內運行的應用程式, 可以觀察到系統上有一個 containerd 正在運行，同時 kuberlet 內關於 container-runtime 的參數有被修改。 此外對於每個運作的 container, containetd 都會產生一個 containerd-shim 來運作。 vagrant@k8s-dev:~/cri/contrib/ansible$ ps -axo command | grep containerd /usr/local/bin/containerd /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/conf ig.yaml --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --runtime-cgroups=/system.slice/containerd.service --co ntainer-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/490d8b240d8ca4c0df7d8440c5002ca795bddb989fb8bc6de2 0da78394eb8ce6 -address /run/containerd/containerd.sock -containerd-binary /usr/local/bin/containerd ... .....  ","version":"Next","tagName":"h2"},{"title":"Docker​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-ii#docker","content":"為了驗證之前說明是否 dockerd 與 kubelet 共用 containerd 但是彼此資源是互相隔離的，這時候我們來安裝一下 docker 並且於背景運行一個 container 試試看 sudo docker run -d hwchiu/netutils  接下來我們使用 docker 指令針對上述的操作都進行一次，來比較看看 vagrant@k8s-dev:~$ sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE hwchiu/netutils latest a0d1dad34d58 13 months ago 222MB vagrant@k8s-dev:~$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 320b653e39d8 hwchiu/netutils &quot;/bin/bash ./entrypo…&quot; 15 minutes ago Up 15 minutes inspiring_haslett  非常乾淨簡單，完全看不到任何 kubernetes 用到的容器資源，此外我們再度透過 ｀ps 觀察 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/320b653e39d8164e3033f1eec079c7772a10e703d24bb7f39f7bdf26fe084fe5 -address /run/containerd/containerd.sock -containerd-binary /usr/local/bin/containerd -runtime-root /var/run/docker/runtime-runc  首先，可以看到 dockerd 這時候也透過參數的方式去連接 containerd, 另外可以看到針對我上述的 docker image 所產生的 containerd-shim 與 kubernetes 產生的 containerd-shim 有明顯的參數不同 -namespace moby v.s k8s.ioworkdir 不同-runtime-root /var/run/docker/runtime-runc 此外我們也可以透過 containerd cli(ctr) and containerd 來觀察更多有趣的資訊 首先可以看到對於 containerd 的確有不同的 namespace，也的確如上面所觀察到的是 moby 與 k8s.io vagrant@k8s-dev:~$ sudo ctr namespaces ls NAME LABELS k8s.io moby  此外我們也可以看到不少關於 containerd 預設的設定，預設情況下 linux 環境中都是採用 runc 作為 OCI Runtime Spec 的解決方案。 而 kubelet 透過 CRI 與 docker 最後選擇都是會採取 plugin.linux 的設定，所以兩者背後最後都是透過 runc 來創建 Container。 sudo containerd config default ... [plugins.cri] stream_server_address = &quot;&quot; stream_server_port = &quot;10010&quot; enable_selinux = false sandbox_image = &quot;k8s.gcr.io/pause:3.1&quot; stats_collect_period = 10 systemd_cgroup = false [plugins.cri.containerd] snapshotter = &quot;overlayfs&quot; [plugins.cri.containerd.default_runtime] runtime_type = &quot;io.containerd.runtime.v1.linux&quot; runtime_engine = &quot;&quot; runtime_root = &quot;&quot; [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = &quot;&quot; runtime_engine = &quot;&quot; runtime_root = &quot;&quot; ... [plugins.linux] shim = &quot;containerd-shim&quot; runtime = &quot;runc&quot; runtime_root = &quot;&quot; no_shim = false shim_debug = false ...  Summary 本文中我們簡述了如何建置一套基於 containerd 而非 docker 的 kubernetes cluster, 並且透過相關指令與工具來觀察在此設定下，kubernetes,containerd, dockerd 等彼此的交互關係，可以更加深對於 CRI, OCI 等概念的理解。 kubelet -&gt; containerd -&gt; containetrd-shin(k8s.io) -&gt; runc docker client -&gt; docker enginer -&gt; containerd -&gt; containerd-shim(moby) -&gt; runc ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o","content":"","keywords":"","version":"Next"},{"title":"安裝 CRI-O​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o#安裝-cri-o","content":"基本上安裝的過程跟 containerd 大同小異，只是安裝的套件不同，同時最後設定 kubelet 的方式不同。 ","version":"Next","tagName":"h2"},{"title":"設定系統相關資訊​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o#設定系統相關資訊","content":"modprobe overlay modprobe br_netfilter echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables echo 1 &gt; /proc/sys/net/ipv4/ip_forward echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-ip6tables  ","version":"Next","tagName":"h3"},{"title":"安裝套件​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o#安裝套件","content":"apt-get update apt-get install software-properties-common add-apt-repository ppa:projectatomic/ppa apt-get update apt-get install cri-o-1.15 systemctl start cri-o  如果執行錯誤發現因為找不到相關的 /usr/local/libexec/crio/crio-wipe/crio-wipe.bash 這個檔案的話，可以手動幫忙建立個 soft link sudo ln -s /usr/libexec /usr/local/libexec  這個問題在 github 上面已經被回報，但是不確定是什麼時候會修復到打包的 deb 之中，至少我2019/09/18測試的時候還是壞掉的。 最後透過指令確認 cri-o 有正確運行 vagrant@k8s-dev:~$ sudo systemctl status cri-o ● crio.service - Container Runtime Interface for OCI (CRI-O) Loaded: loaded (/usr/lib/systemd/system/crio.service; disabled; vendor preset: enabled) Active: active (running) since Thu 2019-09-19 03:31:32 UTC; 20min ago Docs: https://github.com/cri-o/cri-o Main PID: 28333 (crio) Tasks: 16 Memory: 870.2M CPU: 28.468s CGroup: /system.slice/crio.service └─28333 /usr/bin/crio Sep 19 03:31:32 k8s-dev systemd[1]: Starting Open Container Initiative Daemon... Sep 19 03:31:32 k8s-dev systemd[1]: Started Open Container Initiative Daemon. Sep 19 03:31:57 k8s-dev systemd[1]: Started Open Container Initiative Daemon. Sep 19 03:32:10 k8s-dev systemd[1]: Started Container Runtime Interface for OCI (CRI-O).  ","version":"Next","tagName":"h3"},{"title":"安裝 kubernetes​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o#安裝-kubernetes","content":"","version":"Next","tagName":"h2"},{"title":"安裝套件​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o#安裝套件-1","content":"安裝 kubeadm/kubelet/kubectl 相關檔案工具，不再撰述其過程 apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl  接下來要使用 kubeadm 進行安裝·安裝步驟與之前 containerd 大同小異 由於部分需要的設定只能透過 config 的方式來修改，並不能像之前 containerd 的方式去改 systemd 裡面的環境變數，因此請增加 /etc/default/kubelet 這個檔案，內容如下 vagrant@k8s-dev:~$ cat /etc/default/kubelet KUBELET_EXTRA_ARGS=--feature-gates=&quot;AllAlpha=false,RunAsGroup=true&quot; --container-runtime=remote --cgroup-driver=systemd --container-runtime-endpoint='unix:///var/run/crio/crio.sock' --runtime-request-timeout=5m  ","version":"Next","tagName":"h3"},{"title":"建立叢集​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o#建立叢集","content":"透過下列指令依序建立叢集 sudo swapoff -a &amp;&amp; sudo sysctl -w vm.swappiness=0 sudo kubeadm init --pod-network-cidr=10.244.0.0/16 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml kubectl taint nodes --all node-role.kubernetes.io/master-  如果 cri-o 沒有正確安裝的話，會因為找不到相關的 unix socket，使得 kubelet 會嘗試去找 docker 來使用，但是因為我系統上面沒有 docker，因此會使得安裝失敗，訊息如下。 vagrant@k8s-dev:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 [init] Using Kubernetes version: v1.16.0 [preflight] Running pre-flight checks [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: docker is required for container runtime: exec: &quot;docker&quot;: executable file not found in $PATH  ","version":"Next","tagName":"h3"},{"title":"測試​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-cri-o#測試","content":"因為 cri-o 就是完全針對 cri + kubernetes 打造的，所以前述的 crictl 相關的工具都還是可以繼續使用 vagrant@k8s-dev:~$ sudo crictl images IMAGE TAG IMAGE ID SIZE k8s.gcr.io/coredns 1.6.2 bf261d1579144 44.2MB k8s.gcr.io/etcd 3.3.15-0 b2756210eeabf 248MB k8s.gcr.io/kube-apiserver v1.16.0 b305571ca60a5 219MB k8s.gcr.io/kube-controller-manager v1.16.0 06a629a7e51cd 165MB k8s.gcr.io/kube-proxy v1.16.0 c21b0c7400f98 87.9MB k8s.gcr.io/kube-scheduler v1.16.0 301ddc62b80b1 88.8MB k8s.gcr.io/pause 3.1 da86e6ba6ca19 747kB quay.io/coreos/flannel v0.11.0-amd64 ff281650a721f 55.4MB vagrant@k8s-dev:~$ sudo crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID 72d22f82eca39 ff281650a721f46bbe2169292c91031c66411554739c88c861ba78475c1df894 29 minutes ago Running kube-flannel 0 8c7db4df0ae25 bf7f4886d1a59 c21b0c7400f988db4777858edd13b6d3930d62d7ccf026d2415485a52037f384 38 minutes ago Running kube-proxy 0 ead4354c566f9 fa3d24cb95896 b2756210eeabf84f3221da9959e9483f3919dc2aaab4cd45e7cd072fcbde27ed 39 minutes ago Running etcd 0 b219a7e8b9d52 983924dffa404 301ddc62b80b16315d3c2653cf3888370394277afb3187614cfa20edc352ca0a 39 minutes ago Running kube-scheduler 0 6d1c2c8035d10 a04f0f3b253a6 06a629a7e51cdcc81a5ed6a3e6650348312f20c954ac52ee489a023628ec9c7d 39 minutes ago Running kube-controller-manager 0 5fa8e74c08bac a48e40de9a05b b305571ca60a5a7818bda47da122683d75e8a1907475681ee8b1efbd06bff12e 39 minutes ago Running kube-apiserver 0 014ba57340bf8  這時候透過 ps 等指令觀察一下系統中運行的指令 vagrant@k8s-dev:~$ sudo ps -x -awo command | grep cri /usr/bin/crio /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --feature-gates=AllAlpha=false,RunAsGroup=true --container-runtime=remote --cgroup-driver=systemd --container-runtime-endpoint=unix:///var/run/crio/crio.sock --runtime-request-timeout=5m /usr/libexec/crio/conmon -s -c 014ba57340bf8afd3b2ce6982b760ffac82ce1bc2861a2b02f8069c8becde304 -n k8s_POD_kube-apiserver-k8s-dev_kube-system_b00230a3af5f91e5d10118aee4d054c4_0 -u 014ba57340bf8afd3b2ce6982b760ffac82ce1bc2861a2b02f8069c8becde304 -r /usr/lib/cri-o-runc/sbin/runc -b /var/run/containers/s torage/overlay-containers/014ba57340bf8afd3b2ce6982b760ffac82ce1bc2861a2b02f8069c8becde304/userdata -p /var/run/containers/storage/overlay-containers/014ba57340bf8afd3b2ce6982b760ffac82ce1bc2861a2b02f8069c8becde304/userdata/pidfile -l /var/log/pods/kube-system_kube-apiserver-k8s-dev_b00230a3af5f91e5d10118aee4d054c4/014ba57340bf8afd3b2ce6982b760ffac82ce1bc2861a2b02f8069c8becde304.log --exit-dir /var/run/crio/exits --socket-dir-path /var/run/crio --log-level error --runtime-arg --root=/run/runc  可以觀察到 有一個名為 crio 的 daemon 運行kubelet 的參數都修改為去取 cri-o 配合crio 本身會 fork/exec 一個名為 conmon 的 process ，也因為這個 跟 kubernetes 是直接配合的，可以看到很多參數都直接跟 kubernetes 有關，譬如名稱是 k8s_POD_kube-apiserver-k8s-dev_kube-system_b00230a3af5f91e5d10118aee4d054c4_0，裡面描述了其 pod 的名稱，還有namespace。 Summay 到這邊為止，我們已經架設過基於 containerd 與 cri-o 等不同相容於 CRI 的解決方案，唯一可惜的就是我們的背後都是基於 runc 這套純 container 的運行方式。 因此接下來的數天我們將針對這一塊去探討其他滿足 OCI Runtime 卻不同於 runc 的解決方案，特別會開始跟 Virtual Machine 牽扯到一起。 參考 https://cri-o.iohttps://github.com/cri-o/cri-o/blob/master/tutorials/kubeadm.mdhttps://www.opencontainers.org/blog/2018/06/20/cri-o-how-standards-power-a-container-runtimehttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-master-nodehttps://kubernetes.io/docs/setup/production-environment/container-runtimes/ ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-design","content":"前言 Kubernetes 作為近年來討論度最高的容器管理平台，從自行架設，使用公有雲相關服務甚至到尋求第三方廠商解決方案都已經是日常可見的作法。 使用場景來看，各式各樣的場景都在思考與評估是否有機會將 kubernetes 納入其應用範圍，從架設服務器提供服務，配合 GPU 進行大量運算使用甚至將 kubernetes 與 5G網路產業結合。 各式各樣的需求不停的發出，而 kubernetes 是否能夠滿足這些所有的需求則是一個需要好好思考的問題。 為了評估到底 kubernetes 能不能適用於各種使用情境，我們必須先知道什麼是kubernetes 的極限，我認為透過學習其實作原理與設計理念能夠提供一個基本的能力去評估到底 kubernetes 能不能滿足所需。 接下來的系列文內，我會針對 kubernetes 內幾個最大的特點也是所有使用情境最需要考慮的幾個方向來探討，如下 運算單元網路架設與連線儲存空間其他特殊裝置 藉由學習這些不同面向功能的實作原理與設計開發理念，我們都能夠有更好的立足點去評估到底 kubernetes 是否能夠滿足目前所需，甚至說若需要進行第三方開發改善時，該怎麼下手。 架構 Kubernetes 作為一個開放原始碼的專案，其所有原始碼都可以在 Github 看到，同時也可以在看來自世界各地的使用者與開發者如何合作一起開發這個巨大的專案。 對於一個成熟的開源軟體來說，通常都會有所謂的 Release 版本週期，並不是每個開發的新功能都可以很準時的被安排在下一個版本釋出，同時也意味有時候希望某些新功能可以順利的使用，都要等到下個 Release 週期釋出，否則就要自己透過版本控制的方法去 build/compile 來使用最新的功能。 基於以上的軟體開發流程情況，可以試想一下一個情境。 今天某開發者開發了一個有趣的功能，吸引很多使用者都想要趕快嚐鮮使用，然後基於上述的理由，該功能要先經過整體的程式碼評估與測試，最後合併。接者還要等上一段時間產生一個正式公開的建置版本，這一切跑完可能都是數天，數週，甚至數個月後的事情。 而對於 Kubernetes 來說，所謂的 容器管理平台 其涉略的領域實在太多，對於 kubernetes 的眾多開發者來說，要能夠完全掌握這些不同領域的技術與概念其實也是很困難的事情。 舉例來說 今天有一個熱心的開發者想要讓 Kubernetes 支援 GPU 的運算，於是提交了相關的程式碼改動， 如果 kubernetes 的維護者對於 GPU 的運作原理不夠掌握是否有辦法幫他進行程式碼的審查? 同時加上 kubernetes release 週期的規則，會使得這些由來自世界各地貢獻者的結晶沒有辦法很即時的被一般使用者與測試。 整個運作流程如下圖 為了使得整體的開發流程更加順暢，如果能夠針對架構比較需要彈性的功能進行架構改造，將介面與實作給獨立出來各自運作。此架構中， kubernetes 只要設計介面，並且專注於本身與介面的溝通與整合，第三方的開發者則是專注於開發應用，只要該應用符合介面標準即可。 這種狀況下，第三方的開發者可以自行決定其軟體/產品的 release 週期，不需要與 kubernetes 本身掛鉤。不但能夠讓整個平台的擴充功能開發更佳流暢，同時使用者也可以更方便地去嘗試各種不同的底層實現。 改良後的架構可參考下圖，kubernetes 與其他各自的解決方案可以有自己的開發週期 與流程，彼此之間透過事先定義好的介面進行溝通，如此一來就可以提升整體開發的 流暢度。 接下來的29天，會帶領讀者一起探討這些介面的設計以及各種不同應用的實作。 包含了 基於運算單元的 CRI (Container Runtime Interface),提供平台容器網路連接能力的 CNI(Container Network Interface)，提供儲存能力供容器使用的 CSI (Container Storage Interface)以及可以掛載各式各樣系統裝置的 Device Plugin.","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin","content":"","keywords":"","version":"Next"},{"title":"開發者​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin#開發者","content":"對於開發者來說，基於 gRPC 的介面去實現相關功能(詳細的部分下篇文章會探討)，譬如說 service DevicePlugin { // returns a stream of []Device rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} rpc Allocate(AllocateRequest) returns (AllocateResponse) {} }  開發者基於這些介面去開發一個應用程式，該程式滿足上述的介面的功能，譬如回報當前 device 的狀態，根據需求去分配可用的 device。 接者開發者將該應用程式部署到 kubernetes 叢集之中，並且透過 unix socket 的方式與 kubelet 溝通，該路徑通常是 /var/lib/kubelet/device-plugins/，這個路徑跟之前研究 CSI 時候所觀察到的路徑非常類似，都是給 kubelet 使用的。 一但 device plugin 部署到節點之中，主動透過 gRPC 通知 kubelet 目前有新的 device plugin 安裝到系統中，並且準備註冊，一但這個步驟完畢後，整個 kubernetes 叢集中就知道這個 device plugin 的存在，並且使用者就可以開始使用了。 舉例來說，假設該開發者開了一個 hwchiu/test-dev 的 device，則下來都可以透過 kubelet 去查看每個節點上 hwchiu/test-dev 此 device 的總共數量以及當前可用數量。 ","version":"Next","tagName":"h2"},{"title":"使用者​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin#使用者","content":"對於使用者來說，使用起來的方式非常簡單，就是於 Pod 格式中透過 resources 的方式去定義需要什麼 device 且需要多少個 apiVersion: v1 kind: Pod metadata: name: hwchiu-test-dev-pod spec: containers: - name: test-pod image: hwchiu/netutils:latest workingDir: /root resources: limits: hwchiu/test-dev: 1 # requesting a devivce  當使用者提交上述的資源描述到 kubernetes 之中時，kubernetes scheduler 搭配 kubelet 就會去詢問所有節點上的 device plugin，透過上述的 gRPC 介面去詢問當前有多少個可用 device 並且找出所有符合該需求的節點。 當 schedukler 選定節點之後，就會再度透過該節點的 kubelet 透過 gRPC 去戳相關的 device plugin 應用程式去創立一個資源供目標的 Pod 使用。 整理一下流程就是: Pod資源請求Scheduler 搭配 kubelet 去尋找所有符合需求的 節點Scheduler 選定一個節點部署該節點的 kubelet 呼叫 device plugin 解決方案去分配需求數量的 device plugin 供 Pod 使用。 當然當 pod 結束之後會有相對應的函式可以被呼叫來進行資源回收。 Summary 本篇文章簡單簡述了一下關於 Device Plugin 的概念，並且簡單敘述了一下工作流程， 下一篇文章會針對 device plugin 本身的運作原理跟架構進行更仔細的討論。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-implement","content":"","keywords":"","version":"Next"},{"title":"GetDevicePluginOptions​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-implement#getdevicepluginoptions","content":"這個功能滿簡單的，就是回傳一些關於本 device plugin 的一些能力，目前定義的能力非常少，只有一個，就是需不需要容器啟動前呼叫的掛勾點 (pre-start-hook) message DevicePluginOptions { // Indicates if PreStartContainer call is required before each container start bool pre_start_required = 1; }  ","version":"Next","tagName":"h2"},{"title":"ListAndWatch​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-implement#listandwatch","content":"該函式有兩個目的 讓 kubelet 去得知該 devices 的特性以及發現到有多少個 devices由 device plugin 主動通知 kubelet 任何關於 device 狀態的改變 ","version":"Next","tagName":"h2"},{"title":"Alloocate​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-implement#alloocate","content":"kubelet 再創建 Pod/Container 前會呼叫該函式來處理如何使得該 device 可以被容器掛載使用 滿足這個介面的應用程式可透過下列其中之一方式部署到節點上 kubenetes Pod實機部署 接者透過註冊相關的函示主動通知 kubelet 需要註冊一個全新的 device plugin，完成這個步驟之後，接下來 kubelet 就會開始透過上述的兩個 gRPC 介面與解決方案互動 整個過程可以用下圖來解釋上圖節錄自Device Plugin Device Manager Proposal 首先一開始，當 device plugin 安裝完畢之後，會呼叫 Register 這個函式，而這個函式最重要的是背後必須要透過 gRPC 的方式與 kubelet 溝通，將本解決方案的相關資訊都告知 kubelet，所以可以注意到這邊是單線進行。 一但 kubetlet 接收到註冊之後，接下來就會開始根據需求進行 ListWatch 以及 Alloacte 兩個的呼叫，這邊可以特別注意到的是 ListWatch 本身是雙向互動的，意味者兩邊都會根據需求互相呼叫對方的函式。而 Allocate 就是單一方向的。 本圖片雖然來自官方文件，但是右下方的 Unload Drivers During Pre-Stop Hook只是早期設計時的發想，畢竟是 Proposal，實際上這個點反而很難做，譬如 Container crash 算不算 pre-stop? 因此不如透過 pre-start 的方ㄕ確保每個 container 啟動前都可以呼叫的方式去 reset 相關資源。 ","version":"Next","tagName":"h2"},{"title":"PreStartContainer​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-implement#prestartcontainer","content":"如果前述的 GetDevicePluginOptions 有描述有這個需求的話，那這個函式就會再有任何 Pod 要被創造前被呼叫，可以用來進行重設 device 確保下個 Pod 是用到新的。 註冊 為了能夠實現註冊功能， device plugin 一但創立的時候就必須要有能力跟 kubelet 溝通，並且通過 gRPC 的方式去呼叫註冊的函式。 而目前這個方式都會透過 unix socket 的方式來滿足，正巧的是 kubelet 就有配置一個門用來溝通的 unix socket，預設情況下都在 /var/lib/kubelet/device-plugins/kubelet.sock。 也因為這個原因，任何安裝 device-plugin 的 yaml 檔案內都會看到下列的設定 ... volumeMounts: - name: device-plugin mountPath: /var/lib/kubelet/device-plugins volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins ...  一旦這條路通上之後， device plugin 就需要準備一個基於 RegisterRequest 格式的內容去進行註冊，而該格式內包含了幾個重要資訊 version 當前使用的版本基本上可以使用函式庫內的變數即可endpoint 後續進行 ListAndWatch, Allocate 等相關操作使用的 Unix-Socket。resoruce_name 預註冊的 device 名稱，其有規範，通常是 vendor-name/device-nameDevicePluginOptions 當前的 device plugin 有哪些額外參數要設定，目前只有是否需要 pre-start-hook 的選項 一旦註冊成功，之後 kubelet 就會使用 endpoint 中描述的 UNIX Socket 來與 device plugin 進行溝通。 message RegisterRequest { // Version of the API the Device Plugin was built against string version = 1; // Name of the unix socket the device plugin is listening on // PATH = path.Join(DevicePluginPath, endpoint) string endpoint = 2; // Schedulable resource name. As of now it's expected to be a DNS Label string resource_name = 3; // Options to be communicated with Device Manager DevicePluginOptions options = 4; }  這意味者一個有效的 device plugin 會透過兩個不同的 UNIX socket 來與 kubelet 溝通，一個是固定位置，專門用來註冊的，另外一個則是每個 device plugin 自行設定的位置，只要 kubelet 能夠存取即可。 Allocate 接下來探討一下最重要的 Allocate 的函式，主要是要對這個函式有點大概的瞭解，則接下來去看任何的 device plugin 解決方案都會更容易理解其實作邏輯。 首先我們知道，使用者可以於 Pod 中去要求整數數量的 device，當這些需求最後被 kubelet 轉換為實際需求送到 device plugin gRPC 服務器時，就會變成一個以陣列型態表示的 deviceID。 device plugin 本身依據這些流水號的 id 去分配對應的 device 資源，而分配的方式與內容都會於其回傳封包 AllocateResponse 中設定。 // - Allocate is expected to be called during pod creation since allocation // failures for any container would result in pod startup failure. // - Allocate allows kubelet to exposes additional artifacts in a pod's // environment as directed by the plugin. // - Allocate allows Device Plugin to run device specific operations on // the Devices requested message AllocateRequest { repeated ContainerAllocateRequest container_requests = 1; } message ContainerAllocateRequest { repeated string devicesIDs = 1; }  往下探討之前，先來複習一點東西，就是所謂的 device 到底跟容器可以怎麼用? 以 docker 為範例，常常使用 volume 會知道可以透過 -v 的方式標明來源與目的的映射關係，想要複雜一點的設定可以採用 --mount 來處理。 然而對於 device 來說，也有對應的指令，就是 --device 可以參考 Docker 官網範例 $ docker run --device=/dev/sdc:/dev/xvdc \\ --device=/dev/sdd --device=/dev/zero:/dev/nulo \\ -i -t \\ ubuntu ls -l /dev/{xvdc,sdd,nulo} brw-rw---- 1 root disk 8, 2 Feb 9 16:05 /dev/xvdc brw-rw---- 1 root disk 8, 3 Feb 9 16:05 /dev/sdd crw-rw-rw- 1 root root 1, 5 Feb 9 16:05 /dev/nulo  由上述可以看到其使用方式與 volume(mount) 相似，都是透過路徑的方式來處理。 有了這個概念，接下來就可以看一下 Allocate 這個函式的回傳值 AllocateResponse 的格式。 首先 Request 可以要求多個 device，所以 Response 本身的回傳也會是一個陣列，而每個陣列都是一個基於 ContainerAllocateResponse 的格式。 該格式使用了四個欄位來輔助，分別是 envsmountsdevicesannotations // AllocateResponse includes the artifacts that needs to be injected into // a container for accessing 'deviceIDs' that were mentioned as part of // 'AllocateRequest'. // Failure Handling: // if Kubelet sends an allocation request for dev1 and dev2. // Allocation on dev1 succeeds but allocation on dev2 fails. // The Device plugin should send a ListAndWatch update and fail the // Allocation request message AllocateResponse { repeated ContainerAllocateResponse container_responses = 1; } message ContainerAllocateResponse { // List of environment variable to be set in the container to access one of more devices. map&lt;string, string&gt; envs = 1; // Mounts for the container. repeated Mount mounts = 2; // Devices for the container. repeated DeviceSpec devices = 3; // Container annotations to pass to the container runtime map&lt;string, string&gt; annotations = 4; }  其中最重要的就是 mounts 以及 devics，其格式如下，可以看到格式非常類似，也非常清楚 host_pathcontainer_pathread_only/permissions 其中前面兩個資訊是不是與我們前述的 docekr 範例非常相似？ 透過用路徑的方式去表達欲掛載到目標容器內的 volume 或是 devices 所以回傳的格式可能如下 &quot;devics&quot;:[ { container_path: &quot;/dev/sda1&quot;, host_path: &quot;/dev/sda1&quot;, permissoin: &quot;rwm&quot; }, { container_path: &quot;/dev/sda2&quot;, host_path: &quot;/dev/sda2&quot;, permissoin: &quot;rwm&quot; }, { container_path: &quot;/dev/sda3&quot;, host_path: &quot;/dev/sda3&quot;, permissoin: &quot;rwm&quot; }, ]  // Mount specifies a host volume to mount into a container. // where device library or tools are installed on host and container message Mount { // Path of the mount within the container. string container_path = 1; // Path of the mount on the host. string host_path = 2; // If set, the mount is read-only. bool read_only = 3; } // DeviceSpec specifies a host device to mount into a container. message DeviceSpec { // Path of the device within the container. string container_path = 1; // Path of the device on the host. string host_path = 2; // Cgroups permissions of the device, candidates are one or more of // * r - allows container to read from the specified device. // * w - allows container to write to the specified device. // * m - allows container to create device files that do not yet exist. string permissions = 3; }  看到這邊對於 Allocate 已經有一個基本的概念了，對於 device plugin 的解決方案來說，根據需求去配置需要的 device，如果有需要也可以準備相關的 volume，最後一起將這些路徑回傳回去，這樣對應的 Pod 再啟動的時候就會透過類似 docekr --device xxx --mount 的方式把這些資訊都掛載到容器內使用。 Summary 本文透過實際探索 device plugin 框架的面貌來學習其解決方案會怎麼設計，相對於 CRI/CNI/CSI， 整個 device plugin 的內容非常簡單，只有少少四個介面需要設計，也因為如此的設計能夠讓第三方解決方案的人員，專心致力於 device 的處理與開發，就可以很順利的銜接到 kubernetes 叢集運算中。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma","content":"","keywords":"","version":"Next"},{"title":"Zero Copy​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#zero-copy","content":"此特色源自於 DMA，目標就是希望能夠減少 記憶體 的複製，這邊先思考一個情境，任何的資料想要於 User Space 與 Kernel Space 中交換的話，並沒有辦法直接交換，因為其使用的記憶體位置是完全切割的，所以才會有一些 copy_from_user 之類的函式用來處理。 一個常見的 Zero Copy 的範例是，假設今天想要讀取一個檔案，讀取完畢後什麼都不處理就直接輸出，這種情況下其實該筆資料的內容根本沒有再 User Space 被處理，實際上可以一直放在 kernel 後直接輸出，所以就可以減少資料複製的行為藉此提高整個運算速度。 將這個檔案處理的邏輯套用到網路運算上就是網路應用程式封包的傳輸，能否減少必要的複製行為來降低延遲性。 ","version":"Next","tagName":"h2"},{"title":"Kernel Bypass​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#kernel-bypass","content":"另外一個重要的特色就是跳過 Kernel，目前已經有不少的網路技術專案再探討如何透過跳過 Kenrel 來達到更快的處理速度，譬如讓 User Space 的應用程式直接跟網卡對街，直接處理封包，畢竟 Kernel 要處理的事情又多又複雜，並沒有辦法針對網路應用程式最佳化，因此跳過 kernel 儼然成為一個可考慮的解決方案之一。 ","version":"Next","tagName":"h2"},{"title":"No CPU Involvement​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#no-cpu-involvement","content":"除了上述的兩個特性外，還有一個有趣的特色就是可以再不使用遠端 CPU 的情況下直接讀取遠方的記憶體，這部分要仰賴網卡本身的幫忙以及協定的互助，此外對於 CPU來說根本不知道有任何記憶體被讀取，因此對於快取的部分也都不會有任何影響 ","version":"Next","tagName":"h2"},{"title":"Message Based Transactions​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#message-based-transactions","content":"基於 RDMA 封裝後的封包傳輸都是基於message 為單位，是一個已經定義好的的格式來處理，想對於單純直接使用 TCP 這種 streaming 的傳輸格式來說，開發者就不需要自己不停的拆解封包來判斷當前的格式與內容 ","version":"Next","tagName":"h2"},{"title":"Scatter/Gather Entries Support​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#scattergather-entries-support","content":"此功能是提供一次性處理多個 message 的能力，不論是發送封包，接受封包，都可以一次性處理多個 message 的封包。此特色並不是類似迴圈般呼叫多次，而是一次的呼叫可處理多筆的資料。 優勢 看了上述的各種特色後，組合起來能夠為 RDMA 這項技術帶來什麼樣的優勢 低延遲性高傳輸量低 CPU 使用量 但是世界通常沒有這麼美好，實際上上述的三個優勢並不是同時存在的，會需要根據需求台調整不同的設計以及用法來達成，這部分可以看看 Tips and tricks to optimize your RDMA code 這篇文章裡面的描述來如何最佳化你的應用程式 稍微節錄一下裡面的四大章節 Improving the BandwidthReducing the latencyReducing memory consumptionReducing CPU consumption 根據不同的需求都有不同的方法去設定，甚至包含程式碼撰寫的方式都會影響最後的效能，也因此 RDMA 的撰寫難度頗高，整個框架完全不同且使用情境也會影響寫法。 效能 這邊擷取自 Mellanox 關於 NFS 進行 TCP/RDMA 兩者的效能比較，有興趣的可以自行閱讀看看比較說明，以結果論來說大致上都有兩倍左右的提升，不論是傳輸速度，或是每秒可執行的操作數 以上圖片節錄自Double Your Network File System (NFS) Performance with RDMA-Enabled Networking 以上圖片節錄自Double Your Network File System (NFS) Performance with RDMA-Enabled Networking  以上圖片節錄自Double Your Network File System (NFS) Performance with RDMA-Enabled Networking 架構 接下來使用下列這張圖片來解釋一下 RDMA 的架構。 該圖節錄自Mobile D2D RDMA CAAP, Cluster As Application Platforma 首先該圖片上半部分成三層，分別是 Application, User Space, Kernel Space，並且透過這三層次的比對來介紹 RDMA 與傳統 TCP/IP 運作的差異。 下半部分則是介紹當 RDMA 的封包離開網路卡後，要如何跟外網的裝置進行溝通，有什麼樣的協定可以採用。 ","version":"Next","tagName":"h2"},{"title":"上半部分​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#上半部分","content":"首先，圖片左半部分則是傳統的 TCP/IP 運作流程，網路應用程式會透過 systel call 的方式創建一個 socket，並且透過這個 socket 來進行連線，傳輸，接收等相關操作。 而 kernel 部分則是會有一個與上述 socket 對應的接口，一旦該接口接收到封包後，就會進行網路相關的處理，從 TCP 一路處理到 ETH 最後透過相關的驅動程式與網卡對接，讓封包順利送出。 對於 RDMA 來說整個運作模式完全不同，首先其使用的介面完全不同於傳統的 BSD Socket API，所以程式撰寫的部分是需要完全重寫，可以看到圖上使用的是 RDMA Verbs API。 接下來由於 Kernel Bypass 的緣故，應用程式會透過 API 直接與相關的驅動程式溝通，封包不會經過的 Kernel Network Stack 的處理，所以比對起來就會發現其走過的路徑相對較簡單。 ","version":"Next","tagName":"h2"},{"title":"下半部分​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#下半部分","content":"接下來封包到達網卡後若要往外移動，這時候就有不同的選擇可以處理 若今天部署的環境是基於 InfiniBand 的環境，這種情況下環境內的 Swtich 都要是 InfiniBand 的交換機才能夠處理這種格式的封包。 提到右邊的 iWARP 以及 RoCE 前要先有一些背景介紹，上述提到的 RDMA 除了網卡本身支援外，其封包傳遞的格式也就跟 TCP/IP 不相容，因此如果沒有特別處理的話，對於採用 Ethernet 的網路架構中，是不能用這種應用程式的。 為了解決這個問題，有兩個不同相容於 Ethernet 的協議被發展出，分別是 iWRAP 以及 RoCE。 其中 iWARP 基於 TCP 去實作，而 RoCE 則是基於 UDP，所以看到圖 中這兩個協議最後都接上了 Ethernet 交換機。 ","version":"Next","tagName":"h2"},{"title":"lossless network​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#lossless-network","content":"這邊額外提一下 RoCE 的架構下，會希望整個網路是所謂的 lossless network，因為其協定是基於UDP的，所以掉了任何一個封包其實都很麻煩，如果可以讓網路架構本身去確保不會掉封包，這樣整個 RoCE 這邊就可以用更少的事情在處理重送之類的機制，反而可以更專注於效能的發送 這篇文章中提到了三種達到 lossless network 的方法，有興趣可以再研究 Ethernet Flow Control (802.3x) PFC (Priority Flow Control) ECN (Explicit Congestion Notification) kubernetes 前面探討了關於 RDMA 的基本介紹，基本上就是一個希望講求高效率網路傳輸的技術，使用上需要安裝相關的 driver 以及支援的網卡來處理。 接下來看一下 kubernetes 關於 RDMA 的 device plugin ","version":"Next","tagName":"h3"},{"title":"Deployment​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#deployment","content":"其安裝的方式非常簡單，幾乎所有的 device plugin 都一樣，畢竟是一個要跟每個節點的 kubelet 溝通的 gRPC server，引此採用 DaemonSet 的方式來安裝也是合情合理。  apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: rdma-device-plugin-daemonset namespace: kube-system spec: template: metadata: # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler # reserves resources for critical add-on pods so that they can be rescheduled after # a failure. This annotation works in tandem with the toleration below. annotations: scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot; labels: name: rdma-device-plugin-ds spec: tolerations: # Allow this pod to be rescheduled while the node is in &quot;critical add-ons only&quot; mode. # This, along with the annotation above marks this pod as a critical add-on. - key: CriticalAddonsOnly operator: Exists hostNetwork: true containers: - image: carmark/k8s-rdma-device-plugin:latest name: rdma-device-plugin-ctr args: [&quot;-log-level&quot;, &quot;debug&quot;] securityContext: allowPrivilegeEscalation: false capabilities: drop: [&quot;ALL&quot;] volumeMounts: - name: device-plugin mountPath: /var/lib/kubelet/device-plugins volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins  使用上非常簡單，但是前述提到必續要有相關的應用程式才可以使用這種裝置，並不是任何一個目前使用 TCP/IP 的應用程式都可以輕鬆轉換過去，因此為了測試可以使用 Mellanox 提供的容器來測試。 ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-rdma#usage","content":"apiVersion: v1 kind: Pod metadata: name: rdma-pod spec: containers: - name: rdma-container image: mellanox/mofed421_docker:noop securityContext: capabilities: add: [&quot;ALL&quot;] resources: limits: tencent.com/rdma: 1 # requesting 1 RDMA device  使用 device plugin 就是這樣簡單，能夠讓容器內部的應用程式看起來跟在實體機器使用上沒有差異，而最大的問題反而是使用情境以及相關的應用程式要如何搭配這些高速的網路設備來使用。 ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-sriov","content":"","keywords":"","version":"Next"},{"title":"Multus CNI​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-sriov#multus-cni","content":"但是先前於 CNI 的章節有介紹過 CNI 是一個基於節點的設定，且基本上是所有跑在該節點上的容器都會採用該 CNI 來處理。 麻煩的地方在於 SR-IOV 的環境並不一定是每個機器都需要使用，同時 VF 的數量是有上限的，每張網卡的能力不同，能夠支援的 VF 數量也不一致，所以這反而會影響能夠運行的 Pod 數量上限 另外一個麻煩的點在於一旦透過 SR-IOV 的技術，封包就直接從網卡出去了，這導致這些封包不會被 Host 端的 Linux Kernel 處理，這樣的後果就是 Kubernetes service 這個功能會完全壞光，完全不能用了。 為了解決這個問題，幾乎所有使用 SR-IOV 的解決方案，都會採用第三方的 CNI 來達到讓 Pod 動態選擇 CNI 的效果，譬如 Multus, Geneie 解決方案 透過 Multus 的幫忙，管理者可以於系統中管理多套 CNI 解決方案，並且每個 Pod 本身在創建的時候可以透過 Annotation 去決定要使用哪個 CNI 的解決方案。 此架構下，就可以根據需求來決定哪些應用需要引入 SR-IOV 來處理，哪些不需要可以繼續本來常用的 Calico/Flannel 處理。 這邊就不提及太多關於 Multus 的使用方法，有興趣的可以到GitHub閱讀相關文件。 有了上述這種多重 CNI 管理的解決方案後，就可以看看接下來會怎麼使用 SR-IOV。 ","version":"Next","tagName":"h2"},{"title":"Device Plugin​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-sriov#device-plugin","content":"首先必須要安裝 Device Plugin 來管理節點上所有的 SR-IOV 裝置，包含總共數量，當前可用數量，狀態等，這些資訊會透過 ListAndWatch 傳遞給 kubelet 最後就可以到每個 node 上觀察到當前系統上的 SR-IOV 裝置數量。 關於該 device plugin 的詳細資訊，都可以參閱 GitHub，這邊有完整的介紹，包含如何與 Multus 共同使用。 其中我覺得比較有趣的是該 Device Plugin 的設定檔案，因為支援 SR-IOV 的裝置並不是只有一款，每款對應的廠商以及驅動程式都不同，這種情況下要如何讓使用者可以很順利的根據需求選擇需要的 device plugin 來使用？ 為了解決這個問題， SR-IOV device plugin 要使用者事先準備一個設定檔案，範例如下 { &quot;resourceList&quot;: [{ &quot;resourceName&quot;: &quot;intel_sriov_netdevice&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;154c&quot;, &quot;10ed&quot;], &quot;drivers&quot;: [&quot;i40evf&quot;, &quot;ixgbevf&quot;] } }, { &quot;resourceName&quot;: &quot;intel_sriov_dpdk&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;154c&quot;, &quot;10ed&quot;], &quot;drivers&quot;: [&quot;vfio-pci&quot;], &quot;pfNames&quot;: [&quot;enp0s0f0&quot;,&quot;enp2s2f1&quot;] } }, { &quot;resourceName&quot;: &quot;mlnx_sriov_rdma&quot;, &quot;isRdma&quot;: true, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;15b3&quot;], &quot;devices&quot;: [&quot;1018&quot;], &quot;drivers&quot;: [&quot;mlx5_ib&quot;] } }, { &quot;resourceName&quot;: &quot;infiniband_rdma_netdevs&quot;, &quot;isRdma&quot;: true, &quot;selectors&quot;: { &quot;linkTypes&quot;: [&quot;infiniband&quot;] } } ] }  該檔案裡面去描述所有節點中可能會用到的網卡資訊，包含廠商, 驅動程式, 裝置名稱, pf 網卡名稱。 一旦這些資訊準備好後， SR-IOV 的 gRPC 被啟動後就會去讀取這個資訊，然後開始檢查本系統上所有的網卡資訊，根據上述的條件把所有符合的裝置數量都找出來，並且根據 resourceName 的欄位向 kubelet 註冊這個資訊。 換句話說 SR-IOV 最後產生的資源數量不會只有一個，取決於設定檔案怎麼描述以及系統節點上硬體裝置的配置。 ","version":"Next","tagName":"h2"},{"title":"SR-IOV CNI​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-device-plugin-sriov#sr-iov-cni","content":"完成 device plugin 後，接下來就是要處理 VF 的配置(實際上 kernel module啟動時就已經配置完畢，但是需要被創建出來使用) 這個解決方案的 GitHub 於此，很有趣的是其作者跟 RDMA 的是同一個人，是個很專注於網路發展的開發者，非常的厲害! 先來看看開頭的介紹 PF is used by host.Each VFs can be treated as a separate physical NIC and assigned to one container, and configured with separate MAC, VLAN and IP, etc. 針對每個被掛載到 Container 的 Virtual Function(VF) 都可以設定 MAC, VLAN 以及 IP 地址。 來看一下一個範例的 CNI 設定檔案 { &quot;name&quot;: &quot;mynet&quot;, &quot;type&quot;: &quot;sriov&quot;, &quot;master&quot;: &quot;eth1&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;fixipam&quot;, &quot;subnet&quot;: &quot;10.55.206.0/26&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;gateway&quot;: &quot;10.55.206.1&quot; } }  首先 type 代表要執行的 CNI Binary 檔案，名為 SRIOV，而這邊我們透過 master 描述目標的 PF 是誰，預設情況下該 CNI 會自動地從可以用的 VF中挑一個可以用的 VF 出來匹配改容器使用，並且搭配 IPEM, fixipam 來設定相關的 IP 資訊。  if args.VF != 0 { vfIdx = int(args.VF) vfDevName, err = getVFDeviceName(masterName, vfIdx) if err != nil { return err } } else { // alloc a free virtual function if vfIdx, vfDevName, err = allocFreeVF(masterName); err != nil { return err } }  從上述 CNI 的實作可以看如果今天沒有傳遞一個叫做 VF 的參數的話，就會呼叫 allocFreeVF 來配置一個可用的 VF。如果有傳遞的話則是會根據該參數去得到對應的裝置，而實際上 allocFreeVF 的工作就是使用不同的 VF 參數後不停的呼叫 getVFDeviceName 來處理。 func getVFDeviceName(master string, vf int) (string, error) { vfDir := fmt.Sprintf(&quot;/sys/class/net/%s/device/virtfn%d/net&quot;, master, vf) if _, err := os.Lstat(vfDir); err != nil { return &quot;&quot;, fmt.Errorf(&quot;failed to open the virtfn%d dir of the device %q: %v&quot;, vf, master, err) } infos, err := ioutil.ReadDir(vfDir) if err != nil { return &quot;&quot;, fmt.Errorf(&quot;failed to read the virtfn%d dir of the device %q: %v&quot;, vf, master, err) } if len(infos) != 1 { return &quot;&quot;, fmt.Errorf(&quot;no network device in directory %s&quot;, vfDir) } return infos[0].Name(), nil }  Summary 最後用一個架構圖來描述 SR-IOV 使用後的可能架構該圖節錄自常見 CNI (Container Network Interface) Plugin 介紹 首先該 kubernetes cluster 會使用 Multus CNI 來提供動態管理解決方案，對於圖中示範的 Pod 都會使用三個 CNI 分別是呼叫一次 flannel 以及兩次 SR-IOV。 兩次的 SR-IOV 可以針對不同的網卡給予不同的網路參數，包含 IP。更深層的意義在於這些網卡背後的實體網路配置，可以藉此提供不同的 data plan 網路拓墣。 這種情況下，這些 Pod 裡面就會有多張網卡，同時有多個 IP 地址，因此 Routing 的規則更為重要，要是沒有弄好可能就會處理錯誤，導致封包走錯網卡出去。 當然前述也有提過，因為封包都不會經過 Host 的 Kernel Network Stack，因此諸如 Kubernetes Server (ClusterIP/NodePort) 等功能對於 SR-IOV 的介面都不會生效，需要動腦看看該如何處理，甚至是放棄該功能，畢竟作為 data-plan 好像又不是說非常重要。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-operator-pattern","content":"","keywords":"","version":"Next"},{"title":"動機​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-operator-pattern#動機","content":"Operator 的名稱的由來，根據官網的動機介紹，我的解讀是希望能夠模擬系統管理員，或是所謂的操作員在管理大量服務時的各種操作，特別是這些操作本身會有特定的邏輯牽扯，同時這些操作本身也有依賴性。 根據上述的說法，有時候就會有一些腳本或是相關的工具來幫忙自動化的完成這些工作，但是這些腳本或是工具都是基於外部對 kubernetes 的操作來處理。 今天 Operator 希望達到的方式是可以透過內部直接於 kuberentes 來溝通 ，並且透過程式化的方式將這些相關邏輯用程式撰寫來完成。 ","version":"Next","tagName":"h2"},{"title":"組成​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-operator-pattern#組成","content":"接下來就來探討所謂的 custom resources 以及 control loop 這兩個概念。 ","version":"Next","tagName":"h2"},{"title":"CRD​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-operator-pattern#crd","content":"custom resourecs 顧名思義就是客製化資源，目前於 kubernetes 中已經定義了大量的內建資源，譬如 Deployment, Pod, NetworkPolicy, StorageClass 這些都是內建的資源。 而 Custom Resources 則是所謂的 Custom Resources Definition(CRD) 框架下的產物，任何使用者都可以透過 CRD 的格式向 kubernetes 動態的創造一個全新資源，甚至可以使用 kubectl get 的方式來取得這些資源的資訊。 官方文件 - Extend the Kubernetes API with CustomResourceDefinitions 介紹了一個範例 apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt; name: crontabs.stable.example.com spec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: stable.example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct  一旦將上述的檔案加入到 kubernetes 中，接下來就可以使用裡面描述的 names 底下的各種名稱來取得。 譬如 $ kubectl get ct $ kubectl get crontab  同時也可以直接創造一個對應的資源 apiVersion: &quot;stable.example.com/v1&quot; kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: &quot;* * * * */5&quot; image: my-awesome-cron-image  透過 CRD 的方式，我們可以對我們的應用程式，服務跟需求創建一個符合的資源，並且搭配需要的設定檔。 ","version":"Next","tagName":"h3"},{"title":"Control Loop​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-operator-pattern#control-loop","content":"這個概念其實源自於 Kubernetes Control Plane， 對於 kubernetes 來說，master 以及各節點的 kubelet都扮演者 control plane 的角色，幫忙維護各式各樣的資源需求，其中的運作邏輯則是會運行一個無窮的迴圈，不停地監控所有叢集上的資源變化，譬如 Pod 的 Create,Delete,Terminated，接者根據使用者的需求來決定下一個步驟該怎麼做。 而這些運作過程中，都可以直接去監聽各種 kubernetes 資源的變化，除了這些內建的資源之外，連我們透過 CRD 動態創立的資源也可以使用一樣的方式 有了上述兩個概念之後，我們可以簡單歸納一下 Operator Pattern 通常會做的事情。 根據需求創建需要的 CRD，可以更加方便的去管理目標應用的設定撰寫一個應用程式，該應用程式會不停地去聽取 Kubernetes 相關資源的變化，譬如上述 CRD 被創建後，就會根據該資源再去創造所有需要的資源，譬如 Pod, Service，將所有之前需要人為涉入的邏輯都用程式化的方式來重複執行。 ","version":"Next","tagName":"h3"},{"title":"Build Operator​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-operator-pattern#build-operator","content":"接下來可以來看一下，如果想要撰寫一個 operator，可以怎麼完成 畢竟上述提到的都只是相關的該念，實際上要撰寫的話可以怎麼完成 根據官方文件的推薦，目前有四種以上的方法可以完成 using KUDO (Kubernetes Universal Declarative Operator)using kubebuilderusing Metacontroller along with WebHooks that you implement yourselfusing the Operator Framework 就我自己的經驗來說，最基本的方式就是直接使用 client-go 這個官方的 golang library 直接撰寫一個可以跟 kubernetes 溝通的應用程式，並且自己滿足相關的資源監聽，相關的 Control Loop。 而上述提到的四個方式就是將這個步驟再次包裝，期望提供更簡單的方式讓使用者可以開發出一個基於 Operator Pattern 的應用程式。 但是事情沒有絕對完美，框架的問題就在於是否夠靈活彈性與客製化，是否能夠符合所有的應用情境，不能的話是不是還是要退回到最原始自己與 kubernete 溝通? 所以我認為挑選 Operator Framework 前要先釐清自己的使用情境跟需求，接下來去挑選各個工具的時候才能夠判斷是否該工具適合自己的情境。 Summary 最後用一張架構圖來解釋 Operator Pattern 的運作概念該圖節錄自Comparing Kubernetes Operator Pattern with alternatives 該圖片分成左右兩部分，其功能是等價值的。 左邊部分則是最原始的操作過程，右邊則是採用 Operator Pattern 後的過程。 先來看看左邊的架構流程，其將部署分成兩個部分 準備好所有相關的檔案與設定，接者使用 Helm 或是任何工具安裝相關的資源，譬如 Deployment, StatefulSet 等接下來安裝完畢後，就要進入到後續的維護操作，這時候可能會有額外的自動化程式來處理 Deployment/SttatefulSet 相關的變化，並且根據這些變化進行不同的設定 而右邊的部分非常簡單，就是先行安裝該應用程式相關的 Controller，如果這時候沒有額外的特別設定，則上述安裝的 Controller 本身會開始跟 kubernetes 溝通並且開始創造如 Deployment, StatefulSet 相關的資源，並且自行監控所有的變化來處理。 等於說將所有之前人為觀察操作的步驟都程式化於該 Controller 之中。帶來的好處不言而喻，但是其實我認為也帶了不少壞處 除錯困難，一旦所有的運作邏輯都被綁到程式內，對於叢集的管理人員來說更像是一個神秘的黑盒子，遇到問題其實幾乎不能處理，也不能客製化。一但有任何更動就是需要重新建置編譯並且產生 Image 最後部署。 這一連串的流程導致除錯麻煩以及變得非常依賴該專案上游的維護以及專案本身的穩定性 ","version":"Next","tagName":"h2"},{"title":"完賽感想","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/summary","content":"完賽感想 本篇為鐵人賽的最後一篇，本篇基本上沒有什麼要探討的了，主要是回顧一下這三十天以來的所有文章以及想要跟社群大眾分享的重點 從2012左右開始引起一波虛擬化熱潮的 OpenStack 開始，人們開始討論架構的改變，虛擬化帶來的優缺點，基於虛擬機器產生的部署環境開始落地於各種場景之中，不論是研究,測試或是真的商用. 然而在OpenStack出現之前，其實虛擬化的技術早就蓬勃發展，譬如 chroot, jail, lxc 甚至各式各樣的虛擬機器解決方案. 而 OpenStack 的出現，帶來的不單純只是一個虛擬化的範例，而是一整個解決方案，裡面滿滿的不同元件，基於不同的功能一起搭建出完整的解決方案。 我認為也是這一點才使得整個專案可以如此吸人目光與吸引熱潮，還在世界各地的使用者都思緒這如何使用這個 虛擬化叢集管理工具來管理大量的虛擬機器並且提供更有效的管理效率,良好的服務品質 然而軟體世界就是有趣，永遠都無法滿足眾人，隨著 docker 專案的興起，基於容器的虛擬化解決方案也開始吸引了一波目光，容器與虛擬機器的比較從來沒有停過，輕巧快速簡單等特色吸引了大眾的眼球。 就如同 OpenStack 帶來的叢集管理功能，容器方面是否也有基於多節點的管理工具? Docker Swam, Mesos 等諸多的專案都為了這些方向發展，直到 kubernetes 的出現，我個人認為其幾乎打趴了先前的所有管理工具，幾乎一統了基於容器的叢集管理平台解決方案。 綜觀發展歷程，解決方案一直推陳出新，虛擬機器與容器共存，不同平台互相整合已提供更完善的解決方案。 對於使用者來說，看到的反而是一直出現的新專案，每個專案發展的速度比大部分使用學習速度還要快上超級多，根本追都追不上。 但是仔細想想，底層的容器技術早就存在已久，從隔離的 namespace, 網路的 iptables/ipvs, 儲存系統的 mount,file system, 系統的 device 等技術早就行之有年甚至成熟。 對於 kubernetes 來說就是如何把這些存在的功能與平台進行整合，讓使用者使用起來更為方便。 我自己的想法是除了學習新功能如何使用之外，其實多花點時間瞭解所有底層的運作原理並不會吃虧，目前看起來都還是過去那些基礎功能不停的轉換使用，而這些東西也是最為苦悶但是卻最為重要的基底。 一旦瞭解更多的底層原理，其實看到任何的新功能的時候都可以開始思考，這個功能可能怎麼完成的? 如果是我來做我會怎麼做，接者開始驗證自己的想法，藉由這種思考後學習的辦法其實更可以幫助你理解其實做的概念與理由。 很多時候實作上不一定會有註解，甚至文件都只是描述其功能，沒有描述為什麼，這時候如果有相關的經驗與概念，對於思考上都會有很大的幫助。 最後再次重申，學習底層原理，學習閱讀原始碼，都能夠為你帶來很大的好處與進步，這些知識也許短時間之內不能幫你解決問題，但就如同歷久彌新般的內功，你遲早會愛上他的&gt;","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-service-catalog","content":"","keywords":"","version":"Next"},{"title":"安裝 Service Catalog​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-service-catalog#安裝-service-catalog","content":"可以透過 helm 的方式來安裝 Service Catalog 的服務 $ helm repo add svc-cat https://svc-catalog-charts.storage.googleapis.com $ helm install svc-cat/catalog --name catalog --namespace catalog vagrant@k8s-dev:~$ kubectl api-resources | grep servicecatalog.k8s clusterservicebrokers servicecatalog.k8s.io false ClusterServiceBroker clusterserviceclasses servicecatalog.k8s.io false ClusterServiceClass clusterserviceplans servicecatalog.k8s.io false ClusterServicePlan servicebindings servicecatalog.k8s.io true ServiceBinding servicebrokers servicecatalog.k8s.io true ServiceBroker serviceclasses servicecatalog.k8s.io true ServiceClass serviceinstances servicecatalog.k8s.io true ServiceInstance serviceplans servicecatalog.k8s.io true ServicePlan  可以觀察到安裝完畢之後，系統上多出了不少相關的定義，包含了 serviceborkers, serviceclasses, serviceinstances 等各類資訊 ","version":"Next","tagName":"h2"},{"title":"安裝minibroker​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-service-catalog#安裝minibroker","content":"接下來機會安裝一個基於測試開發用的 Serive Broker, minibroker 這部分也可以透過 helm 的方式進行安裝 helm repo add minibroker https://minibroker.blob.core.windows.net/charts helm install --name minibroker --namespace minibroker minibroker/minibroker  接者我們就可以觀察上述的這些資源，來瞭解目前多了哪些資源可以用 vagrant@k8s-dev:~$ kubectl get --all-namespaces clusterservicebrokers NAME URL STATUS AGE minibroker http://minibroker-minibroker.minibroker.svc.cluster.local Ready 65m  首先透過 clusterservicebrokers 可以看到有一個新增的 service broker，包含對應的 URL，這部分是用 Kubernetes service 與 DNS 來處理連接行為的。 接下來看一下這個 minibroker 提供什麼服務可以創造 vagrant@k8s-dev:~$ kubectl get --all-namespaces clusterserviceclasses NAME EXTERNAL-NAME BROKER AGE mariadb mariadb minibroker 66m mongodb mongodb minibroker 66m mysql mysql minibroker 66m postgresql postgresql minibroker 66m redis redis minibroker 66m  透過觀察 clusterserviceclasses 可以看到提供了五種類型，四種資料庫搭配一個 Redis 來創建，非常的有趣是吧 這邊的 class 代表是類別，接下來可以透過 plan 的方式看到更細緻的資訊 vagrant@k8s-dev:~$ kubectl get --all-namespaces clusterserviceplans | grep redis redis-3-2-9 3-2-9 minibroker redis 72m redis-4-0-10 4-0-10 minibroker redis 72m redis-4-0-10-debian-9 4-0-10-debian-9 minibroker redis 72m redis-4-0-11 4-0-11 minibroker redis 72m redis-4-0-12 4-0-12 minibroker redis 72m redis-4-0-13 4-0-13 minibroker redis 72m redis-4-0-14 4-0-14 minibroker redis 72m redis-4-0-2 4-0-2 minibroker redis 72m redis-4-0-6 4-0-6 minibroker redis 72m redis-4-0-7 4-0-7 minibroker redis 72m redis-4-0-8 4-0-8 minibroker redis 72m redis-4-0-9 4-0-9 minibroker redis 72m redis-5-0-4 5-0-4 minibroker redis 72m redis-5-0-5 5-0-5 minibroker redis 72m  各種版本的 Redis 都有提供，接下來我們就用範例試試看來創建這些資源 ","version":"Next","tagName":"h2"},{"title":"Provision​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-service-catalog#provision","content":"apiVersion: servicecatalog.k8s.io/v1beta1 kind: ServiceInstance metadata: name: mini-instance namespace: test-ns spec: clusterServiceClassExternalName: mariadb clusterServicePlanExternalName: 10-1-26 parameters: param-1: value-1 param-2: value-2  上述的 yaml 描述了需要的資源 mariadb 以及對應的版本 10-1-26，同時若該服務有額外參數可以傳遞，也可以透過 parameters 一併傳送到最後的 service broker 去處理。 vagrant@k8s-dev:~$ kubectl get --all-namespaces serviceinstances NAMESPACE NAME CLASS PLAN STATUS AGE test-ns mini-instance ClusterServiceClass/mariadb 10-1-26 Ready 39m  這時候只有單純的資源創造，還沒有對應的連線資訊，所以接下來透過 binding 的方式來處理 apiVersion: servicecatalog.k8s.io/v1beta1 kind: ServiceBinding metadata: name: mini-binding namespace: test-ns spec: instanceRef: name: mini-instance  非常簡單的格式，最重要的時一但這個資源創建完畢後，系統也會順便產生一個 secret 來放置相關的資訊 vagrant@k8s-dev:~$ kubectl -n test-ns describe secret mini-binding Name: mini-binding Namespace: test-ns Labels: &lt;none&gt; Annotations: &lt;none&gt; Type: Opaque Data ==== username: 4 bytes Protocol: 5 bytes host: 49 bytes mariadb-password: 10 bytes mariadb-root-password: 10 bytes password: 10 bytes port: 4 bytes uri: 78 bytes  可以看到這時候系統上產生了對應的資訊，接下來我們嘗試看看這些數值 vagrant@k8s-dev:~$ kubectl -n test-ns get secret mini-binding -o json | jq '.data | map_values(@base64d)' { &quot;Protocol&quot;: &quot;mysql&quot;, &quot;host&quot;: &quot;eager-greyhound-mariadb.test-ns.svc.cluster.local&quot;, &quot;mariadb-password&quot;: &quot;cWmMvYGu9W&quot;, &quot;mariadb-root-password&quot;: &quot;E8KJnmqrrt&quot;, &quot;password&quot;: &quot;E8KJnmqrrt&quot;, &quot;port&quot;: &quot;3306&quot;, &quot;uri&quot;: &quot;mysql://root:E8KJnmqrrt@eager-greyhound-mariadb.test-ns.svc.cluster.local:3306&quot;, &quot;username&quot;: &quot;root&quot; }  這邊可以看到各種連線的資訊，同時也可以觀察 pod 的資訊 vagrant@k8s-dev:~$ kubectl -n test-ns get all NAME READY STATUS RESTARTS AGE pod/eager-greyhound-mariadb-647888fb8b-gdcf8 1/1 Running 0 72m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/eager-greyhound-mariadb ClusterIP 10.103.69.162 &lt;none&gt; 3306/TCP 72m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/eager-greyhound-mariadb 1/1 1 1 72m NAME DESIRED CURRENT READY AGE replicaset.apps/eager-greyhound-mariadb-647888fb8b 1 1 1 72m  可以看到創建了 deployment 以及 service 來幫忙完成這些資源的連動。 Summary 本篇介紹的 Service Catalog 可以讓管理者直接在 Kubernetes 內直接去得到需要的服務資源，譬如 Redis 或是資料庫之類的，當然資源的多寡還是依據每家 Service Broker 去設計與處理，通過這個方式也是可以拿到 IaaC 的架構來建置服務。 參考 https://kubernetes.io/docs/concepts/extend-kubernetes/service-catalog/https://www.youtube.com/watch?v=bm59dpmMhAkhttps://svc-cat.iohttps://svc-cat.io/docs/install/https://svc-cat.io/docs/walkthrough/#step-4---creating-a-new-serviceinstance ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/k8s-concept","content":"","keywords":"kubernetes introduction","version":"Next"},{"title":"Before Kubernetes​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#before-kubernetes","content":"Storage(儲存) 實際上一直都不是一個簡單處理的問題，從軟體面來看實際上牽扯到非常多的層級，譬如 Linux Kernel, FileSystem, Block/File-Level, Cache, Snapshot, Object Storage 等各式各樣的議題可以討論。 以檔案系統來說，光一個 EXT4/BTRFS 兩個檔案系統就有不少的評比與比較，何況是加上了 Distributed FileSystem(分散式作業系統)，譬如 Ceph, GlusterFS 等相關的解決方案進來後，一切事情又變得更加複雜。 此外還可以考慮到其他的軟體相關儲存技術，譬如 RAID, LVM, 甚至是各式各樣的Read/Write Cache 及DRBD 各種不同取向的解決方案，都會因為 使用者的需求而有不一樣的選擇. 異地備援，容錯機制，快照，重複資料刪除等超多相關的議題基本上從來沒有一個完美的解法能夠滿足所有使用情境。 NetApp, Nutanix, 家用/企業 NAS 等眾多廠商專注於儲存解決方案的提供，從單一機器的擴充到超融合架構(HCI)都是服務的對象之一 光這樣看下來就知道儲存技術真的不簡單，Kubernetes 何德何能可以以一個 Container Orchestrator 平台來解決所有事情? 舉一個最簡單的範例來說， NFS(Network File System) 是一個普遍都聽過也滿常被使用的儲存方案，這種 Client/Server 架構下，系統管理者針對 NFS Server 進行設定跟擴充，基本上NFS Client 大部分都不知道，甚至沒有感覺 (Mount options 除外) 譬如除了基本的檔案存取外，可以藉由 RAID 來提供基本硬碟容錯的功能，管理者可能會直接在 NFS Server 上進行 MDADM 來設定相關的 Block Device 並且基於上面提供 Export 供 NFS 使用，甚至底層套用不同的檔案系統 (EXT4/BTF4) 來獲取不同的功能與效能。 這類型的修改都是在 NFS Server 端去完成的，而 NFS Client 端不知道也無權責去進行這些功能的強化。 而 Kubernetes 就只是 NFS Client 的角色，所以整體背後的 NFS Server 能夠提供什麼樣的儲存功能與安全保障，對於 Kubernetes 來說已經超出其權責之外了，其能夠做的就是向 NFS Server 進行連線取得一個可以存取的網路位置罷了。 ","version":"Next","tagName":"h2"},{"title":"Kubernetes​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#kubernetes","content":"對於 Kubernetes 的定位來看，本身平台更專注地在於介面標準的制定與支援，在 Kubernetes 中該介面則是所謂的 CSI(Container Storage Interface)。CSI 本身作為 Kubernetes 與 Storage Solution 的中介層。Kubernetes 這邊專注於本身的元件，PV/PVC/StorageClass 這些元件作為中介層，往上銜接 Pod　等實際應用情形，往下則透過 CSI 與各式各樣的 Storage Solution Provider 銜接. 詳細的用法跟概念可以參閱 Kubernetes Storage 101 若想了解更多 CSI 的設計原理跟組成，可以直接參閱可以參閱 官方 Github Container-Storage-Interface ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#summary","content":"Kubernetes 本身不提供任何儲存功能, 透過標準介面 (CSI) 存取儲存伺服器Kubernetes 本身也不去管什麼 RAID, 快照, 分散式儲存, 資料同步, 這些都是後端儲存伺服器自行完成請針對自己的需求以及認知，選擇一個適合自己的儲存方案來使用不要認為 Kubernetes 可以幫你處理一切事情，沒有這麼強大也不應該這麼強大，請認份的學習儲存方面的概念與知識，然後與 Kubernetes 整合.遇到任何問題，可能是 Kubernetes 使用上的問題，也有可能是儲存伺服器本身的問題，這部分要仰賴管理者的經驗來處理 最後用一張圖來簡單闡述一下整體概念，基本上 Pod 裡面每個 Container 會使用 Volume 這個物件來代表容器內的掛載點，而在外部實際上會透過 PVC 以及 PV 的方式來描述這個 Volume 背後的儲存方案伺服器的資訊。 最後整體會透過 CSI 的元件們與最外面實際上的儲存設備連接，所有儲存相關的功能是否有實現，有支援全部都要仰賴最後面的實際提供者， kubernetes 只透過 CSI 的標準去執行。  Network(網路) 網路這個議題也非常有趣，我認為談到 Kubernetes 與 Networking 的關係時，可以有兩個方向去探討 如何提供網路功能給 Kubernetes 內運行的容器如何將 Kubernetes 應用到網路服務提供商 事實上，大部分(99%)的文章都在探討第一點，如何讓 Kubernetes 內部的容易有網路服務，不論是容器間，容器存取外部或是容器被外部存取。 而第二點其實仔細分析後，其概念最後也是回歸到第一點的需求，只是網路服務提供商內部的容器對於網路效率的要求更嚴苛，譬如更高的流量，更低的延遲，更多的網路介面等。 這些點我們之後再來仔細討論 ","version":"Next","tagName":"h2"},{"title":"Before Kubernetes​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#before-kubernetes-1","content":"如同先前提到的儲存資源的概念一樣，網路概念本身也是包山包海，各式各樣的議題可以討論，包含了層級也是非常的廣，譬如 硬體交換機的設計與建制網路架構的佈建，包含了各式各樣的拓墣 (Fat-Tree, Leaf-Spine..etc), 交換機內的連線 (LAG, MC-LAG, Bonding)各式各樣的路由技術或是路由議題 (BGP, OSPF, DSR, RIP, ECMP..etc)各式各樣的網路協定 (IPv4/IPv6,Unicast/Multicast/Broadcast,TCP,UDP,ICMP,MPTCP,QUIC)以Linux為範例來說，軟體上也有各式各樣的網路封包處理，譬如常用的 iptables/tc, linux bridge, tun/tapSDN 概念的管理與佈建 (SDN Controller, SW/HW Switch, P4, ONOS..etc)各式各樣的邏輯網路部署 (VLAN, VXLAN, GRE, NVGRE)效能優先的網路技術，如 DPDK, RDMA, Smart NIC 等... 基本上講不完，包含的議題實在太多了 上述每個領域都有各自的廠商/軟體在從事這方面的研究，這些領域要互相整合來提供一個更為強大的網路架構才是真正有價值的部分。 所以只要仔細想一下， Kubernetes 本身本來就不可能一口氣支援上述的所有的功能，甚至每個都處理的完美無缺點。 這對於整個 Kubernetes 平台來說是一個多麽大的負擔，可以說是一個不切實際的理想。 就跟儲存一樣，請放下 Kubernetes 是萬能的想法，不是套上 Kubernetes 什麼網路問題都解決了，請不要給 Kubernetes 過多錯誤的期待與期盼 ","version":"Next","tagName":"h2"},{"title":"Kubernetes​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#kubernetes-1","content":"對於 Kubernetes 來說，我個人的認知下， Kubernetes 在網路的部分比儲存 的地方做了更多的支援，除了標準介面之外，也有部分是 Kubernetes 自行實現的功能。 如同 CSI(Container Storage Interface) 這個針對 儲存 所定義的標準介面，在網路部分也有與之對應的 CNI(Container Network Interface). kubernetes 透過 CNI 這個介面來與後方的 網路解決方案 溝通，而該解決方案(我底下就統稱 CNI比較方便) 就我自己的開發經驗與體悟，我認為 CNI 最基本的要求就是在在對應的階段為對應的容器提供網路能力，就這樣非常簡單。 但是什麼叫做 提供網路能力, 這個部分我認為沒有定義，畢竟誰說網路一定是走 IPv4 ? 誰說網路一定要至少到 Layer3 IP 難道不能 point to point 互連嗎? 主要是因為這部分的功能特性對於大部分的使用者都沒有需求，而目前最常見也是 IPv4 + TCP/UDP 的傳輸方式，因此才會看到大部分的 CNI 都在講這些。 這邊使用實際需求來探討一下 CNI 所做的事情，假設我們希望所有容器彼此之間可以透過 IPv4 來互相存取彼此，不論是同節點或是跨節點的容器們都要可以滿足這個需求。 在這個要求下，最常見的步驟如下 容器創建之時，想辦法獲得一個 IPv4 位置，並且將該 IPv4 位置分配到容器內幫容器與外部節點中間建立一個能夠聯繫的通道設定相關的路由條件 (overlay? underlay?) 上面三個步驟，實際上做法百百種 如何取得 IPv4？ 如果要取得不重複的 IP 該怎麼做，需要有集中式的管理？ 還是分散式各自管理?如何讓容器與外部節點有聯繫的通道? 要走 veth？ host-device ? 直接掛載網卡進去 ?如何設定路由條件? 動態路由協議還是靜態路由協議? 要透過集中式資訊傳遞 gateway 嗎? 容器間到底怎麼傳輸的，需不需要封裝，透過什麼網卡，要不要透過 NAT 處理? 這一切都是 CNI 介面背後的實現，對於 kubernetes 來說其實根本沒有想要，也沒有能力去處理這些。 所以不要再幻想 kubernetes 能夠為你建立各式各樣的網路環境了 除了上述的容器間封包傳遞外，還有其他的網路議題 外部網路存取容器服務 (Service/Ingress)DNS 服務ACL (Network Policy) 這三個部份中， CNI 都多多少少有涉獵其中，譬如前兩點就會依賴 Kubernetes 提供的網路抽象層 (Service/Ingress)來使用，而這些部分的最底層則是 CNI 要提供基本的容器存取能力來打造良好的基底，上述的網路抽象層才能夠正常運作。 kubernetes 在 Service/Ingress 中間自行實現了一個模組，大抵上稱為 kube-proxy, 其底層可以使用 iptables, IPVS, user-space software 等不同的實現方法，這部分是跟 CNI 完全無關。 所以不要再看到 IPVS 就覺得好像是什麼全新的功能，其實最原生的概念就只是中間抽象層功能的一種實現而已。 而 ACL 則是一個完全抽象層， Kubernetes 本身只實現接口，不實現底層功能，因為 kubernetes 沒有任何頭緒你的 CNI 是如何讓容器有網路能力的，因此 kubernetes 根本沒有辦法幫你去設定相關的 ACL，則要依賴 CNI 自己去完成了。 ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#summary-1","content":"Kubernetes 本身有定義 CNI 這個網路標準介面，同時也有定義網路服務的中介層CNI 面對的網路提供方案自行想辦法實作功能，讓容器有網路連線能力Kubernetes 本身也有定義的中介層 Service/Ingress 並且透過不同的模組來提供此功能 iptables/IPVS.CNI 跟 Service/Ingress 是會衝突的，也有可能彼此沒有配合，這中間沒有絕對的穩定整合。遇到網路任何問題，可能是 Kubernetes 整合上的問題，也有可能是 CNI 本身的問題，這部分要仰賴管理者的經驗來處理，不可能也沒辦法一定概括誰的問題。 接下來用這張圖做一個總結 圖中虛線的部分則是 CNI 一般會處理的部份，包含了容器內的 網卡數量,網卡名稱,網卡IP, 以及容器與外部節點的連接能力等，左邊就是一個基本的 Bridge CNI 的用法，而右邊則類似一個 host-local CNI 的用法, 所以連接方法百百種，一切都依賴 CNI的實現。 若對於 CNI 標準有興趣的可以參閱下列文章 Containernetworking CNI githubCNI 系列文章 對於 Kubernetes 抽象層可以參閱下列文章了解其原理 What is ServiceHow to Implement Kubernetes Service - ClusterIPHow To Implement Kubernetes Service - NodePortHow To Implement Kubernetes Service - SessionAffinityIntroduction to Kubernetes Ingress Computing(運算) 最後則要講到運算能力這個部分了，眾所皆知的就是 Kubernetes 是一個 Container(容器) 管理協作平台，因此上面的基本運算單位都是基於 Container(容器) 來管理. 不意外的是， Kubernetes 本身又搞了一些相關標準來處理這個部分，如 CRI (Container Runtime Interface) 或是 Device Plugin 等相關的標準。 Container(容器) 與 Virtual Machine(虛擬機器) 之間的討論與比較一直以來都沒有停過，推薦這篇文章 Container vs VM: When and Why?， 以光譜兩端來說，除了完全的Container以及完全的Virtual Machine之外，也有愈來愈多的混合體，希望可以結合 Container 以及 Virtual Machine 各自的優點得到一個更好的虛擬化環境。 譬如(這邊不討論細節) gVisorKata Container 對於 kubernetes 來說，其實本身並不在意到底底下的容器化技術實際上是怎麼實現的，你要用 Docker, rkt, CRI-O 都無所謂，甚至背後是一個偽裝成 Container 的 Virtaul Machine virtlet 都可以。 ","version":"Next","tagName":"h2"},{"title":"Container​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#container","content":"就我個人的觀察來說，最多人在這個議題最大的誤解就是 容器 是萬能的 很多人看到的容器化可帶來的優勢後，一股腦地就要所有東西都容器化，完全沒有去思考到底為什麼自己本身的服務需要容器化，容器化可以帶來什麼優點. 舉個例來說，我想要透過 SRIOV 等相關硬體設定分配給我的應用程式使用時候， Virtual Machine 方面的發展與支援就遠比 Container 來的好多. 當然 RDMA 我個人也是抱持者 Virtual Machine 支援更好的情況來看待。 很多人踩中了第一點認為 容器 是萬能之後，就會開始進行要命的第二步驟，就是將原先的應用程式容器化. 太多太多的人都認為只要寫一個 Dockerfile 將原先的應用程式們全部包裝起來放在一起就是一個很好的容器 來使用了。 這就是常常會看到有一些的 Dockerfile 內同時跑了一堆Daemon(守護行程) 的容器，然後彼此之間相互依賴，對於外部的 Signal 以及 生命週期確認目標 都沒有辦法搞得清楚 最後在使用的時候又會發現各種軟體版本相依，日誌混雜，升級麻煩，監控重啟等都遇到各種問題，然後又會產生各種 Workaround 來使用。 最後就會發現根本把 Container 當作 Virtual Machine 來使用，然後再補一句 Contaienr 根本不好用啊. ","version":"Next","tagName":"h2"},{"title":"GPU​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#gpu","content":"除了 Container 本身的使用之外，還有一個近年因應 AI 產業蓬勃發展而對於 Kubernetes 最大的誤解。 就是 GPU 虛擬化 由於 AI 的蓬勃發展，帶動了 GPU 的使用需求, 而眾多的運算框架本身在使用 GPU 都是以 GPU張數 作為基本單位來使用的，譬如一台機器上面只有 一張 GPU 卡，同時間就只能有一個應用程式來使用該 GPU 並且使用一張 GPU 卡。 當然 Nvidia 本身也有推出一些虛擬化解決方案，譬如 Virtual GPU 來解決這方面的問題，希望能夠讓最上層的虛擬化環境可以不需要考慮底層的 GPU 真實數量。 除此之外也有一些廠商，如 Bitfusion 等也有提供對應的 GPU 虛擬化解決方案，底層還會使用如 GPUDirect RDMA(supported by Nvidia) 等技術來提供快速的 GPU Pool 概念。 但是仔細思考上面的文件，會觀察到這類型的技術都還是廠商自行研發或是提供介面來開發，而這部分牽扯到 kubernetes 之後又變得很有趣了。 Kubernetes 本身是依賴 Device Plugin 這個外部裝置的標準介面來存取外部裝置, 包含了 GPU, RDMA, SRIOV 等 NVIDIA 針對 Device Plugin 開發的一個簡單的 GPU 分配模組，可以將節點上的 GPU 分配到 Container 內部，而目前就是以張為基本單位去分配，因此 Container 可以看到的 GPU 就是真實主機上面的 真實數量。 然而由於 AI 的需求發展，加上 kubernetes 聽起來很棒的說法，兩者結合再一起之後就產生了無懈可擊的期盼，kubernetes 能夠虛擬化 GPU卡，讓容器同時存取多台節點上的 GPU 舉例來說，節點上只有 2張 GPU 卡，卻總是期盼一個要求四張 GPU卡的容器可以正常運作並且 Kubernetes 有一個漂亮的演算法可以從叢集中自動分配 GPU卡 來盡可能的提升使用效率. 光問 GPU 之間彼此怎麼溝通，這個事情就不是一個好處理的事情，透過 Process 不停地交換資料導致大量的 IO Copy 反而會造成效能下降， 若要透過 GPU 直接跨節點交換資料，又要透過 GPUDirect RDMA 等技術來處理，而這個部分又是要 GPU 應用程式 自行處理。 除此之外還有很多相關的議題要處理， kubernetes 本身根本不是因應 GPU 使用需求而產生的平台，抱有 kubernetes 可以完全處理一切問題實在是擁有太過美好的幻想了。 ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/k8s-concept#summary-2","content":"運算容器方面， Container 與 Virtaul Machine 都有各自的使用情境，不要一昧追求 Container 或是 Virtual Machine， 請多多思考自己的需求容器化 不是把直接 Virtual Machine 的使用習慣換個環境使用就叫做 容器化，而是要從概念上去暸解與使用千萬不要為了容器化而容器化，不是用來解決問題的改動都只是在製造更多的問題。硬體裝置部分，GPU 很夯沒錯，但是 GPU 本身的複雜程度沒有這麼好處理， kubernetes 不是為了 GPU 而誕生的，沒有辦法什麼都辦得到，要特定的功能還是想辦法自己去修改 kubernetes 或是搭建其他的服務來處理 Summary 講了這麼多，想傳達的就是 Kubernetes 不是萬能的，作為一個平台其強大之處在於透過各式各樣的介面與框架來相容第三方解決方案的整合。 在此模式下彼此都可以專注於自身能力的發展並且互相合作來提供更好的使用價值 不要對 Kubernetes 抱有太大的期盼，不要一昧的跟風，看到什麼是主流就馬上換什麼，請好好的思考到底自己需要什麼，到底 kubernetes 帶來的價值是否值得採用 儲存，網路，運算 這些常見的使用資源中，請仔細研究與考慮自己使用情境的需求，不要一昧看到大家說什麼就用什麼，最後苦的只是自己。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2020/cncf-tech-radar-cd","content":"","keywords":"","version":"Next"},{"title":"Level​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cncf-tech-radar-cd#level","content":"為了簡單量化這些調查報告，所有的調查都會要求使用者對於是否推薦這個專案給予下列答案之一 Adopt 這個答案代表該使用者(通常是廠商)是明確的推薦這個技術，使用者已經使用這個專案一段時間，而且也被團隊內證實的確是穩定且有幫助的Trail 這個答案代表使用者有成功的使用過這些技術且推薦大家要多關注這些技術的發展Assess 這個答案代表使用者有嘗試使用過且認為他們是有未來的，推薦大家當你專案內有特別需求的時候可以去看看這些專案 基本上我的認知就是信心程度，由上到下遞減。 除了上述三個答案之外，還有一個選項就是 HOLD，顧名思義就是可以停一下，不要太執著這個專案甚至不要使用。 關於這個專案的一些運作，譬如題目跟專案的選擇，甚至一些概念的介紹都可以參閱官方網站 Continuous Delivery ","version":"Next","tagName":"h2"},{"title":"資料來源​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cncf-tech-radar-cd#資料來源","content":"上述影片中說有 33 個廠商回報，但是根據網頁的結果大概只有 28 個，所以實際上到底會有多少我不確定，不過大概就是 30 上下左右。 這些公司散落於不同產業，同時公司內的員工數量也不一定，這些數據都可以從官方網頁中看到，如下圖  ","version":"Next","tagName":"h2"},{"title":"報告​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cncf-tech-radar-cd#報告","content":"下列就是報告的 Radar 圖 這邊就是根據最上面的三個標準，讓參與的 CNCF 使用者廠商來回報對這些專案的想法。 這邊要注意： 這邊的結果是粗略的統計結果，沒有太多明確的數學定義多少廠商回報 ADOPT 就可以歸類為 ADOPT，就當過一個普遍的趨勢去看待就好。 從上圖可以看到，目前被歸類為 ADOPT 的有兩個專案，分別是 Helm 以及 Flux Helm 大家應該都很熟悉， Kubernetes 應用程式的打包及部署解決方案，而 Flux 則是一個基於 GitOps 所實作的 CD 解決方案。 以下是根據影片內投影片的結果來統計其每個 Level 的數量 --\tAdopt\tTrial\tAssess\tHoldFlux\t7\t1\t3\t0 Helm\t12\t4\t3\t0 接下來看到 TRIAL 這列表，包含了 Circle CIKustomizeGitLab --\tAdopt\tTrial\tAssess\tHoldCircle CI\t5\t0\t1\t3 Kustomize\t5\t3\t3\t1 GitLab\t4\t2\t3\t0 最後看到 Assess 列表，會被放到 Assess 的專案通常都是大家沒有一個很明確的共識。 這邊的專案很多，包含 ArgoCDJenkins XSpinnakerTeamCityGitHub ActionsTekton CDTravis CIJenkinsjsonnet --\tAdopt\tTrial\tAssess\tHoldArgoCD\t2\t3\t10\t0 Jenkins X\t0\t1\t3\t2 Spinnaker\t0\t3\t6\t4 TeamCity\t1\t0\t0\t1 GitHub Actions\t2\t7\t7\t0 Tekton CD\t0\t0\t2\t0 Travis CI\t1\t1\t1\t2 Jenkins\t6\t0\t2\t17 jsonnet\t1\t1\t2\t2 這份清單的確可以看到大部分的專案都沒有明顯的共識，有些專案可能使用者數量也少，導致回應的統計數量也少。 比較有趣的點有 喜愛 Jenkins 的是裡面最多的，但是認為不要碰 Jenkins 的也是最多的，更甚一個量級，比 Adopt 的 Helm 支持者還要...GitHub Action 跟 ArgoCD 看起來算是相對獲得好評，本身沒有任何 HOLD 的紀錄Jenkins X / Teckon CD 這兩套看起來使用者還不算太多jsonnet 相對於 Kustomize 以及 Helm 還是小眾 ","version":"Next","tagName":"h2"},{"title":"結論​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cncf-tech-radar-cd#結論","content":"Publicly avaliable solutions are combined with in-house tools 根據調查結果，大部分的使用者都嘗試過至少十套以上的解決方案，最後收斂到2-4套左右去使用。有些大公司會建置自己的 CD 工具並且開源部分，譬如 release-manager, kube-applier 以及 stackset-controller。 另外一個有趣的是沒有一個公有雲廠商的 CD 解決方案被使用者給建議，這部分是個有趣的結論。 Helm is more than packaging applications. 雖然 Helm 不會被認為是一個 CD 工具(其本質更像是一個 Kubernetes 應用程式管理者)，但是 Helm 卻是一廣泛被應用於各種 CD 解決方案中的管理工具。 Jenkins is still broadly deployed, while cloud native-first options emerge. Jenkins 以及其相關生態系的相關工具 (Jenkins X, Jenkins Blue Ocean) 還是被廣泛地使用。然後滿多廠商都表示 Jenkins 主要是用在存在已久的系統，如果是全新的應用環境，都會嘗試使用其他的解決方案，譬如更加支援 GitOps 的 Flux 等 個人心得 這個專案我認為算滿有趣的，透過 CNCF 的使用者社群來調查大家使用過哪些工具，對於這些工具有什麼想法，可以整理出對於滿山滿谷專案下，一個普遍的共識（或是沒有共識）。 事實上這個專案調查的時候也沒有辦法列出世界上所有的 CD 解決方案，所以可能會有些專案沒有被列入考慮中，但是我認為這類型的報導跟調查都還是有其價值所在，雖然受訪群體不多，就 30 多個廠商為主，但是畢竟是以廠商為單位，不是以人為單位，本來就很難動不動就湊到幾千幾萬個人來回報資料。 就結果的分析來看， Helm 這個工具還是有其地位所在，再來就是 Kustomize，兩者我都喜歡，畢竟各有各適合的場景。 對於系統來說, Jenkins 真的是一個又愛又恨的選項，彈性高，維護麻煩，除此之外，可以看到也有滿多開源專案跟 SaaS 平台上榜，其中 Flux/ArgoCD 的排名有點出乎我意料就是了，比我想像多的公司在使用 GitOps 類似的部署流程 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2020/cncf-tech-radar-observability","content":"","keywords":"","version":"Next"},{"title":"資料來源​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cncf-tech-radar-observability#資料來源","content":"這次的報告總共有來自 32 個 CNCF 會員參與，全部票數有 283 票，參與的廠商規模有大小，領域也不同，下圖節錄自官方報告  從人數規模來看，基本上每個公司都是百人規模以上，甚至一半以上都是千人等級，還有六家公司是萬人等級。 這數字我個人認為台灣很難找到如此規模的公司再探討 CNCF 可觀測性的應用，此外這些公司裡面，大部分都來自於軟體公司 ","version":"Next","tagName":"h2"},{"title":"報告​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cncf-tech-radar-observability#報告","content":"下圖節錄自官方的結論報告 該報告就是根據上面的標準，讓參與的 CNCF 會員來回報對這些專案的推薦程度 這邊要注意，這邊的結果是粗略的統計結果，沒有太多明確的數學定義到底什麼樣的等級可以歸類為 ADOPT，所以觀看時就當做一個參考看看即可 ADOPT 該圖片中，歸類為 ADOPT 也就是非常推薦使用的解決方案有五個，分別是 Elastic Elastic 這邊沒有說明是開源專案還是商業解決方案，畢竟 Elastic 實際上還包含了很多專案一起，常用的 ELK 可以算是其中之一。 Datadog Datadog 本身是一個商業解決方案，提供客戶一種視覺化的服務來監控與觀測系統上的各式各樣資料，這公司自疫情以來，股價已經翻了三倍，財報屢屢創新高 PrometheusOpenMetricsGrafana 這三個幾乎可以一起談，Prometheus 以及 Grafana 這兩套軟體大家使用上都會一起使用，很少看到單獨使用的。 透過 Prometheus 的介面，可以串皆各式各樣的 Metrics 並且透過 Grafana 來將這些資訊用自己喜歡的方式呈現 OpenMetrics 本身也是 CNCF 的專案之一，其目的主要是探討 Metrics 的格式，希望透過制定標準來讓各解決方案輕鬆整合，詳細的介紹可以參閱這個 CNCF to host OpenMetrics in the Sandbox TRIAL 這邊總共有六個工具，代表的是有使用，並且強烈推薦觀望其發展，認為其有使用的潛力，這邊包含了 Splunk 商業的日誌收集解決方案，算是非常老牌的服務，印象中價格會根據日誌容量來計費，所以如果今天服務開啟了大量的 debug模式的話，可能開銷會突然增加不少 Sentry 一套針對應用程式的觀測與除錯解決方案，使用起來非常方便且好用，特別是當出現問題的時候能夠提供更多友善的資訊幫忙除錯，我個人是滿喜歡的。 Cloudwatch AWS 內建的觀測平台，這部份就沒有什麼特別好說，我想應該不會有人沒有使用 AWS 卻跑來使用這套系統 Lightstep 查了一下也是一套商業解決方案，本身並沒有使用過的經驗，所以也不好說 Statsd 個人沒有聽過也沒有用過 Jaeger 作為 Uber 所開源的 Opentracing 解決方案，我之前有一個影片詳細介紹 Opentracing 與 Jaeger，有興趣的可以觀看 SDN x Cloud Native Meetup - Webinar 邱牛上菜 #3 OpenTracing ","version":"Next","tagName":"h2"},{"title":"ASSESS​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cncf-tech-radar-observability#assess","content":"這個類別中只有三個選項，分別是 OpenTelementry, Thanos, Kiali ，代表這些專案是有使用過，且認為不錯，但是只有真的需要的時候才特別需要去研究。 OpenTelementry 這部份我個人認為可能是個趨勢，該組織已經將 OpenTracing 給整合進去，希望能夠提供一個更為通用的函式庫以及介面來使用，就我所知 Jaeger 的部份程式碼都已經被整合進去。 詳細的也可以觀看 SDN x Cloud Native Meetup - Webinar 邱牛上菜 #3 OpenTracing 這個影片，最後面有跟大家分享 OpenTracing, Jaeger, OpenTelementry 三者的差異 Thanos 作為 Prometheus HA 的解決方案，就我所知用過的人都覺得還不錯，除了這個解決方案之外，不確定還有什麼好方式可以幫 Prometheus 搭建起 HA 的環境 Kiali 作為一個 Service Mesh Istio的管理工具，我本身是沒有使用經驗，所以也無從判斷 最後用長條統計圖來再次觀看一下三個類別的資訊 結論 文章與影片中，針對這些報告給了三個結論，這邊簡單節錄部分內容，有興趣的可以觀看 原文 The most commonly adopetd tools are open source. 整份報告裡面，三個收集到最多 Adopt 投票回饋的工具 (Prometheus, Grafana, Elastic) 以及五個收集到最多投票回饋的工具 (Prometheus, Grafana, Elastic, Jaeger, OpenTelemetry) 全部都是開源軟體。 作者認為這個議題滿有趣，可以看到大部分的公司都決定自己去維護這些開源軟體，從佈署，維護甚至支持更大規模的挑戰全部都自己處理。這些公司想必也是有針對商業解決方案去進行一些探討，並且從中比較各自的優缺點，最後才選擇自行架設使用 There's no consolidation in the observabilibty space. 大部分的公司都使用非常多的工具於可觀測性這個領域，有超過一半的公司使用五個工具以上，還有 33%左右的公司擁有十個工具以上的使用經驗 可觀測性這個議題其實非常廣泛，每個人使用時想要獲得的資訊都不同，同時每個工具的強項也都不同，這可能也是造成這個領域並沒有一個主宰的工具，反而是群雄割據。 此外對於大部分的使用者來說， 可觀測性並不是整體供的核心服務，因此可能也不會有太多的資源讓團隊去研究如何整合切換，這也可能就是為什麼會同時使用多套解決方案的理由之一 Promethesu and Grafana are frequently used together. 報告中表示，有超過 66% 的使用者是同時使用這兩套解決方案的。市面上有很多的教學文章或是解決方案都是將兩者整合，讓使用者可以很輕鬆的同時使用兩者 個人心得 我個人滿喜歡 CNCF 技術雷達的文章，可以作為一個參考來看看各領域當前主流的用法有哪些，雖然主流不代表正確，但是這也是一個信心支撐的來源，至少你有機會跟別人說，這樣用法很常見，還有 CNCF 的 文章可以背書。不過重要的是自己的團隊適合什麼工具。 就像文章中有提到，可觀測性是一個非常廣泛的議題，你要收集什麼資料，想要用這些資料回答什麼問題，再套用這些工具之前一定要先想清楚這個問題，從這個問題在去發想自己缺乏什麼工具，以及有哪些工具可以解決這些問題。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2019/iThome_Challenge/k8s-security","content":"","keywords":"","version":"Next"},{"title":"Container Security​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#container-security","content":"排除特定基於虛擬機器的 CRI 解決方案的話，Container 是 kubernetes 運作的最基本單元，container 本身的安全性牽扯範圍不少，譬如運行環境的權限設定，避免過度提權導致該 container 有過大的權力。或是 container 內部安裝的軟體是否本身就有安全性漏洞，而這些軟體是產生 image 本身的時候就已經安裝好還是運行後動態安裝的？ 這意味者 container image 本身也是有相關的安全性問題需要檢查，譬如檢查整個系統內是否有任何軟體有安全性漏洞 基於上述 container image 產生的安全性隱憂，目前也有相關的專案再處理這一塊，譬如CoreOS's Clair 專案 Clair is an open source project for the static analysis of vulnerabilities in application containers (currently including appc and docker). 除了 Image 內軟體的安全性之外，image 本身的數位簽章也是一個需要考慮的部分 舉例來說，對於 kubernetes 這個 container 管理平台，是否針對任何 Pod Yaml內描述的 Container 都需要幫忙創建? 如果該 Container 可能本身是來路不明，無法保證其使用安全性，這種情況下是否可以拒絕創建 基於這個情況下我們可以採用簽名的方式來幫每個 Container Image 簽署名稱，同時讓 kubernetes 本身信任簽署的單位。 其概念有點類似 SSL 憑證及 CA 的運作。 以 Docker 為範例，其本身有個功能名為 Docker Container Trust，有興趣的可以自行研究。 如果是基於 kubernetes 使用情況的話，可以參考由 IBM 推出的專案portieris， Portieris is a Kubernetes admission controller for enforcing Content Trust. You can create image security policies for each Kubernetes namespace, or at the cluster level, and enforce different levels of trust for different images. 最後則是關於 Container 本身的權限控管，不論是運行的使用者身份，群組，甚至是相關 namespace 的共用，或是基於 system call 層級來限制的功能。 這部分我們來仔細探討 ","version":"Next","tagName":"h2"},{"title":"Container Permission​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#container-permission","content":"這邊基於 Kubernetes內創建 Container 相關的參數來一一探討，這些參數每個的效用都有範圍，也許單獨只看一個會覺得影響不大，但是如果不同的權限功能互相疊加後，就可能產生一個極大權力的 Container，大到要整個破壞 Kubernetes 節點本身都不是問題。 接下來的討論是基於 Pod Security Policy 的內容來討論 ","version":"Next","tagName":"h2"},{"title":"Host namespaces​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#host-namespaces","content":"之前談過 Linux 的環境下是基於 Linux Kernel Namesapce 來創建一個與原生系統 隔離的虛擬化環境。 於 CNI 的章節中也有介紹過這些 namespace 本身除了可以創建新的來隔離之外，也可以與舊有的進行共用，譬如 Infrastructure Pod(Pause)。 目前 kubernetes 有開放下列幾種 namespace 來共用。 HostPIDProcess ID 與節點共用，這意味就可以於 Container 內部直接觀看到節點上運行的所有 Process HostIPCContainer 與節點共用 Inter-Process Communications Namespace，如果對 IPC 概念有興趣的可以參考這篇 Introduction to Linux namespaces - Part 2: IPC HostNetwork 開啟這個功能將使得 Container 本身的網路與節點是完全共用的，這意味可以從 Container 內部看到節點上面的網路資訊，譬如網卡數量， IP 地址，相關路由規則甚至是 Iptables 防火牆。 事實上 kubernetes 很多內建的服務都會開啟這些功能，最簡單的概念就是 CNI 都還沒有安裝的情況下，那些被預設安裝好的 Pod 到底是怎麼互通的? 其實就是透過這個方式直接使用節點上的網路功能來互通。 ","version":"Next","tagName":"h3"},{"title":"Volumes and file systems​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#volumes-and-file-systems","content":"此功能是 Kubernetes 自行實作的，單純用來限制該 Pod/Container 可以使用哪些儲存空間類型與模式，譬如 ConfigMap, HostPath, PVC 等。 其實這類型的安全設定都秉持者一個概念，針對用到的部分去給予權限，也許會覺得管理起來很麻煩，但是就是一種限縮的概念 ","version":"Next","tagName":"h3"},{"title":"Users and groups​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#users-and-groups","content":"期望系統用什麼樣的身份去運行該容器，目前於 Linux 中是透過 UID/GID 等數值搭配系統上的 /etc/passwd, /etc/group 來配對出該運行的角色是什麼身份。 我認為目前大部分的 Docker Image 還是都基於 root 的身份去創建的，這個帶來的一些隱性問題就是如果今天該 Container 透過 Volumes 的方式把一些系統上面的檔案都掛載到 container 內，那因為檔案系統的權限也是基於 UID/GID 去比對的，所以其實容器的 root 是有機會去修改掛載進來的檔案。 如果今天該 container 是個惡意的應用程式，就代表有機會可以存取到節點外的系統資訊，甚至對於其進行寫入造成影響。 所以比較好的方式是不要使用 root 來運行你的應用程式，創立特定的使用者與群組來處理。 此外如果對於 NFS 熟悉的人，也會知道 NFS 的存取權限也是基於 UID/GID 的處理，所以如果是一個以 root 身份去使用 NFS 的話，產生出來的所有檔案都會是 root/root，對於整個檔案分級的架構可能就會造成不預期的行為。 ","version":"Next","tagName":"h3"},{"title":"Capabilities​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#capabilities","content":"針對 Linux 本身更深層的處理，有個名為 Capabilities 的權限控管工具可以使用，詳細的內容可以參考 man capabilities 根據說明，其淵源以及功能為 For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero). Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process's credentials (usually: effective UID, effective GID, and supplementary group list). Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled. Capabilities are a per-thread attribute. 透過 Capabilites 將本來全部賦予給 privileged 權限的功能給拆出來，可以避免一個擁有無上功能的使用者，藉此來達到 有使用才給予 的原則。 不知道有多少人知道，其實如果沒有賦予權限的話，是不能使用 ping 這個功能的，是因為 ping 的底層是透過 raw socket 的方式去實現，而 raw socket 本身就是屬於直接收送封包的方式，本身就會有權限使用上的考量，因此必須要搭配 CAP_NET_RAW 這樣的權限才有辦法使用 ping。 但是這個功能因為太常用，所以其實這個能力已經變成預設值(以 Docker為範例，可參考Runtime privilege and Linux capabilities) 此外還有一個能力叫做 CAP_NET_ADMIN，一但開啟這個功能，就可以對所有的 network stack 進行操作，包括改 IP 地址，改路由規則，修改任何運行網卡設定，非常的強大。 這時候仔細想想，如果有一個 Container 本身被賦予 CAP_NET_ADMIN 的權限，同時也透過 hostnetwork 的方式與節點共享網路。 這意味者該 Container 擁有完全修改節點網路內容的能力，只要該應用程式想要作怪，整個節點直接斷線並且讓網路功能喪失都不是什麼問題，非常輕鬆。 所以使用者要非常謹慎小心，哪些能力需要額外賦予應用程式請斟酌考量，並且確實的了解其用途。 ","version":"Next","tagName":"h3"},{"title":"AppArmor​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#apparmor","content":"可以參考Kubernetes Apparmor 的介紹 AppArmor is a Linux kernel security module that supplements the standard Linux user and group based permissions to confine programs to a limited set of resources. AppArmor can be configured for any application to reduce its potential attack surface and provide greater in-depth defense. It is configured through profiles tuned to whitelist the access needed by a specific program or container, such as Linux capabilities, network access, file permissions, etc. Each profile can be run in either enforcing mode, which blocks access to disallowed resources, or complain mode, which only reports violations. 基本上是個非常厭煩的功能，以 profile 為基本單位去限制相關應用程式能夠存取的所有東西，譬如 capabilities, network, file permissions。 譬如以下範例 #include &lt;tunables/global&gt; /bin/ping flags=(complain) { #include &lt;abstractions/base&gt; #include &lt;abstractions/consoles&gt; #include &lt;abstractions/nameservice&gt; capability net_raw, capability setuid, network inet raw, /bin/ping mixr, /etc/modules.conf r, }  上述這個範例是針對 /bin/ping 這個應用程式去設定的，就如同上述提到的，需要有 CAP_NET_RAW 的能力，一旦只要 ping 本身被修改過使用到超過標注的，就會被 apparmor 給阻止而不能使用。 其使用上非常麻煩，但是可以限制非常多不必要的功能。 ","version":"Next","tagName":"h3"},{"title":"Privileged​","type":1,"pageTitle":"前言","url":"/docs/techPost/2019/iThome_Challenge/k8s-security#privileged","content":"只要打開此功能，上述探討的一些特性都會一起被打開來創造一個非常有力的應用程式，包含可以讀取所有的裝置，有滿滿的 capabilities，請斟酌小心使用，不要對來路不明的應用程式使用這個權限。 Summary 除了上述之外討論到的功能之外，還有其他非常多的細節，更不用說 4C 中其他領域都有各自的範圍與概念需要學習與探討。 資訊安全就是一個沒出事情前大家不會在意，甚至不覺得有幫助，但是一旦出了問題，可能就是一個動搖整個公司的問題。就我的角度這類型的概念就是會愈多愈好，你未來執行任何操作，撰寫任何程式時都能夠把安全的概念給套用，其實無形中就是增加整個系統與產品的安全。 參考 https://kubernetes.io/docs/concepts/security/overview/https://github.com/coreos/clair/https://blog.yadutaf.fr/2013/12/28/introduction-to-linux-namespaces-part-2-ipc/https://kubernetes.io/docs/tutorials/clusters/apparmor/https://help.ubuntu.com/lts/serverguide/apparmor.html ","version":"Next","tagName":"h3"},{"title":"Docker Network - 網路模型","type":0,"sectionRef":"#","url":"/docs/techPost/2020/docker-network-model","content":"","keywords":"docker network","version":"Next"},{"title":"None​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#none","content":"None 這個參數的意思就是告訴 Docker Engine 不要幫我管理任何任何網路功能，只要建立一個隔離網路空間（Network namespace）就好。 ","version":"Next","tagName":"h2"},{"title":"範例指令​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#範例指令","content":"$ docker run --network=none -d --name none hwchiu/netutils $ docker exec none ifconfig -a lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  在我們的範例當中可以看到，預設就只有一個 lo(loopback) 的介面，沒有其他任何網路卡介面，所以這個 container 也沒有對外上網的能力。 ","version":"Next","tagName":"h3"},{"title":"網路環境觀察​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#網路環境觀察","content":"這邊用兩張不同視角來看待這個行為，由於本篇文章都會採用相同的概念去解讀不同網路模型，因此這邊詳細介紹兩種視角的概念。 圖左：這邊想要介紹的是以系統底層的角度去觀察網路，中間的灰色線將其分為上半部分的 UserSpace, 以及下半部分的 Kernel Space。 本範例中下圖會有不同顏色變化，代表不同的網路空間，每個網路空間彼此網路隔離。 圖右：這邊提供一個比較簡略的介紹，主要會從使用者的角度去觀察，由圖例來說明網路元件的關係上有什麼變化。 在瞭解上述概念後，我們再來看一下如何去理解這張圖片： 圖左： 當創建一個全新的 Container 後，系統會幫我們在 Kernel 內創建一個全新的網路空間（黃色區塊）來達到網路隔離的效果。不過因為我們沒有對這個隔離環境做任何設定，所以這個網路空間中只會有一個 lo 的網卡。 至於淺藍色的部份則代表著系統原先的網路空間，在此假設已經存在預設網卡 eth0。 圖右: 系統中產生了一個全新的 Container, 但是這個 Container 跟原生主機沒有任何互動，網路關係上就是 毫無關係。  ","version":"Next","tagName":"h3"},{"title":"使用情境​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#使用情境","content":"想要開發網路模型，或想要研究 Docker 網路，以及想要開發 CNI 的人都適合用這種模式創建乾淨網路，然後開始透過各種方式讓其能夠上網。 ","version":"Next","tagName":"h3"},{"title":"Host​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#host","content":"Host 這個參數的意思就是告訴 Docker，請不要幫我創造 network namespace，我不需要網路隔離，和宿主機共用相同的網路模型即可。 共用內容包含了 網卡，路由表，防火牆 ... 等 ","version":"Next","tagName":"h2"},{"title":"範例指令​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#範例指令-1","content":"$ docker run --network=host -d --name host hwchiu/netutils $ docker exec -it host ip link 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 0e:e5:e9:25:d8:41 brd ff:ff:ff:ff:ff:ff 3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default link/ether 02:42:71:98:4c:2a brd ff:ff:ff:ff:ff:ff $ ip link 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 0e:e5:e9:25:d8:41 brd ff:ff:ff:ff:ff:ff 3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default link/ether 02:42:71:98:4c:2a brd ff:ff:ff:ff:ff:ff  透過上述指令的創造，可以發現該 container 內所看到的網卡資訊與外部主機的內容是完全一樣的，除了使用 ip link外，其他的指令如 ip addr, ifconfig, iptables-save, ipvsadm 等都會看到相同內容。 network port 都會共用，因此如果外面已經有服務使用 port 80, 你就不能再跑一個 port 80 於相同的 address 上。 ","version":"Next","tagName":"h3"},{"title":"網路環境觀察​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#網路環境觀察-1","content":"圖左： 當創建一個全新的 Container 後，我們不需要任何網路隔離功能，因此 Kernel 內並沒有任何新的網路空間被創造。我們的 Container 會直接使用預設網路空間內的所有網路資源，譬如網卡 eth0。 圖右: 系統中產生了一個全新的 Container，這個 Container 跟原生主機共用網路空間，因此 Container 看到的網路環境會與宿主機是完全一致。  ","version":"Next","tagName":"h3"},{"title":"使用情境​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#使用情境-1","content":"想要直接存取 Container，不希望封包會被其他路徑處理 a. 網路效能與存取本機相同特別注意 Port Number 的使用，若啟用相同服務時就很容易發生衝突容器需要使用特殊硬體資源，但掛載到容器導致相對麻煩時，我們則會使用這種方式共用宿主機的資源容器本身會需要對宿主機的網路環境進行操作或監控時 ","version":"Next","tagName":"h3"},{"title":"Bridge​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#bridge","content":"Bridge 這個參數的意思就是告訴 Docker 請幫我創造全新的 network namespace，然後我想要透過 Linux Bridge 來與原生網路有互動的能力 這部份先忽略 iptables 的任何規則 這個模型也是 Docker 中預設的網路模型，會幫你執行下列步驟： 你於容器內創建一張網卡，並且指派相關的 IP addresses於主機創建一個 Linux Bridge透過 veth 幫你把 container 與 主機 這兩個不同的網路空間給串連一起 veth 概念複雜，在此暫時不探討太多，只要知道是一個特殊的方式來串連不同網路空間。 ","version":"Next","tagName":"h2"},{"title":"範例指令​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#範例指令-2","content":"下述的指令比較複雜，首先我們會先創建一個使用 bridge 模型的 container。 然後我們可以透過 ip addr 觀察到裡面有兩張網卡，分別是 lo 以及 eth0， 這邊要注意的是 eth0 後面有一個數字 183，這個數字跟 veth 的概念有關，不過請容我在此先略過。 接者我們到主機方面去呼叫 ip link 然後用 183 來搜尋，會發現系統上有一個網路介面跟 183 有關係。 最後我們透過 brctl 工具可以發現系統上有一個 Linux Bridge 叫做 docker0，而剛剛發現的網卡正是被綁定在這個 docker0 當中。 $ docker run --network=bridge -d --name bridge hwchiu/netutils $ docker exec bridge ip addr 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 182: eth0@if183: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever $ ip link | grep 183 183: veth980af09@if182: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default $ brctl show | grep veth980af09 docker0 8000.024271984c2a no veth980af09  ","version":"Next","tagName":"h3"},{"title":"網路環境觀察​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#網路環境觀察-2","content":"圖左: 這個架構比較複雜，分成幾個角度來看： 容器創造後，會於自己的網路空間（黃色區塊）內額外創造一張新的網卡 eth0，因此，網路空間內就會有兩張網卡，分別是 lo 以及 eth0外部主機的預設網路空間（水藍色區塊）中，會創建一個全新的 Linux Bridge：docker0，並且透過 veth 的機制與 Container 內的 eth0 串接起來 veth 概念複雜，在此暫時不探討太多，只要知道是一個特殊的方式來串連不同網路空間。 圖右：系統中除了產生 Container 之外，還會產生一個全新的 Linux Bridge，並且透過 veth 的方式將 Linux Bridge 與 Container 串連一起。  如果這時候我們再額外建立一個新的 Container（粉色區塊），那結果會如下圖 全新的 Contaienr 會創造一個新的網路空間（綠色區塊），並且設定好網路介面。因為當前系統上已經有 docker0 可以使用，因此 Docker 不會創建新的 Bridge 來橋接 Container透過 veth 將新的網路空間（綠色區塊）與外部主機的網路空間（水藍色區塊）串連起來從右邊圖片來看，系統上有愈來愈多的 veth 網卡，這些網卡其實都連接到不同的 Container  ","version":"Next","tagName":"h3"},{"title":"使用情境​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#使用情境-2","content":"Docker 預設網路模型，能夠對外上網 需要與 iptables 合作來達到存取外網，這篇文章先跳過 iptables 部分 Container 之間可以透過 IP 的方式互通對網路沒有要求，單節點彼此能通就好 ","version":"Next","tagName":"h3"},{"title":"Container​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#container","content":"Container:$ID 這個參數的意思就是告訴 Docker 不要幫我創造新的網路空間，取而代之，使用現有的 Container 的網路空間，和它共處於相同的網路環境中。因此，這兩個 Container 將會看到一樣的網路介面、路由表 ... 等網路相關資訊。 ","version":"Next","tagName":"h2"},{"title":"範例指令​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#範例指令-3","content":"$ docker run --network=container:$(docker ps --filter name=bridge -q) -d --name co_container hwchiu/netutils $ docker exec co_container ifconfig eth0 Link encap:Ethernet HWaddr 02:42:ac:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:17 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1366 (1.3 KB) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) $ docker exec bridge ifconfig eth0 Link encap:Ethernet HWaddr 02:42:ac:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:17 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1366 (1.3 KB) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  可以看到創建後，這兩個容器內所看到的網卡內容都會一模一樣。 ","version":"Next","tagName":"h3"},{"title":"網路環境觀察​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#網路環境觀察-3","content":"圖左: 這個架構比較複雜，分成幾個角度來看： 原本的容器（淺藍色）以 Bridge 模式創立，因此系統會產生 bridge 環境所使用到的所有資源接著新的容器（粉紅色）以共享網路的方式創立，新容器要掛載到藍色容器的網路空間中，所以不會有新的網路空間，會共享藍色容器的網路空間（黃色區塊），包含 eth0, lo 等 圖右: 系統中產生 Container 後，會直接與目標 Container 共享網路空間，因此會看到這兩個 Container 共享同一張網卡 eth0。 如果本來的 Container 是 None, 那這種情況就是兩個人一起 None，簡而言之就是共享網路環境而已。  ","version":"Next","tagName":"h3"},{"title":"使用情境​","type":1,"pageTitle":"Docker Network - 網路模型","url":"/docs/techPost/2020/docker-network-model#使用情境-3","content":"相同網路空間內的容器因為共享 lo，所以可以使用 localhost 來存取彼此服務不同容器有存取需求，可以透過此方式享受到更快的網路存取速度Kubernetes 的 Pods 基於這個模型去實作，所以 Kubernetes Pod 裡面可以有多個 Containers 且彼此可以使用 localhost 來互相存取彼此的服務。 結論 本篇文章跟大家介紹了 Docker 的基本模型，並沒有涉及太多底層的技術細節，從不同的網路環境來認識這些模型的差異，同時我們也比較每種環境的使用時機，讓各位對於這些網路模型有更多認識。 這些虛擬網路環境其實都和我們時刻相處，瞭解這些網路架構的不同也會有助於思考整套系統的架構。 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2019/keel","content":"","keywords":"k8s cicd keel","version":"Next"},{"title":"Problem​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#problem","content":"再正式介紹 Keel 之前，我們先自己思考一下，如果遇到今天下列的情況，你可能會怎麼做? 開發者提交程式碼，並且與 CI 系統有良好整合，確保程式品質希望最新版的程式碼能夠自動的部署到 Kubernetes 集群，不論是 dev/staging/production/...etc 基於上述的環境下，其實有滿多的方法可以完成，但是不同的方式其實都牽扯到不同的條件。譬如 使用的 CI 是哪一套，是 Drone/Jenkins/CircleCI/Travis ? 是自架維運還是直接採用企業雲端服務?容器倉庫放置的地點是哪裡? Docker hub ? 還是公有雲相關的服務?Kubernetes 部署的位置在哪裡? 是放置在公有雲裡面? 還是私人集群? 公有雲裡面是採用 kubernetes service (GKE/EKS/AKS) 還是透過 VM 的方式自行架設? 不同的情境，對應到上述的問題都會有不同的解法。而本文的 Keel 則是用來解決上述問題的其中一種的解決方案 假設今天使用了 雲端服務 為主的 CI 解決系統，而此時相關的 Kubernetes 則是直接部署在三大公有雲裡面。 這時候整個運作流程就是 程式碼提交(這邊使用 Github 為範例), 並且觸發相關的 CI 流程該流程中，我們進行了相關的功能與測試，並且建置好相關的容器映像檔(Container Image)將該容器映像檔案部署到特定的容器倉庫 (Container Registry)透過特定的方式想辦法更新 Kubernetes 內運行的容器 上述的流程牽扯到不同雲端服務，這意味者就牽扯到不同的權限控管。 舉例來說 Github 與 CI 相關的資訊授權(不是本文重點)CI 系統與容器倉庫平台的權限控管 通常來說可能只需要 Push Image 相關的權限.Push Image 可能意味 Write 配上一些 Read. 這部分沒有唯一，看平台而定 CI 系統與 Kubernetes 平台的權限控管 這部分應該會基於 Kubernetes 的 Role (RBAC) 來決定，根據應用程式的使用方式，可能會需要更強大的權限來部署相關的資源，如 Pod/Deployment/Service/Ingress/Secret/ConfigMap 所以對我來說最困難的反而不是這些如何串聯起來，反而是就授權方面，該如何處理才是合宜的。 我認為權限控管沒有一定的答案，根據自己所在環境的政策以及需求，並且針對風險與開發流程去評估最適合自己的處理方式才對。 ","version":"Next","tagName":"h2"},{"title":"Keel​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#keel","content":"基於上述的流程，如果今天不希望在 CI 系統擁有太多關於 Kubernetes 相關的權限，即不希望 CI 系統能夠主動的去更新 kubernetes 內的資源狀態。 一種相對應的解法就是，有沒有辦法讓 Kubernetes 自己去更新相關的資源狀態? 相關的概念可以是 CI 建置並且更新最新的 Container Image 後想辦法通知 Kubernetes 內部CI 建置並且更新最新的 Container Image 後, kubernetes 本身自己去偵測是否需要更新相關的資源 而 keel 就是上述概念的解決方案，透過相關的設定，自動偵測是否有新的 Container Image 並且更新相關的 kubernetes 資源 Keel 的官方網站 上面有更多的介紹，這邊我就直接使用其架構圖來介紹 Keel 以下的架構圖是基於 Keel V1 的版本。 在整個 Keel 的架構中，我們可以分成三大部分來看待 Keel ","version":"Next","tagName":"h2"},{"title":"Registry​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#registry","content":"在 Keel 裡面會需要針對 Container Image 去偵測，判別是否有新的版本，因此必須要與 Container Registry 有所連動，本身要有相關的權限可以去讀取相關的 Container Image Information 來判斷是否有新版. 因此該架構圖左半部分描述的就是相關的 Container Registry. 從公有雲服務到自架的服務都有支援，譬如 Dockerhub, Quay, GCR. 此外 程式碼比文件新，實際上連 AWS ECR 也有支援. 詳細的支援列表可以參閱Keel Trigger ","version":"Next","tagName":"h2"},{"title":"Detection Approach​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#detection-approach","content":"Keel 要怎麼去偵測是否有新的 Container Image. 目前 Keel 裡面實現了兩種概念，分別為被動接收 Webhook 以及主動定時詢問(Polling)) Webhook 的概念就是到各個 Container Registry 去設定的 webhook，並且將他指向 keel. 當該 Contaienr Registry 收到對應的 Push Image Event 時會主動通知 Keel 有新版的 Image 被更新了. Polling 則相對簡單，Keel 對定期的去檢視相關的 Container Registry/Container Image 是否有產生新版。 相對於 Trigger 來說, Polling 的反應時間會比較長. 此外，這兩個比較像是 Webhook 是一定會採用，而 Polling 是可以決定要不要使用，因此可以同時使用 webhook + polling 來監控，或是只有單純 webhook. ","version":"Next","tagName":"h3"},{"title":"Rules​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#rules","content":"由於是根據 Container Image 的更新來判別是否有新版, 因此在比對的規則上就特別的重要。 到底什麼叫做新版? 什麼叫做舊版? 如果每次的 image tag 都只是一些看似亂碼的 git commit hash tag 的話，其實 keel 根本搞不清楚誰是新版本，誰是舊版本. 因此 Keel 嚴格遵守 Semantic Versioning 2.0.0, 基於 ${Major}.${Minor}.${Patch}-${Labels} 的規範來比對 詳細的比對規則可以參閱 Semantic Versioning 2.0.0 來學習更多。 此外，有部分的需求是希望使用 latest 這種不會改變名稱的 tag 來進行更新。因此 keel 也有針對這類型的需求提供了設定方式。 針對特定不變的 tag 名稱, keel 會去讀去該 image digest 去判別其產生的時間，根據創建時間來判別版本的大小。 ","version":"Next","tagName":"h3"},{"title":"Configuration Monitor​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#configuration-monitor","content":"前述設定好相關的 Container Registry 之後, Keel 要怎麼知道去監控 Kubernetes 裡面的何種資源? 這部分總共提供兩種方式, 分別是 Native Yaml 以及 Helm Chart 基本上兩者的使用概念完全一樣，只是設定的方式有所不同 ","version":"Next","tagName":"h2"},{"title":"Native Yaml​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#native-yaml","content":"在 Native Yaml 中，由於 Yaml 會被 Kubernetes API Server 先行處理，因此在格式上不能有太多的變化，因此所有的設定都是基於 Annotation 以及 Labels 的方式來設定 譬如下列範例是透過 keel.sh/policy: major 去設定 karolisr/webhook-demo:0.0.8 的版本更新。 如果有收到任何 webhook(預設是接收 webhook 更新) 並且告知 karolish/webhook-demo 的 image version 有 Major 的版本更新，就幫我更新該運行的 StatefulSet apiVersion: apps/v1 kind: StatefulSet metadata: name: wd namespace: default labels: name: &quot;wd&quot; keel.sh/policy: major spec: ... spec: containers: - image: karolisr/webhook-demo:0.0.8 imagePullPolicy: Always ...  下列範例則是採用 Polling 的方式，並且定期每10分鐘去檢查一下目標的 Container Image 是否有更新. 此外 policy:force 代表的意思就是不考慮任何版本(SemVer), 而是針對 Image Digest 內的創建時間來更新新版即可，因此使用 Latest 的時候就會使用此作法。 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: wd namespace: default labels: name: &quot;wd&quot; keel.sh/policy: force keel.sh/trigger: poll annotations: keel.sh/pollSchedule: &quot;@every 10m&quot; spec: ...  ","version":"Next","tagName":"h3"},{"title":"Helm Chart​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#helm-chart","content":"Helm Chart 的部分相對簡單,針對 values.yaml 去進行撰寫相關設定即可。 以下述範例來說，會透過 Polling 的方式每兩分鐘 就去嘗試看看是否有 karolisr/webhook-demo 的版本更新了. 而特別注意的是 policy:all 代表的完整的 SemVer 有任何新版的就會直接採用。 另外，版本的部分要透過 keel.images.tag 以及 keel.image.repository 的方式來設定，此範例則是參考到上面的 image.repository 以及 image.tag. replicaCount: 1 image: repository: karolisr/webhook-demo tag: &quot;0.0.8&quot; pullPolicy: IfNotPresent service: name: webhookdemo type: ClusterIP externalPort: 8090 internalPort: 8090 keel: # keel policy (all/major/minor/patch/force) policy: all # trigger type, defaults to events such as pubsub, webhooks trigger: poll # polling schedule pollSchedule: &quot;@every 2m&quot; # images to track and update images: - repository: image.repository tag: image.tag  ","version":"Next","tagName":"h3"},{"title":"Notification​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#notification","content":"假如一切都更新完成後，最後的部分就是通知了，如何將相關的部屬結果通知給管理者。 這方面 keel 整合了一些常見的溝通工具，譬如 slack, mattermost, hipchat. 詳細的整合方式請參閱 Keel Notificaion ","version":"Next","tagName":"h2"},{"title":"Misc​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#misc","content":"除了上述的部分外，還有一些功能可以使用，譬如 Approval 等投票機制，確認同意才會部署等相關的行為。 這些都可以在 Keel Documentation 找到。 Example 接下來會直接採用一個簡單的範例來測試 keel 的功能。 ","version":"Next","tagName":"h2"},{"title":"Install Keel​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#install-keel","content":"安裝的部分可以採用 Helm 或是直接部署相關的 Yaml 即可。 基本上會在 Kubernetes 內安裝相對應的 Deployment 來提供上述所描述的所有功能。 helm repo add keel-charts https://charts.keel.sh helm repo update helm upgrade --install keel --namespace=kube-system keel-charts/keel  詳細的安裝流程可參閱 Keel Installation 安裝完畢後執行下列指令確認安裝完成 hwchiu~$ kubectl --namespace=kube-system get pods -l &quot;app=keel&quot; NAME READY STATUS RESTARTS AGE keel-8b8447549-wsfqr 1/1 Running 0 5s  ","version":"Next","tagName":"h2"},{"title":"Deploy Application​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#deploy-application","content":"這邊我是採用自己的 Container Image 來測試部署，並且打算採用Dockhub 作為後端的 Container Registry。 同時採用 Polling 的方式來偵測 Container Image 是否有更新 apiVersion: apps/v1 kind: Deployment metadata: name: keel-demo namespace: default labels: name: &quot;keel-demo&quot; keel.sh/policy: all keel.sh/trigger: poll annotations: keel.sh/pollSchedule: &quot;@every 1m&quot; spec: selector: matchLabels: name: keel-demo template: metadata: labels: name: keel-demo spec: containers: - name: keel-demo image: hwchiu/netutils:0.1.0 imagePullPolicy: Always name: keel-demo  執行下列指令確認當前運行的 Image. hwchiu:~$ kubectl get pods -l&quot;name=keel-demo&quot; -o jsonpath=&quot;{.items[0].spec.containers[0].image}&quot; hwchiu/netutils:0.1.0 hwchiu:~$  ","version":"Next","tagName":"h2"},{"title":"Update Image​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#update-image","content":"接下來我們透過 docker tag/docker push 的方式來更新相關的docker image. hwchiu:~$ sudo docker tag hwchiu/netutils:0.1.0 hwchiu/netutils:0.1.1 hwchiu:~$ sudo docker push hwchiu/netutils:0.1.1 The push refers to a repository [docker.io/hwchiu/netutils] ab42d9bbb598: Layer already exists 98d902303c2d: Layer already exists bcff331e13e3: Layer already exists 2166dba7c95b: Layer already exists 5e95929b2798: Layer already exists c2af38e6b250: Layer already exists 0a42ee6ceccb: Layer already exists 0.1.1: digest: sha256:f1a3643b8b10c98b4aa9e4ac8269b7587c1d9f415f134ff359f920b8539a6f76 size: 1776  ","version":"Next","tagName":"h2"},{"title":"CheckResources​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2019/keel#checkresources","content":"接下來我們透過 watch 指令來定時觀看 watch kubectl get pods -l&quot;name=keel-demo&quot; -o jsonpath=&quot;{.items[0].spec.containers[0].image}&quot;  會得到下列的結果 Every 2.0s: kubectl get pods -lname=keel-demo -o jsonpath={.items[0].spec.containers[0].image} Sat Jan 12 07:55:42 2019 hwchiu/netutils:0.1.1  可以觀察到版本真的改變了，接下來嘗試修正 Major 版本看看 hwchiu:~$ sudo docker tag hwchiu/netutils:0.1.1 hwchiu/netutils:1.1.1 hwchiu:~$ sudo docker push hwchiu/netutils:1.1.1 The push refers to a repository [docker.io/hwchiu/netutils] ab42d9bbb598: Layer already exists 98d902303c2d: Layer already exists bcff331e13e3: Layer already exists 2166dba7c95b: Layer already exists 5e95929b2798: Layer already exists c2af38e6b250: Layer already exists 0a42ee6ceccb: Layer already exists sleep 601.1.1: digest: sha256:f1a3643b8b10c98b4aa9e4ac8269b7587c1d9f415f134ff359f920b8539a6f76 size: 1776 hwchiu:~$ sleep 60;^C hwchiu:~$ kubectl get pods -l&quot;name=keel-demo&quot; -o jsonpath=&quot;{.items[0].spec.containers[0].image}&quot; hwchiu/netutils:0.1.1 hwchiu:~$ sleep 60 hwchiu:~$ kubectl get pods -l&quot;name=keel-demo&quot; -o jsonpath=&quot;{.items[0].spec.containers[0].image}&quot; hwchiu/netutils:1.1.1  Summary 這次跟大家介紹一款針對 Kubernetes 量身打造的 CD 工具，不過適不適合各位的環境並不是一個絕對的答案，就讓各位自己去評估是否有這個需求。 畢竟通常越方便使用，有時候其彈性反而愈少，當未來有任何客製化需求的時候可能反而會綁手綁腳。 Reference KeelSemantic Versioning 2.0.0 ","version":"Next","tagName":"h2"},{"title":"GitOps 帶來的痛點與反思","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops-bad-and-ugly","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"不適合使用於程式化的更新​","type":1,"pageTitle":"GitOps 帶來的痛點與反思","url":"/docs/techPost/2020/gitops-bad-and-ugly#不適合使用於程式化的更新","content":"如果今天 CI 流水線本身結束後，會需要將測試的結果送入到測試的叢集中去更新，那就意味者我們要透過修改 Git Repo 的內容來觸發 GitOps 的更新，因此需要在 CI 流水線的最後面對 Git Repo 進行修改。 然而 Git 這個工具的設計本身就不是很適合太自自動化的操作，特別是遇到衝突時，需要有人為的解讀來修復。所以當有多個 CI 流水線同時運作時，就會有機會導致有部分的 CI 流水線會遇到 Git 衝突導致沒有辦法順利完成作業。 這部分的衝突通常只是 Git 的紀錄更新，必須要一次又一次的 Pull 來抓到最新版才可以Push. 現實中很難真的發生會有單一檔案的衝突 為了解決這個問題，作者於他們的工作環境中又設計了其他的系統來不停重試這些環節，這過程花費大量的時間。 ","version":"Next","tagName":"h2"},{"title":"Git Repo 增長帶來的問題​","type":1,"pageTitle":"GitOps 帶來的痛點與反思","url":"/docs/techPost/2020/gitops-bad-and-ugly#git-repo-增長帶來的問題","content":"GitOps 強調一切都放在 Git Repo 上，因此隨者系統專案的增加， Git Repo 也會愈開愈多。而每一個 Repo 都需要進行相關設定，譬如權限，整合等 作者的親身經歷是他們有 30% 以上的時間來設定相關的 Git Repo。 作者認為減少 Git Repo 的數量可以減少這種痛苦，譬如每一個環境就用一個 Git Repo, 裡面放置該環境會用到的全部應用程式，但是這種架構下帶來的反而是一個 Repo 本身的權責太大，擁有的東西過多，然後搭配(1)的問題，就會產生更多衝突 ","version":"Next","tagName":"h2"},{"title":"缺乏視覺化​","type":1,"pageTitle":"GitOps 帶來的痛點與反思","url":"/docs/techPost/2020/gitops-bad-and-ugly#缺乏視覺化","content":"GitOps 強調一切都是 Git，所有的差異變化都可以透過 Git History 來追蹤，這也代表這些文字內容可以幫忙管理人員回答諸多部署的相關問題 然而作者的經驗是當環境愈來愈複雜時，有更多問題是沒有辦法依賴 GitOps 的特性來回答的，譬如說「某個應用程式被部署的頻率」。 作者認為 Git 內容的改動不太容易對應到特定應用程式的更動，舉例來說某些改動可能一次會觸發多個應用程式重新部署，而有些改動卻可能只有部分設定檔案改動，而沒有任何應用程式重新部署。 ","version":"Next","tagName":"h2"},{"title":"Secret 的管理問題依然沒有解決​","type":1,"pageTitle":"GitOps 帶來的痛點與反思","url":"/docs/techPost/2020/gitops-bad-and-ugly#secret-的管理問題依然沒有解決","content":"滿多的複雜應用環境都會遇到 Secret 的管理問題，特別是當 CI/CD 的流水線放置於系統外部服務時，這些機密資訊到底該如何保存? 舉例來說，凡舉資料庫的密碼或是私有鑰匙等需高度保存的東西都需要有一個很好的解決方案來保護。作者認為透過 Hashicorp 的 Vault 來進行集中化管理是一個比較合理的解法。 雖然 GitOps 本身沒有強調自己對於安全性管理有更好的效益與優勢，但是就實務經驗上來看，並沒有讓事情變得簡單。 畢竟 Git Repo 本身不是一個適合存放各種機密資訊的地方，就算透過加解密的方式只存放加密後的結果，這些過往的資訊還是會被 Git History 給永久保存。 此外當 Git Repo 的數量擴大變多時，這些加密後的機密資訊也就會散落愈多地方，會導致管理與追蹤這些機密資訊要花費更大的功夫來處理 ","version":"Next","tagName":"h2"},{"title":"缺少檔案資源的驗證性​","type":1,"pageTitle":"GitOps 帶來的痛點與反思","url":"/docs/techPost/2020/gitops-bad-and-ugly#缺少檔案資源的驗證性","content":"作者認為 GitOps 的架構下, Git Repo 作為一個 Kubernetes 與 CI/CD 流水線中間的介面，沒有一個很好的方式去驗證所有修改過的資源是否有效與合法。 此架構下，則是開發者要確保自己的 Helm/Yaml 是否有格式問題，不然相關的修改被合併到 Git Repo 的話就會造成部署錯誤 我認為這個問題倒是不大，因為不管透過哪種部署思維都會有這樣的問題，反而是 CI/CD 的過程中能不能驗證 Yaml 的正確與否，如果可以提早驗證，並且阻止沒通過測試的程式碼被合併，那這些問題應該都不會發生 不同於 GitOps 的解決方案 上述的問題只是作者遇到的實戰問題，並非代表 GitOps 就毫無用處。 GitOps 依然帶來了很多好的優點，作者強調大家只需要心中知道你可能會遇到這些問題，然後遇到的時候可以想想要怎麼解決這些問題。 作者開始思考，什麼樣的解決方案可以保留 GitOps 的優點同事又可以增進上述的缺點，其心目中有一些基本要有的特性 有能力記錄叢集環境上的一切變化使用宣告式(Declarative)的文件格式來描述或是設定環境上要用到的所有資源所有的環境變化都可支援審核機制，要通過審核才會往下運作權限控管，控制誰有能力去對環境資源進行更改有辦法針對 期望的狀態與運行的狀態進行比對 作者也認為一個好的架構中會有一個代理人運行在所有的 Kubernetes 之前，然後所有的操作請求都不應該直接面對 Kubernetes，而是跟該代理人溝通 如下圖 該代理人本身提供 API 介面來接受各式各樣的請求，最後把這些請求更新到後方管理的 Kubernetes 叢集。 原文中還有更多關於這個架構的一些想法，有興趣的可以閱讀原文 作者也提到這部分會花很多的時間在設計這套解決方案，重新實作這個專案花的成本可能比專心用 GitOps 還來得高 剛好現在有一個專案 Spinnaker 就是在實現上述的架構，其下一代開發 Humanitec 更是完全針對 Kubernetes 使用。 這些專案有上述架構提到的一些特性，有些還在開發藍圖，作者期望有一天他們能夠成為 GitOps 以外的選擇 個人心得: 作者提到的痛點我認為其實並不是每個使用 GitOps 的人都會遇到的，畢竟 GitOps 我認為是一個概念，底層要怎麼實作並沒有強烈限制。使用的開源軟體不同，最後的工作流程也會完全不同，因此不一定完全是 GitOps 的問題，更有可能是當前架構下的問題。 此外 Secret 的管理問題也並非是 GitOps 才會有的，而是任何在玩 CI/CD 的人都會遇到的問題，只是最後你要怎麼解決而已。 我也同意沒有最好的部署策略與解決方案，還是要根據自己的使用環境跟內部架構來挑選適合的工具 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2020/docker-network-model-lab","content":"","keywords":"docker network","version":"Next"},{"title":"創建兩個 None 容器​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#創建兩個-none-容器","content":"首先，我們要先創造兩個乾淨的容器，最後目標是打造成 Bridge 的網路模型並確保這兩個容器能夠互相存取。 這邊會先使用 --network=none 的形式要求 Docker 不要對該網路模型動手腳，讓我們自己處理即可。 這邊我們創造容器的時候，我們會特別給予一個參數 --privileged 來要求特別權限，原因後面會解釋，這邊先直接使用 ","version":"Next","tagName":"h2"},{"title":"示範程式碼​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#示範程式碼","content":"透過下列的程式碼，我們會於系統中創造兩個容器，分別名為 c1 及 c2. 之後透過 docker 指令進去看，確認兩個容器內除了 lo 之外沒有其他任何網卡。 $ docker run --privileged -d --network=none --name c1 hwchiu/netutils $ docker run --privileged -d --network=none --name c2 hwchiu/netutils $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 868f54ee0b32 hwchiu/netutils &quot;/bin/bash ./entrypo…&quot; 11 minutes ago Up 11 minutes c2 5df4ed8e756a hwchiu/netutils &quot;/bin/bash ./entrypo…&quot; 11 minutes ago Up 11 minutes c1 $ docker exec c1 ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  ","version":"Next","tagName":"h3"},{"title":"模型分析​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#模型分析","content":"這邊用兩張不同視角來看待這個行為，由於本篇文章都會採用相同的概念去解讀不同網路模型，因此這邊詳細介紹兩種視角的概念。 圖左：這邊想要介紹的是以系統底層的角度去觀察網路，中間的灰色線將其分為上半部分的 UserSpace, 以及下半部分的 Kernel Space。 本範例中下圖會有不同顏色變化，代表不同的網路空間，每個網路空間彼此網路隔離。 圖右：這邊提供一個比較簡略的介紹，主要會從使用者的角度去觀察，由圖例來說明網路元件的關係上有什麼變化。  當我們透過 None 創建兩個容器時，同時也會於系統內創建兩個全新的 netns(Network Namespace)，如下圖黃色及淺綠色所述，而淺藍色則是原生宿主機所使用的。預設情況下，這些 netns 裡面都只會有 lo 這張預設網卡假設 eth0 是宿主機本身擁有的網卡  ","version":"Next","tagName":"h3"},{"title":"創建 Linux Bridge​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#創建-linux-bridge","content":"接下來我們先於系統中創建一個 Linux Bridge，這也是 Docker 預設網路模型的作法。 這部份會需要使用 brctl 這個工具， Ubuntu 系統可以透過安裝 brctl-utils 來取得。 ","version":"Next","tagName":"h2"},{"title":"範例程式碼​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#範例程式碼","content":"這邊我們會使用兩個 brctl 的指令來處理，分別是 brctl add-br $namebrctl show 第一個指令會於系統中創建一個名為 $name 的 Linux Bridge 第二個指令則是會顯示目前系統中有多少 Linux Bridge，以及其相關資訊 範例中我們會創立一個名為 hwchiu0 的 Linux Bridge，最後透過 ifconfig 這個指令讓 Bridge 給叫起來，讓他處於一個可運行的狀態。 $ sudo brctl addbr hwchiu0 $ sudo brctl show bridge name bridge id STP enabled interfaces hwchiu0 8000.000000000000 no $ sudo ifconfig hwchiu0 up  ","version":"Next","tagName":"h3"},{"title":"網路模型​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#網路模型","content":"執行完這個階段後，我們系統會於宿主機的 netns (Network Namespace) 中創造一個全新的 Linux Bridge (hwchiu0)。 這時候的系統架構圖如下，基本上跟上述沒有太多變化，單純就是多了一個元件。  ","version":"Next","tagName":"h3"},{"title":"創建 Veth Pair​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#創建-veth-pair","content":"目前我們擁有 兩個空蕩蕩的容器，包含其本身的 netns一個 Linux Bridge 因此接下來我們要做的就是想辦法將其串連起來，這邊我們會使用一個名為 veth 的特殊網路設備，透過該設備我們可以於系統中創造一條特殊的連結。 該連結會有兩個端口，分別都會有對應的網卡名稱，從一端進去的封包都會馬上從另外一端出來，可以想像成一個雙向水管的概念。 因此這個步驟我們要先於宿主機上面創造兩條雙向水管，也就是兩條 veth pair. ","version":"Next","tagName":"h2"},{"title":"程式碼示範​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#程式碼示範","content":"這個範例中，我們需要透過 ip 這個指令來創建該 veth 水管，指令變化很多，這邊示範一種用法 ip link add dev ${name} type veth 上述指令會要求 kernel 幫忙創造一條基於 veth 型態的連結，並且其中一個端頭命名為 ${name}，至於另一端則讓 kernel 幫忙處理 當該指令完畢後，系統中就會多出兩張虛擬網卡，分別是 ${name} 以及 kernel 幫忙創造的，通常是 veth.... $ sudo ip link add dev c1-eth0 type veth $ sudo ip link add dev c2-eth0 type veth $ sudo ip link | grep veth 23: veth0@c1-eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 24: c1-eth0@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 25: veth1@c2-eth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 26: c2-eth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000  透過上述的指令，我們創建了兩條基於 veth 的網卡，因此系統中產生了四個虛擬網卡，其配對分別為 veth1 --&gt; c2-eth0veth0 --&gt; c1-eth0 ","version":"Next","tagName":"h3"},{"title":"網路模型​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#網路模型-1","content":"網卡建立完畢後，這時候的網路模型如下 左圖: 當前都是於宿主機內操作，所以四張虛擬網卡都坐落於宿主機的 netns 裡面，圖中用兩種不同顏色的連線來代表這些網卡的關係 右圖: 系統創建完畢後，宿主機多出四張虛擬網卡，這四張虛擬網卡跟容器無關，跟 Linux 無關，可以想成四個孤兒  ","version":"Next","tagName":"h3"},{"title":"移動 Veth 到容器​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#移動-veth-到容器","content":"當我們創建好相關的 veth 水管後，下一個步驟就是要將 veth 的一個端頭給放到容器之中，這樣我們就可以利用 veth 的特性於不同 netns 之中傳遞封包。 上述創造的配對如下 veth1 --&gt; c2-eth0veth0 --&gt; c1-eth0 因此我們的目標就是 將 c1-eth0 這個虛擬網卡放到 c1 容器將 c2-eth0 這個虛擬網卡放到 c2 容器 然而，準確的說，我們其實不是要放到容器內，而是要放到容器所屬的 netns (network namespace) 內，所以我們要先有辦法接觸到這些容器所使用的 netns. 這邊我們會使用 ip netns 指令來進行操作，這個指令預設會去讀取 /var/run/netns 底下的資料來顯示相關的 netns。 然而 docker 其本身設計預設則是會避開 /var/run/netns 可能是怕有人直接用系統指令 ip netns 來操作導致容器崩壞。 因此透過 docker 所創造的容器其所屬的 netns 都會放到 /var/run/docker/netns。 這邊我們只需要透過一個 soft link 的方式將這兩個位置串接起來，我們就可以使用 ip netns 來觀察 c1,c2 兩個容器的 netns 了 ","version":"Next","tagName":"h2"},{"title":"程式碼範例​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#程式碼範例","content":"根據上述概念，我們先執行 ln 指令來創造 soft link，然後透過 ip netns show 來展示出系統當前看到的 netns 名稱 $ sudo ln -s /var/run/docker/netns /var/run/netns $ sudo ip netns show 792fedcf97d8 1bb2e0141544  這邊的兩個名稱其實是 docker 容器裡面的 NetworkSettings.SnadboxID我們可以透過下列指令來觀察 $ docker inspect c1 | jq '.[0].NetworkSettings.SandboxID' &quot;1bb2e0141544758fe79387ebf4b7297556fb65efacc7d9ed7e068099744babee&quot; $ docker inspect c2 | jq '.[0].NetworkSettings.SandboxID' &quot;792fedcf97d8ae10ec0a29f5aa41813ad00825ff8127fd4d9c25b66a5714d7ca&quot;  因此當前範例中， 792fedcf97d8 代表是的 c2 容器的 netns，而 1bb2e0141544 則是 c1 容器的 netns。 接下來終於要進入正題，把我們事先創建好的虛擬網卡(veth的一端)給放到對應的 netns 中，這邊要借助 ip link set 這指令，該指令可以把虛擬網卡給放到不同的 netns 內，同時也可以重新命名 $ sudo ip link set c1-eth0 netns 1bb2e0141544 name eth0 $ sudo ip link set c2-eth0 netns 792fedcf97d8 name eth0  執行完畢上述指令後，我們接下來可以透過 docker 指令再次觀察是否有任何變化 $ sudo docker exec c2 ifconfig -a eth0 Link encap:Ethernet HWaddr ea:51:1c:2c:a4:15 BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) $ sudo docker exec c1 ifconfig -a eth0 Link encap:Ethernet HWaddr be:a7:29:1b:e0:13 BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) $ ip link | grep veth 23: veth0@if24: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 25: veth1@if26: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000  最後我們可以再次透過 ip link 指令觀察，可以發現宿主機上面的四張虛擬網卡只剩下兩張了，因為有兩張都被搬移到容器內。 ","version":"Next","tagName":"h3"},{"title":"網路模型​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#網路模型-2","content":"當相關的虛擬網卡都被移動到對應的容器後，這時候的模型有一點點小小變化 左圖: veth 的一端都被移動到所屬容器的 netns 內，並且重新命名為 eth0。 右圖: 這邊的改變就是 veth 的端口被搬移到容器中，並且重新命名為 eth0。 ","version":"Next","tagName":"h3"},{"title":"veth 綁定 Bridge​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#veth-綁定-bridge","content":"接下來，我們要將 veth 跟 Linux Bridge 給整合一起，希望藉由 Linux Bridge 的功能來幫我們轉發封包 所以這邊的概念很簡單，就是把宿主機上面關於 veth 的網卡通通都綁到我們事先創造好的 Linux Bridge hwchiu0 身上 ","version":"Next","tagName":"h2"},{"title":"程式碼範例​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#程式碼範例-1","content":"這邊我們會使用 brctl addif 這個指令來達成目標，該指令用法如下 brctl addif ${bridge_name} ${nic_name} 該指令會將名為 ${nic_name} 的網卡給加入到 ${bridge_name} 的 Linux Bridge 上。 當加入完畢後，我們也順便透過 ifconfig (其實 ip link 也可以) 將 veth 虛擬網卡給叫起來讓其運作 $ sudo brctl addif hwchiu0 veth0 $ sudo brctl addif hwchiu0 veth1 $ sudo ifconfig veth0 up $ sudo ifconfig veth1 up $ sudo brctl show bridge name bridge id STP enabled interfaces hwchiu0 8000.266248dc8ca1 no veth0 veth1  最後我們可以透過 brctl show 來顯示當前系統上面關於 Linux Bridge 的資訊，可以觀察到 veth0 以及 veth1 都已經綁定上去。 ","version":"Next","tagName":"h3"},{"title":"網路模型​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#網路模型-3","content":"這個步驟主要針對的是 veth 於宿主機內的搬移，將其綁到對應的 Linux Bridge 上。 左圖/右圖: 差異就是 veth 的兩個端頭現在都不流浪，而是歸屬於 Linux Bridge 底下  ","version":"Next","tagName":"h3"},{"title":"設定 Container IP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#設定-container-ip","content":"現在我們幾乎已經將整個路線給串通了！剩下最後一個簡單步驟就是設定 IP 我們兩個容器內的 eth0 網卡都還沒有 IP，因此這個步驟我們就來幫忙設定 IP， IP 本身議題也很多，這邊我們為了避免更多問題，因此這兩個容器的 IP 我們設定同網段，就如同我們使用 Docker 容器般，大家都是同網段 目標: c1 容器使用 10.55.66.2 的IP, 網段為 10.55.66.0/24c2 容器使用 10.55.66.3 的IP, 網段為 10.55.66.0/24 ","version":"Next","tagName":"h2"},{"title":"程式碼範例​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#程式碼範例-2","content":"還記得一開始創造容器時有特別要求 --privileged 這個參數嗎? 這是因為我們接下來要於容器內透過一些網路工具去修改網卡資訊，這部份我們會需要相關的權限，否則執行的時候會得到下列錯誤 SIOCSIFADDR: Operation not permitted SIOCSIFFLAGS: Operation not permitted SIOCSIFNETMASK: Operation not permitted SIOCSIFFLAGS: Operation not permitted  $ sudo docker exec c1 ifconfig eth0 10.55.66.2 netmask 255.255.255.0 up $ sudo docker exec c2 ifconfig eth0 10.55.66.3 netmask 255.255.255.0 up $ sudo docker exec c1 ifconfig eth0 Link encap:Ethernet HWaddr be:a7:29:1b:e0:13 inet addr:10.55.66.2 Bcast:10.55.66.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:11 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:906 (906.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) $ sudo docker exec c2 ifconfig eth0 Link encap:Ethernet HWaddr ea:51:1c:2c:a4:15 inet addr:10.55.66.3 Bcast:10.55.66.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:10 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:796 (796.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  最後透過 ifconfig 等指令再次觀察，可以看到相關的網卡都起來了，同時 IP 等資訊也都有了，可以準備邁入最後階段 ","version":"Next","tagName":"h3"},{"title":"Ping 測試​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#ping-測試","content":"這個階段，我們要來透過 PING 的指令來打測試我們的連線，整個封包流程會是 c1 容器內的 ping -&gt; c1 容器內的網卡 eth0 -&gt; 宿主機內的 veth0 -&gt; 宿主機內的 Linux Bridge -&gt; 宿主機內 Bridge 上的 veth1 -&gt; c2 容器內的網卡 eth0 這部份我們直接進入到容器內執行 ping 10.55.66.3 指令即可 ","version":"Next","tagName":"h2"},{"title":"程式碼模擬​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#程式碼模擬","content":"$ docker exec -it c1 ping 10.55.66.3 -c5  這時候你會發現網路不通， ping 都沒有反應，這部份我們執行下面的神祕指令 $ sudo iptables --policy FORWARD ACCEPT $ docker exec -it c1 ping 10.55.66.3 -c5  你就會發現網路通了，一切都正常了，至於到底上面那個神祕指令做了什麼手腳，為什麼會影響封包，這部份等我們下一章節再來慢慢細談 iptables 的概念 ","version":"Next","tagName":"h3"},{"title":"網路模型​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab#網路模型-4","content":"最後這邊就用來顯示一下封包的流程，基本上就是概括了本章節提到的所有元件，包含了 veth, Linux Bridge, 容器 IP 等資訊。  Summary 到這邊為止，我們已經瞭解如何將一個什麼都沒有的容器給打造為使用 Bridge 模型的容器，然而現在這種情況下，我們的兩個容器只能彼此互相存取，還沒有辦法對外上網，譬如 ping 8.8.8.8 。 所以下篇文章我們會來探討這後半部分，來把整個 Docker Bridge 網路模型給摸熟，透過這些步驟來瞭解到底每個容器起來時實際上系統做了什麼手腳 題外話： 上述的流程中有一個部分其實不好處理，稱為 IPAM，也就是 IP 分配管理的相關議題，我們要如何對每個容器去分配一個不重複的 IP 地址，同時當容器掛掉時，也要能夠回收這些 IP。 另外本篇文章的詳細內容其實跟 Bridge CNI 的步驟大概有 80% 一樣, 差異只剩下 IP 分配問題，以及後續會探討到的 IP 路由表管理。 ","version":"Next","tagName":"h3"},{"title":"[書本導讀]-GitOps 到底解決了什麼問題","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops-book-ch1","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"Deplyoment By Hand​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#deplyoment-by-hand","content":"軟體開發團隊現在都非常習慣使用版本控制等工具來開發與測試軟體，同時也會建置相關的部署產物。對於開發者來說，手動部署這些應用程式到目標環境中並不是一個不常見的選項。 雖然大部分的情況下，都可以透過撰寫一些腳本來盡可能的自動化這些部署行為，但是開發團隊跟維運團隊兩者大多情況下還是會有交集，需要合作來處理問題，特別是一些特定的環境，譬如整合環境，正式生產環境等。而這些合作可能還是會有一些手動操作介入。 ","version":"Next","tagName":"h2"},{"title":"State In A Spreadsheet​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#state-in-a-spreadsheet","content":"維運團隊總是會希望能夠透過一些權限控管即可分享的系統來追蹤系統上的各種狀態，一個經典的範例就是，維運團隊透過 Excel 試算表來管理每個網路介面目前到底開啟了哪些連接埠。今天如果有人想要對連接埠的使用進行修改，必須要對該 Excel 試算表發出一個修改請求，一旦該請求通過後，就會有專屬的人員將這份修改給套用到正式機器上。 ","version":"Next","tagName":"h2"},{"title":"Deployment Pipeline Created By GUI​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#deployment-pipeline-created-by-gui","content":"隨者 DevOps 的概念近年成為軟體開發的標準配備，愈來愈多的團隊會透過類似 Jenkins 這種工具來打造建置與部署的流水線。 然而打造這些流水線的做法很多時候都是透過 GUI 直覺的操作，之後的更新與維護也都是基於 GUI 來完成 上述這些流程其實會導致一些維運上的問題，而這些問題就是 GitOps 想要解決的，因此接下來我們來看一下這些問題 ","version":"Next","tagName":"h2"},{"title":"Uncertain State​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#uncertain-state","content":"上面這些不良的操作流程其實都會導致我們難以瞭解當前系統的狀態，舉例來說。今天有一個工程師想要對測試環境的 database 的 table schema 進行一些手動微調來處理 bug，那未來其他工程師要怎麼知道這個操作，以及其他環境要如何一起套用這個修正。 或是有一個團隊想要把其他人的流水線設計也設計一份到自己環境中，如果都依賴 GUI 來管理的話，那該怎麼達成？ ","version":"Next","tagName":"h2"},{"title":"Gap Between Desired And Actual State​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#gap-between-desired-and-actual-state","content":"相同的，如果今天有任何人員的主動修改但是卻沒有任何文件紀錄，那就會產生一個系統狀態認知的差距，維運團隊期望的狀態以及正式系統運行的狀態這之間的差異會因為愈來愈多的手動介入而愈來愈大。 如果你很幸運的剛好有一些文件幫你記住當前運行的狀態，譬如上述的 Excel 試算表，那你可以幸運的解決這些問題。但是更多的情況下，很多系統上的狀態，譬如軟體版本，軟體用的設定檔案可能都沒有很好的被記錄跟追蹤。 當系統一旦發生這些情況時，往往都依賴一些經驗老道，系統熟悉的工程師幫忙整理跟善後，盡可能的解決這些問題，讓系統變成可接受的狀態。 ","version":"Next","tagName":"h2"},{"title":"Control Challenges​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#control-challenges","content":"上述這些流程還會造成一個很嚴重的問題就是，對於系統上的任何修改，非常困難的去回答 為什麼會有這些修改系統發生什麼事情誰提出這個請求 上述這個需求對於很多公司來說都是必備的，但是一旦今天沒有辦法滿足這個條件，那工程團隊就要花更多的時間去解決這些未知的答案。 大型企業通常都會使用一些如 ServiceNow 或是 Remedy 之類的工具來管理整個工作流程，透過這些工具去進行一些稽核的動作來追蹤是誰執行了這些變化。 但是這些工具跟真實運行的系統其實還是有脫鉤，很多系統上的變化都沒有辦法即時的被該系統追蹤，所以就現實面來說，還是會有一些問題沒有辦法回答上述的三個問題。 ","version":"Next","tagName":"h2"},{"title":"How GitOps Helps​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#how-gitops-helps","content":"談了這些問題後，我們來看一下 GitOps 期望怎麼解決 ","version":"Next","tagName":"h2"},{"title":"Git to Provide​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#git-to-provide","content":"GitOps 希望透過 Git 這套版本系統來提供下列特性 提供一個 single source of truth 的特令來維護系統的期望狀態有提供歷史紀錄可以追蹤每次的修改，包含修改內容，誰造成修改透過 git 原本的整合系統，可以提供管理機制來確保能不能修改 ","version":"Next","tagName":"h3"},{"title":"Declarative data formats (such as YAML or HCL) to:​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#declarative-data-formats-such-as-yaml-or-hcl-to","content":"所有的檔案格式都希望基於宣告式內容，用這些格式來描述系統的期望狀態透過檔案的方式來描述狀態而不是透過程式化來產生 ","version":"Next","tagName":"h3"},{"title":"Control loop tools to​","type":1,"pageTitle":"[書本導讀]-GitOps 到底解決了什麼問題","url":"/docs/techPost/2020/gitops-book-ch1#control-loop-tools-to","content":"透過一個循環的控制邏輯將系統與 Git 連動 確保系統上的運行狀態都能夠與 Git 內描述的期望狀態一致如果系統上有任何期望的修改，這些修改可以被寫回到 Git 中去保存 上述這些提到的問題，以及解決問題的方法其實早就存在，並不是一個全新的想法。 然而 GitOps 做的事情就是將這些方法集中起來，提供一個系統化的解決方法讓工程團隊能夠有一個共識來解決維運上的各種挑戰 到這邊為止，我們已經瞭解了關於 GitOps 的基本概念以及其期望解決的問題。 下一篇我們會開始探討 GitOps 理論上的定義以及實作上的細節。 ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2020/docker-network-model-lab-dnat","content":"","keywords":"docker network","version":"Next"},{"title":"準備 Web Server​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab-dnat#準備-web-server","content":"為了讓整個 Demo 順利與方便，我們會於準備好的容器中透過 python 創建一個 web server。 $ docker exec -it c1 apt-get install -y python3 $ docker exec c1 python3 -m http.server&amp; $ curl 10.55.66.2:8000 &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=ascii&quot;&gt; &lt;title&gt;Directory listing for /&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Directory listing for /&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;.dockerenv&quot;&gt;.dockerenv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;bin/&quot;&gt;bin/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;boot/&quot;&gt;boot/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;dev/&quot;&gt;dev/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;entrypoint.bash&quot;&gt;entrypoint.bash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;etc/&quot;&gt;etc/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;home/&quot;&gt;home/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;lib/&quot;&gt;lib/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;lib64/&quot;&gt;lib64/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;media/&quot;&gt;media/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;mnt/&quot;&gt;mnt/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;opt/&quot;&gt;opt/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;proc/&quot;&gt;proc/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;root/&quot;&gt;root/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;run/&quot;&gt;run/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;sbin/&quot;&gt;sbin/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;srv/&quot;&gt;srv/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;sys/&quot;&gt;sys/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;tmp/&quot;&gt;tmp/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;usr/&quot;&gt;usr/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;var/&quot;&gt;var/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr&gt; &lt;/body&gt; &lt;/html&gt;  透過上述的指令我們已經創建了一個網頁伺服器，並且透過 curl 順利存取，確保該服務沒問題。 因此接下來，我們要做的就是如何透過 iptables 讓外界網路能夠存取該容器 ","version":"Next","tagName":"h2"},{"title":"DNAT​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab-dnat#dnat","content":"基本上網路概念全部都一樣，我們要探討的問題都是一樣的，就是封包該怎麼送，誰來負責轉發。 今天外界網路要存取我們內部的容器，想法就是 就是封包要先到達宿主機，接下來宿主機想辦法幫你轉送到內部容器。相對於 SNAT，這邊要採用的策略是 Destination Network Address Translation(DNAT)，將封包的目標地址進行修改。 接下來我們來進行一個範例，我們希望做到 -p 2345:8000 的範例，將送到本地的 2345 送到容器內的 8000 port 內。 $ sudo iptables -t nat -A PREROUTING -p tcp -m tcp --dport 2345 -j DNAT --to-destination 10.55.66.2:8000  上述的指令概念意思是，看到 TCP 的封包，且目標是 2345 port，則透過 DNAT 幫我把封包的目標 IP 改成 10.55.66.2, Port 改成 8000 接下來到其他環境，對宿主機使用 curl 來存取看看 port 2345 $ curl 172.17.9.103:2345 &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=ascii&quot;&gt; &lt;title&gt;Directory listing for /&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Directory listing for /&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;.dockerenv&quot;&gt;.dockerenv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;bin/&quot;&gt;bin/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;boot/&quot;&gt;boot/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;dev/&quot;&gt;dev/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;entrypoint.bash&quot;&gt;entrypoint.bash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;etc/&quot;&gt;etc/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;home/&quot;&gt;home/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;lib/&quot;&gt;lib/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;lib64/&quot;&gt;lib64/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;media/&quot;&gt;media/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;mnt/&quot;&gt;mnt/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;opt/&quot;&gt;opt/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;proc/&quot;&gt;proc/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;root/&quot;&gt;root/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;run/&quot;&gt;run/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;sbin/&quot;&gt;sbin/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;srv/&quot;&gt;srv/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;sys/&quot;&gt;sys/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;tmp/&quot;&gt;tmp/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;usr/&quot;&gt;usr/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;var/&quot;&gt;var/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr&gt; &lt;/body&gt; &lt;/html&gt;  到這邊為止，基本上我們幾乎實現 docker -p 的功能，能夠讓外界存取到內部容器了，其流程如下 ","version":"Next","tagName":"h2"},{"title":"其他問題​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab-dnat#其他問題","content":"看似完美解決問題，其實背後還是有一些東西要處理 ","version":"Next","tagName":"h2"},{"title":"本地存取​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab-dnat#本地存取","content":"完成上述規則後，我們嘗試於宿主機內針對設定的 2345 去進行存取 $ curl 172.17.9.103:2345 curl: (7) Failed to connect to 172.17.9.103 port 2345: Connection refused $ curl 127.0.0.1:2345 curl: (7) Failed to connect to 127.0.0.1 port 2345: Connection refused $ curl localhost:2345 curl: (7) Failed to connect to localhost port 2345: Connection refused  結果看起來我們設定的 iptables 規則對於這種本地直接存取自己是有問題的，但是如果使用的是 docker -p 去創造服務的話，會發現其實這些是通的，那到底 docker 是怎麼解決這些問題? ","version":"Next","tagName":"h3"},{"title":"重複使用​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab-dnat#重複使用","content":"第二個要解決的問題是，如果今天有程式想要於系統中去使用 port 2345，這時候問題就來了。 我們事先透過 iptables 去描述 DNAT，將 2345 轉為容器內80，這部分其實對於 socket programming 來說是不知情的，所以任何人都還是有機會去使用本地機器的 2345 port。 在這種情況下，會跑出一個疑問，請問送到 2345 的封包到底是要依據我們的規則去轉發給容器，還是要送給這個使用 2345 的本地程式? ","version":"Next","tagName":"h3"},{"title":"解決方式​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-lab-dnat#解決方式","content":"為了解決這兩個問題，這邊我們先看個範例 $ docker run -d -p 12345:80 nginx $ curl localhost:12345 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt; &lt;style&gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome to nginx!&lt;/h1&gt; &lt;p&gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&lt;/p&gt; &lt;p&gt;For online documentation and support please refer to &lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt; Commercial support is available at &lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; $ ps auxw | grep docker-proxy root 20060 0.0 0.1 552996 4060 ? Sl 05:20 0:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 12345 -container-ip 172.18.0.2 -container-port 80 $ sudo netstat -anlpt | grep 12345 tcp6 0 0 :::12345 :::* LISTEN 20060/docker-proxy ....  這邊我們透過 docker 快速起一個 nginx 的服務，並且設定 12345:80 的關係。 可以看到如果 docker 的話，我們是可以透過 localhost:12345 來存取容器的，這是我們剛剛自己設定 iptables 辦不到的。 解法是，docker 於本地端偷偷部署了一個新的程式，叫做 docker-proxy，可以看一下其運行的參數 -proto tcp -host-ip 0.0.0.0 -host-port 12345 -container-ip 172.18.0.2 -container-port 80  有沒有覺得這些內容跟我們 iptables 的規則很類似? iptables -t nat -A PREROUTING -p tcp -m tcp --dport 2345 -j DNAT --to-destination 10.55.66.2:8000  -proto tcp &lt;----&gt; -p tcp-host-port &lt;----&gt; -m tcp --dport--container-ip, --contianer-port &lt;-----&gt; -j DNAT --to-destination 所以其實這是一個，針對 loclahost 流量而設計的 Proxy 程式，幫忙處理 DNAT，將封包轉送到內部去。 此外，這個應用程式本身也會去聽 12345 這個 port，因此未來如果有任何應用程式想要註冊 12345 的話，就會沒有辦法註冊，因為已經被搶走 總結 docker -p 做法非常簡單，就是整合了 iptables DNAT 以及 docker-proxy透過 iptables DNAT 的功能，我們可以讓外部網路透過宿主機來存取內部容器，然而這種情況下，你是沒有辦法於宿主機內透過一樣的方式去存取容器的，這部分的原因是 iptables 的執行點跟 localhost 這種直接存取是有落差的docker透過 docker-proxy 的方式來彌補上述的缺陷，專門用來處理本地封包同時， docker-proxy 也會佔據本地的 port，避免未來有其他的程式想要使用相同的 port 而造成混淆與服務不如預期。 到這邊為止，我們透過四篇文章來探討了 Docker 的網路模型，並且嘗試透過手工的方式，打造一個相同的 Bridge 網路模型。過程中，我們也一併探討要如何讓容器擁有網路存取能力，不論是容器間互相存取，容器主動對外存取或是外界主動存取容器。 這些步驟背後原理非常複雜，實際的實作細節牽扯到 Linux Kernel 內的很多元件，包含了 netfilter, conntrack, iptables 等，而本系列文目標是從概念出發，從大方向去理解到底怎麼運作，docker 怎麼做，有了這些基本概念後 未來有興趣去學習底層運作時，也比較會有方向知道該怎麼下手。 ","version":"Next","tagName":"h3"},{"title":"[書本導讀]-什麼是GitOps","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops-book-ch2","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"Audited Change Management of Source Code​","type":1,"pageTitle":"[書本導讀]-什麼是GitOps","url":"/docs/techPost/2020/gitops-book-ch2#audited-change-management-of-source-code","content":"當程式碼改變後，現代的原始碼管理工具必須要能儲存全部的管理資訊，包含誰修改了資訊，做了什麼修改，以及誰准許這次修改 為了達到這些功能，這工具必須要可以提供 對於程式碼能夠簡單的達到分支與複製的功能透過 hashing 的方式來確保歷史修改紀錄的唯一性支持 Pull Request 類似的功能跟一些身份認證系統的整合，譬如 LDAP 就目前而言，只有一套工具可以被考慮來滿足上列的要求，這也是為什麼Git這個詞會在這個概念中被強調，直接用 GitOps 來描述一切。 但是這邊要特別注意，除了 Git 之外還是有其他工具有潛力可以滿足上述之些條件，最重要的就是能夠滿足 GitOps 的概念即可。 目前沒有一個很強勁的競爭對手於原始碼控制市場中可以與 Git 匹敵 ","version":"Next","tagName":"h2"},{"title":"Declarative Data Definition of Systems​","type":1,"pageTitle":"[書本導讀]-什麼是GitOps","url":"/docs/techPost/2020/gitops-book-ch2#declarative-data-definition-of-systems","content":"整個概念的原則就是不論是軟體或是架構的部署，目標都是透過原始碼內的檔案來定義，藉由這些檔案來定義其最終的期望狀態，但是到底這些期望狀態怎麼達到，則不是這些檔案要關注的。 最知名的一個關於這個概念的範例就是 Kubernetes 系統中的 YAML 檔案， Kubernetes 透過 Yaml 來定義到底什麼樣的資源要被部署到叢集中，或是什麼樣的資訊應該要被儲存在叢集中來輔助其他的應用程式。 其他的工具譬如 Pupper 都有類似的方式。 舉例來說，下圖是一個 YAML 的範例，其中就定義了 Secret 這個資訊，並且將其掛載到 Kubernetes 內的容器應用程式。這邊要注意的是下述的範例並沒有定義到底 Secret 要怎麼掛載，或是定義檔案內不同的資源被處理的先後順序。 透過原始碼內來宣告宣告一個理想的系統，最後怎麼部署跟處理的則是由其他的工具處理  相對於其他不同的部署方式，不論是明確定義部署先後順序，或是將定義狀態與實際部署邏輯混雜一起處理。 GitOps 這種將狀態與部署邏輯分離的方式可以更簡單與輕鬆的去管理系統。 ","version":"Next","tagName":"h2"},{"title":"Control Loop Configuration Management of Systems​","type":1,"pageTitle":"[書本導讀]-什麼是GitOps","url":"/docs/techPost/2020/gitops-book-ch2#control-loop-configuration-management-of-systems","content":"Control Loop 是一個不停運行的程式，運作邏輯滿簡單 檢查當前目標系統內的運行狀態跟期望狀態是否一致如果不一致，則將系統內的運行狀態改成跟期望狀態一致 一個經典的範例就是家中的空調系統，其不停地監控當前的溫度並且確認設定的溫度是否一致，如果不夠冷就讓他更冷點，直到兩邊的溫度一致。 Control loop 在大型的分散式架構系統中是一個很強力的部署應用程式工具，透過這種簡單的演算法就可以將應用程式同時部署到多個叢集中。 Key GitOps Tools 先前已經提過 Git 是實現整個 GitOps 的核心的重要工具，接下來會列出一些跟 GitOps 有關的工具。 ","version":"Next","tagName":"h2"},{"title":"Kubernetes​","type":1,"pageTitle":"[書本導讀]-什麼是GitOps","url":"/docs/techPost/2020/gitops-book-ch2#kubernetes","content":"Kubernetes(K8s) 是一個開源的容器管理平台，旨在提供自動化的應用程式部署，縮放與管理。 Kubernetes 自從 Google 2014 開源以來就發展迅速，基本上 Kubernetes 在其領域的統治力就跟 Git 差不多強烈。 K8S 主要是使用 YAML 作為其宣告式語言的基礎，透過 YAML 來定義各式資源的期望狀態。 此外， Kubernetes 內部也有 Control Loops 的核心概念，叢集中的每個節點上面也都會運行一個 Control Loops 來確保每一台節點上應該要運行什麼諮詢，不應該要運行什麼資源。 ","version":"Next","tagName":"h2"},{"title":"Terraform​","type":1,"pageTitle":"[書本導讀]-什麼是GitOps","url":"/docs/techPost/2020/gitops-book-ch2#terraform","content":"Terraform 是一個開源的 Infrastructure as code(架構即程式碼) 的開源工具，Terraform 的設定檔案都是基於 Hashicorp 所開發的 HCL 這種宣告式語言，藉由這種語言使用者可以用來定義其架構中的期望狀態。 其他的雲端廠商也有提供類似的工具來創建其基礎架構，譬如 CloudFormation 這種工具，不過 Terraform 則是透過 plugin 系統來支援更多的廠商。 ","version":"Next","tagName":"h2"},{"title":"Docker/Containers​","type":1,"pageTitle":"[書本導讀]-什麼是GitOps","url":"/docs/techPost/2020/gitops-book-ch2#dockercontainers","content":"容器這個技術發展已久，甚至在 Docker 之前就已經存在與發展，但是自從 2013 年 Docker 發展後，Docker 成為一個非常強勁的容器解決方案。 透過 Docker 的技術，使用者可以很輕鬆的將其應用程式給打包起來並且將其運行到各式各樣的環境之中。 Summary 結論來說， Gitops 的核心概念基於三個概念 透過宣告式定義應用程式的期望狀態透過原始碼控制來管理整個修改歷史Control loop 的概念來管理整個應用程式的交付 這些概念一直以來都不是全新的發想，但是因為 GitOps 常用的工具們(Git, Docker, Kubernetes 以及 Terraform)自從 2015 後都逐漸發展並且壯大出名。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2020/docker-network-model-snat","content":"","keywords":"docker network","version":"Next"},{"title":"封包要送給誰​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#封包要送給誰","content":"之前我們透過 ifconfig 幫 eth0 設定 IP 後，Kernel 會針對該網卡設定一個 Routing 規則，告訴系統說，什麼樣的封包，往什麼樣的網卡送出去 我們來看一下當前兩個容器 C1 &amp; C2 分別的狀態長怎樣 ○ → docker exec c1 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.55.66.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 ○ → docker exec c2 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.55.66.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0  這兩個訊息的概念都很簡單，就是告訴系統，當未來看到 10.55.66.0/24 的封包，就往 eth0 這張網卡送出去，但是因為 eth0 網卡是由 veth 組成的， veth 就如同水管一樣，從左邊進去，右邊就會出來，因此送到 eth0 的封包就會馬上從另一端的 veth0/veth1 跑出來。 概念如下圖 根據上述觀念，當前兩個容器往 10.55.66.0/24 的封包最後都會從宿主機身上的 veth0/veth1 上出現。  ","version":"Next","tagName":"h2"},{"title":"誰來處理封包​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#誰來處理封包","content":"當封包進入 eth0 網卡，之後從 veth0/veth1 出現後，因為 veth0/veth1 本身是掛在 Linux Bridge (hwchiu0) 身上，因此封包就會交由 Linux Bridge 來處理！ 這邊的處理分成兩個部分(這邊不談實際封包處理順序) Linux Bridge 內部會有一個機制用來決定如何轉送封包ebtables (本文忽略這個概念) Linux Bridge 的轉送機制是基於 MAC Address 來決定封包怎麼轉送，而這份轉換表可以稱為 Forwarding Table，根據 MAC Address 來轉發封包 下圖為一個範例，當封包進入到 Linux Bridge 後，其會根據封包的目標 MAC Address 是誰，來決定從哪個 Port 送出去。 因此我們的範例中，C1/C2 容器之間的封包就是透過這種機制來處理轉發。 ","version":"Next","tagName":"h2"},{"title":"誰來過濾封包​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#誰來過濾封包","content":"有了上述的規則後，C1/C2 容器之間要可以互相傳輸封包了，但是實務上卻發現會有問題，我們的 PING 不會通，原因是因為 iptables 偷偷介入來進行處理，並且將不符合規則的封包都丟棄。 這邊有兩個議題 iptables 為什麼偷偷介入來處理為什麼不符合規則的封包要丟棄，而不是符合規則的封包才丟棄 iptables 預設不會干擾任何 Linux Bridge 轉發的封包，這邊是有個開關要打開，也是 docker 安裝後會幫忙打開的開關 → cat /proc/sys/net/bridge/bridge-nf-call-iptables 1  這邊我們可以做個實驗，步驟是 確認當前網路不通將上述開關關閉，讓 iptables 不要干涉重新發送 ping 這時候就會發現容器之間可以透過 ICMP 傳送封包了 $ docker exec -it c1 ping 10.55.66.3 -c1 -W1 PING 10.55.66.3 (10.55.66.3) 56(84) bytes of data. --- 10.55.66.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms $ echo &quot;0&quot; | sudo tee /proc/sys/net/bridge/bridge-nf-call-iptables 0 $ docker exec -it c1 ping 10.55.66.3 -c1 -W1 PING 10.55.66.3 (10.55.66.3) 56(84) bytes of data. 64 bytes from 10.55.66.3: icmp_seq=1 ttl=64 time=0.025 ms --- 10.55.66.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.025/0.025/0.025/0.000 ms $ echo &quot;1&quot; | sudo tee /proc/sys/net/bridge/bridge-nf-call-iptables 1  當 iptables 被告知要干涉 Linux Bridge 的封包管理後，所有經過的封包都會讓 iptables 來進行檢查 Docker 的環境之中，採取的是白名單機制，若沒有告知要通過，則將該封包給丟棄，所以這也是為什麼我們的封包不會通過。 這邊有兩種解決方法 修改成黑名單的概念，預設通過封包加入相關規則，讓我們的封包可以通過 前一篇文章的實驗就是針對 (1) 進行操作,這邊透過 iptables -P FORWARD ACCEPT 去說明，對於 FORAWRD 封包的處理，預設就是 ACCEPT 去接納他們 $ docker exec -it c1 ping 10.55.66.3 -c1 -W1 PING 10.55.66.3 (10.55.66.3) 56(84) bytes of data. --- 10.55.66.3 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms $ sudo iptables -P FORWARD ACCEPT $ docker exec -it c1 ping 10.55.66.3 -c1 -W1 PING 10.55.66.3 (10.55.66.3) 56(84) bytes of data. 64 bytes from 10.55.66.3: icmp_seq=1 ttl=64 time=0.039 ms --- 10.55.66.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.039/0.039/0.039/0.000 ms # recover $ sudo iptables -P FORWARD DROP  ","version":"Next","tagName":"h2"},{"title":"結論​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#結論","content":"針對同節點上不同容器之間的傳輸，基本上都是依靠 Linux Bridge 來幫忙處理，藉由 Forwarding Table 來決定該怎麼傳輸封包，透過 iptables 來決定封包能不能通行。 Docker 預設的情況下會讓 iptables 介入 Linux Bridge 的處理，同時採由白名單機制，沒有符合規則的就一律丟棄，而 Docker 這邊的作法則是會加入相關的規則來打通封包連接。  容器如何主動存取外部服務 上述討論了容器間的基本存取方式，有了上述的概念之後，我們接下來可以往下邁進去探討，如果容器想要存取外部服務，譬如 8.8.8.8 之類的外部網站時，到底該怎麼處理，這也是容器服務中最常使用的類型，畢竟沒有對外上網能力，很多事情都沒辦法完成。 這個範例中將探討如何讓一個容器作為我們的 Client 端，能夠透過 PING 這個指令來存取外部 8.8.8.8 服務器 架構圖如下方，最終目標是容器 C1 能夠用 ping 存取 8.8.8.8 ","version":"Next","tagName":"h2"},{"title":"封包要送給誰​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#封包要送給誰-1","content":"封包的轉送我們一開始都需要先決定，到底送給誰，這邊我們再次回顧一下當前容器 c1 內的路由表 $ docker exec -it c1 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.55.66.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0  這時候如果我們強行連接 8.8.8.8 的話，會得到相關錯誤,因為系統中根本不知道要怎麼轉送 8.8.8.8 的封包，完全沒有可以符合的規則使用 $ docker exec -it c1 ping 8.8.8.8 connect: Network is unreachable  網路除錯時要格外小心與謹慎，我建議所有網路除錯都是先畫出架構圖，然後思考一下你認為封包該怎麼運作，對於所有關鍵點你有沒有辦法舉證他是對或是錯，藉由這個過程縮小可能出錯的範圍。 為了解決這個問題，我們可以 明確告訴系統，看到 8.8.8.8 的封包該怎麼送但是這種思路造成的問題就是如果今天你想要存取 1.1.1.1，你就要額外的新規則，所以我們可以透過另外一種作法。 沒有符合任何規則的話，就走預設走法吧！這也是系統實務上常見的作法，因為沒有人有辦法預料到你會想要連哪個網站，針對每個網站都寫一條規則實在是不太合理 有了這個想法後，我們下一個問題就是，那我要送給誰幫忙處理，這邊因為牽扯到 L3 路由的概念相對複雜，直接講結論就是 我們希望透過 宿主機 幫我們處理，如果宿主機本身就有能力存取外部網路，我們是不是能夠依賴宿主機幫我們處理，我們只要想辦法將封包送給宿主機，讓宿主機知道有封包要處理，請繼續往下弄 為了達成這個條件，我們需要進行下列設定 給 Linux Bridge (hwchiu0) 一個 IP 地址告訴容器說，預設走法就是將封包送給 Linux Bridge (hwchiu0) 上述兩個概念轉換成系統指令如下 $ sudo ifconfig hwchiu0 10.55.66.1 netmask 255.255.255.0 $ sudo docker exec -it c1 ip route add default via 10.55.66.1 $ sudo docker exec -it c1 ip route show default via 10.55.66.1 dev eth0 10.55.66.0/24 dev eth0 proto kernel scope link src 10.55.66.2  設定完畢後，我們容器現在多了一個規則，預設情況下，就將封包透過 eth0 送出去，並且將其 閘道(Gateway) 設定成 10.55.66.1。 這邊不解釋 Gateway 的概念，想成我們想要透過 Linux Bridge 幫我們轉發，封包送給他就對了！ 接下來針對系統上的 eth0 去監聽封包，看看這時候從 container c1 發送封包到 8.8.8.8 會怎麼樣 為了避免 iptables 又來干擾過濾功能，我們先修改成預設封包都會通 開兩個視窗執行 $ sudo iptables -P FORWARD ACCEPT $ sudo docker exec -it c1 ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.  $ sudo tcpdump -vvvnn -i eth0 icmp tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:01:20.425931 IP (tos 0x0, ttl 63, id 60779, offset 0, flags [DF], proto ICMP (1), length 84) 10.55.66.2 &gt; 8.8.8.8: ICMP echo request, id 16700, seq 1, length 64 19:01:21.445051 IP (tos 0x0, ttl 63, id 60899, offset 0, flags [DF], proto ICMP (1), length 84) 10.55.66.2 &gt; 8.8.8.8: ICMP echo request, id 16700, seq 2, length 64  透過上述指令可以觀察到，封包還是不通，沒有辦法得到正確的 ICMP 回應，但是我們可以從宿主機上面的 eth0 觀察到相關封包了，這些封包標示 10.55.66.2 想要送給 8.8.8.8。 只是目前都只有看到 ICMP 的請求，而沒有回應。 上述的所有流程我們用下列流程圖再次解釋 該圖片怎麼觀看 上面白色框框代表不同元件，若元件上方有IP，則代表該元件的 IP 地址每個元件之間透過箭頭來描述封包流向，並且表明該流向中，封包內的 IP 地址是什麼，由誰送給誰封包流向下方描述的則是當前過程中，有哪些元件涉入進行處理  重新整理目前已知流程與思路 當容器內發出一個送往 8.8.8.8 的封包，因為規則的關係，該封包會透過 eth0 送出去當封包從 eth0 送出去後，因為 veth 的特性，封包會到達系統上的 veth0 虛擬網卡從 veth0 進入到 Linux Bridge 的世界，這時候透過 Forwarding Table 的概念，最終該封包會進入到 Linux Bridge(hwchiu0) 本身。hwchiu0 收到封包後，接下來都是 kernel 內的事情了，這邊太複雜，我們忽略Kernel 最後透過本身的 routing talbe 去查詢，該怎麼轉送 8.8.8.8 的封包，然後根據規則送往宿主機上的 eth0 網卡封包送了出去，但是我們沒有辦法監聽到回來的封包 ","version":"Next","tagName":"h2"},{"title":"誰來處理封包​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#誰來處理封包-1","content":"為什麼收不到封包，理由很簡單 我們送出去的封包來源是 10.55.66.2，大部分情況下我們這個網段都是 private，也就是私有網段，世界上可能有很多人都會使用 10.55.66.2，那這種情況下， 8.8.8.8 根本不知道要怎麼把封包送回給 10.55.66.2 這時候我們要來思考，我們的宿主機是不是可以上網，是不是他的封包都回得來？ 如果是的話，我們能不能叫宿主機幫忙好人作到底，把封包的來源也變成宿主機的 IP ? 然後宿主機再想辦法把封包轉送回給我們後面的容器即可 這個概念就是所謂的 Network Address Translation (NAT)，這個範例中我們想要修改封包的來源 IP 地址，因此這個行為我們會稱為 Source NAT (SNAT)。 為了達成這個目的，我們要透過 iptables 的規則來幫我們做，而 iptables 有多種用法可以滿足這個需求，我們決定採用最簡單的也是最常用的方式, MASQUERADE，這種動態 SNAT 的功能來處理 工商我的其他文章，從 Linux Kernel Source Code 來探討這個行為，不適合初學者看 Linux NAT Masquerade 研究(上) 我們使用下列規則，告訴 iptables 說，以後你只要看到 10.55.66.2/32 的封包，而且要從宿主機上面的 eth0 送出去的，請你順便幫忙修改封包來源，改成自己 $ sudo iptables -t nat -I POSTROUTING -s 10.55.66.2/32 -o eth0 -j MASQUERADE $ sudo docker exec -it c1 ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=61 time=18.2 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=61 time=15.0 ms ^C  這時候我們再透過 tcpdump 來監聽封包，就會觀察到一切都不同了! $ sudo tcpdump -vvvnn -i eth0 icmp tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:40:52.893673 IP (tos 0x0, ttl 63, id 39804, offset 0, flags [DF], proto ICMP (1), length 84) 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 19069, seq 3, length 64 19:40:52.906130 IP (tos 0x0, ttl 62, id 11543, offset 0, flags [DF], proto ICMP (1), length 84) 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 19069, seq 3, length 64 19:40:54.962322 IP (tos 0x0, ttl 63, id 40179, offset 0, flags [DF], proto ICMP (1), length 84) 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 19079, seq 1, length 64 19:40:54.977470 IP (tos 0x0, ttl 62, id 11559, offset 0, flags [DF], proto ICMP (1), length 84) 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 19079, seq 1, length 64  裡面的內容我們分成兩筆來看 19:40:52.893673 IP (tos 0x0, ttl 63, id 39804, offset 0, flags [DF], proto ICMP (1), length 84) 10.0.2.15 &gt; 8.8.8.8: ICMP echo request, id 19069, seq 3, length 64  我們可以看到出去的封包 IP 再也不是 10.55.66.2 了，而是宿主機本身的 10.0.2.15，你可能會想說 10.0.2.15 也是私有 IP，為什麼 8.8.8.8 本身可以回應，這是因為我的系統環境外面還有一層 SNAT，一個封包經歷過多次 SNAT 是滿正常且合理的，但是整個運作邏輯都是一致的 [DF], proto ICMP (1), length 84) 8.8.8.8 &gt; 10.0.2.15: ICMP echo reply, id 19069, seq 3, length 64  這個情況下我們也可以看到封包順利回來了，同時從容器中也可以看到 ICMP 有正常回應。 這邊補充一下，如果我們針對 hwchiu0 這張 Linux Bridge 的網卡去監聽封包，會得到下列資訊 $ sudo tcpdump -vvvnn -i hwchiu0 icmp tcpdump: listening on hwchiu0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:47:55.128471 IP (tos 0x0, ttl 64, id 18861, offset 0, flags [DF], proto ICMP (1), length 84) 10.55.66.2 &gt; 8.8.8.8: ICMP echo request, id 19515, seq 1, length 64 19:47:55.146004 IP (tos 0x0, ttl 61, id 11872, offset 0, flags [DF], proto ICMP (1), length 84) 8.8.8.8 &gt; 10.55.66.2: ICMP echo reply, id 19515, seq 1, length 64  從 hwchiu0 的角度來看，他看到的封包都是 10.55.66.2，跟宿主機上面的 IP 沒有任何關係。 所以上述的概念我們用一樣的流程圖來看，這時候會變成怎麼樣 這份圖中，我們可以順利接收封包，所以多了回來封包的路線，其中 IP 特別用紅色底標示代表的是該封包是有被改過的。  這邊重新整理所有思路，將其條列下來 封包按照前面所有概念，一路送到 Linux KernelLinux Kernel 這時候查詢完 Routing Table 後，確認封包要送給 eth0iptables 這時候會介入，透過 MASQUERADE 的功能來修改封包的來源 ip 地址封包的來源被修改成 10.0.2.15，最後 8.8.8.8 收到這些封包後一路送回來回應的封包到達 eth0 之後，iptables 再度介入，畢竟誰幫你轉換封包，誰就要幫你轉回來，幫你把封包的目標從 10.0.2.15 轉回最初的 10.55.66.2 其實更精準的說這邊還有 conntrack 的介入來處理，但是過於複雜，因此這邊忽略，我們懂大概念就好 封包一路透過 Linux Bridge + Forwardnig Table + Veth 等元件回到容器手上 ","version":"Next","tagName":"h2"},{"title":"誰來過濾封包​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#誰來過濾封包-1","content":"到這邊為止，基本上我們的容器已經可以對外存取了，但是這邊有個前提是，我們事先修改 iptables 的規則，讓他預設是轉送封包，而不是丟棄 Docker 安裝完畢後，會讓 iptables FORWARD 預設丟棄封包，所以 docker 會針對這一塊去撰寫規則讓你的容器可以對外上網 因此這個小節我們就來看看，要如何透過 iptables 來允許我們的封包通過 根據我們前面的流程加上最初的運作，其實 iptables 運作的地方有兩個 Linux Bridge 這邊會偷偷請 iptables 處理一次Linux Kernel 往宿主機 eth0 這邊發送時也會有一次 這兩個環節於這個情境下，概念完全不同。 對於 Linux Bridge(hwchiu0) 來說，容器的封包會到達他身上，這邊是&quot;到達&quot;他身上，因此 iptables 內的規則就不是 FORWARD 這種轉發概念來處理，而是 INPUT 來處理。因此這邊我們就不需要特別處理 INPUT chain 本身沒有被改成預設丟棄，因此都會通 相反的，第二個流程是 LINUX KERNEL 轉發封包，從 eth0 出發，因此這邊 iptables 的 FORWARD 規則表就要被考慮，所以我們要處理的就是這一段過程 首先我們將 iptables 中的 FORWARD 修改回預設丟棄封包，回歸到安裝好 docker 的設定。 $ sudo iptables -P FORWARD DROP $ sudo docker exec -it c1 ping 8.8.8.8  這時候你會發現封包不會通，透過 tcpdump 去監聽 eth0 的封包也沒有任何訊息，不過若是監聽 hwchiu0 則是會有封包 $ sudo tcpdump -vvvnn -i eth0 icmp tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes ^C 0 packets captured 0 packets received by filter 0 packets dropped by kernel $ sudo tcpdump -vvvnn -i hwchiu0 icmp tcpdump: listening on hwchiu0, link-type EN10MB (Ethernet), capture size 262144 bytes 20:16:28.313865 IP (tos 0x0, ttl 64, id 60164, offset 0, flags [DF], proto ICMP (1), length 84) 10.55.66.2 &gt; 8.8.8.8: ICMP echo request, id 21202, seq 33, length 64  所以我們要透過 iptables 告訴系統，請允許我們的封包通過，這邊有非常多種思路 針對來源與目的 IP 去處理針對來源與目的 網卡 去處理 用網卡會簡單很多，這部份沒有一定，完全看你設計， docker 會使用網卡來處理，這樣規則數量不會因為容器過多而變多。 以下是我們使用的規則，我們告訴 iptables 說 從 hwchiu0 到 eth0 的封包，給他過！從 eth0 到 hwchiu0 的封包，給他過 ! $ sudo iptables -t filter -I FORWARD -i hwchiu0 -o eth0 -j ACCEPT $ sudo iptables -t filter -I FORWARD -i eth0 -o hwchiu0 -j ACCEPT $ sudo docker exec -it c1 ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=61 time=16.3 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=61 time=15.8 ms  這時後來看架構圖，這個範例中， iptables 會有兩個地方干涉，分別是進入到 hwchiu0 以及宿主機 eth0 兩個地方。 但是兩個地方因為概念不同，使用的是 iptables 下不同的表，而我們這邊針對 FORWARD 表去處理，因為其預設是丟棄封包 ","version":"Next","tagName":"h2"},{"title":"總結​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/docker-network-model-snat#總結","content":"到這邊為止，我盡可能的用簡單的方式去描述，到底一個容器要對外上網中間會發生什麼事情，而這些事情平常我們都沒有感覺，是因為 docker 本身都幫我們處理完畢了，整體思路都完全一樣，不過對於 iptables 的規則會因為細度不同，所以下法也不太一樣。 總結 由於本篇文章已經過長，因此如何從外部存取容器就留到下篇文章再來分享 綜合本文與前文，這邊幫大家整理一下，基於 Bridge 網路模型下， Docker 容器是如何對外上網 擁有 Linux Bridge，並且設定一個 IP創造容器，透過 veth 將容器與宿主機的 Linux Bridge 連接對容器內的網卡設定 IP，並且設定一個預設路由規則，讓 Linux Bridge 幫忙轉發對外封包設定相關 iptables 規則，讓系統轉發時，封包不會被丟棄設定 iptables SNAT 規則，讓我們往外的封包能夠有機會回來，最後輾轉回到容器手上 這一系列的規則看起來很多，但是其實整體都圍繞再 TCP/IP 的網路規則下，簡單的說就是 封包該怎麼走，誰幫忙處理封包，會不會有人攔截封包 將這三個思路整理下來，就可以很清晰的去分析封包的走向與除錯 ","version":"Next","tagName":"h2"},{"title":"介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"傳統部署方式​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#傳統部署方式","content":"下圖是一個常見的 CD 流水線流程(常見，非唯一，非絕對) 這個範例中，我們有一些基本元件 Git Repo 這個 Git Repository 中至少會放置關於 Kubernetes 資源的描述檔案(Manifests), 可以是原生的 Yaml， Helm Charts 或是 Kustomize 的檔案。CI/CD Pipeline 譬如 Jenkins/GitHub Action/CirecleCI 等常見系統，這個 Pipeline 裡面會有一些工作，譬如測試，建置 Contianer Image, 更新 Container Image，以及跟 Kubernetes 溝通Operator/Devloper 實際上操作的人員，譬如專案開發人員或是架構維運人員。 對於開發人員來說，可能會透過 1) PullRequest 的產生， 2) PR 被合併或是有特定的 Tag 被產生 等諸多條件觸發 CI/CD Pipeline 來進行後續行為 對於架構維運人員來說可已透過 1)直接要求 CI/CD Pipeline 運行來進行後續行為 2) 直接對目標 Kubernetes Cluster 進行操控 從上述的範例中可以看到，如果沒有嚴厲的去限制 直接對 Kubernetes 進行更新 這個行為，就有兩個不同的觸發點可以對 Kubernetes 進行更新，這種情況可能會造成的問題是沒有辦法控管Kubernetes 正在運行的資源狀態 (Living Status) 與Kubernetes 被期望的資源狀態 (Desired Yaml) 一致 特別是當有人員會直接透過 kubectl edit, kubectl patch 等方式直接修改運行狀態卻又沒有要更新描述資源 (Yaml) 時更容易發生。 除此之外，這個部署模型還有一個容易引人詬病的問題就是 KUBECONFIG 的散佈，為了讓外部環境(CI/CD Pipeline, 特定人員 laptop) 能夠對 Kubernetes 進行操作，必須要將連接 Kubernetes Cluster 所需要的 KUBECONFIG (Token/Credentail/API Address:Port) 等資訊給分享出去，這無疑造成了一些安全性的隱憂。 這意味者只要任何人有機會取得這份 KUBECONFIG，就有能力對你欲使用的 Kubernetes Cluster 進行各式各樣的操作，這部分就算透過 RBAC 去限制基本上也沒有太大用途。 因為你本來期望 1) CI/CD Pipeline, 2)維運人員 能夠擁有的操作權限也都一併跟者該份 KUBECONFIG 一併洩漏出去。 簡單整理以下上述部署流程可能的缺失 1) Kubernetes Cluster 的連線資訊必須要暴露在外，一旦被外界拿到則整個 Cluster 都有被破壞的可能 2) 難以確保 Kubernetes 內運行的狀態與描述的資源檔案 (Yaml/Helm Chart) 一致，因為有太多地方可以進行操作 接下來我們就看一下對於 GitOps 來說，要如何解決上面兩個潛在問題，並且是透過何種方式來處理。 ","version":"Next","tagName":"h2"},{"title":"GitOps​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#gitops","content":"GitOps 的概念如下圖呈現與前述的部署方式最大的差距就是 CI/CD Pipeline 內不進行任何部署動作資源的描述狀態 (Yaml/Helm Chart) 放在 Git 裡面，Git 作為 Single Source of Truth的角色Kubernetes 內部有一個 Controller 會定期去偵測 Git 的變化，並且把 Git 內的變動都更新到 Kubernetes 裡面 這意味者任何人如果想要對 Kubernetes Cluster 進行修改，只有一個辦法就是 更新 Git Repo，一旦 Git Repo 內描述的 Yaml/Helm Chart 有任何修改，Kubernetes Cluster內的 Controller 會負責將這些變動的差異性更新到 Kubernetes Cluster 內。 因此 Git repo 就是唯一的來源，同時透過 Git 版本控制的特性，如果想要針對資源進行跳板, rollback 等操作，直接針對 Git 管理（譬如 git revert). 此外，GitOps 的過程中，任何人都不應該直接對 Kubernetes Cluster 直接操作，因此也不需要將 KUBECONFIG 這個檔案給分享出去，因此安全性的隱憂也就迎刃而解. 所以這邊針對 GitOps 下個一個總結 KUBECONFIG 不分享出去，因此不希望任何人/環境有辦法直接從外部去操作 Kubernetes將所有描述 Kubernetes 資源狀態的檔案（Yaml/Helm Chart)用 Git 保存Kubernetes 內會安裝相對應的 controller 來監控 Git Repo 的更新並且將一切的更新都更新到 kubernetes cluster 內(可能的優勢)，透過 GitOps 這種架構，Kubernetes Cluster 本身不需要將 API Server 的存取方式給暴露出去，可以透過 Firewall 的方式把常見的 6443 連接埠給關閉，對於安全性來說也是減少了一些潛在的問題。 開源專案介紹 接下來我們將探討兩個有實現 GitOps 的開源專案，這邊不會有太仔細的 Demo 與操作，主要是針對架構來進行介紹，並且最後針對兩個專案進行一些比較，之後有時間會再針對這兩個專案寫篇比較細節的介紹 ","version":"Next","tagName":"h2"},{"title":"ArgoCD​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#argocd","content":"","version":"Next","tagName":"h2"},{"title":"介紹​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#介紹-1","content":"ArgoCD 的架構引用其官網  這個架構圖分成幾個部分來看 ArgoCD 於 Kubernetes Cluster 內安裝的服務，這些服務包含了 - Argo API Server 提供介面給外界操控 ArgoCD 服務，本身提供了 CLI, GUI 以及 gRPC/REST 等介面，最簡單的 Demo 可以使用 GUI 來操作。 - Repository Service. 本身作為一個遠方 Git Repo 的本地快取，主要是儲存該 Git Repo 內的資源預期檔案 (Yaml/Helm Chart) - Application Controller 負責跟 Kubernetes 溝通，會比較 Kubernetes Cluster 內的運行狀態 (living status) 以及 Git Repository 內所描述的資源預期狀態 (Yaml/Helm Chart), 並且更新 Kubernetes Cluster 一旦發現 Git Repository 內有更動 上方的 Git Repo, 該 Git Repo 本身有兩種方式與 ArgoCD 互動 Webhook 主動告知 ArgoCDArgoCD 本身定期去觀察 Git Repo 的狀態，並且一旦有更新就會讓 Controller 來更新相關的應用程式 左方的開發人員/維護人員，這些人員可以透過 UI/CLI/gRPC/REST 等方式來操作 ArgoCD 的服務。 開發人員可以透過對 Git Repo 的修改與合併來觸發 ArgoCD部署人員可以透過 UI/CLI 等方式來觀察與設定當前應用程式的狀態 左下方的是 Hook 點， ArgoCD 本身針對 Git Repo 更新前後有提供相關的 Hook，可以在這邊將相關的訊息與其他系統銜接，譬如 Slack, Webhook 等，讓你的系統能夠有辦法接收到當前部署的狀況 右下方則是目標的 Kubernetes Cluster 對於 ArgoCD 來說，本身要先透過一套 Kubernetes 來架設服務，接者該 ArgoCD 可以針對不同的 Kubernetes Cluster 進行設定與存取。舉例來說，如果你環境內有多套 Kubernetes Cluster, 可以只用一套 ArgoCD 來管理這些 Kubernetes Cluster，將關注的 Git Repo 給部署到不同 Kubernetes Cluster 內的不同 namespace 下。 ","version":"Next","tagName":"h3"},{"title":"使用範例​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#使用範例","content":"官網的使用範例, 有興趣的可以參考連結來設定玩看看，基本上全部都是基於 GUI 來進行操作 以下圖片都來自官網範例上圖範例就是建立一個 Application, 其追蹤的 Git Repo 是 https://github.com/argoproj/argocd-example-apps，並且追蹤的是目標 Repo 的 HEAD(latest) branch 下的 guestbook 資料夾內的資源描述檔案. 此外當有任何更新的時候，會將相關的應用程式部署到 https://kubernetes.default.svc 該 API server 所對應的 Kubernetes Cluster 內，並且放到 default namespace ArgoCD 本身也會透過 GUI 的方式來視覺化呈現部署的資源，包含其狀態，名稱以及彼此的關係 ","version":"Next","tagName":"h3"},{"title":"Flux​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#flux","content":"","version":"Next","tagName":"h2"},{"title":"介紹​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#介紹-2","content":"Flux 的架構圖參考自其官網 Ｆlux 這個專案的開發者也就是本文一開始所提到的 Weave Net 這間公司，此專案早期與該公司的產品 Weave Cloud 有緊密的連結性，後期的開發則是逐漸讓此專案變成一個獨立專案，更能夠被應用到不同的環境之中。 此專案與 ArgoCD 最大的差異性有幾點 除了偵測 Git Repo 之外，也可以偵測 Container Image 的變動，一旦 Image 有變動，也會自行更新部署的應用程式由於 Image 版本變動造成 Kubernetes 內運行資源的更新，同時又要維持 Git Repo 是 Single Source of Truth 的概念，因此 Flex 本身會對該 Git Repo 進行修改，將相關的 Image tag 給修改並且自己上一筆 Commit由於上述需求， Flex 必要時要有能對 GitRepo 擁有寫入的權限，同時為了避免混淆整個專案， Flex會希望將 程式碼 以及 描述資源狀態的檔案(Yaml/Helm Chart) 分開放， 所以由上方的架構圖可以看到， Flux Controller 本身會關注 1) Git Repo, 2) Container Registry。 如果是 Git Repo 有任何更動，就將更動部署到 Kubernetes Cluster 內，與 ArgoCD 的邏輯雷同，如果是 Container Registry 有任何更動，就將更動部署到 Kubernetes Cluster 內，同時將新的版本號給寫回到關注的 Git Repo 並且產生一筆 Commit ","version":"Next","tagName":"h3"},{"title":"使用範例​","type":1,"pageTitle":"介紹","url":"/docs/techPost/2020/gitops#使用範例-1","content":"有興趣的可以參考官方教學 來操作，基本上指令弄過去就可以架起整個環境，此外官方也有準備示範用的 Manifest(Yaml/Helm Chart) Repo，可以非常無腦的架起整個環境來觀察這一切。 整個操作結果可能如下圖可以看到 Flux 本身會幫忙上 Commit, 除了 自動更新版本號 之外，透過 Flux 工具對應用程式進行的任何設定修改也都會被寫回到 Git Repo 之中，非常堅持的去滿足 Git Repo is a single source of truth 的概念。 上圖是個範例，當觀察的 Image Tag 出現如預期般的變化時，就會自動更新並且寫入一筆 Commit. 總結 GitOps 關注的是如何自動化部署(CD)，跟 CI, Pipeline 關係不大GitOps 要求的是以 Git Repo 為來源，如果要對應用程式進版或是退版，都直接對 Git 修正即可，後續讓相關的 Controller 來幫忙同步資源狀態(個人想法): Flux 相對於 ArgoCD 來得更加強韌，除了監控 Git Repo 外也可以針對 Contaienr Image 來處理針對不能夠被外界存取的 Kubernetes Cluster來說， GitOps 的概念可以解決自動部署的困境，同時也能夠減少外洩 KUBECONFIG 這個檔案的潛在性如果對上述專案有興趣，根據官網的操作連結，大概 10mins 內就可以架起一個測試環境來玩耍Flux 本身還有更多功能，包含還可以針對特定版本鎖定，禁止任何更新等功能 ","version":"Next","tagName":"h3"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2020/cni-performance-2020","content":"","keywords":"","version":"Next"},{"title":"環境​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#環境","content":"本篇文章的環境基於下列版本 Kubernetes: 1.19Ubuntu: 18.04受測 CNI: Antrea v.0.9.1Calico v3.16Canal v3.16 (Flannel network + Calico Network Policies)Cilium 1.8.2Flannel 0.12.0Kube-router latest (2020–08–25)WeaveNet 2.7.0 內容是 2020 8月份時進行的實測結果 該文用到的所有測試工具全部都開源並放到 Github 上，對其專案有興趣的可以到這邊觀看內容 benchmark-k8s-cni-2020-08 或是閱讀本文的第一大章節，有介紹一些工具的使用。 MTU 的影響 文章中針對三款 CNI (Calico, Canal, WeaveNet) 測試看看 偵測MTU的功能 基於 TCP/UDP 下的效能如何 ","version":"Next","tagName":"h2"},{"title":"TCP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#tcp","content":"CNI\\MTU\tAuto MTU\tCustom MTUCalico\t8876\t9834 Canal\t8530\t9817 Weave Net\t4880\t9759 以上數字都是 Mbit/s ","version":"Next","tagName":"h2"},{"title":"UDP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#udp","content":"CNI\\MTU\tAuto MTU\tCustom MTU1Calico\t1901\t9757 Canal\t1820\t9742 Weave Net\t1809\t9593 以上數字都是 Mbit/s ","version":"Next","tagName":"h2"},{"title":"結論​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#結論","content":"上述的結論可以看到 Auto MTU 的效能都非常差，原因並不是 Auto MTU 沒有效，而是因為這些 CNI 目前根本沒有支持 Auto MTU 的做法，而 Calico 直到 3.7 版本才正式支持 Auto MTU 這個功能，而且根據作者的測試其功能良好。 作者認為對於這種需要設定 Jumbo frames 的環境下，如果沒有 Auto MTU 的話，管理員則需要手動去設定這些 MTU，所以非常希望每個 CNI 能夠去實作 Auto MTU 的功能來自動偵測並且設定，減少管理員需要人工介入的需求。 至於其他的 CNI 為什麼沒有測試，因為他們都有實作 Auto-MTU 的功能 AntreaCiliumFlannelKube-OVN 其中 Kube-router 這邊作者標示為 None, 估計可能是根本不能設定 MTU 使用 Raw Data 來進行 CNI 的評測 這章節主要會用來比較這些再正確 MTU 設定下不同 CNI 之間的效能比較 ","version":"Next","tagName":"h2"},{"title":"資源效能評比​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#資源效能評比","content":"一開始作者先比較基於閒置狀況下，不同 CNI 所消耗的資源狀況，包含了 CPU 以及 Memory。 原文中是用 CPU(%) 以及 Memory (MB) 來畫圖，我則是將這些數字用幾個等級來區分，數字愈低代表使用量愈低 CNI\\資源類型\tCPU\tMemoryAntrea\t3\t4 Calico\t3\t4 Canal\t3\t4 Cilium\t5\t5 Flannel\t2\t3 Kube-OVN\t8\t4 Kube-router\t3\t3 Weave Net\t2\t3 裸機\t1\t2 這邊可以觀察到 Weave Net/Flannel 的資源使用量最低 Cilium 資源使用量偏高 Kube-OVN 資源使用量最高 剩下的資源使用量都差不多，我認為可以算是同一個等級 Kube-OVN &gt; Cilium &gt; 剩下全部 &gt; WeaveNet/Flannel ","version":"Next","tagName":"h2"},{"title":"Pod to Pod​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#pod-to-pod","content":"接下來看一下 Pod to Pod 的存取，這邊的方式是直接用 Pod 的 IP 來存去，並不是任何用 Service 這種方式來存取。 ","version":"Next","tagName":"h2"},{"title":"TCP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#tcp-1","content":"CNI\\Performance\tMbit/sAntrea\t9826 Calico\t9834 Canal\t9817 Cilium\t9426 Flannel\t9690 Kube-OVN\t9029 Kube-router\t8051 Weave Net\t9759 裸機\t9906 從上面的數據可以觀察到 Kube-router 的數據最差Kube-OVN 也沒有很好，大概就 9Gb/s 左右Cilium 大概介於 9.5Gb/s剩下的都 CNI 效能跟裸機都不會差太多 接下來觀察一下這個實驗中，不同 CNI 的資源消耗量，原文中是用 CPU(%) 以及 Memory (MB) 來畫圖，我則是將這些數字用幾個等級來區分，數字愈低代表使用量愈低 CNI\\資源類型\tCPU\tMemoryAntrea\t2\t3 Calico\t2\t3 Canal\t2\t3 Cilium\t3\t4 Flannel\t2\t3 Kube-OVN\t4\t4 Kube-router\t2\t3 Weave Net\t3\t3 裸機\t1\t2 從上面的結論觀察，我認為跟 閒置 的情況差不多，唯一的差異就是 Weavenet 從最少使用量的 CNI 變成第三高 Kube-OVN &gt; Cilium &gt; WeaveNet &gt; 剩下全部 ","version":"Next","tagName":"h3"},{"title":"UDP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#udp-1","content":"CNI\\Performance\tMbit/sAntrea\t9796 Calico\t9757 Canal\t9742 Cilium\t9726 Flannel\t9846 Kube-OVN\t5833 Kube-router\t2810 Weave Net\t9539 裸機\t9885 從上面的數據可以觀察到 Kube-router 的數據最差，連 3Gb/s 都不到，非常慘，不到 30% 的效能Kube-OVN 也很不好，大概只有 6Gb/s剩下的都 CNI 效能跟裸機都不會差太多 接下來觀察一下這個實驗中，不同 CNI 的資源消耗量，原文中是用 CPU(%) 以及 Memory (MB) 來畫圖，我則是將這些數字用幾個等級來區分，數字愈低代表使用量愈低 CNI\\資源類型\tCPU\tMemoryAntrea\t4\t4 Calico\t3\t4 Canal\t3\t4 Cilium\t4\t5 Flannel\t3\t4 Kube-OVN\t5\t5 Kube-router\t4\t4 Weave Net\t4\t4 裸機\t2\t2 從上面的結論觀察，跟 閒置 比較起來比較有一些變化 Kube-OVN 永遠都是使用資源第一名Cilium 還是第二名第三名則是 WeaveNet/Antrea/Kube-Router剩下的等級差不多 Kube-OVN &gt; Cilium &gt; WeaveNet/Antrea/Kube-Router &gt; Calico/Canal/Flannel &gt; 裸機 ","version":"Next","tagName":"h3"},{"title":"Pod to Service​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#pod-to-service","content":"這個情況下則是探討透過 Service 來存取目標 Pod，也是基於 TCP/UDP 來測試，其中 Service 則是基於 ClusterIP 的設定才測試。 ","version":"Next","tagName":"h2"},{"title":"TCP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#tcp-2","content":"CNI\\Performance\tMbit/sAntrea\t9789 Calico\t9841 Canal\t9757 Cilium\t9630 Flannel\t9826 Kube-OVN\t9409 Kube-router\t8380 Weave Net\t9749 從上面的數據可以觀察到 Kube-router 的數據最差, 大概只剩下 85% 效能Kube-OVN 還行，大概 95%剩下的都 CNI 效能都差不多， 97% +-1%。 接下來觀察一下這個實驗中，不同 CNI 的資源消耗量，原文中是用 CPU(%) 以及 Memory (MB) 來畫圖，我則是將這些數字用幾個等級來區分，數字愈低代表使用量愈低 CNI\\資源類型\tCPU\tMemoryAntrea\t3\t4 Calico\t2\t4 Canal\t2\t4 Cilium\t3\t5 Flannel\t2\t4 Kube-OVN\t4\t5 Kube-router\t2\t4 Weave Net\t3\t4 從上面的結論觀察，跟 閒置 比較起來比較有一些變化 Kube-OVN 永遠都是使用資源第一名Cilium 還是第二名第三名則是 WeaveNet/Antrea剩下的等級差不多 Kube-OVN &gt; Cilium &gt; WeaveNet/Antrea &gt; Kube-Router/Calico/Canal/Flannel &gt; 裸機 相對於 Pod to Pod 的情況來看， Pod to Service 中 Antrea 的效能消耗更高，從第四名那群躍升到第三名。 ","version":"Next","tagName":"h3"},{"title":"UDP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#udp-2","content":"CNI\\Performance\tMbit/sAntrea\t8618 Calico\t9420 Canal\t9800 Cilium\t9712 Flannel\t9825 Kube-OVN\t5380 Kube-router\t2781 Weave Net\t9154 從上面的數據可以觀察到 Kube-router 的數據最差，連 3Gb/s 都不到，非常慘，不到 30% 的效能Kube-OVN 也很不好，大概只有 5Gb/sAntrea 的效能也不好了，大概只有 8.6 Gb/sCalico 以及 WeaveNet 的效能也都降到 95% 以下，不到 9.5Gb/s剩下的都 CNI 效能都差不多 (Canal/Cilium/Flannel) 接下來觀察一下這個實驗中，不同 CNI 的資源消耗量，原文中是用 CPU(%) 以及 Memory (MB) 來畫圖，我則是將這些數字用幾個等級來區分，數字愈低代表使用量愈低 CNI\\資源類型\tCPU\tMemoryAntrea\t4\t4 Calico\t3\t4 Canal\t3\t4 Cilium\t4\t5 Flannel\t3\t4 Kube-OVN\t4\t5 Kube-router\t3\t4 Weave Net\t4\t4 從上面的結論觀察，跟 閒置 比較起來比較有一些變化 Kube-OVN 跟 Cilium 兩個各有千秋，一個 CPU 用比較多，一個則是記憶體比較多Antrea/WeaveNet/Kube-router 三者消耗的層級差不多Calico/Canal/Flannel 三者差不多 Kube-OVN/Cilium &gt; WeaveNet/Antrea/Kube-Router &gt; Calico/Canal/Flannel ","version":"Next","tagName":"h3"},{"title":"Network Policies​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#network-policies","content":"這邊沒有任何的數據測試，除了 Flannel 外，剩下的 CNI 都有實現 Ingress/Egress(往內/往外) 的 Network Policies，很棒! CNI 加密 測試的 CNI 解決方案中，只有四套有支援資料加密的部分，分別是 Antrea (IPSec)Calico (wireguard)Cilium (IPSec)WeaveNet (IPSec) ","version":"Next","tagName":"h2"},{"title":"效能部分​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#效能部分","content":"因為這邊效能比較少，所以我們將 TCP/UDP 放在一起評比 ","version":"Next","tagName":"h2"},{"title":"Pod to Pod TCP/UDP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#pod-to-pod-tcpudp","content":"CNI\\Performance\tTCP (Mbit/s)\tUDP (Mbit/s)Antrea\t1392\t742 Calico\t9786\t4877 Cilium\t1657\t869 WeaveNet\t1384\t432 這邊可以觀察到 TCP 的效能遠勝於 UDP使用 wireguard 的效能完全輾壓 IPSec 技術的其他 CNI三個都使用 IPSec 的 CNI，其中 WeaveNet 效能是其中最差的，而 Cilium 則是效能最好的 ","version":"Next","tagName":"h2"},{"title":"資源效能評比 TCP/UDP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#資源效能評比-tcpudp","content":"","version":"Next","tagName":"h2"},{"title":"TCP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#tcp-3","content":"接下來觀察一下這個實驗中，不同 CNI 的資源消耗量，原文中是用 CPU(%) 以及 Memory (MB) 來畫圖，我則是將這些數字用幾個等級來區分，數字愈低代表使用量愈低 CNI\\資源類型\tCPU\tMemoryAntrea\t2\t4 Calico\t4\t4 Cilium\t2\t5 WeaveNet\t2\t4 從上面的結論觀察，跟 閒置 比較起來比較有一些變化 Calico 使用的資源最多剩下三者差不多 ","version":"Next","tagName":"h3"},{"title":"UDP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#udp-3","content":"接下來觀察一下這個實驗中，不同 CNI 的資源消耗量，原文中是用 CPU(%) 以及 Memory (MB) 來畫圖，我則是將這些數字用幾個等級來區分，數字愈低代表使用量愈低 CNI\\資源類型\tCPU\tMemoryAntrea\t2\t4 Calico\t4\t4 Cilium\t2\t5 WeaveNet\t2\t4 從上面的結論觀察，跟 閒置 比較起來比較有一些變化 Calico 使用的資源最多剩下三者差不多 ","version":"Next","tagName":"h3"},{"title":"Pod to Service TCP/UDP​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#pod-to-service-tcpudp","content":"CNI\\Performance\tTCP (Mbit/s)\tUDP (Mbit/s)Antrea\t1390\t735 Calico\t9808\t4872 Cilium\t1661\t866 WeaveNet\t1381\t451 這邊可以觀察到其結果與 Pod-to-Pod 是差不多的，因此結論完全一致 TCP 的效能遠勝於 UDP使用 wireguard 的效能完全輾壓 IPSec 技術的其他 CNI三個都使用 IPSec 的 CNI，其中 WeaveNet 效能是其中最差的，而 Cilium 則是效能最好的 總結 根據上述的全部來源，我們可以繪製數張總結表格，效能的部分採用相對比較，對原始數字有興趣的可以參考其公開的 Github 專案。 評比標準: 好&gt;普通&gt;不好 ","version":"Next","tagName":"h2"},{"title":"MTU/加密效果​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#mtu加密效果","content":"---\tMTU設定\tNetwork Policy(往內)\tNetwork Policy(往外)\t加密設定\t加密後傳輸效能\t加密資源消耗量Antrea\t自動偵測\t支援\t支援\t支援(部署時設定)\t普通\t普通 Calico\t手動設定\t支援\t支援\t動態設定\t好\t不好 Canal\t手動設定\t支援\t支援\t不支援\tn/a\tn/a Cilium\t自動偵測\t支援\t支援\t支援(部署時設定)\t普通\t普通 Flannel\t自動偵測\t不支援\t不支援\t不支援\tn/a\tn/a Kube-OVN\t自動偵測\t支援\t支援\t不支援\tn/a\tn/a Kube-router\t無\t支援\t支援\t不支援\tn/a\tn/a Weave Net\t手動設定\t支援\t支援\t支援(部署時設定)\t普通\t普通 ","version":"Next","tagName":"h2"},{"title":"流量評比​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#流量評比","content":"這邊使用一些縮寫 P2P -&gt; Pod to Pod P2S -&gt; Pod to Service ---\tP2P/TCP\tP2P/UDP\tP2S/TCP\tP2S/UDPAntrea\t很好\t很好\t很好\t普通 Calico\t很好\t很好\t很好\t好 Canal\t很好\t很好\t很好\t很好 Cilium\t好\t很好\t很好\t很好 Flannel\t很好\t很好\t很好\t很好 Kube-OVN\t好\t很差\t好\t很差 Kube-router\t很差\t很差\t很差\t超級差 Weave Net\t很好\t很好\t很好\t好 ","version":"Next","tagName":"h2"},{"title":"資源消耗評比​","type":1,"pageTitle":"前言","url":"/docs/techPost/2020/cni-performance-2020#資源消耗評比","content":"這邊使用一些縮寫 P2P -&gt; Pod to Pod P2S -&gt; Pod to Service 同時評比的概念是使用的資源多寡，採用相對等級 超高&gt;有點高&gt;普通&gt;少 ---\t閒置\tP2P/TCP\tP2P/UDP\tP2S/TCP\tP2S/UDPAntrea\t普通\t普通\t普通\t普通\t普通 Calico\t普通\t普通\t少\t少\t少 Canal\t普通\t普通\t少\t少\t少 Cilium\t有點高\t超高\t有點高\t有點高\t超高 Flannel\t少\t普通\t少\t少\t少 Kube-OVN\t超高\t超高\t超高\t有點高\t超高 Kube-router\t普通\t普通\t普通\t少\t普通 Weave Net\t少\t有點高\t普通\t普通\t普通 透謝圖表可以觀察到 Kube-OVN 不但資源吃很多，效能還很不好Canal/Calico/Flannel 三者的運算資源使用量都不多，且效能都很好Kube-Router 的效能都很差，資源使用方便也不是特別出色WeaveNet 與 Cilium 效能都不差，但是 Cilium 吃的效能很高，可說跟 Kube-OVN 同等級，而 WeaveNet 用到的資源少 個人心得 這次的實驗評比我認為其實能看到的東西有限，主要是不同的 CNI 所搭配的解決方案不同，目標要配合的情境也不同，雖然從圖表中可以看到 Kube-OVN 的綜合評比最差，但是其要用的場景本身就不太一樣，單純用最原始的流量互打來判別優劣其實不太對如果今天要選擇網路 CNI 的時候，可以看到效能跟資源方面， Flannel/Calico/Canal 幾乎等級都差不多，而且 Calico 還支援加密與 Network Policy 等功能。此外，目前 Flannel 也從 Kubeadm 的官方教學頁面中移除，因為太多問題且維護者沒有要修復。所以我認為如果沒有特別使用情境需求的話，可以考慮使用 Calico.Cilium 對於安全性以及 load-balancing 方面也有別的功能，就如同(1)點所提到，不同的場景有不同的需求，有些功能是獨占的。 參考來源 https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-updated-august-2020-6e1b757b9e49 ","version":"Next","tagName":"h2"},{"title":"[書本導讀]-淺談GitOps過往","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops-book-ch3","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"Everything As Code 以及 DevOps​","type":1,"pageTitle":"[書本導讀]-淺談GitOps過往","url":"/docs/techPost/2020/gitops-book-ch3#everything-as-code-以及-devops","content":"GitOps 可以很合理的視為是 Everything As Code 的延續發展，都強調透過原始碼工具來管理這些編碼後的應用開發與交付產物。 這個概念在 2015 因為 Terraform 的發展與崛起而被更廣泛的支持，透過 Terraform 的幫忙，使用者可以透過程式化的方式來自動完成基礎建設的交付，而不再需要手動的部署。 談到 DevOps，這個相關且更早的概念則是於 2009 年由 DevOpsDays 這個會議開始茁壯發展。雖然 DevOps 本身並沒有任何正式的定義，其精神則是開發工具與技術應該要應用到維運的實戰上。 舉例來說，一個經典貫策 DevOps 精神的就是 CALMS(Culture, Automation, Lean, measurement, Sharing)以及其中的 Automation 流水性則是 GitOps 定義中不可缺少的一塊。 ","version":"Next","tagName":"h2"},{"title":"Declarative Code​","type":1,"pageTitle":"[書本導讀]-淺談GitOps過往","url":"/docs/techPost/2020/gitops-book-ch3#declarative-code","content":"宣告式程式碼本身不是一個全新的概念，譬如 Makefile 或是 HTML 都是宣告式的概念，舉例來說。宣告式的概念從過去到現在則是一直發展，譬如 Chef, Puppet 甚至 Ansible 等工具都有透過自己的語言(Domain-Specific Languages DSLs)來完成這些目的。 上述這些設定檔管理工具都透過宣告式的方式來處理不同維度的事情。 譬如， Chef 則是透過宣告式的方式來處理高維度的事情，但是如果將其修改套用到每個節點上，這些修改就會變成立即性的，而非宣告式的處理。相對於 Chef 來說， Puppet 則將宣告式的概念發展得更好。 2015 後， Kubernetes 與 Terraform 的崛起將整個宣告式的概念給集大成，這兩者的結合提供了一條宣告式的道路來滿足任何點到點服務所需要的所有資源。 ","version":"Next","tagName":"h2"},{"title":"The Control/Reconciler Loop​","type":1,"pageTitle":"[書本導讀]-淺談GitOps過往","url":"/docs/techPost/2020/gitops-book-ch3#the-controlreconciler-loop","content":"Control loops 這個概念自從軟體存在以來已經發展許久，過去十年來，這個概念被廣泛 的應用到各式各樣的設定管理工具，譬如 Chef 以及 Puppet。 這些工具通常都會安裝一個代理人到環境中的所有節點，透過這個代理人來觀察當前系統狀態是什麼，並且根據其差異與需求來進行後續處理。 Kubernetes 其架構的核心概念就是 Control/Loop，每個節點上都會有一個代理人程式kubelet，其負責協調所有該節點上的資源是否符合預期狀態。 全部整合一起 GitOps 可以被視為是 Everything as Code 的後續發展，自從 2015 開始，由 Everything as Code 為基底開始，接者遇到各式各樣合適的工具，最後全部組織再一起，就成為了 GitOps，其特性 強調 Git, Kubernetes 以及 Terraform 是目前 GitOps 發展中的主流，但是不是唯一標準，任何工具可以滿足這些概念都可以。為宣告式程式碼以及 Control-Loop 概念的擁護者，透過這個機制來維護狀態與部署應用強調除了 Everything as Code 之外的第二個好處， ","version":"Next","tagName":"h2"},{"title":"[書本導讀]-GitOps工具的選擇","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops-book-ch4","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"'Push' GitOps Tools​","type":1,"pageTitle":"[書本導讀]-GitOps工具的選擇","url":"/docs/techPost/2020/gitops-book-ch4#push-gitops-tools","content":"Push GitOps 的工具(技術上來說，可以稱為基於 Client-API 的工具)也許可以被視為相對老舊的工具。本質上來說，其實作 Control Loop 的概念於 CI/CD 流水線系統上，在 CI/CD 的過程中，去管理系統上的應用程式狀態，並且將差異性給部署到系統中。 這種架構中，這個 CI/CD 的工作可以有很多種被觸發的方式，譬如是排程的觸發(每天晚上)，或是任何程式碼更動導致的觸發(PR,Merge)。 一個比較容易理解範例是，假設有一個公司使用 Jenkins 作為其 CI/CD 流水線系統來處理與 Kubernetes 上的應用程式。當流水線被觸發後，程式碼中那些宣告式的檔案就會被重新部署到 Kubernetes 叢集內來想辦法達到期望的狀態。 這部分可以借助一些指令列工具，譬如 kubectl, kustomized 等。 此外如 kubestack 這類型的工具也是基於相同的策略來完成，不過實作則是透過 Terraform 等工具來完成 Kubernetes 內的部署。 最近這幾年，一些Git管理服務譬如 GitLab 以及 GitHub 都發展了屬於自己的一套 CI/CD 流水線，如 Github Action 以及 GitLab CI/CD，其架構中提供了豐富的第三方套件系統讓其系統能夠提供更廣泛與強大的功能。 對於設定檔管理工具來說， Ansible 可以被視為是一個 push GitOps 工具，其底層實作是基於 SSH 連線來將所有的修改都套用到遠方環境。 Push GitOps 的工具的特性使得開發團隊能夠用相對簡單的概念去理解與維護整個部署流程。然而，隨者時間發展，有一些團隊開始因為這個流程對於 Control Loop 的無力感而感到灰心。 無力感的理由如下 對一個已經存在的資源進行更新時，如果本次的修改有包含一些資源的刪除，那要將遠方系統上這些相對應的資源也一併刪除是一個實作上的挑戰通常來說，將修改內容套用到系統上通常會需要一系列的腳本幫忙輔助，而這些腳本彼此運行的順序尤其重要，一但順序錯誤可能整個邏輯就大亂，最後部署的結果也會不如預期。 ","version":"Next","tagName":"h2"},{"title":"'Pull' GitOps Tools​","type":1,"pageTitle":"[書本導讀]-GitOps工具的選擇","url":"/docs/techPost/2020/gitops-book-ch4#pull-gitops-tools","content":"'Pull' GitOps 工具(技術上來說可以說是基於代理人架構)完全採用不同的方式，這些工具會先於目標環境中部署一個代理人服務，而該代理人服務則會等待各種狀態變動的請求，並且根據這些請求來修正系統上的狀態。 如果是設定檔管理工具的話， Puppet 則是一個基於 Pull 機制的管理工具，所有的更動都要環境中的代理人幫忙管理 使用這種模式的好處就是這些代理人因為本身就處在這些目標環境中，所以可以被視為是一個可信任的代理人。以 Puppet 來說，其代理人可以基於 privileged 這種比較有權限的使用者去運行。 另外，對於 Kubernetes 來說，內部的 Operator 概念就是完全符合這種模式，透過授權的代理人能夠存取 Kubernetes API 來獲得當前系統上資源的狀態，甚至對其修改。 相關的工具譬如 Flux/ArgoCD 就會於 Kubernetes 內運行一個代理人，透過適當的設定後，這些代理人就會專注於特定的 Git Repo，當有任何更動時就會把差異內容套用到 Kubernetes 內，甚至有必要的話還可以把系統上的更動寫回到 Git Repo 內確保狀態一致。 其他工具譬如 Tekton 則是一套基於 Kubernetes 內的 CI/CD 工具，能夠輕鬆地做到上述 GitHub Action 或 GitLab CI/CD 與kubectl結合的各種事項。 ","version":"Next","tagName":"h2"},{"title":"Infrastructure Provisioning Tools​","type":1,"pageTitle":"[書本導讀]-GitOps工具的選擇","url":"/docs/techPost/2020/gitops-book-ch4#infrastructure-provisioning-tools","content":"對於基礎建設來說，GitOps 領域中最成熟的使用工具則是 Terraform。其能夠成為這個領域的佼佼者主要依賴於其乾淨且簡單的宣告式語法，讓使用者可以順利的專注於基礎架構的建置。 相對於 Terrafrom 來說，不同的公有雲廠商都有推出針對自己環境的工具，譬如 AWS 的 Cloud Formation, Azure 的 Azure Automation 或是 Goolge 的 Deployment Manager。 這些工具都專注於其開發廠商，然而 Terraform 則透過套件系統讓其能夠支援的環境非常多樣化且具有發展性。 最後，Kubernetes 本身則是透過 ClusterAPI 這個專案來達成 GitOps 版本的基礎架構交付工作，透過這個專案我們可以用 Kubernetes 內習慣的宣告式語言來定義 Kubernetes Cluster 並創建。 過往的狀態下，使用者都是基於各家廠商自有的 API 去開發相關的工具，更多情況下則是手動操作。 但是透過 ClusterAPI 的發展，我們可以利用 Kubernetes 本身的機制來幫我們創建與管理其他的 Kubernetes 叢集，只要透過 ClusterAPI 去規範與定義目標狀態即可。 ","version":"Next","tagName":"h2"},{"title":"Curated GitOps Products​","type":1,"pageTitle":"[書本導讀]-GitOps工具的選擇","url":"/docs/techPost/2020/gitops-book-ch4#curated-gitops-products","content":"因為 DevOps 這個領域實在太深太廣，工具的選擇完全沒有結束的一天，大家總是針對不同的環境下，對不同的工具進行優缺點的比較。基於此概念下，愈來愈多全新的開放原始碼軟體就被發展了，其中最重要的就是 Jenkins X. JenkinsX 其目的並不是要取代 Jenkins，其本身運行在 Kubernetes 叢集內，透過與其他工具譬如 Skaffold,Tekton,Lighthouse, Kaniko 的整合來打造一個基於 GitOps 架構的 Cloud Native CI/CD 工具。 過去數年來， GitLab 也發展了不少的工具來強化他們本身與 Kubernetes 的整合，提供更多針對容器環境的自動化的測試與部署。 藉由 Gitlab Autodevops 的幫忙，你可以享受到程式碼的安全性掃描，容器的安全性掃描，動態以及靜態的應用程式安全性測試，不同的部署策略以及更多。 GitOps Tooling Summary 以下是上述工具的總結  GitOps 'Push' Tools GitLab CI/CD + Kubectl/HelmGitHub Actions + Kubectl/HelmKubestack  GitOps 'Pull' Tools ArgoCDFlux  GitOps Infrastructure Provisioning Tools TerraformPulumi  Curated GitOps Productis: Jenkins X 這章節就是很粗略的介紹目前 GitOps 中的一些可用工具，當然實際上有各式各樣的工具可以使用，同時也會有更多新的工具被發展出來，解決出不同的問題。此外工具的整合也是進行式，特別是 ArgoCD 與 Flux 於 2019 宣布要加入此專案Argo Flux，後續的發展指日可待。 ","version":"Next","tagName":"h2"},{"title":"[書本導讀]- GitOps實作上的挑戰","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops-book-ch5","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"SealedSecrets​","type":1,"pageTitle":"[書本導讀]- GitOps實作上的挑戰","url":"/docs/techPost/2020/gitops-book-ch5#sealedsecrets","content":"Bitnami Implementation ","version":"Next","tagName":"h2"},{"title":"Storing Encrpyted Secrets directly in your source reposiroty​","type":1,"pageTitle":"[書本導讀]- GitOps實作上的挑戰","url":"/docs/techPost/2020/gitops-book-ch5#storing-encrpyted-secrets-directly-in-your-source-reposiroty","content":"git-secretgit-cryptBlackBox ","version":"Next","tagName":"h2"},{"title":"Storing secrts with source control separately from source​","type":1,"pageTitle":"[書本導讀]- GitOps實作上的挑戰","url":"/docs/techPost/2020/gitops-book-ch5#storing-secrts-with-source-control-separately-from-source","content":"GitLab protected variables ","version":"Next","tagName":"h2"},{"title":"Storing encrypted secrets with your source-control tool separately from source​","type":1,"pageTitle":"[書本導讀]- GitOps實作上的挑戰","url":"/docs/techPost/2020/gitops-book-ch5#storing-encrypted-secrets-with-your-source-control-tool-separately-from-source","content":"GitHub encrypted secrets ","version":"Next","tagName":"h2"},{"title":"Storing secrets with your cloud vendor in a secrets-management system​","type":1,"pageTitle":"[書本導讀]- GitOps實作上的挑戰","url":"/docs/techPost/2020/gitops-book-ch5#storing-secrets-with-your-cloud-vendor-in-a-secrets-management-system","content":"AWS Secrets Manager[Google Cloud Secets Manager])(https://cloud.google.com/secret-manager)Azure Key Vault ","version":"Next","tagName":"h2"},{"title":"Integrating with a third party secrets-management tool​","type":1,"pageTitle":"[書本導讀]- GitOps實作上的挑戰","url":"/docs/techPost/2020/gitops-book-ch5#integrating-with-a-third-party-secrets-management-tool","content":"Hashicorp VaultMozilla SOPS 上述只是粗略的列出了跟安全性相關的專案，每個專案的用法與情境都截然不同。對於團隊來說，要花多少心力於這些安全性解決方案上取決於你們團隊對於安全性的重視度有多高及需求有什麼。 此外，最好定期檢視這些安全性的設定來確保一切設定都符合安全需求。 Staffing 第二個問題與第一個問題息息相關，要找到一位對 Cloud Native 相關技術有經驗的人已經屬實困難，更何況要找到一位還要對 GitOps 熟悉有經驗的負責人。除此之外，要如何將這些概念擴散到整個團隊，讓團隊成員有相同的能力與背景共同處理這些流程，其難度更高。 一個比較可行的做法是透過學習的方式，讓員工之間花時間去學習分享與研究，藉此降低每個人之間對於 GitOps 認知的鴻溝，最後讓彼此都能夠掌握整個系統 Regulatory and Legacy Tooling/Processes GitOps 本身對於有監管需求的團隊來說是非常值得嘗試的，透過公開透明的檢查及瀏覽機制，可以讓團隊更能夠有效率地去知道什麼時候被修改，誰進行了修改，什麼內容被修改。 GitOps 下的做法相對簡單，只要將其與現有的技術與工具整合。舉例來說，如果今天團隊內使用 LDAP 或 AD 這種權限控管工具，可以很輕鬆的將其與 GitLab 進行整合。這樣就可以透過 LDAP/AD 來限制員工的權限，什麼群組的人可以觀看什麼 Repo，進行什麼操作。 這種概念的整合是個非常有效率的做法，特別是對大型的組織來說，能夠用這種整合的方式直接把現有的規則與政策都直接套用到新產品架構上，而不用重新打造一特全新作法。 然而，GitOps 的這些作法再某些領域上可能不會這麼順利，特別是跟些已經存在的工具有相反思維時。這邊就以 Pull Request 這種工作流程為範例，如果過去的開發習慣是手動硬上且一個帳號專門使用的系統，那就與 GitOps 的流程非常不和，因為沒有辦法做到稽核的效果。 另一方面，對於一些資深且不熟悉 Git 操作的管理人員來說，要其登入 Git 並且發送 Pull Request 可能會有些操作上的困難。 最後導致的就是，當你導入 GitOps 到團隊時，為了配合團隊舊有的工具或是稽核流程， GitOps 本身所強調的特性可能會被犧牲一些。 你甚至可能要花一些時間來研究現有機制，並且想辦法說服上層説為什麼採用全新的 GitOps 流程會是更好的解決方案。 這一切都沒有標準解答，完全是看各團隊到底習慣用什麼，怎麼用符合大家需求 Time to Market GitOps 方式帶來的好處非常容易透過白板解釋給技術背景的員工，但是要將其實踐並且整合則需要花費不少時間。 整個實踐過程需要仰賴非常紀律的方式去部署應用程式，手動介入的操作都要盡量避免，所有的測試都要寫好寫滿來確保工作流程。 事實上這種紀律的要求不是只有 GitOps 流派下才會需要，不論是測試驅動開發或是 DevOps 等都需要一定程度的紀律與準則來要求整個團隊。 這些紀律短期上可能看不出好處，但是其效益都是為了長期所打算的，當然這樣的做法帶來的缺點就是如果你要向上層展示其好處與優點，短期內可能很難展現。 上述提到的所有挑戰與困難都會增加團隊產品與市場接軌的時間，這可能會是 GitOps 實作上最大的挑戰。當面對一些真實市場的壓力與需求，團隊可能會傾向使用舊方法來處理產品部署的方式。 作者認為如果你沒有堅持下去，而是放棄使用過去的舊方法，那隨者時間久了，有一些競爭對手開始享受到 GitOps 帶來的長期效益時，這時候你的團隊在各方面就會追不上對方。 ","version":"Next","tagName":"h2"},{"title":"[書本導讀]-GitOps後續","type":0,"sectionRef":"#","url":"/docs/techPost/2020/gitops-book-ch6","content":"","keywords":"gitops pros and cons","version":"Next"},{"title":"Emergent Patterns​","type":1,"pageTitle":"[書本導讀]-GitOps後續","url":"/docs/techPost/2020/gitops-book-ch6#emergent-patterns","content":"基於 GitOps 這種操作方式，也有一些新的想法被逐漸提出來探討如何將 GitOps 給套用真實世界的應用場景。一個常見的範例就是將不同環境的流水線給分開獨立，當今天有任何更動時，可以先於A環境進行測試，都沒有問題時則自動的套用到B環境。 一個實際的範例就是 kubestack 所推出的 cluster_pair 模型，可以讓一個 Repo 內同時管理多個 Cluster。 另外一個有趣的模式則是如何有效的將基礎架構(infrastructure)與應用程式(applications)所使用到的狀態檔案分開成不同的 Git repo。 這兩個 repo 本身的層級不同，存取權限也不同，設計與實作上則會有諸多考量要處理，同時也要確保任何新的應用程式都可以順利部署不會出錯。 作者認為目前上述的一些模式都還比較屬於早期階段，更多都是手動創建與處理，期望有一天能夠有愈來愈多的解決方案將這些模式給實作並整合近來。 Conclustion 接下來針對這六篇文章進行個總結 GitOps 這個概念起源自 2017 年，此後就一直有者良好的發展 GitOps 本身圍繞者三個主要核心概念 透過單一控管的方式來維護歷史紀錄與操作透過宣告式的方法來定義資源的期望狀態透過 Control Loop 的方式來同步資源狀態 目前市面上有一些軟體非常適合組合出 GitOps 的解決方案 GitTerraformKubernetesFlux 直到現在，有愈來愈多的組織與廠商開始嘗試 GitOps 這樣的操作，雖然其工具與產品都還在處於非常早期階段，我們還是可以未來期待會有愈來愈多的選擇。 接下來我們則是可以期待去看看 GitOps 是否可以在這個市場上存活，畢竟 GitOps 嘗試解決許多過去的部署問題，譬如 Control: Zero-touch 的系統讓你能夠去限制誰能夠對你的系統做什麼Audit: 版本控制加上宣告式的格式可以讓你知道是誰做的，做了什麼，什麼時候做的Inventory: 宣告式的做法可以讓你確保你的系統於任意時間點應該要長怎樣 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2020/ipvs-1","content":"","keywords":"linux ipvs","version":"Next"},{"title":"創建 Virtual Service​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-1#創建-virtual-service","content":"$ ipvsadm -A virtual-service [-s scheduler] [-p [timeout]] [-M netmask] [--pe persistence_engine] [-b sched-flags] virtual-service: --tcp-service|-t service-address service-address is host[:port] --udp-service|-u service-address service-address is host[:port] --sctp-service service-address service-address is host[:port] --fwmark-service|-f fwmark fwmark is an integer greater than zero  ipvsadm 提供了眾多指令用來管理，其中 -A(add) 可以用來創建一組服務 Virtual Service, 大部分的 Virtual Service 都是由 VIP:Port + Layer 4 協定所組成，目前共支援 tcp(-t),udp(-u),(sctp) 這三種協定，此外由於跟 Linux Kernel 綁定，所以其實也可以透過 sk_buff 內的 mark 來進行處理。 $ sudo ipvsadm -A -t 172.17.8.101:80  上述指令會在系統中創建一個 virtual service, 其 VIP是 172.17.8.101 並且聽在 port 80 上 接下來可以透過 -a 的方式針對已經存在的 virtual service來加入 real servers k8s-dev:06:39:18 [~]root $sudo ipvsadm -a -t 172.17.8.101:80 -r 172.18.0.2 -m k8s-dev:06:39:24 [~]root $sudo ipvsadm -a -t 172.17.8.101:80 -r 172.18.0.3 -m  其中 -m 的部分主要是設定 director 如何跟 real server 溝通， 這部分牽扯到不同的網路運作模式，本章節不進行太詳細探討，主要是先觀察 service 與 real server 的運作關係。 接下來可以透過 -l(-L) 來觀察相關的資訊 $sudo ipvsadm -L IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 172.17.8.101:http wlc -&gt; 172.18.0.2:http Masq 1 0 0 -&gt; 172.18.0.3:http Masq 1 0 0  上述可以看到幾個資訊 Service VIP (172.17.8.101), port =80 (HTTP port)Real Servers 172.18.0.2:80172.18.0.3:80 目前沒有特別設定，所以上述兩個 Real Server 的權重一樣，這意味 Director 分配時會基於 50:50 去分配 Experiment 接下來我們會透過下列的架構來進行實驗，該架構中我們會 創立一個 Service透過 Docker 的方式建立多個 Real Server 並且加入到上述的 Service 中透過 curl 指令觀察結果嘗試改變 Real Servers 的權重，並且再次觀察結果 創建一個 Service, 該 IP 172.17.8.101 真實存在於系統上 ipvsadm -A -t 172.17.8.101:80  接下來透過 Docker 的方式創建兩個 Real Server，這邊採用 Nginx 並且配置不同的首頁內容 sudo mkdir -p /nginx/A /nginx/B echo &quot;This is A&quot; &gt; /nginx/A/index.html echo &quot;This is B&quot; &gt; /nginx/B/index.html docker run --rm -d -v &quot;/nginx/A:/usr/share/nginx/html&quot; --name nginx-A nginx docker run --rm -d -v &quot;/nginx/B:/usr/share/nginx/html&quot; --name nginx-B nginx  接下來取得上述 docker container 的 IP 地址，並且加入到 ipvs service 中 IP_A=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' nginx-A) IP_B=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' nginx-B) ipvsadm -a -t 172.17.8.101:80 -r $IP_A -m ipvsadm -a -t 172.17.8.101:80 -r $IP_B -m  接下來我們可以透過 curl 的方式來透過 IPVS 來存取背後的 Nginx Docker Container k8s-dev:06:00:35 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:00:39 [/home/vagrant]root $curl 172.17.8.101 This is A k8s-dev:06:00:39 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:00:39 [/home/vagrant]root $curl 172.17.8.101 This is A k8s-dev:06:00:39 [/home/vagrant]root $curl 172.17.8.101 This is B ... $ipvsadm -L -n --stats --rate IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes -&gt; RemoteAddress:Port TCP 172.17.8.101:80 13 85 45 5396 4617 -&gt; 172.18.0.2:80 6 40 20 2548 2052 -&gt; 172.18.0.3:80 7 45 25 2848 2565  預設情況下是 50:50 的權重分配，所以理論上兩者個 conns 比例應該要差不多 接下來嘗試改變看看權重 k8s-dev:06:06:44 [/home/vagrant]root $ipvsadm -e -t 172.17.8.101:80 -r $IP_B:80 -m -w 9 k8s-dev:06:09:50 [/home/vagrant]root $ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 172.17.8.101:80 wlc -&gt; 172.18.0.2:80 Masq 1 0 0 -&gt; 172.18.0.3:80 Masq 9 0 0  這時候權重變成 1:9 了，透過 curl 繼續瘋狂敲打看看 k8s-dev:06:10:25 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:13 [/home/vagrant]root $curl 172.17.8.101 This is A k8s-dev:06:12:13 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:13 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:14 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:14 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:14 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:14 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:14 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:14 [/home/vagrant]root $curl 172.17.8.101 This is B k8s-dev:06:12:15 [/home/vagrant]root $ipvsadm -L -n --stats --rate IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes -&gt; RemoteAddress:Port TCP 172.17.8.101:80 46 316 210 20180 21546 -&gt; 172.18.0.2:80 10 68 40 4340 4104 -&gt; 172.18.0.3:80 36 248 170 15840 17442  這時候再次觀察比例，會發現 B 明顯比 A 還要更多 Summary 本文透過一個非常簡單也非常簡略的方式去介紹 IPVS 的用途以及用法，實際上背後還有很多原理需要理解，包含 Director 如何跟 Real Server 溝通，不同的 Load Balancing演算法, 底層如何實現以及要如何跟 Kubernetes 整合。 接下來的文章會嘗試一一解答上述的問題 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2020/ipvs-2","content":"","keywords":"linux ipvs","version":"Next"},{"title":"ClusterIP/NodePort​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-2#clusteripnodeport","content":"vagrant@k8s-dev:~$ kubectl apply -f network-study/ipvs/service.yml -f network-study/ipvs/hello.yml vagrant@k8s-dev:~$ kubectl get svc,ep NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cluster-demo ClusterIP 10.97.35.96 &lt;none&gt; 80/TCP 25s service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 18m service/nodeport-demo NodePort 10.100.61.228 &lt;none&gt; 80:31543/TCP 25s NAME ENDPOINTS AGE endpoints/cluster-demo 10.244.0.4:8080,10.244.0.5:8080,10.244.0.6:8080 25s endpoints/kubernetes 10.0.2.15:6443 18m endpoints/nodeport-demo 10.244.0.4:8080,10.244.0.5:8080,10.244.0.6:8080 25s  先確認一下 ClusterIP 使用的是 10.97.35.96NodePort 使用的是 10.100.61.228 同時使用的 Port是 31543。 由於輸出過長，我們就忽略 Real Servers 的部分，專心看 IPVS Service。 vagrant@k8s-dev:~$ sudo ipvsadm -Ln | grep rr TCP 10.96.0.1:443 rr TCP 10.96.0.10:53 rr TCP 10.96.0.10:9153 rr TCP 10.97.35.96:80 rr TCP 10.100.61.228:80 rr TCP 127.0.0.1:31543 rr TCP 172.17.8.111:31543 rr TCP 172.18.0.1:31543 rr TCP 10.0.2.15:31543 rr TCP 10.244.0.0:31543 rr TCP 10.244.0.1:31543 rr UDP 10.96.0.10:53 rr  首先 ClusterIP 的部分非常簡單，就 TCP 10.97.35.96:80 rr 一個規則而已，但是對於 NodePort 來說，這邊則是要針對系統上全部的 IP 都去設定，所以會看到總共有六個 IPVS Service，分別對應系統上六個IP，且都指向 31543 這個連接埠。 TCP 127.0.0.1:31543 rr TCP 172.17.8.111:31543 rr TCP 172.18.0.1:31543 rr TCP 10.0.2.15:31543 rr TCP 10.244.0.0:31543 rr TCP 10.244.0.1:31543 rr  藉由上述的觀察，我們可以知道 IPVS 目前創造的規則如同下方所述，首先讓我們假設該 Serivce 有開放 n 個連接方式(L3+L4)： 對於 ClusterIP 會創造 n 個 IPVS Service對於 NodePort 來說, 對於系統上每一個網卡，都會創造 n 個 IPVS Service，所以假如系統中有五個對外 IP, 那就會有 5*n 個 IPVS Service。 ","version":"Next","tagName":"h2"},{"title":"IPSET​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-2#ipset","content":"除了 IPVS Service 以及 IPVS Real Servers 的組合外，kube-proxy(IPVS) 本身也會透過 IPSET 來輔佐整個網路連線的處理，舉例來說： 防火牆SNAT (Masquerade) 這兩個功能還是要依賴 IPtables 來幫忙完成，但是這邊為了讓 IPtables 的規則盡量得少，不想要每一個 IP 就一條規則，進而提升整體規則的匹配效能，於是採用了 IPSET 的方式來幫忙處理。 用一個最快速的方式來講就是將一堆 IP:PORT 透過不同的方式放到一個 SET 裡面，而 IPTABLES 本身就針對這個 SET 去比較。 直接以下列範例來看 vagrant@k8s-dev:~$ sudo ipset list KUBE-NODE-PORT-TCP Name: KUBE-NODE-PORT-TCP Type: bitmap:port Revision: 3 Header: range 0-65535 Size in memory: 8268 References: 1 Number of entries: 1 Members: 31543 vagrant@k8s-dev:~$ sudo ipset list KUBE-CLUSTER-IP Name: KUBE-CLUSTER-IP Type: hash:ip,port Revision: 5 Header: family inet hashsize 1024 maxelem 65536 Size in memory: 472 References: 2 Number of entries: 6 Members: 10.96.0.10,udp:53 10.96.0.10,tcp:53 10.96.0.10,tcp:9153 10.97.35.96,tcp:80 10.100.61.228,tcp:80 10.96.0.1,tcp:443  上面可以看到兩組 IPSET，其中第一組是針對 NODE PORT 去使用，其型態為 bitmap:port，這部分只針對 Port 去比對，所以裡面可以看到 31543 而已 至於第二組則是針對所有的 ClusterIP 去使用，他的型態則是 hash:ip,port，所以每個資料都是 IP:Port 的規則，可以看到我們之前用到的全部 ClusterIP:Port 都在裡面。 有了這兩組 IPSET 後，我們稍微看一下 IPTABLES 會怎麼使用 IPSET KUBE-NODE-PORT-TCP 來減少需要的規則數量。 ","version":"Next","tagName":"h2"},{"title":"NODE-PORT​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-2#node-port","content":"vagrant@k8s-dev:~$ sudo iptables-save | grep KUBE-NODE -A KUBE-NODE-PORT -p tcp -m comment --comment &quot;Kubernetes nodeport TCP port for masquerade purpose&quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT  先觀察的第一個規則是 KUBE-SERVICES，本身使用 --dst-type LOCAL 來判斷封包是不是針對 LOCAL (本地網卡)是的話就跳到KUBE-NODE-PORT 去二次處理。KUBE-NODE-PORT 裡面透過 -m set --match-set KUBE-NODE-PORT-TCP dst 來判斷封包的目標 Port 有在 KUBE-NODE-PORT-TCP 這個 ipset 裡面，就去弄 KUBE-MARK-MASQ 相關的動作。 因為最外層已經透過 dst-type LOCAL 來判斷是不是送往本地介面，所以這邊的 IPSET 只需要處理 Port 即可，就不用管 IP。 更多關於 IPSET 的使用意思以及使用情境可以參考官方文件 ","version":"Next","tagName":"h3"},{"title":"Dummy Interface​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-2#dummy-interface","content":"最後最後，我們來探討一個有趣的事情，這些 ClusterIP 本身都是一個不存在的IP, 那我們到底是如何成功地讓封包被 IPVS 接手處理的？ vagrant@k8s-dev:~$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cluster-demo ClusterIP 10.97.35.96 &lt;none&gt; 80/TCP 63m kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 81m nodeport-demo NodePort 10.100.61.228 &lt;none&gt; 80:31543/TCP 63m vagrant@k8s-dev:~$ curl 10.97.35.96 ... ...  譬如上述，我們可以在節點內直接用 curl 的方式透過 clusterIP 去存取服務，可是之前在探討 IPVS 的時候，那時候你的 Service IP(VIP) 必須要真實存在才可以處理。 而這些 ClusterIP 本身又不在系統之中，那到底怎麼辦？ 為了解決這個問題，kube-proxy於系統內偷偷創造了一個 dummy interface kube-ipvs0，並且把所有的 clusterIP 都設定到該網卡上面了，可以用下面的指令觀察到： vagrant@k8s-dev:~$ ip link show type dummy 5: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default link/ether 62:c9:fc:74:c4:f8 brd ff:ff:ff:ff:ff:ff vagrant@k8s-dev:~$ ip addr show dev kube-ipvs0 5: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default link/ether 62:c9:fc:74:c4:f8 brd ff:ff:ff:ff:ff:ff inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.97.35.96/32 brd 10.97.35.96 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.100.61.228/32 brd 10.100.61.228 scope global kube-ipvs0 valid_lft forever preferred_lft forever  上述的資料有一個要注意的就是，其狀態 DOWN, 這意味該 Interface 本身不是一個可運作的狀態，單純只是一個沒有被叫起來運作的 Interface。 實際上 dummy interface 本身跟 IPVS 的協同合作只有一個目的，讓封包可以往 Kernel 送，只要封包可以順利送進去，接下來就可以被 IPVS 給接手處理 這部分的原理要等到下篇文章從 kernel + netfilter 看起才比較好說明原理 這邊直接快速用一個實驗來驗證上面推論 ","version":"Next","tagName":"h2"},{"title":"Experiment​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-2#experiment","content":"我們把該 Interface kube-ipvs0 叫起來隨便捏造一個假的 IP, 並且設定一條靜態路由指向 kube-ipvs0手動用 ipvsadm 根據上面假的 IP 去創造一個新的 IPVS Service手動將我們的 pod IP 加入到上述創造的 IPVS Service透過 Curl 去連接我們創造的假 IP 假設我們捏造一個 1.2.3.4/32 的 IP 地址，然後後端的 POD IP:PORT 是 10.244.0.4:8080,10.244.0.5:8080,10.244.0.6:8080。 sudo ifconfig kube-ipvs0 up sudo ip route add 1.2.3.4/32 table local dev kube-ipvs0 sudo ipvsadm -A -t 1.2.3.4:80 sudo ipvsadm -a -t 1.2.3.4:80 -r 10.244.0.4:8080 -m sudo ipvsadm -a -t 1.2.3.4:80 -r 10.244.0.5:8080 -m sudo ipvsadm -a -t 1.2.3.4:80 -r 10.244.0.6:8080 -m curl 1.2.3.4  執行完 curl 就會順利的存取到後面伺服器的網頁內容，但是要注意的是這邊因為 IPVS 會根據 SyncPeriod 的設定定期去更新規則，所以上述創造的規則放一段時間就會被刪除 根據這個實驗可以驗證我們的猜想，其實 kube-ipvs0 這個 interface 本身根本不需要有任何的 IP Address，其目的只是一開始產生的 IP address 能夠產生一個對應的 Route Entry，把封包往系統內送，當封包走到系統內後，便會與 Netfilter 交互作用將封包轉接給 IPVS 的底層實作去處理，這時候就會根據 IPVS 的 Service 來決定是否有匹配的資料並且將其轉發到後端伺服器。 下篇文章就會來開始探討到底 IPVS 與 IPTables 的差異在哪裏，並且嘗試解釋上面的推論過程其背後的實作原理 ","version":"Next","tagName":"h3"},{"title":"鐵人賽系列文章 - DevOps 與 Kubernetes 的愛恨情仇","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd","content":"鐵人賽系列文章 - DevOps 與 Kubernetes 的愛恨情仇 這次的鐵人賽的題目則著重於 Kubernetes 與 DevOps 之間的關係， DevOps 這詞發展多年以來，似乎成為一個顯學 每間公司都會朗朗上口需要招聘專門負責 DevOps 的人，工作內容百百種，實在讓人難以一言就斷定到底什麼是 DevOps。 本次系列文則不會針對 DevOps 去進行細部探討，到底 DevOps 的日常生活中可能會有哪些事情要處理，相反的 本系列文主要會針對 CI/CD 的流程去探討，看看當 CI/CD 與 Kubernetes 整合的過程中，要怎麼處理 舉例來說，一個最大的差別就是當所有應用程式都容器化後，要如何透過 CI/CD 流水線將新版本部署到 Kubernetes 之中 這中間有哪些議題可以探討，針對不同議題有哪些解決方案可以使用，以及這些解決方案彼此的優缺點 下圖是一個參考流程，從開發階段到部署階段中， Kubernetes 可能會扮演哪些角色，而 CI/CD 流水線則會跟哪些角色有所互動 上次的流程中有非常多的環節可以探討，每個環節中都有不同的解決方案與取捨，這些歡節包含 Kubernetes 內的應用程式該如何包裝? 原生 Yaml 還是 Helm?本地開發者需要 Kubernetes 來測試 Kubernetes 嗎?CI 流水線系統要選擇哪一套? 流水線工作要如何被觸發？CI 流水線過程中，需要 Kubernetes 來測試應用程式嗎 ?Container Registry 要使用雲端服務還是要自架，自架的話該怎麼使用以及如何與 Kubernetes 整合CD 流水線系統要選擇哪一套? 流水線工作要如何被觸發?CD 流水線過程中，要怎麼將應用程式更新到遠方 Kubernetes? 雲端架構或是地端架構會有什麼差異CD 更新過程中，如果有機密資料，該怎麼處理 接下來的章節會針對上述環節進行介紹，每個環節都會透過下述流程來介紹 概念介紹相關專案介紹使用範例 再次強調，上面的流程圖不是一個唯一的流程圖，而是一個範例，真正的運作流程都會根據不同的環境與需求而有所差異 但是選擇跟設計解決方案的思路則是不變的，透過培養思考的能力，才能夠遇到任何環境都有辦法構建出一套符合的解決流程。","keywords":"","version":"Next"},{"title":"CI 與 Kubernetes 的整合","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-10","content":"","keywords":"","version":"Next"},{"title":"遠方固定一個 Kubernetes 叢集​","type":1,"pageTitle":"CI 與 Kubernetes 的整合","url":"/docs/techPost/2020/iThome_Challenge/cicd-10#遠方固定一個-kubernetes-叢集","content":"架構概念如下，這情境下會有一個遠方的 Kubernetes 叢集，我們希望所有的 CI 測試都會使用這個遠方的 Kuberentes 叢集。 同時，我們系統中會有兩個 Job(假設多個開發者同時開發，各自的修改都會觸發 Pipeline 去執行)，每個 Job 中都會有很多個 Stage 要執 行，其中最重要的 Testing 我們會希望將應用程式部署到 Kubernetes 內去測試。  這種狀況下就會有一些問題產生，譬如 每次的測試是否有完整的清理資源，確保系統資源測試前後一致(我認為這是很重要的一點，任何的測試都不應該殘留資源於系統上，導致二次測試失敗)如果有多個工作同時要使用該 Kubernetes，是否會有衝突? 雖然可以透過 namespace 來區分，但是 Kubernetes 內有些資源是沒有 namespace 概念的，譬如 PV為了讓 Pipeline 有能力存取 Kubernetes，勢必要把 KUBECONFIG 等資訊存放到 pipeline 系統中，這對很多人來說是個安全性的隱憂，畢竟只要讓 KUBECONFIG 流出去，其他人就有能力操控你的 Kubernetes，如果權限弄得不好甚至可以搞壞整個 Kubernetes 叢集。 這種架構的好處就是， pipeline 系統內只要專注處理如何測試，這些 pipeline 到底是運行在 VM 或是 Container 上都沒有關係，只要能夠透過 kubectl/helm 等指令存取遠方 Kubernetes 叢集即可。 此外，如果測試過程中發現任何錯誤，我們都可以直接到遠方的 Kubernetes 去檢查失敗後的環境，來釐清到底為什麼會測試失敗 ","version":"Next","tagName":"h2"},{"title":"CI 過程動態產生 Kubernetes 叢集​","type":1,"pageTitle":"CI 與 Kubernetes 的整合","url":"/docs/techPost/2020/iThome_Challenge/cicd-10#ci-過程動態產生-kubernetes-叢集","content":"這種架構與上述不同，主要的差異是該 Kubernetes 叢集並非固定的，而是於 pipeline 過程中動態產生  這種架構下來我們來看看到底有什麼樣的好壞 由於 Kuberentes 都是獨立產生，每個 Job 都會有自己的 Kubernetes，所以彼此環境不衝突，甚至也不用擔心資源沒有清理乾淨，因為每次測試都是全新的環境也因為 Kubernetes 是獨立且動態的， KUBECONFIG 是動態產生，所以不用擔心會有額外的安全性問題 但是這種架構下也會有其他的缺點 如果今天測試失敗時，可能這個 Kubernetes Cluster 就被移除了，導致沒有相關的環境可以用來釐清出錯的原因，變得更難除錯有些測試需要一些前置作業，這些前置作業會不會不好處理，譬如需要一個額外的檔案系統，額外的環境架設pipeline 環境中要思考如何架設 Kuberentes，如果你的 pipeline 環境是基於 docker, 那就要思考如何在 docker上創建 kubernetes，這部分還要考慮使用的 pipeline 系統有沒有辦法做到這些事情。 這兩種架構各自有其優缺點，並沒有絕對的對錯，接下來我們會嘗試使用第二種架構，於 GitHub Action 中去創建一個 Kuberentres Clsuter，並且透過 Kubectl 指令來確認該 Kubernetes 叢集是運作正常的 ","version":"Next","tagName":"h2"},{"title":"GitHub Action & Kubernetes​","type":1,"pageTitle":"CI 與 Kubernetes 的整合","url":"/docs/techPost/2020/iThome_Challenge/cicd-10#github-action--kubernetes","content":"Github Action 中有非常豐富的 Plugin，其實可以查到有非常多的 action 再幫忙創建 Kubernetes 叢集，譬如 1.action-k3s 2.kind 3.setup-minikube 可以直接到 Github Action Marketplace 去搜尋就可以看到滿多跟 Kubernetes 相關的範例。 由於之前的章節中我們介紹過用 KIND 與 K3D 來部署本地的 Kubernetes，那這次我們就嘗試使用 K3S 來部署看看 Kubernetes。 ","version":"Next","tagName":"h2"},{"title":"使用​","type":1,"pageTitle":"CI 與 Kubernetes 的整合","url":"/docs/techPost/2020/iThome_Challenge/cicd-10#使用","content":"這邊不會介紹太多關於 GitHub Action 的詳細用法，有興趣可以參考官網教學，其實非常簡單，每個 GitHub Repo 只要準備一個檔案就可以設定。 於專案中的下列資料夾中 .github/workflows ，準備一個名為 main.yml 的檔案，其內容如下 # This is a basic workflow to help you get started with Actions name: CI # Controls when the action will run. Triggers the workflow on push or pull request # events but only for the master branch on: push: branches: [ master ] pull_request: branches: [ master ] # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called &quot;build&quot; build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 - uses: debianmaster/actions-k3s@master id: k3s with: version: 'v1.18.2-k3s1' - run: | kubectl get nodes kubectl version  基本就是一個最基本的 GitHub Action 範例，只是最後我們改成使用 k3s 的 GitHub Action ，根據 action-k3s 的描述，我們只要指定 k3s 的版本就可以獲得對應的 Kubernetes 版本，因此我們指定 v.18.2-k3s1。 最後我們補上兩個指令 kubectl get nodes 以及 kubectl version 來確保我們有在 GitHub Action 中獲得一個 Kubernetes 叢集並且可以操控。 這邊要注意的GitHub Action預設都是提供 Virtual Machine 供所有測試任務使用，所以我們可以相對簡單的於這個 VM 上面去運行相關的操作。反之如果今天提供的是 Container 為基底的環境，那要在上面再次安裝 Kubernetes 就不是這麼簡單了。 ","version":"Next","tagName":"h2"},{"title":"執行過程​","type":1,"pageTitle":"CI 與 Kubernetes 的整合","url":"/docs/techPost/2020/iThome_Challenge/cicd-10#執行過程","content":"下圖是執行過程，可以看到最上面是執行 actions-k3s 的內容，透過 docker 指令創建相關的 k3s Cluster，最後透過 kubectl 來觀看相關的內容，包含節點資訓以及對應的版本  到這邊為止我們就有辦法於 GitHub Action 中動態創立 Kubernetes 叢集了，如果有什麼測試都可以把這些部分整合到 GitHub action 中了。 ","version":"Next","tagName":"h2"},{"title":"初探 IPTABLES 流動之路 - 以 Docker 為範例","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iptables-1","content":"","keywords":"","version":"Next"},{"title":"User/Kernel​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#userkernel","content":"所有你可以直接在系統上操作的 iptables- 系列工具 (ebtables,arptables雷同)全部都是 User Space 的工具，其功能都是用來管理 規則, 但是規則真正運行被觸發的時機點都是在 Kernel 內。 實際上， iptables 會透過 getsockopt/setsockopt 等 IPC 方式與 Kernel 進行溝通，不論是讀取當前的規則，或是寫入新規則，這邊也有一個重要的概念就是 規則 本身不會被儲存，所以當機器重新開機的時候， kernel 內的規則就會全部消失，需要仰賴 userspace 的工具重新寫入規則到 kernel 內， 上述的架構其實也會讓整個 iptables 的觀察與管理變得相對困難，因為大部分情況下我們都是使用 iptables-* 系列工具來進行觀察與管理，而實際上封包到底怎麼流動，被哪些規則給丟棄，被哪些規則給修改，一切都是在 kernel 內進行。 這意味者我們必須要相信 iptablse 與 kernel 的溝通是沒有問題的，不然單純依靠 userspace 的工具來觀察結果其實是會有一些不確定性。 但是如果要觀察這一切的訊息都要去改 Kernel 來幫助除錯，這方面的工與精力也花費太大，所以一般情況下都還是基於 iptables 的規則來解讀封包與 iptables 之間的關係。 ","version":"Next","tagName":"h2"},{"title":"規則組成​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#規則組成","content":"大抵上， iptables/ebtables/arptables 的規則組成是有一訂的規範，一個使用範例如下 這個規範中可以分成四個部分 Chain: 最簡單的想法就是封包發生的時間點，譬如上述的 OUTPUT, INPUTTable: 一群相同規則的集合體，譬如 nat, filter, 該 table 內的規則都有類似的目的Match: 符合的條件，可以想成該規則要被觸發的條件，譬如上述的 ! -d 127.0.0.0/8 -m addrtype --dst-tppe LOCAL, 這部分可以是 iptables 內建的基本條件，或是透過 -m 來動態載入的其他的 moduleTarget: 當規則符合條件後，要做什麼事情，譬如上述的 -j DOCKER, --log-level debug, 如同 Match 一樣，有內建的 Target 外也可以動態載入其他 module 來處理。 所以今天如果要寫入一個 iptables規則，思路大概是 我想要撰寫一個規則，這個規則會在 什麼時間點 被觸發，什麼樣的封包 符合條件，最後要執行什麼動作. 而根據動作的類型再把這個規則放到對應的 Table 裡面。 ","version":"Next","tagName":"h2"},{"title":"觀察方式​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#觀察方式","content":"有了上面的概念後，本文就會使用 LOG 概念的 Target 來觀察封包流向，整個規則的含義就是 在什麼時間點,針對我們想要觀察的流向封包,輸出相關資訊。 藉由這些資訊，我們可以組合出封包於 iptables/ebtalbes/arptables 內的流向注意，這邊只能做到 iptables/ebtables/arptables 內的流向，其餘更細部的 network stack 處理則沒有辦法 EBTABLES EBTABLES 相對於 IPTABLES 來說比較陌生，主要是其運作的層次更低，基於 ethernet 來處理，一般使用情況下，大家都比較依賴 iptables，也是因為透過 IP 的描述方式會比使用 MAC Address 來得更佳容易記憶與管理。 即管如此，ebtables 的存在還是不可忽視，也許某一天你的應用場景就會需要使用到 ebtables 來管理 ","version":"Next","tagName":"h2"},{"title":"Chain​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#chain","content":"EBTABLES 內總共有六條 Chain，也就是六個時機點 INPUT: 根據 Forwarding table, 該封包要送到 Linux Bridge 前FORWARD: 根據 Forwarding table，該封包要被轉發到其他連接埠前 (不能是 Linux Bridge 本身，否則會走 INPUT)OUTPUT: 本地產生的封包，最後目的是 Linux Bridge 底下的連接埠，基本上與 FORWARD 非常相似，只是一個來源是其他人，一個是自己機器本身PRE-ROUTING: 我看過一些評論，跟我的想法滿類似的就是這邊準確的說應該是 PRE-FORWARDING, 因為這個層級我們不探討 IP，不會用 Routing 這個字，但是理論與實作本來就會有所差異，與 IPTABLES 共用相同的結構在實作上會比較簡單些。 這個時間點主要是用在剛剛從 Linux Bridge 相關連接埠收到封包後，還沒有透過 Forwarding table 決定目標前，這也是為什麼稱為 PRE- 的原因，主要都是用來修改封包內容 題外話：因為還沒有被 Forwarding Table 決定怎麼轉送，所以通常這時候都可以修改封包的目的地 POST-ROUTING: 基本上跟 PRE-ROUTING 是雷同的，概念改成 封包要從網卡出去前 會觸發的時間點，也會用來修改封包 題外話：因為還已經被 Forwarding Table 決定怎麼轉送，通常這時候不可以修改封包的目的地，但是可以修改封包的來源 BROUTING: 這是一個 ebtalbes 獨有的點，非常少用，觸發時間點非常早，封包收到後就會先進入到這邊去處理 這個時間點能做的事情只有 封包要不要直接送到 Layer3 去處理 的判斷。 以上就是 ebtalbe 的六個時機點，接下來看看有哪些 table 最後如何組合再一起 ","version":"Next","tagName":"h2"},{"title":"Tables​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#tables","content":"Table方面則是會有三個 Filter 用來過濾封包，可以決定要不要丟棄或是通過NAT 針對 PRE-ROUTING/POST-ROUTING 等時機點使用，去修改封包的 MAC AddressBrout 這個是非常特別，就針對 BROUTING 這個 chain 去使用而已 每個 Table 都有自己搭配的 Chain, 所有的規則都定義於 Linux Kernel 內，初始化這些 Table 的時候就會去決定有哪些 Chain 可以與之匹配。 ","version":"Next","tagName":"h2"},{"title":"Workflow​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#workflow","content":"將上述的 Chain 與 Table 結合後會得到這張圖片 圖片中先忽略所有 iptables 的處理，專心觀察 ebtables 的架構，圖中的 Bridging Decision 也就是本文上述提的 Forwarding table, 用來決定方向。 ","version":"Next","tagName":"h2"},{"title":"Lab​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#lab","content":"接下來要透過實際操作來驗證上述的行為，會使用類似於下面用法的規則來顯示資訊這樣當封包經過時就會將資訊吐出，可以透過 dmesg 來觀察 完整規則如下 ","version":"Next","tagName":"h2"},{"title":"Container to Container​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#container-to-container","content":"Contaienr to Contaienr 的方式非常簡單，就直接讓兩個 Container 透過 ICMP request/reply 來產生流量即可 sudo docker exec netutils ping 172.18.0.2 -c1  最後觀察 kernel log 會得到類似下列的輸出 將這些輸出結果與前面的流程圖合併後，可以得到下列的資訊 基本上 Container to Container 就只會經過這四個點 ","version":"Next","tagName":"h3"},{"title":"Host to Container​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#host-to-container","content":"Host to Container 也是使用 ICMP Request/Reply 來處理，不過是由 host 本身發起，所以概念如下 sudo ping 172.18.0.2 -c1  觀察 kernel log會得到類似下列輸出整理後會得到類似這種走向 可以看到 Host to Contaienr 則會走 output 出去，而 Container to Host 最後則是在 Bridging Decision 決策後走 INPUT 上去。 更多詳細的操作過程可以觀看影片，裡面有實際的 Demo 以及相關的操作。 IPTABLES IPTABLES 概念非常雷同，有四個 Table, 5個 Chain，此外還有 Conntrack(Connnection Track) 在輔佐幫忙。 這邊我們專注於 IPTABLES 本身， Conntrack 的概念就不紀錄太多，影片中有一些段落再介紹其概念與影響。 ","version":"Next","tagName":"h3"},{"title":"Chain​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#chain-1","content":"INPUTFORWARDOUTPUTPREROUTINGPOSTROUTING 這邊的概念跟 EBTALBE 完全一樣，只是 全部的 Forwarding Table 都要換成基於 IP 查詢的 Routing Table，並且沒有了 BROUTING 這個點。 ","version":"Next","tagName":"h2"},{"title":"Table​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#table","content":"除了原有了 Filter 以及 NAT 之外，多出了 RAW 以及 MANGLE 兩張 Table. RAW RAW 這個 Table 本身被呼叫的順序就很早，基本是被 Conntrack 處理前會呼叫的 TableMangel 這個可以用來進行一些修改封包的內容，譬如 mark 等之類的資訊 最常用的還是 filter 以及 NAT。 ","version":"Next","tagName":"h2"},{"title":"Workflow​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#workflow-1","content":"iptables 本身的流向簡化版本如下  概念跟 ebtables 非常像，這邊透過 Routing 來決定封包走向。 由於前面有 ebtables 的經驗，我們先將兩者合併的圖片展示，接下來再透過觀察的方式驗證這張圖片中的封包流向 ","version":"Next","tagName":"h2"},{"title":"Lab​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#lab-1","content":"觀察的方式跟 ebtable 一樣，透過 LOG 的方式來觀察封包，譬如 iptables -t mangle -I PREROUTING -p tcp -d 172.18.0.0/16 -j LOG --log-prefix '/iptable/mangle-PREROUTE' --log-level debug  完整的規則如下 insert() { iptables -t raw -I PREROUTING -p icmp -j LOG --log-prefix 'iptable/raw-PREROUTE' --log-level debug iptables -t mangle -I PREROUTING -p icmp -j LOG --log-prefix 'iptable/mangle-PREROUTE' --log-level debug iptables -t nat -I PREROUTING -p icmp -j LOG --log-prefix 'iptable/nat-PREROUTE' --log-level debug iptables -t mangle -I FORWARD -p icmp -j LOG --log-prefix 'iptable/mangle-FORWARD' --log-level debug iptables -t filter -I FORWARD -p icmp -j LOG --log-prefix 'iptable/filter-FORWARD' --log-level debug iptables -t mangle -I INPUT -p icmp -j LOG --log-prefix 'iptable/mangle-INPUT' --log-level debug iptables -t filter -I INPUT -p icmp -j LOG --log-prefix 'iptable/filter-INPUT' --log-level debug iptables -t raw -I OUTPUT -p icmp -j LOG --log-prefix 'iptable/raw-OUTPUT' --log-level debug iptables -t mangle -I OUTPUT -p icmp -j LOG --log-prefix 'iptable/mangle-OUTPUT' --log-level debug iptables -t nat -I OUTPUT -p icmp -j LOG --log-prefix 'iptable/nat-OUTPUT' --log-level debug iptables -t filter -I OUTPUT -p icmp -j LOG --log-prefix 'iptable/filter-OUTPUT' --log-level debug iptables -t mangle -I POSTROUTING -p icmp -j LOG --log-prefix 'iptable/mangle-POSTROUTE' --log-level debug iptables -t nat -I POSTROUTING -p icmp -j LOG --log-prefix 'iptable/nat-POSTROUTE' --log-level debug }  ebtables 的規則也一併保留一起使用，就可以觀察 iptables+ebtables 的流向 ","version":"Next","tagName":"h2"},{"title":"Contaienr to Container​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#contaienr-to-container","content":"指令如下 sudo docker exec netutils ping 172.18.0.2 -c 1  下面兩張圖就是去回流向，這邊有兩個點要注意 回來的封包不會進入到 NAT table, 主要是這些封包被 Conntion Track(Conntrack) 給處理過，接下來都不會進入 NAT 處理。ebtables 內會偷偷呼叫 iptables 來進行處理，這部分是個動態開關，可以透過/proc/sys/net/bridge/bridge-nf-call-iptables 來告訴 kernel 要不要偷偷呼叫 iptables。 這樣的好處就是對於 Contaienr to Container 的封包流向，可以使用 iptables 來管理，不然就單純只能使用 ebtables 來使用。 ","version":"Next","tagName":"h3"},{"title":"Host to Container​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#host-to-container-1","content":"執行指令 ping 172.18.0.2 -c 1  相關的 Log 請參閱影片結果，底下就紀錄最後結果 Host to Contaienr Container to Host ","version":"Next","tagName":"h3"},{"title":"Container to WAN​","type":1,"pageTitle":"初探 IPTABLES 流動之路 - 以 Docker 為範例","url":"/docs/techPost/2020/iptables-1#container-to-wan","content":"執行指令 sudo docker exec netutils ping 8.8.8.8 -c 1  相關的 Log 請參閱影片結果，底下就紀錄最後結果 Container to WAN WAN to Container Summary 經過這次的觀察，大致上的結論是 網路很難，除錯很麻煩IPTABLES 很難，除錯很麻煩Linux Kernel 很複雜，除錯很麻煩 如果除錯這麼難，那網路出問題到底該怎麼辦 網路基本概念的理解Linux 網路運作的理解TCPDUMP 等相關工具的使用檢查 IPTABLES/EBTABLES/ARPTABLES 等規則檢查 Routing Table/ARP Tables/Forwarding Tables 的紀錄相關資訊加上經驗整合，最後歸納出一套封包的走向，並且去驗證 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2020/ipvs-4","content":"","keywords":"linux ipvs","version":"Next"},{"title":"User Space​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-4#user-space","content":"就如同上述所說， user space 的工具就是 ipvsadm, 其本身都是透過 get/set socketopt 這個方式與底層溝通，其相關的原始碼可以參考這份專案 其中一個有趣的地方在於， ipvsadm 本身使用上不需要先自己安裝相關的 Kerenl Module，這部分是因為 ipvsadm 的原始碼裡面會先幫忙檢查當前系統是否已經存在對應的 ipvs.ko，如果不存在，則透過 modprobe 這個工具來安裝 ip_vs。 也是因為透過這個機制，大部分的 Linux 機器如果要成為 Kubernetes 節點並且啟動 IPVS 的功能，都不需要事先去安裝所需的 kernel module，對於系統設定來說省了一些工 以下內容都可以於 ipvsadm.c 中找到 int main(int argc, char **argv) { int result; if (ipvs_init()) { /* try to insmod the ip_vs module if ipvs_init failed */ if (modprobe_ipvs() || ipvs_init()) fail(2, &quot;Can't initialize ipvs: %s\\n&quot; &quot;Are you sure that IP Virtual Server is &quot; &quot;built in the kernel or as module?&quot;, ipvs_strerror(errno)); } /* warn the user if the IPVS version is out of date */ check_ipvs_version(); /* list the table if there is no other arguement */ if (argc == 1){ list_all(FMT_NONE); ipvs_close(); return 0; } /* process command line arguments */ result = process_options(argc, argv, 0); ipvs_close(); return result; }  static int modprobe_ipvs(void) { char *argv[] = { &quot;/sbin/modprobe&quot;, &quot;--&quot;, &quot;ip_vs&quot;, NULL }; int child; int status; int rc; if (!(child = fork())) { execv(argv[0], argv); exit(1); } rc = waitpid(child, &amp;status, 0); if (rc == -1 || !WIFEXITED(status) || WEXITSTATUS(status)) { return 1; } return 0; }  ","version":"Next","tagName":"h2"},{"title":"Kernel Space​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-4#kernel-space","content":"IPVS 最主要的功能都是由 ip_vs.ko 這個模組提供的，該模組的功能粗略可以分成三大項 處理與 UserSpace 的溝通，譬如提供接口供 Set/Get Socketopt 介面使用處理與 Schduleing Algorithm 的溝通，每個 Schduleing 本身都是一個獨立的 kernel module，譬如 ipvs_rr.ko ipvs.ko 本身也會自動插入對應的 kernel module，所以使用者並不需要事先安裝，只要確保系統上有相關的 kernel module 檔案，而這部分 Ubuntu 發行版本都有，其餘的發行版本則不確定 真正處理封包的核心邏輯，這部分分成 什麼時間處理封包 以及 怎麼處理封包 ipvs.ko 本身包含了眾多的 function 來解決 怎麼處理封包，這部分包含查找對應的資料結構，呼叫對應的 scheduling algorithm 選擇後端伺服器以及封包轉發netfilter 的架構則提供了 什麼時間處理封包，這部分與常見的 IPTABLES 時間點一樣，譬如 PREROUTING, POSTROUTING, INPUT, OUTPUT 等 實際上，(3)處理封包核心邏輯的實作則是將兩個概念結合， ipvs.ko 將處理封包的 function 透過 Linux Kernel 的介面去跟 netfilter 註冊，要求當封包經過特定的 HOOK 點的時候，呼叫相關的 function 來處理封包 下圖是一個基本的 IPTABLES/Netfilter 流程圖，基本上就是五個時間點，每個時間點內都會有不同對應的 TABLE，而這些 TABLE 內也有相對應的 Rules實際上的運作流程就是封包到達這些時間點後，會依序跳到個 TABLE 裡面，並且依序執行 RULES。 下圖則是將 IPVS 與 IPTABLES/Netfilter 結合後的圖片，差異性就是五個時間點內中有三個時間點變動，分別是 INPUT/OUTPUT/FORWARD.ipvs.ko 針對這三個時間點分別去註冊三個不同的 Function，而這些 Function 的執行點都不同。 INPUT: 註冊的 function 會在 Filter TABLE 後執行，而 function 完畢後則會跳去執行 NAT TABLE.OUTPUT: 註冊的 function 會在 nat TABLE 後執行，而 function 完畢後則會跳去執行 filter TABLE.FORWARD: 註冊的 function 會在 Filter TABLE 後執行，而 function 完畢後則會結束整個 Forward 時間點的處理，讓 Netfilter 繼續往下處理 這邊針對整個架構做一個總結 UserSpace 的工具會透過事先定義好的介面與 Kernel 溝通，而定義該介面的則是 ip_vs.ko 這個 Kernel Moduleip_vs.ko 做非常多事情，包含了 a. 定義封包處理的 function，並且透過 netfilter 框架註冊這些 function. 未來封包在 kernel 內遊走的時候就會被這些 function 處理 b. 根據參數設立相關的資料結構，包含有哪些 backend server, 採用哪套 scheduling 演算法 c. 根據需求動態載入對應 scheduling 的 kernel module. 接下來我們就會從原始碼的部分稍微看一下 ip_vs.ko 這些事情實際上是怎麼實作的 Source Code 原始碼的部分，基於 Linux Kernel 4.15 為基準去閱讀, 可以從 Github 或是 LXR 來進行線上閱讀 ","version":"Next","tagName":"h2"},{"title":"載入 Scheduling Kernel Module​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-4#載入-scheduling-kernel-module","content":"這部分的程式碼都在 ip_vs_sched.c 裡面，重要的有幾個 提供介面根據字串去找出對應的 Scheduler, 如果不存在，就嘗試幫忙載入提供介面給呼叫者去註冊，闡明自己是個 Scheduler 1)的部分主要是下列的程式碼，首先嘗試取得，如果取得失敗就去幫忙載入，並且再次讀取 /* * Lookup scheduler and try to load it if it doesn't exist */ struct ip_vs_scheduler *ip_vs_scheduler_get(const char *sched_name) { struct ip_vs_scheduler *sched; /* * Search for the scheduler by sched_name */ sched = ip_vs_sched_getbyname(sched_name); /* * If scheduler not found, load the module and search again */ if (sched == NULL) { request_module(&quot;ip_vs_%s&quot;, sched_name); sched = ip_vs_sched_getbyname(sched_name); } return sched; }  2)的部分如下，基本上就是一些資料結構的處理，呼叫者則會是那些不同算法的 Scheduler int register_ip_vs_scheduler(struct ip_vs_scheduler *scheduler) { struct ip_vs_scheduler *sched; if (!scheduler) { pr_err(&quot;%s(): NULL arg\\n&quot;, __func__); return -EINVAL; } if (!scheduler-&gt;name) { pr_err(&quot;%s(): NULL scheduler_name\\n&quot;, __func__); return -EINVAL; } ..... }  基本上所有的 IPVS Scheduling 的實作都會再 Kernel Module 初始化的過程去呼叫 register_ip_vs_scheduler，舉例來說 ip_vs_rr.c 這個 kernel module 被系統載入後，第一件事情就是呼叫 register_ip_vs_scheduler，將自己的物件註冊進去，所以之後有任何人透過 ipvsadm 去表示我要使用 rr 這個算法的時候，底下的 kernel module(ip_vs.ko) 就有辦法找到對應的 scheduler 來使用。  static int __init ip_vs_rr_init(void) { return register_ip_vs_scheduler(&amp;ip_vs_rr_scheduler); } static void __exit ip_vs_rr_cleanup(void) { unregister_ip_vs_scheduler(&amp;ip_vs_rr_scheduler); synchronize_rcu(); } module_init(ip_vs_rr_init); module_exit(ip_vs_rr_cleanup); MODULE_LICENSE(&quot;GPL&quot;);  ","version":"Next","tagName":"h2"},{"title":"註冊 Netfilter Function​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-4#註冊-netfilter-function","content":"ip_vs 這個 Kernel Module 本身被載入後，也會有相關的初始步驟，該步驟會其實是呼叫的 __ip_vs_init 這個函式 /* * Initialize IP Virtual Server netns mem. */ static int __net_init __ip_vs_init(struct net *net) { struct netns_ipvs *ipvs; int ret; ipvs = net_generic(net, ip_vs_net_id); if (ipvs == NULL) return -ENOMEM; /* Hold the beast until a service is registerd */ ipvs-&gt;enable = 0; ipvs-&gt;net = net; /* Counters used for creating unique names */ ipvs-&gt;gen = atomic_read(&amp;ipvs_netns_cnt); atomic_inc(&amp;ipvs_netns_cnt); net-&gt;ipvs = ipvs; if (ip_vs_estimator_net_init(ipvs) &lt; 0) goto estimator_fail; if (ip_vs_control_net_init(ipvs) &lt; 0) goto control_fail; if (ip_vs_protocol_net_init(ipvs) &lt; 0) goto protocol_fail; if (ip_vs_app_net_init(ipvs) &lt; 0) goto app_fail; if (ip_vs_conn_net_init(ipvs) &lt; 0) goto conn_fail; if (ip_vs_sync_net_init(ipvs) &lt; 0) goto sync_fail; ret = nf_register_net_hooks(net, ip_vs_ops, ARRAY_SIZE(ip_vs_ops)); if (ret &lt; 0) goto hook_fail; return 0; /* * Error handling */ hook_fail: ip_vs_sync_net_cleanup(ipvs); sync_fail: ip_vs_conn_net_cleanup(ipvs); conn_fail: ip_vs_app_net_cleanup(ipvs); app_fail: ip_vs_protocol_net_cleanup(ipvs); protocol_fail: ip_vs_control_net_cleanup(ipvs); control_fail: ip_vs_estimator_net_cleanup(ipvs); estimator_fail: net-&gt;ipvs = NULL; return -ENOMEM; }  這邊最重要的就是 nf_register_net_hooks 這個函式，該函式會傳入 ip_vs_ops 這個物件，而這個函式就是跟 netfilter 去註冊，接下來實際看一下 ip_vs_ops 物件(忽略 ipv6, 因為跟 ipv4 概念雷同) static const struct nf_hook_ops ip_vs_ops[] = { /* After packet filtering, change source only for VS/NAT */ { .hook = ip_vs_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC - 2, }, /* After packet filtering, forward packet through VS/DR, VS/TUN, * or VS/NAT(change destination), so that filtering rules can be * applied to IPVS. */ { .hook = ip_vs_remote_request4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC - 1, }, /* Before ip_vs_in, change source only for VS/NAT */ { .hook = ip_vs_local_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_OUT, .priority = NF_IP_PRI_NAT_DST + 1, }, /* After mangle, schedule and forward local requests */ { .hook = ip_vs_local_request4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_OUT, .priority = NF_IP_PRI_NAT_DST + 2, }, /* After packet filtering (but before ip_vs_out_icmp), catch icmp * destined for 0.0.0.0/0, which is for incoming IPVS connections */ { .hook = ip_vs_forward_icmp, .pf = NFPROTO_IPV4, .hooknum = NF_INET_FORWARD, .priority = 99, }, /* After packet filtering, change source only for VS/NAT */ { .hook = ip_vs_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_FORWARD, .priority = 100, }, ...  ip_vs_ops 是一個 nf_hook_ops 的陣列物件，每個物件內都有四個成員被使用，分別是 hookpfhooknumpriority 這四個組合起來的含義就是，我準備了一個 hook 的函式，希望當封包符合 pf 描述的格式，且封包目前運行到 hooknum 這個時間點的時候，根據我描述的 proiroty 於正確的時間點呼叫我準備的 hook 函式。 我們取其中一個範例來進行比較詳細的解釋  { .hook = ip_vs_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC - 2, },  首先，hook 的值是 ip_vs_reply4，這個其實是一個 function，內容如下 /* * It is hooked at the NF_INET_FORWARD and NF_INET_LOCAL_IN chain, * used only for VS/NAT. * Check if packet is reply for established ip_vs_conn. */ static unsigned int ip_vs_reply4(void *priv, struct sk_buff *skb, const struct nf_hook_state *state) { return ip_vs_out(net_ipvs(state-&gt;net), state-&gt;hook, skb, AF_INET); }  pf 的值則是 NFPROTO_IPV4，其他相關的變數則定義於 netfilter.h enum { NFPROTO_UNSPEC = 0, NFPROTO_INET = 1, NFPROTO_IPV4 = 2, NFPROTO_ARP = 3, NFPROTO_NETDEV = 5, NFPROTO_BRIDGE = 7, NFPROTO_IPV6 = 10, NFPROTO_DECNET = 12, NFPROTO_NUMPROTO, };  hooknum 則是我們一直強調的時間點，對應到 netfilter/iptables 的架構圖，就是 LOCAL_IN，封包經由 routing 後決定要送到本機處理的時間點，其他相關的變數則定義於netfilter.h enum nf_inet_hooks { NF_INET_PRE_ROUTING, NF_INET_LOCAL_IN, NF_INET_FORWARD, NF_INET_LOCAL_OUT, NF_INET_POST_ROUTING, NF_INET_NUMHOOKS };  可以由上面的原始碼看到這些變數的名稱都與我們熟悉的 IPTABLES 用法熟悉，這是因為都基於 Netfilter 架構的原因 最後的 priority 則是一個優先度，數值愈小代表優先度愈高，也先執行，其他相關的變數則定義於netfilter_ipv4.h enum nf_ip_hook_priorities { NF_IP_PRI_FIRST = INT_MIN, NF_IP_PRI_CONNTRACK_DEFRAG = -400, NF_IP_PRI_RAW = -300, NF_IP_PRI_SELINUX_FIRST = -225, NF_IP_PRI_CONNTRACK = -200, NF_IP_PRI_MANGLE = -150, NF_IP_PRI_NAT_DST = -100, NF_IP_PRI_FILTER = 0, NF_IP_PRI_SECURITY = 50, NF_IP_PRI_NAT_SRC = 100, NF_IP_PRI_SELINUX_LAST = 225, NF_IP_PRI_CONNTRACK_HELPER = 300, NF_IP_PRI_CONNTRACK_CONFIRM = INT_MAX, NF_IP_PRI_LAST = INT_MAX, };  透過這些資訊，我們可以瞭解到上述的含義就是請幫我於 LOCAL_IN 這個時間點註冊一個 function(ip_vs_reply4)，若封包是 IPV4 的格式且請比 NAT_SRC(SOURCE NAT) 還要提早處理。  { .hook = ip_vs_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC - 2, },  這也是為什麼我們架構圖會於 INPUT(LOCAL_IN) 裡面將 IPVS 放到 NAT 前面執行 其他的 FORWARD, OUTPUT(LOCAL_OUT) 的概念是完全一樣的 這邊還有一個有趣的概念就是，其實這些 Talbe (NAT/FILTER/MANGLE) 的本質都是一個函式，舉例來說 iptable_net.c 裡面就定義了 NAT 相關的操作 static const struct nf_hook_ops nf_nat_ipv4_ops[] = { /* Before packet filtering, change destination */ { .hook = iptable_nat_ipv4_in, .pf = NFPROTO_IPV4, .hooknum = NF_INET_PRE_ROUTING, .priority = NF_IP_PRI_NAT_DST, }, /* After packet filtering, change source */ { .hook = iptable_nat_ipv4_out, .pf = NFPROTO_IPV4, .hooknum = NF_INET_POST_ROUTING, .priority = NF_IP_PRI_NAT_SRC, }, /* Before packet filtering, change destination */ { .hook = iptable_nat_ipv4_local_fn, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_OUT, .priority = NF_IP_PRI_NAT_DST, }, /* After packet filtering, change source */ { .hook = iptable_nat_ipv4_fn, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC, }, };  ，而所謂的 NAT Table 的概念其實就被上述的 hook 給處理掉，如下圖，其中 ipt_do_table 作為一個 function pointer 被上述的 hook 裡面使用，而這個函式內則是會透過 do...while 去依序執行相關的規則 static unsigned int iptable_nat_do_chain(void *priv, struct sk_buff *skb, const struct nf_hook_state *state, struct nf_conn *ct) { return ipt_do_table(skb, state, state-&gt;net-&gt;ipv4.nat_table); } static unsigned int iptable_nat_ipv4_fn(void *priv, struct sk_buff *skb, const struct nf_hook_state *state) { return nf_nat_ipv4_fn(priv, skb, state, iptable_nat_do_chain); } static unsigned int iptable_nat_ipv4_in(void *priv, struct sk_buff *skb, const struct nf_hook_state *state) { return nf_nat_ipv4_in(priv, skb, state, iptable_nat_do_chain); } static unsigned int iptable_nat_ipv4_out(void *priv, struct sk_buff *skb, const struct nf_hook_state *state) { return nf_nat_ipv4_out(priv, skb, state, iptable_nat_do_chain); } static unsigned int iptable_nat_ipv4_local_fn(void *priv, struct sk_buff *skb, const struct nf_hook_state *state) { return nf_nat_ipv4_local_fn(priv, skb, state, iptable_nat_do_chain); }  其他的 table 概念一樣，只是使用的介面不太一樣，都改使用 xt 系列的 API來處理 IPTABLES 比較 藉由上述原始碼的觀察，我們可以觀察到一個很重要的事情就是 IPVS 本身的運作是依賴 netfilter 架構來處理封包，而很剛好的是 IPTABLES 本身也是大量依賴 netfilter 來使用，這邊的比較我認為有幾個重點 ","version":"Next","tagName":"h2"},{"title":"IPVS 與 IPTABLES 的取代問題​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-4#ipvs-與-iptables-的取代問題","content":"我認為沒有辦法將 IPVS 與 IPTABLES 直接評比優劣，做出誰能夠取代誰的結論。 原因是兩者個功能面向完全不同， IPVS 能夠提供的功能很少，就是完全針對 LoadBalancing 實作而已，然而 IPTABLES 本身可以做的事情非常多，特別是透過 -m, -j 這些 module擴充後可以辦到的事情更多了。 大部分的評比文章其實都是針對於 Load-Balancing 這件事情來比較，如果真的夠熟悉其底層架構就會理解到， IPVS 是專門針對 LB 去實作的，然而 IPTABLES 並不是，而是採用一些比較獵奇的方式來達到類似的功能 因此效能上會有差異也不足為奇，甚至說若 IPVS 沒有更好，根本沒有發展的必要(我認為)。 ","version":"Next","tagName":"h2"},{"title":"IPVS 除錯不易​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-4#ipvs-除錯不易","content":"綜觀 IPVS 的使用文件與架構說明，基本上都不太會講到 IPVS 與 Netfilter 的關係，但是其實對於每個封包來說，其實都是基於 netfilter 的架構在跑，會先被 iptables 的規則處理(mangle/filter)，接下來才被 IPVS 擷取去處理。 這種情況下，當發現問題時，其實不太容易知道問題點在哪裡，到底是 IPTABLES 的規則出錯，還是 IPVS 本身的功能或是設定出錯。 此外，對於 IPVS 使用者來說能夠觀察到的資訊大部分都是依賴 ipvsadm 這個工具，顯示的就只有 設定了哪些 server, 分配了多少封包 等比較上層的資訊，對於除錯能夠提供的線索有限，最終還是要連同 iptables 一起觀看研究，才能夠鎖定問題的發生點。 結論 拖了很久的第四篇 IPVS 文章終於寫完，這中間也經歷了一次的線上 Meetup 來濃縮四篇文章的介紹，該次 Meetup 也有線上錄影, 有興趣的也可以觀看 藉由這四篇文章的學習，讓我對於 IPVS 的概念有更深的理解，同時也更能說出 IPVS 與 IPTABLES 的異同之處，對於相關文章也能夠有更深的背景去探討與思考。 ","version":"Next","tagName":"h2"},{"title":"CD 系統的選擇議題","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-13","content":"","keywords":"","version":"Next"},{"title":"應用工具包裝與部署​","type":1,"pageTitle":"CD 系統的選擇議題","url":"/docs/techPost/2020/iThome_Challenge/cicd-13#應用工具包裝與部署","content":"首先，雷達圖片中有三個工具在描述上述的問題 (2)，分別是 Helm, Kustomize,jsonnet，剛好推薦信心也是按照這個排序 最多廠商選擇使用 Helm ，再來是 Kustomize 以及 jsonnet。 我個人的經驗會根據情況使用 Helm 以及 Kustomize，比較不會使用 jsonnet，覺得帶來的效益跟並不是很大， ","version":"Next","tagName":"h2"},{"title":"部署平台/策略選擇​","type":1,"pageTitle":"CD 系統的選擇議題","url":"/docs/techPost/2020/iThome_Challenge/cicd-13#部署平台策略選擇","content":"再來上圖中剩下的技術都跟 CD 部署有關，其中 Flux 以及 ArgoCD 這兩個主要是主打 GitOps 的部署工具，本身沒有任何流水線的設計，完完全全就是針對部署去設計的，之後也會有章節再探討 GitOps 的概念與示範。 剩下的平台基本是都有提供 Pipeline 系統來處理，這部分有開源軟體，也有 SaaS 軟體，這之中包含了 CircleCIGitLabJenkinsJenkins XGithub ActionTeamCityTravisCI 從使用者廠商回報的結果來看， CircleCI 以及 Gitlab 比較有明確的共識，推薦大家使用，其他的內容包含 Github Action 擁有完全正面的回饋，只是正式使用的數量還不夠多 Jenkins 則是一面倒，擁有數量不少的推薦數量，但是也有最高票的強烈不推薦票 眾多人的意見表示，對舊有系統來說 Jenkins 已經運行的很好了，但是對於全新的系統會願意嘗試不同的系統，而非使用 Jenkins。 所以針對 (1) 到底該怎麼選擇？ 這部分我認為目前有兩個主流，一個就是透過 Pipeline 系統直接與 Kubernetes 叢集互動，另外一個則是透過 GItOps 的概念讓 Kubernetes 叢集自己更新，不仰賴額外的 Pipeline 系統。 接下來我們這兩種概念都會去探討，並且介紹這兩種概念下可能的部署流程會長怎樣。 部署策略 這部分要探討的主要是部署過程中，你要如何將新的應用部署到生產環境，同時對系統以及使用者造成的影響最少，這部分有不少的流派在跑，譬如說 Recreate 這是個最簡單的策略，將舊版本全部移除，接者部署新版本。這種策略下的 downtime 取決於舊版本移除的時間以及新版本的部署時間 Ramped 透過 one by one 的替換策略，每次都會部署一個新版本的實體，透過 load-balancer 確認該新版本實體可以接受到網路流量且正常運作後，就會把舊版本的一份實體移除，反覆執行直到舊版本全部結束。 Blue/Green 相對於 Ramped 部署， BG 部署則是一口氣部署全部的新版本實體，譬如 3 份副本，部署完畢且測試完成後，一口氣將所有流量導向新版本，並且移除舊版本。 Canary Canary 強調的是逐步切換的概念，首先一口氣部署全部的新版本實體，接下來透過 load-balancer 的方式與權重的設定，慢慢的將流量從舊版本導向新版本，譬如 90%:10% -&gt; 80%:20% -&gt; 50%:50% -&gt; 10%:90% -&gt; 0%:100% 這樣的進展 A/B testing 這種部署策略更大的應用是商業上的判斷，最常見的就是針對不同使用者給予不同的介面，譬如 Facebook 每次升級新版本的時候，都會有一部分的使用者開始使用新版本，而剩下的依然使用舊版本。其運作邏輯跟上述的 Canary 部署雷同。 Shadow 這個版本的部署也是新部署版本的全部實體，接下來針對所有流向舊版本的流量都複製一份，將其送到新版本去跑，當一切都沒有問題後才會移除舊版本。 更多詳細的介紹可以參閱這篇文章 Six Strategies for Applications Deployment 實際上這些策略跟你使用的 CD 工具會有很大的關係，礙於每個工具的技術與架構，並不是上述所有策略都可以輕鬆的於任何架構中實現 機密資料控管 最後一個要探討的問題就是機密資料管理，舉例來說，今天要部署一款新的應用程式到 Kuberntes 叢集中，而該應用程式需要知道資料庫的帳號密碼，假設今天使用的是 Helm 的方式來部署。 雖然 Helm 有提供 --set, values.yaml 的方式來客製化內容，但是要如何在 CD 的過程中取得這些機密資料並且部署到 Kubernetes 內，同時又不希望有任何的地方可以看到這些機密資料的明碼。 這部分之後的章節會再來仔細探討相關議題以及一些解決方案 ","version":"Next","tagName":"h2"},{"title":"CI Pipeline x Kubernetes 結論","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-12","content":"CI Pipeline x Kubernetes 結論 過去幾天我們探討了關於 Pipeline 的架構， CI 過程中 Kubernetes 的設計，以及 Yaml 相關的測試方式，今天這篇文章就來針對這些內容進行一些心得總結 一開始我們先用一張圖片來介紹可能的 pipeline 架構(此架構只是一個可能性，不是唯一，每個團隊需要運行的架構都不盡相同，沒有最好的架構只有最適合自己環境的架構) 這張架構中，我們首先要先選擇一套自己喜歡的 Pipeline 系統，這套系統可能是自架，也可能是 SaaS 服務，這中間的取捨條件非常多， 不論是成本考量，維護考量，擴充靈活性等，每個細項都需要團隊經過評估討論後決定 接下來於該 Pipeline 中我們會設定相關的 Job 來處理我們的工作，目前我們都只專注於 CI 方面的工作，因此上述的區塊都跟測試相關， 主要用來驗證與確保每次開發者的程式碼修正有通過團隊的一些測試要求。 這邊的架構其實非常多變，譬如 Git Repo 的設計，是否要將應用程式與 Yamls 放一起 放一起的架構下，就變成整條 pipeine 的測試中要同時包含程式碼的測試，以及 Yaml 的測試，會比較類似上述的架構如果分開放，則有些測試就可以分開，譬如 Yaml Repo 就只需要針對 Yaml 進行測試，甚至配上一些整合測試確保部署後功能沒有問題。當然應用程式本身可以依靠程式語言的測試框架進行基本測試，接者搭配一些整合測試即可。 Yaml 測試方面前一天的文章有探討一些工具的使用與介紹，這些工具的用法與面向都不相同，甚至裡面要測試的檔案也不一定只有 Kubernetes 可以使用，所以多方測試總是會有幫助的 一切通過之後可以開始建置相關的 Container Image，並且準備將其送到測試用的 Kubernetes 叢集中，這邊要特別注意的環節就是 Image Tag 的處理。假設前述過程中產生的 Image Tag 是 5b1f94025b2，那後續測試的過程要有能力把這個 5b1f94025b2 給傳遞到 相關的 Yaml 裡面，這樣才可以確保 Kubernetes 內使用的是這個剛建置好的 Container Image. 如果使用的是 Helm 來部署的話，我們可以透過 --set image.tag=5b1f94025b2 之類的方式來修改 image tag，如果是使用原生的 Yaml 檔案，可能就要利用 sed 等指令來修改，這部分腳本的撰寫要特別小心。 當一切都測試完畢後就可以將最後的 Image Tag 給推上去成為一個經過測試認可的 Image。 那上述的流程有沒有哪些部分可以改善或是引入不同的工具來提升效率呢? 事實上是可以的，我們可以嘗試引入之前介紹過的本地開發工作 skaffold 來幫我們處理建置 Image 及將 Image 推到 Kubernetes 叢集內的這段過程，其架構如下。 於 Skaffold 的設定檔案中我們可以設定測試用的指令，將我們運行整合測試的指令整合進去，就可以把 Build Image 到測試這過程全部 讓 Skaffold 來搞定，同時藉由 Skaffold 的幫忙，我們就不需要自己去處理修改 Image Tag 的這個過程，一切讓 Skaffold 去修改對應的 image tag, 不論是 Helm, Kustomize 或是原生 Yaml 都能搞定。 結論來說，我認為現在一個簡單的 CI 流水線內其實會引入各式各樣的開源軟體來幫忙處理，每個軟體都有自己擅長與不擅長的地方，很 多時候就算找不到相關的開源軟體我們都可以秉持 給我一個 bash, 我給你全世界 的概念來自行處理。但是有時候要去思考一下到底現在 團隊最要緊的任務是什麼，哪個工作是公司最需要的，有時候架構上的完美不一定是商業上的完美，這部分的取捨往往需要一些溝通與協 調，來找到一個公司開心，工程師開心，大家開心的節奏。","keywords":"","version":"Next"},{"title":"GitOps 的介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-16","content":"","keywords":"","version":"Next"},{"title":"Git 作為單一來源​","type":1,"pageTitle":"GitOps 的介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-16#git-作為單一來源","content":"GitOps 中強調，所有的資源描述檔案，都集中放於 Git，不論是原生的 Yaml，Kustomize 或是 Helm。 這些內容都要放到 Git 裡面 同時也只能有這個來源，當有人問起你這個 Kubernetes 資源的描述檔案在哪裡，唯一的答案就是 Git 身上 透過使用 Git 帶來一些好處 任何檔案的變化都可以使用 Git History 來觀察，藉此追蹤每個版本的差異如果有任何修改有問題，想要修復的話，都可以透過 Git 的指令操作，譬如 Revert, 或是再次修正 你要用哪一套 Git 其實沒差，其實概念源自於 VCS 版本控制系統 此外， Git 中所放置的資源描述檔案都希望是基於 Declarative 的概念，一種宣告式描述希望狀態的格式，擁有這個要求才可以滿足第二個核心概念 ","version":"Next","tagName":"h2"},{"title":"狀態同步​","type":1,"pageTitle":"GitOps 的介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-16#狀態同步","content":"第二個核心概念是完全建築在第一個概念的實現，要先完成第一個核心狀態的建置，接下來才可以處理這個 探討這個概念前，我們要先定義兩個資源狀態 使用者渴望的資源狀態 這個狀態指的是 Git 內所維護的狀態，譬如使用者希望我的 Deployment 有 3 個副本，同時 image 的版本是 1.2.4。 這也是為什麼前述有說 Git 專案內要使用的是 Declarative 的格式，透過這類型的概念來描述開發者渴望的狀態正在運行的實際狀態 這個狀態指的是目標資源目前於 Kubernetes(舉例)內運行的狀態，譬如當前運行的 Deployment 有 2 個副本，使用的 image 版本是 1.2.3 GitOps 會希望有一個代理人(Controller)，這個代理人權責很重，他左邊觀看(1)的渴望狀態，同時右邊監控(2)系統上的運行狀態 這個代理人的最終目標就是要確保 (1) 與 (2) 的狀態一致，大部分的情況下都是把 (1) 的狀態給覆蓋到系統內，讓(2)最後會成為(1)所描述的樣子。 部分情況下，管理人員會直接使用一些工具來直接對運行的 Kuberentes 資源進行修改，譬如 kubectl patch, kubectl edit 等工具來修改其運行狀態。一但這種事情發生，就會導致最初描述這些資源的 Yaml 檔案與運行狀態不一致 ","version":"Next","tagName":"h2"},{"title":"更新方式單一來源​","type":1,"pageTitle":"GitOps 的介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-16#更新方式單一來源","content":"最後要講的則是 GitOps 的更新方式，鑑於前面兩個核心概念的組合，所有的更新都要從 Git 出發 舉例來說，我今天想要更新 Deployment 的 image tag, 我就針對該檔案進行修改，並且遞交一個修正的 Git Commit. 當一切都合併完畢後， GitOps 內的代理人接下來就會負責將 Git 上面的狀態資源給同步到目標的 Kubernetes 叢集中，藉此更新 Kubernetes 內的資源。 這種方式帶來幾個好處 Git Commit 是唯一的更新來源，禁止其他人透過 kubectl 等工具直接對 Kubernetes 進行部署與修改。這樣當問題發生的時候也比較好追蹤問題來源與除錯今天版本有問題想要進行退版的時候，可以直接對 Git 進行版本的處理，譬如修正，退版等。只要 Git 這邊搞定，後續就等待代理人將 Kubernetes 叢集內的狀態修正成符合 GIt 上面的格式就算今天有任何繞過規則，手動對 Kubernetes 內的資源進行手動修改，這些修改都可以被代理人追蹤，可以自動更新回去，迫使所有運行資源都要與 Git 所描述的一致 上述三個核心概念組建出 GitOps 的操作模式，然而這邊都只是概念上的敘述，下一篇會再用圖片跟大家介紹 Kuberentes 架構下的 GitOps 實作方式，當然實作方式也是百百種，不同的開源作案做法也都不一樣。 缺點 當然每個技術都不可能完美無瑕沒有任何缺失，接下來將列舉一些別人於 GitOps 實戰中遇到的痛點以及一些領悟，由於內文過長，對於詳細內容有興趣以參閱 GitOps 帶來的痛點與反思 內的分析與介紹。 以下列舉文章內的缺點 不適合使用於程式化的更新 Git Repo 增長帶來的問題 缺乏視覺化 Secret 的管理問題依然沒有解決 缺少檔案資源的驗證性 最後，其實 GitOps 的概念並沒有侷限於 Kubernetes 身上，畢竟 GitOps 就是一個概念，不是一個實作的規格，用任何的工具都有辦法 打造出符合這個核心概念的工作流程，甚至目標部署不是 Kubernetes 也沒有問題。 ","version":"Next","tagName":"h2"},{"title":"CD 與 Kubernetes 的整合","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-14","content":"CD 與 Kubernetes 的整合 上篇文章中我們探討了 CD 過程的各種議題，而本篇文章則會開始探討 CD 與 Kubernetes 的部署整合 這邊要特別強調的是 CI 跟 CD 兩件事情本來就不需要一定在一起，最簡單的情況下就是將 CI 與 CD 兩個步驟整合到同一個 pipeline 系統 上。但是有時候會希望透過手動部署，但是部署中間的過程希望自動化，所以會透過手動觸發 CD 的流程來達成自動部署。 此外， CI 與 CD 使用的流水線系統也不一定要用同一套系統，就如同前一篇文章提到專門針對 CD 這個步驟去列出相關的工具。 以下將會列出四種用法，這四種用法可以分成兩大類 Push ModePull Mode 其中 Push Mode 的概念是由我們的 CD Pipeline 主動將新的應用程式推到遠方的 Kubernetes Cluster 內 然而 Pull Mode 的概念是由 Kuberentes 主動去更新，藉由監聽遠方目標的變化來確保是否要自動更新版本 CI/CD pipeline (Push) 第一個是我認為最直接且直觀，我們把 CI/CD 兩個流程都放到同一個 Pipeline 系統內，其設計上也相對簡單 當 CI 流程結束後，接下來就跑下個步驟，這個步驟包含 a. 準備相關執行檔案 b. 透過相關工具部署到遠方的 Kubernetes這種架構下，因為要存取遠方的 kubernetes，也是會需要將 KUBECONFIG 這個檔案放到 Pipeline 系統中，所以使用上要特別注意 安全性的問題，避免別人存取到這個 KUBECONFIG，否則攻擊者可以控制你的 Kubernetes 叢集 人員觸發 (Push) 這種架構下，我們將 CI pipeline 與 CD pipeline 給分開執行，這兩套 pipeline 要不要使用同一套系統無所謂，至少 Job 是分開的。 叢集管理員或是其他有權限的人可以透過直接執行當前的 CD pipeline 來觸發自動化部屬。這種的好處在於，對於一些正式的生產環境 下，太過於自動的部署不一定是完全好的，有時候會需要一些人為介入的確認，確認一切都沒有問題後才會繼續執行自動部署。 因此這個架構下可能的一個流程是 透過 CI pipeline 通過測試以及產生出最後要使用的 Image 檔案部署團隊與 PM 等經過確認，公告更新時間後在手動觸發自動部署的工作來完成部署如同前面部署，這邊也會需要將 Kubernetes 存取所需要的 KUBECONFIG 放到 CD pipeline 內，所以也是有安全性的問題需要注意 Container Image 觸發 (Push) 這是另外一種不同的架構，我們將人為觸發的部分提供了一個新的選擇，當 Container Registry 本身發現有新版本的 Container Image 更新時，會透過不同的方式通知遠方的 CD pipeline 去觸發自動更新。 這個使用方法會依賴你使用的 Container Registry 是否有支援這種的架構，譬如 Harbor 這個開源專案就有支援，當 image 更新後可以透過 webhook 的方式將訊息打到遠方。而遠方的 CD pipeline 如果也有這種機制可以透過 webhook 來觸發的話，就可以實作上面的機制。 由於是透過 container registry 所觸發的工作，所以這種架構可以支援更多的觸發方式，譬如管理員今天緊急需求，手動推動新版的 Container Image 到遠方 Registry，這樣也能夠觸發 因為跟前述架構完全類似，所以 KUBECONFIG 也是會放到環境之中，必須要有安全性的考量。 Pull Mode 最後我們來看另外一種不同的架構，這種架構下我們就不會從 Pipeline 系統中主動地將新版應用程式推到 Kubernetes 中，相反的是我們的 Kubernetes 內會有一個 Controller，這個 Controller 會自己去判斷是否要更新這些應用程式，譬如說當遠方的 Contaienr Image 有新版更新時，就會自動抓取新的 Image 並且更新到系統之中。 這種架構下，我們不需要一個 CD Piepline 來維護這些事情，此外，因為沒有主動與 Kubernetes 溝通的需求，所以也不需要把 KUBECONFIG 給放到外部系統 (CD Pipeline) 中，算是減少了一個可能的安全性隱憂。 當然這種架構下，整個部署的流程都必須依賴 Controller 的邏輯來處理，如果今天有任何客製化的需求就變成全部都讓 Controller 來處理，可能要自行修改開源軟體，或是依賴對方更新，相較於完全使用 CD Pipeline 處理來說，彈性會比較低，擴充性也比較低，同時整個架構的極限都會被侷限在 Controller 本身的能力。 最後要說的是，以上介紹的架構沒有一個是完美的，都只是一個參考架構，真正適合的架構還是取決於使用者團隊，透過理解不同部署方 式所帶來的優缺點，評估哪些優勢我團隊需要，哪些缺點是團隊可以接受，不可以接受，最後綜合評量後取捨出一套適合團隊工作的方式。","keywords":"","version":"Next"},{"title":"GitOps 與 Kubernetes 的整合","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-17","content":"GitOps 與 Kubernetes 的整合 上篇文章我們探討了 GitOps 的概念，但是概念歸概念，實作歸實作，有時候實作出來的結果跟概念不會完全一致，因此最後的使用方式與優缺點還是要看實作的細節。 今天我們就來看看 GItOps 這個概念要怎麼與 Kubernetes 整合。 首先，前述 GitOps 的概念中，我們提到一個代理人程式，這個程式要能夠管理左看 Git Repo, 右看 Kubernetes ，那由於這個程式本身要能夠有能力去讀取 Kubernetes 內的資源狀態，同時也要有能力對其修改，勢必要獲得一些操控權限。 設想一個情境，如果今天這個代理人程式其座落於 Kubernetes 外，我們終究還是要為他準備一份 KUBECONFIG，這樣其實也是會增加安全性的風險，但是如果把這個程式放到 Kubernetes 裡面，讓其擁有存取 Kubernetes 能力的部分就相對好解決，這樣可以減少一些安全性的風險。 程式碼架構 GitOps 的架構下，因為都會把資源的狀態檔案都放在 Git，所以這時候就會有一些不同的做法，舉例來說 應用程式原始碼以及相關的 Yaml 放一起應用程式原始碼以及相關的 Yaml 分開到不同的 Repo 去放 GitOps 的原則我認為採用 (2) 是比較好實現的，因為我們可以很明確地將應用程式與部署資源給分開，這兩個 Repo 所維護跟控管的團隊也有所不同。同時 Yaml Repo 內的所有更動跟只會跟部署資源的狀態有關，這樣對於維護，追蹤任何變動，甚至要退版等需求都比較好實現。 如果將程式碼跟相關 Yaml 放在同樣一個 Repo 內，那想要針對部署狀況進行退版，就有可能也會導致程式碼本身功能也一併被退版，這就不是期望的結果。 不過這邊所提的都只是一些各自的特性與優缺點，沒有一個絕對的解決方案跟絕對的正確與否，還是要根據 GitOps 的實作方式以及團隊的習慣方式去選擇。 接下來的架構圖都會基於 (2) 的方式去介紹 架構一 我們用下面的架構圖來看一下一個 GitOps 與 Kubernetes 整合範例一 這個架構下，我們會在 Kubernetes 內安裝一個 Controller，也就是前文所提的代理人程式，這個程式本身要有辦法與遠方的 Git(Yaml) Repo 溝通。 接下來其運作流程可能是 開發者對 Git(Application) Repo 產生修改，這份修改觸發了相關的 CI Pipeline ，這過程中經歷過測試等階段，最後將相關的 Image 給推到遠方的 Container Registry。系統管理者針對 Git(Yaml) Repo 產生修改，這份修改觸發了相關的 CI Pipeline, 這過程中會針對 Yaml 本身的格式與內容進行測試，確保沒有任何出錯。當 Git(Yaml) Repo 通過 CI Pipeline 而合併程式碼後，接下來 Kubernetes 內的 Controller 會知道 Git(Yaml) Repo 有更新 一種是 Git 那邊透過 Webhook 的方式告訴 Controller一種是 Controller 定期輪詢後得到這個結果 同步 Git(Yaml) Repo 裡面的狀態描述檔案與 Kubernetes 叢集內的狀態，確保目前運行狀態與 Git 內的檔案描述一致如果今天不想要 (3) 這個步驟的自動化，也可以由管理員經過確認後，手動要求 Controller 去同步 Git 並更新 上述的架構聽起來運作起來都滿順暢，但是對於開發者的叢集來說(假設開發者會有一個遠方的 Kubernetes 用來測試功能)可能會不便利，只要這些更新非常頻繁，那就意味者要一直不停的去修改 Git Yaml Repo 的內容，雖然一切都按照概念在運行，但是操作起來可能會覺得效率不一定更好。 因此也會有人將上述的一些架構整合，譬如當 Image 推向到遠方 Container Registry 後，利用程式化的功能於 CI Pipeline 的最後階段自動的對 Git(Yaml) Code Changed 去送一筆 Commit 並自動觸發後面的 Controller 同步行為。 GitOps 的概念很活，很多種做法都可以，並沒有要求一定要怎麼實作才可以稱為 GitOps，最重要的是你們的工作流程是否可以達到如同 GitOps 所宣稱的效果，就算不走 GitOps，只要可以增加開發跟部署的效率，減少問題就是一個適合的架構。 架構二 接下來我們來看一下另外一種參考架構，這種架構希望可以解決 Contaienr Image 頻繁更新的問題，因此 Controller 本身又會多了監聽 Container Registry 的能力。 當 Controller 發現有新的版本的時候，只要這個版本號碼有符合規則，就會把新的版本資訊給套用到 Kubernetes 裡面。 但是因為 GitOps 的原則是希望以 Git 作為單一檔案來源，如果這樣做就會破壞這個規則，因此這時候 Controller 就要根據當前 Image Tag 的變化，把變化內容給寫回到 Git(Yaml) Repo 之中。 這也是為什麼下圖中 Controller 要對 Git(Yaml) Repo 進行更新與撰寫新 Commit 的原因。 也因為這個原因，我們的 Controller 也必須要對該 Git(Yaml) Repo 擁有讀寫的能力，這方面對於系統又會增加一些設定要處理 以上兩種架構並不是互斥，是可以同時存在的，功能面方面就讓各位自己取捨，哪種功能我們需要，帶來的利弊誰比較大。 下一篇我們將帶來其中一個開源專案 ArgoCD 的介紹，看看 ArgoCD 如何實踐 GitOps 的原則","keywords":"","version":"Next"},{"title":"Container Registry 的介紹及需求","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-19","content":"Container Registry 的介紹及需求 本篇開始要來介紹一些 CI/CD 過程中都一定會用到的一個元件， Container Registry Container Registry 顧名思義就是用來保存 Container Image 的一個倉庫，我認為 Container Registry 也有一些有趣的議題可以探討，譬如說 要使用 SaaS 服務還是自己架設是否支持 Private registry多人合作下是否支持權限控管是否有 web hook 可以與後續的 pipeline 或是其他系統連動跟 Git 等相關專案是否有自動連動與處理是否支援弱點掃描，可以用來檢查當前 Registry 內的 image 是否有潛在安全性的問題 上述的每個議題也都滿有趣的，我們可以先來聊聊 (1) 這個議題到底有什麼要探討的地方 我認為近8年大部分踏入容器化世界的開發者或使用者，第一個接觸的解決方案基本上都是 Docker Container，後續開始使用時通常都會使用 Docker Hub 這個由 Docker 所提供的 Container Registry 作為第一個接觸的 Contaienr Registry 解決方案。 Docker Hub 使用起來，我認為算是非常方便，特別是跟 GitHub/Bitbucket 的連動非常輕鬆設定，通常只要在專案內準備一個 Dockerfile 的檔案，就可以讓 Docker Hub 自動地幫你部署相關的 Image 並且存放到 Docker Hub 上。 這種情況下對於一些要準備自己 Image 的開發者來說非常便利，都不需要額外的 CI Pipeline 系統來處理，只要將程式碼合併，等待一段時間後相關的 Image 就出現了。 然而隨者專案的擴大，使用環境的改變， Docker Hub 並不一定可以適合所有情境 舉例來說，很多落地的工作環境中，會基於保密，機密等安全性要求，不希望運行的 Contianer Image 放置雲端，這時候就會思考是否有辦法自架一個本地端的 Container Registry。 此外更多的情境是網路問題，因為 Container Image 的容量說大不大，說小不小，幾百 MB 到幾 GB 都有，如果遇到網路速度瓶頸問題，就會發生抓一個 Image 花上長時間等待。 這部分的問題其實常常看到，舉例來說 Extremely Slow Image Pulls hub-feedback issue about slow 這些連結都可以看到滿滿的關於下載速度的問題，有時候還會牽扯到 docker hub CDN 的問題，問題發生的時候還真的什麼都不能做，只能祈禱 docker hub 快點修復。 Docker Hub 方案比較 此外，部分工作團隊也會有一些 contaienr image 的需求，但是又不想要公開相關的內容，這時候會需要 private registry 的支援，可惜的是對於 Docker Hub 來說，這部分會取決於方案的選擇，譬如下圖的方案比較 免費方案只能有一個 Private，付費又會取決於你是個人用戶還是一個團隊，對於團隊來說，其價格還會根據使用者數量而有所增加， 所以如果今天團隊內會希望根據架構有不同的權限控管，因此使用者的數量可能會有不少的時候，整個成本又會大幅度增加。 總總考量之下，自架 Container Registry 的需求就會逐漸出現，不論是為了成本，為了功能或是其他因素， SaaS 與 自架的選擇從來沒有 停止過，就如同之前探討 pipeline 系統一樣，每個系統都會有 SaaS 與自架的需求比較，但是哪一種比較適合貴團隊就沒有答案 此外，不同的開源專案提供的 container registry 的功能也都不盡相同，這種情況下就需要有人去針對每套軟體進行評估，找出一套適合自己團隊的服務，或是最後轉回使用 SaaS 商用解決方案都有可能。 DockerHub 使用者條款 最後要提的是，使用 SaaS 服務也不是就沒有完全痛點，譬如 2020 八月份 Docker Hub 的使用者條款更新，該更新中有幾個更動令很多無付費使用者都在思考該怎麼處理，是否要轉換到其他的 SaaS 服務或是都要改成自架來處理。主要更新有 當一個 Image repository 六個月內沒有任何動作(push/pull)，則該 image repository 會被自動刪除針對無認證用戶或是免費版本用戶有下載量的限制。 無認證用戶每六小時只能 Pull Image 100 次認證的免費用戶每六小時只能 pull 200 次 對於很多使用者或是開發者來說，這兩個問題都會造成一些使用上的困擾，特別是 (2) 的限制，因此不少人開始思考要如何於不花錢的情況下解決這些問題，譬如 avoiding the docker hub retention limit, 或是轉戰到其他的 SaaS。 只能說天下沒有白吃的午餐，享受免費方案的同時，也要多注意所謂的使用者條款，如果發現這些條款的修正會影響自己的使用情境，可能就要開始考慮搬移，自架或是付費等選擇 接下來的文章，我們就會探討自架 Contaienr Registry 的各種選擇與示範，最後會在展示如何將自架 Contianer Registry 與 Kubernetes 結合，讓你的 Kubernetes 叢集能夠接受 Docker Hub 以外的 Contaeinr Registry.","keywords":"","version":"Next"},{"title":"Kubernetes 物件管理","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-2","content":"","keywords":"","version":"Next"},{"title":"分享與散佈​","type":1,"pageTitle":"Kubernetes 物件管理","url":"/docs/techPost/2020/iThome_Challenge/cicd-2#分享與散佈","content":"今天一個應用程式如果有需要給外部使用，譬如可以透過類似 apt install 的方式來外部安裝，這種情況下我們會需要一些方式來包裝 應用程式，所謂的包裝除了原本所需要的眾多 Kubernetes 物件外，可能也會牽扯到下列議題 文件系統，如何讓外部使用者可以清楚地知道該怎麼使用，以及使用上有什麼要注意的部分依賴性系統，如果該應用程式本身又依賴其他應用程式，這種情況下要如何讓使用者可以順利安裝全部所需的資源物件一套發布系統，可以讓開發者跟使用者都方便去上傳/下載這些應用程式 ","version":"Next","tagName":"h3"},{"title":"版本控制​","type":1,"pageTitle":"Kubernetes 物件管理","url":"/docs/techPost/2020/iThome_Challenge/cicd-2#版本控制","content":"版本控制的議題相對單純，今天一個程式開發本身就會有版本的變化，其所需要的 Kuberentes 物件資源是不是也會有版本的差異？ 譬如 1.0.0 版本需要 ConfigMap 而 2.0.0 則移除了這個限制，所以今天 Kubernetes 的應用程式，本身是否也可以有版本控制的概念來控 管，這樣使用上時就可以更有彈性的去選擇所需要的版本 ","version":"Next","tagName":"h3"},{"title":"客製化​","type":1,"pageTitle":"Kubernetes 物件管理","url":"/docs/techPost/2020/iThome_Challenge/cicd-2#客製化","content":"客製化的議題也是單純，對於 Kubernetes 的物件資源來說，針對不同的使用環境，會需要不同的設定檔案，譬如同樣一個 Kubernetes Service, 有些環境覺得使用 ClusterIP 就可以，有些環境會需要使用 NodePort 來存取，甚至有些會使用 LoadBalancer 所以今天應用程式是否有辦法讓使用者很方便的去進行客製化的設定，最簡單的做法也許就是一個環境一大包設定檔案，但是這樣建置起 來非常沒有效率，同時維護上也會有眾多問題 ","version":"Next","tagName":"h3"},{"title":"解決方案​","type":1,"pageTitle":"Kubernetes 物件管理","url":"/docs/techPost/2020/iThome_Challenge/cicd-2#解決方案","content":"看了上面這些議題之後，接下來要思考的就是到底有什麼方式可以處理上述這些議題? 如果使用最原生的 Yaml ，是否能完成上述的要求？ 這個答案我認為可以，雖然麻煩但是有效。 事實上也滿多服務都透過 Yaml 配上 Git 的方式來散步其應用程式，使用者根據不同的 URL 來安裝不同的 Yaml 檔案，同時如果有需求就 自己直接修改該 Yaml 來滿足，譬如一個知名的 CNI Flannel 就是透過這種方式，將 Yaml 的內容全部寫到一個檔案中，然後透過 Github 來維護不同版本，使用者根據不同的 URL 來安裝不同版本的 Flannel。 如果不想要走原生 Yaml 檔案，那可以怎麼做? 相關的開源專案滿多的，我認為主流有兩套，分別是 Helm 以及 Kustomize，這兩套解決方法都用不同的設計思維來讓解決上述問題(部 分，非全部)，就我個人認為，目前除了 Helm 以外，還沒有任何一套開源專案可以滿足 分享與散佈 的需求，然而 Helm 於某些情況下又 受到大家的厭惡，輾轉改用 Kustomize 來部署，這中間的取捨沒有絕對，完全是根據應用場景選擇 整個 Kubernetes 生態系的概念也是這樣，沒有一個絕對的解決方案，每個方案都有適合自己的應用場景，最困難的點一直都是如何分析 自己的使用情境並且找到合適的解決方案 整個系列文中我們都會使用 Helm 作為我們的應用程式安裝解決方案，如果對 Kustomize 有興趣的朋友，歡迎自己閱讀官方文件學習怎 麼使用 ","version":"Next","tagName":"h2"},{"title":"Container Registry 的方案介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-20","content":"Container Registry 的方案介紹 本篇文章要來跟大家分享其他 Contaienr Registry 的選擇及相關議題，這些議題包含(包含但不限於) 使用者登入權限控管與整合 硬碟空間處理機制 Registry 的空間處理問題非常重要，處理不好很容易造成使用者沒有辦法繼續推送 Image UI 介面的操作 潛在漏洞與安全性檢查 SSL 憑證的支援性 此外這邊要特別注意，自架 Container Registry 不一定是免費的，有時候自架的會需要有相關授權等花費。 SaaS 不一定要錢，只是免費的通常都會有一些限定 接下來我們就來看一下方案介紹與比較 Docker Registry 2.0 Docker Registry 2.0 是由 Docker 所維護的開源專案，提供開發者一個自架 Docker Registry 的選項，使用上非常簡單，透過 Docker Container 的方式就可以輕鬆創建出一個 docker registry 2.0 的服務器。 舉例來說，下列指令就可以創建完畢 $ docker run -d -p 5000:5000 --restart always --name registry registry:2 不過我個人對於 docker registry 沒有很愛，主要是其預設情況下並沒有提供任何 UI 的支援，一切的操控都只能透過 docker 指令或是 curl 等指令來處理，對於多人控管以及操作上非常不便利。 網路上也有相關的專案，譬如 docker-registry-ui 這些第三方專案在幫忙實作 UI，讓使用者有一個比較好的方式可以管理，但是這種情況下會變成 UI 與 Server 兩個程式是由不同的維護團隊在維護，功能上的整合， Issue 的問題等都不一定夠順暢，所以如果不是為了本地簡單測試的情況下，我通常不會採用 Docker Registry 作為一個長期的解決方案。 儲存方面， Customize the storage location 以及 Customize the stoage back-end 等來自官方的文章再介紹相關的設定 對於外部存取的話，其也有支援 Let's Encrypt 等機制，讓其自動幫你 renew 快過期的憑證，使用上相對方便。 權限認證方面我認為功能比較少，滿多的認證方式都需要自行透過額外的伺服器幫忙處理，可以參考 restricting-access 或是 reverse proxy + SSL + LDAP for Docker Registry Harbor Harbor 是由 VMWare 所開源的 Container Registry 專案，我認為 Harbor 一個很值得推薦的原因是該專案是 CNCF 畢業專案，要成為 CNCF 畢業專案必須要滿足一些條件，雖然沒有一個專案可以完美的適合所有情形，但是就社群使用程度與社群貢獻程度來看， Harbor 算是滿優良的，這部分至少可以證明其本身是不少使用者在使用，而不是一個乏人問津的專案。 Harbor 的目標很簡單，源自期官網的介紹 Our mission is to be the trusted cloud native repository for Kubernetes Harbor 本身使用上不會太困難，可以透更 docker-compose 的方式去安裝，同時本身也有提供簡單的 UI 供使用操作， 詳細的架構可以參考這個 Architecture Overview of Harbor, 大概條列一下幾個重點功能 登入授權方式支援 LDAP/AD 以及 OIDC(OpenID Connect)，基本上銜接到 OIDC 就可以支援超多種登入，譬如 google, microsoft, saml, github, gitlab 等眾多方式都有機會整合進來Harbor v2.0 架構大改，成為一個 OCI (Open Container Initiative) 相容的 Artifacct Registry, 這意味者 Harbor 不單純只是一個 Container Image Registry，而是只要符合 OCI 檔案格式的產物都可以存放，影響最大的就是 Helm3 的打包內容。 未來是有機會透過一個 Harbor 來同時維護 Container Image 以及 Helm Charts支援不同的潛在安全性掃描引擎本身可跟其他知名的 Container Registry 進行連動，譬如複製，或是中繼轉發都可以除此之外還有很多特性，有興趣的點選上方連結瞭解更多 Cloud Provider Registry 三大公有雲 Azure, AWS, GCP 都有針對自己的平台提供基於雲端的 Container Registry，使用這些 Registry 的好處就是他們與自家的運算平台都會有良好的整合，同時服務方面也會有比較好的支援。 當然這些 SaaS 服務本身都會有免費與收費版本，就拿 AWS(ECR) 為範例，一開始會有一個嚐鮮方案，大概是每個月有 500GB 的容量使用，但是接下來更多的容量就會開始計費，計費的方式則是用(1)容量計費，每多少 GB 多少錢，(2)流量計費，流量的價錢是一個區間價格，使用愈高後的單位平均價格愈低。 就如同之前提過，使用 SaaS 服務有很多的優點，包含不需要自行維護伺服器，從軟體到硬體都可以全部交由服務供應商去處理，自己只要專心處理應用的邏輯即可，但是成本考量就是一個需要注意的事項。 Others 除此之外還有不少專案有提供 Self-Hosted 的服務，譬如由 SUSE 所維護的開源專案 Portus ，其專注整合 Docker Registry 並提供友善的介面與更多進階的功能，譬如 LDAP 控管，更進階的搜尋等。 Portus is an open source authorization service and user interface for the next generation Docker Registry.​ 不過觀察該專案的Github 顯示已經數個月沒有更新，甚至其最新的 Issue 都在探討是否該專案已經被放棄，Is Portus no longer being worked on， 有其他網友發現 SUSE 後來開了一個新的專案 harbor-helm，大膽猜測可能 SUSE 也在研究採用 Harbor 作為其容器管理平台而放棄自主研發的 Portus。 如果本身已經是使用 Gitlab 的團隊，可以考慮直接使用 GItlab Container Registry，其直接整合 Gitlab 與 Docker Registry 提供了良好的介面讓你控管 Container Registry，好處是可以將程式碼的管理與 Image 的管理都同時透過 Gitlab 來整合。","keywords":"","version":"Next"},{"title":"CD 之 Pull Mode 介紹: Keel","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-15","content":"CD 之 Pull Mode 介紹: Keel 上篇文章中，我們探討了不同類型的部署架構，今天我們就針對最後一種 Pull 的方式來進行一個介紹，同時使用開源專案 keel 來展示一 下這種模式下的操作過程與結果 本文中的圖片都節錄自 Keel官網 介紹 Keel 的官網介紹如下 Keel - automated Kubernetes deployments for the rest of us Keel 是一款 CD 部署的工具，其實作方式除了我們介紹的 Pull Mode 之外，他也支援 Push Mode 的方式，讓 Container Registry 主動通知 Keel 去進行自動部署。 下圖是一個最快理解 Keel 的運作流程，該圖片有五個步驟，分別是 修改程式碼推到 Github透過 CI pipeline 來產生最後的 Container Image，並且把 Container Image 給推到遠方 RegistryContainer Registry 知道有新版出現後，透過 relay 的方式把 web hook 的資訊往下傳遞當 Webhook 最終到達 Keel 的 Controller 後， Keel 根據設定來準備更新相關資源將差異性更新到 Kubernetes 內 上述的運作方式跟我們前篇提到的 Pull-Mode 不太一樣，因為還是透過 webhook 的方式主動通知 Keel 去更新，但是 Keel 本身也有提供別的機制來實現不同的架構，如同其官方內的文章介紹 Polling Since only the owners of docker registries can control webhooks - it's often convenient to use polling. Be aware that registries can be rate limited so it's a good practice to set up reasonable polling intervals. While configuration for each provider can be slightly different - each provider has to accept several polling parameters: 如圖上篇文章所說，不是每個 container registry 都能去控管 web hooks 的架構，我們等等的示範中會使用 docker registry 配上 Polling 的機制來實現這種稍微被動一點的更新 此外，相對於 Push mode, Pull 則是透過定期詢問的方式去確認有沒有新版本，因此更新的速度上可能會比 webhook 還來得慢一點。 下圖是比較完整的架構，用來敘述 Keel 整個專案的架構 整個架構圖非常簡單，首先右邊代表的是 Keel Controller 以及控管的 Kubernetes Cluster，其中可以 Keel 下方還有 Helm 的標誌，這意 味者 Keel 對於應用程式可以支援原生的 Yaml 也可以支援用 Helm 控管的應用程式 左邊有三個框架，最上面代表的是 KEEL 支援的 Kubernetes 版本，不論是官方原生， Rancher 或者 Openshift 都支援。 下面則是所支援的 Container Registry 版本，譬如是 Quay, Harbor, Docker 或是其他公有雲的 Cloud Registry。 最下面則是一些通知系統，包含 Slack, Mattermost, Hipchat 等，此外 keel 還可以支援審核機制，當要部署的時候會發通知到 slack 等系統，需要有人按下同意後，才會繼續執行後續的動作。 有更多的興趣可以參閱官方網站 安裝 安裝方面提供兩種做法，可以透過 helm 去安裝或是直接透過 kubectl 安裝原生 yaml 檔案 $ helm repo add keel https://charts.keel.sh &quot;keel&quot; has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the &quot;keel&quot; chart repository Update Complete. ⎈Happy Helming!⎈ $ helm upgrade --install keel --namespace=kube-system keel/keel Release &quot;keel&quot; does not exist. Installing it now. NAME: keel LAST DEPLOYED: Sun Sep 13 03:28:06 2020 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. The keel is getting provisioned in your cluster. After a few minutes, you can run the following to verify. To verify that keel has started, run: kubectl --namespace=kube-system get pods -l &quot;app=keel&quot; 到這邊完畢我們就將 Keel Controller 安裝到 Kubernetes 叢集內了，接下來就來試試看如何使用 Keel 來完成自動部署 示範 接下來的示範流程如下 透過 Deployment 準備一個自己準備的 Container Image讓 Keel 幫忙部署該應用程式手動於別的畫面更新 Container Image觀察 Keel 的 log 以及 Kubernetes 狀況，確認該 container 有更新 首先，因為 Keel 本身沒有額外的 CRD 去告訴 Keel 到底哪些應用程式想要被 Keel 控管，因此控制的方式就是在應用程式的 Yaml 內增加 label，然後 Keel 的 controller 就會去監聽所有有設定這些規則的應用程式，再根據應用程式的內容來決定如何更新 下面是一個簡單的 deployment 的範例，該範例中我們於 metadata.labels 裡面增加兩個關於 keel 的敘述 apiVersion: apps/v1 kind: Deployment metadata: name: ithome namespace: default labels: name: &quot;ithome&quot; keel.sh/policy: all keel.sh/trigger: poll spec: replicas: 3 selector: matchLabels: app: ithome template: metadata: name: ithome labels: app: ithome spec: containers: - image: hwchiu/netutils:3.4.5 name: ithome keel.sh/policy: 這邊描述怎樣的 image tag 的變化是認可為有新版，keel 會希望image tag的版本都可以根據 SemVer 的方式使用 $major.$minor.$patch 來描述。 而今天我們使用 all 的含義是三者有任何一個版本更新，我們就會更新，預設會使用最新的版本。keel.sh/trigger 這邊描述我們不使用 webhook 的方式，而是改用去定期詢問遠方 image 是否有更新 接下來我們就來部署看看 $ kubectl apply -f deployment.yaml $ kubectl get deployment ithome -o jsonpath='{.spec.template.spec.containers[0].image}' hwchiu/netutils:3.4.5 接下來開啟其他視窗，嘗試部署一個全新的 image tag, 其版本必須大於 3.4.5，譬如我們使用 4.5.6 試試看 $ docekr push hwchiu/netutils:4.5.6 .... Successfully tagged hwchiu/netutils:4.5.6 The push refers to repository [docker.io/hwchiu/netutils] de527d59ee7c: Layer already exists 0c98ba7dbe5c: Layer already exists 64d2e4aaa54c: Layer already exists 0d3833376c2f: Layer already exists 4a048ea09024: Layer already exists b592b5433bbf: Layer already exists 4.5.6: digest: sha256:f2956ee6b5aafb43ec22aeeda10cfd20b92b3d01d9048908a25ef4430671b8a3 size: 1569 $ kubectl get deployment ithome -o jsonpath='{.spec.template.spec.containers[0].image}' hwchiu/netutils:4.5.6 不久後就可以觀察到系統上的 image 已經被改變了，此時去觀察中間層的 replicaset，就可以看到有 4.5.6 的出現 $ kubectl get rs -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR ithome-7d44545847 3 3 3 2m49s ithome hwchiu/netutils:4.5.6 app=ithome,pod-template-hash=7d44545847 ithome-7d5fb6757f 0 0 0 12m ithome hwchiu/netutils:3.4.5 app=ithome,pod-template-hash=7d5fb6757f 透過這樣的 Demo 過程，我們算是跑了一個基本的 Pull Mode 的更新，我們透過 Container Image 版本的更新來自動更新 Kubernetes 內部資源的狀態，這中間沒有牽扯到任何 CD Pipeline 的運作。 實際上這種運作模式後來也有一種更好的架構，稱為 GitOps，下篇開始我們就來認真學習一下 GitOps 的概念!","keywords":"","version":"Next"},{"title":"Secret 的部署問題與參考解法(上)","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-23","content":"","keywords":"","version":"Next"},{"title":"Helm​","type":1,"pageTitle":"Secret 的部署問題與參考解法(上)","url":"/docs/techPost/2020/iThome_Challenge/cicd-23#helm","content":"試想一個範例，我們的應用程式由 Helm 組成，會透過讀取檔案(Kubernetes Secret)的方式來獲取遠方資料庫的密碼。 今天要透過 Helm 部署這個應用程式的時候，我們會透過準備自己的 values.yaml 或是透過 --set dbpassword=xxx 等方式來客製化這個 secret 檔案，最後把全部的內容送到 Kubernetes 裡面。 ","version":"Next","tagName":"h3"},{"title":"原生 Yaml​","type":1,"pageTitle":"Secret 的部署問題與參考解法(上)","url":"/docs/techPost/2020/iThome_Challenge/cicd-23#原生-yaml","content":"如果是原生 Yaml 的情況下，我們沒有 --set 這類型的東西可以使用，變成我們要透過腳本的方式自行實現類似 Go-Template 的方法，或是動態產生一個 Secret 來使用。這部分不會太困難，只是就會讓人覺得有沒有更好的解決方案 ","version":"Next","tagName":"h3"},{"title":"Kustomize​","type":1,"pageTitle":"Secret 的部署問題與參考解法(上)","url":"/docs/techPost/2020/iThome_Challenge/cicd-23#kustomize","content":"基本上 Kustomize 是基於 overlay 的概念去組合出最後的 Yaml 檔案，所以作法跟原生 Yaml非常類似，好加在 Kustomize 本身有提供 secretGenerator 的語法，讓你更輕鬆的產生 Kubernetes Secret 物件檔案 cat &lt;&lt;'EOF' &gt; ./kustomization.yaml secretGenerator: - name: mysecrets envs: - foo.env files: - longsecret.txt literals: - FRUIT=apple - VEGETABLE=carrot EOF  Pipeline System 第一種架構是 Pipeline 系統本身有提供一些機密資訊的保護，譬如 Jenkins, Github Action, CircleCI..等。 系統中有一塊特別的資訊，讓使用者可以填入想要的 key:value 的數值，然後於 Pipeline 運作過程中，可以透過一些該平台限定的語法來取得。舉例來說 ","version":"Next","tagName":"h3"},{"title":"Github Action​","type":1,"pageTitle":"Secret 的部署問題與參考解法(上)","url":"/docs/techPost/2020/iThome_Challenge/cicd-23#github-action","content":"使用者先在專案列表中，把你想要用到的 key:value 加進去，接下來於 Github Action workflow 中使用 {{ secrets.xxxxx }} 的方式可以取出這些數值，然後這類型的數值再運行的 log 中會被系統給過濾掉，以 **** 的方式呈現。 steps: - name: Hello world action with: # Set the secret as an input super_secret: ${{ secrets.SuperSecret }} env: # Or as an environment variable super_secret: ${{ secrets.SuperSecret }}  其他的如 Jenkins/CircleCI 等不同系統都有一樣的機制可以使用，但是這種用法對於我們的 Kubernetes 應用程式來說要怎麼整合? 接下來我們嘗試將上述三種情境套入到這個架構中，看看會怎麼執行 ","version":"Next","tagName":"h3"},{"title":"Helm​","type":1,"pageTitle":"Secret 的部署問題與參考解法(上)","url":"/docs/techPost/2020/iThome_Challenge/cicd-23#helm-1","content":"CI/CD pipeline 運行到後面階段後，從系統中取出資料庫的帳號密碼，假設這個變數叫做 $password接下來要透過 helm 的方式來安裝我們的應用程式，因此會執行 helm upgrade --install --set dbpassword=$password . 等類似這樣的指令產生出最後的 secret 以及 pod，然後一起部署到 Kubernetes 裡面 ","version":"Next","tagName":"h3"},{"title":"Kustomize​","type":1,"pageTitle":"Secret 的部署問題與參考解法(上)","url":"/docs/techPost/2020/iThome_Challenge/cicd-23#kustomize-1","content":"CI/CD pipeline 運行到後面階段後，從系統中取出資料庫的帳號密碼，假設這個變數叫做 $password接下來透過腳本的方式，產生對應的 secretGenerator 寫入到相關的 Yaml 之中，之後呼叫 kubectl -k 以及 kustomize 這兩個指令最後去部署 ","version":"Next","tagName":"h3"},{"title":"原生 Yaml​","type":1,"pageTitle":"Secret 的部署問題與參考解法(上)","url":"/docs/techPost/2020/iThome_Challenge/cicd-23#原生-yaml-1","content":"CI/CD pipeline 運行到後面階段後，從系統中取出資料庫的帳號密碼，假設這個變數叫做 $password接下來透過 kubectl create secret ...... -o yaml 的方式產生對應的 Yaml 檔案，然後跟剩餘的內容一起部署到 kubernetes 內部 這樣的流程看起來似乎沒有問題，但是我認為有幾個地方要注意 假設今天應用程式需要用到的機密資訊很多，譬如 db_table, db_username, db_password, 甚至一些連接其他服務的帳號密碼，可能需要設定的東西就會非常多，變成你的 pipeline 那邊的設定變得非常多，同時大部分的 pipeline 系統都不會讓你編輯，有要修改就要整條換掉，同時通常也不會顯示明碼給你。呈上，當你要使用一個 pipeline 系統對應多個環境，譬如 dev/QA/staging/production 等多環境，你上述的變數量就會直接翻倍，然後那邊的數量就愈來愈多上述 helm upgrade --install ... 的部分一定是於 shell 去執行，這時候如果有些應用程式需要的機密資料本身就有雙引號，單引號等討人厭字元，就要特別注意跳脫的問題。我過往還遇過某些機密資訊本身是由一個 JSON 檔案組成的，裡面可說是雙引號滿天下，這時候的處理就變得非常頭疼今天這個作法是將機密資訊於 pipeline 系統來處理，但是如果採用的 GitOps 的做法，那就不會有 CD pipeline，因此這種解法也不可行。如果今天因為一些需求，需要替換整個 pipeline 系統，那管理人員會覺得很崩潰，因為整個系統要大搬移。 整個流程如下圖  今天我們就先探討這個架構，下一篇文章我們再來探討別的架構會如何解決這個問題 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iptables-masquerade-handson","content":"","keywords":"linux iptables nat","version":"Next"},{"title":"設定環境​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#設定環境","content":"git clone https://github.com/hwchiu/network-study pushd network-study/iptables/masquerade vagrant up  透過上述方式即可進入到測試環境中 觀察實作 這次的觀察著重於 Kernel Module 的部分，所以主要是針對 ipt_MASQUERADE.c 這個檔案，針對這檔案我進行了一些修改，主要是增加一個輸出相關資訊的函式，並且根據 NAT 呼叫前後來呼叫該函式輸出 ","version":"Next","tagName":"h2"},{"title":"修改程式碼​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#修改程式碼","content":" static unsigned int masquerade_tg(struct sk_buff *skb, const struct xt_action_param *par) { + unsigned int ret =0; struct nf_nat_range range; const struct nf_nat_ipv4_multi_range_compat *mr; @@ -71,13 +70,8 @@ masquerade_tg(struct sk_buff *skb, const struct xt_action_param *par) range.min_proto = mr-&gt;range[0].min; range.max_proto = mr-&gt;range[0].max; + printk(&quot;before nat\\n&quot;); + show_status(skb, par); + ret = nf_nat_masquerade_ipv4(skb, xt_hooknum(par), &amp;range, - return nf_nat_masquerade_ipv4(skb, xt_hooknum(par), &amp;range, xt_out(par)); + printk(&quot;after nat\\n&quot;); + show_status(skb, par); + return ret; }  最主要的修改就是執行 nf_nat_masquerade_ipv4 前後去呼叫 show_status 來輸出相關資訊。 ","version":"Next","tagName":"h2"},{"title":"Conntrack​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#conntrack","content":"往下繼續講之前，先稍微簡單提一下 Conntrack 的概念，對於每一條網路連線, Linux Kernel 都會用 Conntrack 這個結構去紀錄該連線的資訊。 每一條網路連線都代表者雙向的傳輸方向，這邊不管是 TCP/UDP 都享有這類型結構的紀錄，以下圖為例 每個 節點 都會有一份屬於自己的 conntrack 紀錄，而每個 conntrack 都會有Original,Reply 兩個方向的紀錄。 最簡單的情況下， Original 以及 Reply 完全顛倒的，就是 Original 的來源端相同於 Reply 的目的端。  ","version":"Next","tagName":"h2"},{"title":"結果觀察​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#結果觀察","content":" static void show_status(struct sk_buff *skb, const struct xt_action_param *par) { struct nf_conn *ct; struct nf_conntrack_tuple *tuple; enum ip_conntrack_info ctinfo; ct = nf_ct_get(skb, &amp;ctinfo); tuple = &amp;(ct-&gt;tuplehash[IP_CT_DIR_ORIGINAL].tuple); printk(&quot;ORIGINAL: outgoing interface: %s, src-ipv4:%pI4, src-port:%u, dst-ipv4:%pI4, dst-port:%u\\n&quot;, xt_out(par)-&gt;name, (void*)&amp;tuple-&gt;src.u3.ip, ntohs(tuple-&gt;src.u.tcp.port),(void*)&amp;tuple-&gt;dst.u3.ip, ntohs(tuple-&gt;dst.u.tcp.port)); tuple = &amp;(ct-&gt;tuplehash[IP_CT_DIR_REPLY].tuple); printk(&quot;REPLY: outgoing interface: %s, src-ipv4:%pI4, src-port:%u, dst-ipv4:%pI4, dst-port:%u\\n&quot;, xt_out(par)-&gt;name, (void*)&amp;tuple-&gt;src.u3.ip, ntohs(tuple-&gt;src.u.tcp.port),(void*)&amp;tuple-&gt;dst.u3.ip, ntohs(tuple-&gt;dst.u.tcp.port)); return; }  而 show_status 本身則是會去輸出，conntrack 的兩個方向，也就是所謂的 ORIGINAL 以及 REPLY。 輸出的內容概括了 封包出去的網卡該 conntrack 記錄到的來源 IP以及來源連接埠該 conntrack 記錄到的目標 IP以及目標連接埠 所以我們真正觀察的內容就是執行 NAT 前後 Conntrack 的變化 實驗 ","version":"Next","tagName":"h2"},{"title":"環境介紹​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#環境介紹","content":"本文章的所有環境都已經透過 Vagrant 來建立一個獨立且可重複的環境，主要的邏輯是 安裝 Linux Kernel header下載修改後的 kernel module 原始碼編譯該 Kernel module 並且安裝安裝 Docker 準備測試環境  # Install modified kernel module git clone https://github.com/hwchiu/network-study sudo apt-get install -y linux-headers-$(uname -r) cd network-study/iptables/masquerade/module sudo make sudo cp ipt_MASQUERADE.ko /lib/modules/$(uname -r)/kernel/net/ipv4/ netfilter/ipt_MASQUERADE.ko # Install ntp sudo apt-get install -y ntp # Install Docker export DOCKER_VERSION=&quot;18.06.3~ce~3-0~ubuntu&quot; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; sudo apt-get update sudo apt-get install -y docker-ce=${DOCKER_VERSION}  事先準備好 Vagrant 的環境，可以用下列的指令進入到實驗環境 git clone https://github.com/hwchiu/network-study pushd network-study/iptables/masquerade vagrant up vagrant ssh  ","version":"Next","tagName":"h2"},{"title":"連接埠觀察​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#連接埠觀察","content":"第一個實驗則是觀察參數的運作，即 --to-ports 的運作方式即結果，為了完成這個測試，我們需要做相關的操作 刪除 docker 所安裝的 iptables 指令安裝 kernel module (非必要，因為 vagrant 啟動時已經安裝到系統內)重新安裝 iptables 指令，並且設定 --to-ports 這邊要注意的是 --to-ports 本身只有特定的 Layer4 協定有支援，但是 DNS 並不支援，所以需要額外額外一條 iptables 規則來處理 DNS 的查詢。 透過 docker run 的方式去產生 TCP 封包，並且觀察 kernel 的輸出結果 ","version":"Next","tagName":"h2"},{"title":"設定 iptables​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#設定-iptables","content":"pushd ~/network-study/iptables/masquerade/module/ sudo iptables -t nat -D POSTROUTING -s 172.18.0.0/16 ! -o docker0 -j MASQUERADE || true sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/16 -p tcp ! -o docker0 -j MASQUERADE --to-ports 55666-55680|| true sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/16 ! -o docker0 -j MASQUERADE || true  如果今天自己有任何修改 kernel module 需求，則需要採用下列方式 pushd ~/network-study/iptables/masquerade/module/ sudo iptables -t nat -D POSTROUTING -s 172.18.0.0/16 ! -o docker0 -j MASQUERADE || true make sudo rmmod ipt_MASQUERADE sudo insmod ipt_MASQUERADE.ko sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/16 -p tcp ! -o docker0 -j MASQUERADE --to-ports 55666-55680|| true sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/16 ! -o docker0 -j MASQUERADE || true  主要差異在於我們需要重新安裝建制後的 kernel module 到系統內 ","version":"Next","tagName":"h3"},{"title":"測試流量​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#測試流量","content":"由於我們是透過 docker 的環境來使用 MASQUERADE，所以隨便找一個可以對外連線的 docker 來測試即可 sudo docker run --rm --entrypoint &quot;/usr/bin/wget&quot; hwchiu/netutils google.com  ","version":"Next","tagName":"h3"},{"title":"觀察結果​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#觀察結果","content":"sudo dmesg -c  輸入上述指令後，會看到類似於下列的輸出，我們挑重點來看就好 [ 371.727940] original [ 371.727943] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:33894, dst-ipv4:8.8.8.8, dst-port:53 [ 371.727944] REPLY: outgoing interface: eth0, src-ipv4:8.8.8.8, src-port:53, dst-ipv4:172.18.0.2, dst-port:33894 [ 371.727945] after nat [ 371.727946] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:33894, dst-ipv4:8.8.8.8, dst-port:53 [ 371.727947] REPLY: outgoing interface: eth0, src-ipv4:8.8.8.8, src-port:53, dst-ipv4:10.0.2.15, dst-port:33894 [ 371.754760] original [ 371.754763] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:37622, dst-ipv4:216.58.200.46, dst-port:80 [ 371.754764] REPLY: outgoing interface: eth0, src-ipv4:216.58.200.46, src-port:80, dst-ipv4:172.18.0.2, dst-port:37622 [ 371.754766] after nat [ 371.754767] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:37622, dst-ipv4:216.58.200.46, dst-port:80 [ 371.754768] REPLY: outgoing interface: eth0, src-ipv4:216.58.200.46, src-port:80, dst-ipv4:10.0.2.15, dst-port:55666 [ 371.792826] original [ 371.792851] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:42676, dst-ipv4:8.8.8.8, dst-port:53 [ 371.792859] REPLY: outgoing interface: eth0, src-ipv4:8.8.8.8, src-port:53, dst-ipv4:172.18.0.2, dst-port:42676 [ 371.792862] after nat [ 371.792863] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:42676, dst-ipv4:8.8.8.8, dst-port:53 [ 371.792864] REPLY: outgoing interface: eth0, src-ipv4:8.8.8.8, src-port:53, dst-ipv4:10.0.2.15, dst-port:42676 [ 371.807542] original [ 371.807545] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:34276, dst-ipv4:172.217.160.100, dst-port:80 [ 371.807546] REPLY: outgoing interface: eth0, src-ipv4:172.217.160.100, src-port:80, dst-ipv4:172.18.0.2, dst-port:34276 [ 371.807548] after nat [ 371.807549] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:34276, dst-ipv4:172.217.160.100, dst-port:80 [ 371.807550] REPLY: outgoing interface: eth0, src-ipv4:172.217.160.100, src-port:80, dst-ipv4:10.0.2.15, dst-port:55666 [ 371.982956] docker0: port 1(veth1cf6a5c) entered disabled state [ 371.984851] veth96adc5c: renamed from eth0 [ 372.014944] docker0: port 1(veth1cf6a5c) entered disabled state [ 372.016375] device veth1cf6a5c left promiscuous mode [ 372.016378] docker0: port 1(veth1cf6a5c) entered disabled state  因為 wget 抓取網頁的過程中，包含了 HTTP 連線以及 DNS 查詢，所以會有很多條 conntrack 產生，不太意外，最後的 docker 則是因為 docker --rm 跑完就結束的關係，導致相關的 veth 被收回。 我們抓前兩條 conntrack 來看即可，這邊要注意我們的參數是 --to-ports 55666-55680 [ 371.727940] before nat [ 371.727943] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:33894, dst-ipv4:8.8.8.8, dst-port:53 [ 371.727944] REPLY: outgoing interface: eth0, src-ipv4:8.8.8.8, src-port:53, dst-ipv4:172.18.0.2, dst-port:33894 [ 371.727945] after nat [ 371.727946] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:33894, dst-ipv4:8.8.8.8, dst-port:53 [ 371.727947] REPLY: outgoing interface: eth0, src-ipv4:8.8.8.8, src-port:53, dst-ipv4:10.0.2.15, dst-port:33894  針對第一條 conntrack，觀察如下 Before NAT ORIGINAL source_ip: 172.18.0.2source_port: 33894destination_ip: 8.8.8.8destination_port: 53 REPLY source_ip: 8.8.8.8source_port: 53destination_ip: 172.18.0.2destination_port: 33894 預設情況下系統都會假設 ORIGINAL 跟 REPLY 是完全對稱的，畢竟這是最簡單的模式。 After NAT ORIGINAL source_ip: 172.18.0.2source_port: 33894destination_ip: 8.8.8.8destination_port: 53 REPLY source_ip: 8.8.8.8source_port: 53destination_ip: 10.0.2.15destination_port: 33894 這邊可以看到一旦執行 NAT 後， ORIGINAL 完全沒有改變，但是 REPLY 裡面的 destination_ip 則更動了。 對於這樣的改動可以有下列的解讀方式 預設情況下， MASQUERADE 對於 DNS 不會特別幫你挑選連接埠，而是你本來送過來前是什麼，就是什麼。 TCP也是，可以自行修改規則去驗證對於 Linux Kernel 來說，這條 conntrack 的兩個方向分別對應 來源封包從容器送到網卡回應封包從外部到網卡 當這條 conntrack 被確認之後，接下來所有從 ORIGINAL 的封包就會自動地被修正其 source ip，就不會被 iptables 的規則再次執行了，算是一個小加速。 [ 371.754760] before nat [ 371.754763] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:37622, dst-ipv4:216.58.200.46, dst-port:80 [ 371.754764] REPLY: outgoing interface: eth0, src-ipv4:216.58.200.46, src-port:80, dst-ipv4:172.18.0.2, dst-port:37622 [ 371.754766] after nat [ 371.754767] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:37622, dst-ipv4:216.58.200.46, dst-port:80 [ 371.754768] REPLY: outgoing interface: eth0, src-ipv4:216.58.200.46, src-port:80, dst-ipv4:10.0.2.15, dst-port:55666  接下來看一下 wget 指令的主體， *HTTP 連線的狀況 針對第一條 conntrack，觀察如下 Before NAT ORIGINAL source_ip: 172.18.0.2source_port: 37622destination_ip: 216.58.200.46destination_port: 80 REPLY source_ip: 216.58.200.46source_port: 80destination_ip: 172.18.0.2destination_port: 37622 預設情況下系統都會假設 ORIGINAL 跟 REPLY 是完全對稱的，畢竟這是最簡單的模式。 After NAT ORIGINAL source_ip: 172.18.0.2source_port: 37622destination_ip: 216.58.200.46destination_port: 80 REPLY source_ip: 216.58.200.46source_port: 53destination_ip: 10.0.2.15destination_port: 55666 這邊可以看到一旦執行 NAT 後， ORIGINAL 完全沒有改變，但是 REPLY 裡面的 destination_ip 以及 destination_port 則更動了。 這邊的解讀去上述相同，唯一不同的是 port 最後變成 55666，這個就是我們先前透過 --to-ports 55666-55680 去設定的區間。 ","version":"Next","tagName":"h3"},{"title":"IP 選擇觀察​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#ip-選擇觀察","content":"第二個實驗則是要觀察如果網卡有多重 IP 的話，會怎麼選擇，為了完成這個測試我們需要做相關的操作 對主要輸出網卡新增一個不同網段的 IP 地址透過 docker run 的方式去產生 TCP 封包，並且觀察 kernel 的輸出結果 這部分我們可以直接沿用上個實驗的 iptables 規則，所以不需要額外處理，當然如果有需要修改 kernel module 來觀察的話，就需要進行一樣的動作 ","version":"Next","tagName":"h2"},{"title":"設定IP​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#設定ip","content":"Vagrant 的環境中，eth0 是主要的對外網卡，我們對其增加一個不同網段的 IP，同時也增加了一個路由規則，希望送往 1.1.1.1 的封包會嘗試先走到 1.2.3.4 去 sudo ip addr add 1.2.3.56/24 dev eth0 sudo route add 1.1.1.1 gw 1.2.3.4 dev eth0  接者我們觀察預設的路由表與 IP 設定 vagrant@linux-study:~/network-study/iptables/masquerade/module$ sudo route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.0.2.2 0.0.0.0 UG 100 0 0 eth0 1.1.1.1 1.2.3.4 255.255.255.255 UGH 0 0 0 eth0 1.2.3.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 10.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 10.0.2.2 0.0.0.0 255.255.255.255 UH 100 0 0 eth0 172.17.8.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 172.18.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 vagrant@linux-study:~/network-study/iptables/masquerade/module$ ip addr show dev eth0 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:c2:be:11 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0 valid_lft 81500sec preferred_lft 81500sec inet 1.2.3.56/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fec2:be11/64 scope link valid_lft forever preferred_lft forever  這邊只要確認 1.1.1.1/32 會走 eth0 並且 gateway 是 1.2.3.41.2.3.0/24 會走 eth0，因為 eth0 本身有相同網段的 IP，因此就不需要 gateway 的處理，直接轉發即可。 ","version":"Next","tagName":"h3"},{"title":"測試流量​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/iptables-masquerade-handson#測試流量-1","content":"vagrant@linux-study:~/network-study/iptables/masquerade/module$ sudo docker run --rm --entrypoint &quot;/usr/bin/wget&quot; hwchiu/netutils 1.1.1.1 --2020-01-02 04:28:29-- http://1.1.1.1/ Connecting to 1.1.1.1:80... failed: No route to host.  這邊我們不需要考慮目的地的網頁伺服器是否真的存在，同時這個範例中也不考慮 DNS 的查詢，我們直接觀察 TCP 的結果即可 根據前篇文章的探討， MASQUERADE 會先查詢 routing table 中的規則得到要送出的 next-hop。接者從目標網卡的眾多 非 SECONDARY IP 地址中去挑選一個符合的來送出。 按照上述的猜想，整個過程會 根據 routing table, 1.1.1.1 的 next-hop 是 1.2.3.4eth0 上面能夠符合 1.2.3.4 的 IP 是 1.2.3.56/24 所以最後預期送出去的 IP 必須是 **1.2.3.56 接下來觀察 1.1.1.1 的結果並驗證猜測 [ 1879.823479] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:59734, dst-ipv4:1.1.1.1, dst-port:80 [ 1879.823480] REPLY: outgoing interface: eth0, src-ipv4:1.1.1.1, src-port:80, dst-ipv4:172.18.0.2, dst-port:59734 [ 1879.823482] after nat [ 1879.823483] ORIGINAL: outgoing interface: eth0, src-ipv4:172.18.0.2, src-port:59734, dst-ipv4:1.1.1.1, dst-port:80 [ 1879.823484] REPLY: outgoing interface: eth0, src-ipv4:1.1.1.1, src-port:80, dst-ipv4:1.2.3.56, dst-port:55666  Before NAT ORIGINAL source_ip: 172.18.0.2source_port: 59734destination_ip: 1.1.1.1destination_port: 80 REPLY source_ip: 1.1.1.1source_port: 80destination_ip: 172.18.0.2destination_port: 59734 預設情況下系統都會假設 ORIGINAL 跟 REPLY 是完全對稱的，畢竟這是最簡單的模式。 After NAT ORIGINAL source_ip: 172.18.0.2source_port: 59734destination_ip: 1.1.1.1destination_port: 80 REPLY source_ip: 1.1.1.1source_port: 80destination_ip: 1.2.3.56destination_port: 55666 這邊可以觀察到最後轉換的 IP 地址則是 1.2.3.56 而不是最初 eth0 上面的 10.0.2.15/24, 也算是稍微驗證了一下多重 IP 的選擇，並非單純的先來後到，而是還要考慮到網段的符合。 Summary 本篇文章透過修改 MASQUERADE 原始碼的方式來觀察其運作的方式，得到的結論是 不論是 SNAT 或是 DNAT，其最後的修改都體現在 conntrack 這個結構上，透過修改 ORIGINAL 或是 REPLY 兩個方向來預期最後看到的封包結構。 對於 MASQUERADE(TCP/DNS) 來說，預設的情況下不會幫你修改封包的來源連結埠，若有需要修改必須要透過之前提過的參數 --to-ports 以及 --random 來修改，本文中沒有特別提到 --random 的部分是因為與 --to-ports 大同小異，自行修改相關指令就可以進行測試。 ","version":"Next","tagName":"h3"},{"title":"Secret 的部署問題與參考解法(下)","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-24","content":"","keywords":"","version":"Next"},{"title":"Helm​","type":1,"pageTitle":"Secret 的部署問題與參考解法(下)","url":"/docs/techPost/2020/iThome_Challenge/cicd-24#helm","content":"一開始 Helm 裡面就要準備好相關的設定檔案，包含帳號密碼要用什麼東西問等，這部分取決於實作的需求CI/CD pipeline 運行到後面階段後，沒有什麼事情要特別處理，因此直接透過 helm upgrade --install ... 的方式部署到 kubernetes內部當應用程式部署進去後，應用程式或是 sidecar 就會根據當前的設定去詢問需要的機密資訊，最後產生相對應的檔案給應用程式讀取 ","version":"Next","tagName":"h3"},{"title":"Kustomize/Yaml​","type":1,"pageTitle":"Secret 的部署問題與參考解法(下)","url":"/docs/techPost/2020/iThome_Challenge/cicd-24#kustomizeyaml","content":"基本上這兩者的做法一樣，其實也跟 Helm 差不多，因為不需要於 pipeline 系統中動態取得這些機密資訊，所以這個步驟也不太需要客製化。一切的運作邏輯跟 Helm 一樣，只要確保部署進去的資源能夠與 secret 管理者溝通即可。 這種架構下帶來的好處有 因為與 pipeline 的部分都抽離了，所以 GItOps 就有機會實現大部分機密資訊的保存與管理都跟 pipeline 系統無關，要抽換容易這類型 secret 管理者的介面跟操作能力都比 pipeline 系統提供的介面強超級多，用起來相對彈性使用上與設計上稍嫌複雜，同時 secret 管理者的存活要自己負責，如果要設計 HA 的架構就要花一些心思去探索  Centralized Management(ii) 這種架構是上一種的不同實作方式，就如同所說的其實 secret manager 位置放在哪邊不是重點，重點是什麼時間點去呼叫來取得資訊。 如果今天我們依然還是在 Pipeline 系統中去取得這些資訊，那基本上我們還是會得到跟第一篇架構一樣的結果。 由於這類型的服務本身也需要一些 token/key 才可以存取，而現在存取的時間點是 pipeline 系統上，所以我們只要利用 pipeline 系統本 身的管理機制去管理這些 token/key 即可。剩下應用程式所需要的機密資訊就跟遠方的 secret manager 去獲取即可。 但是這邊有一個要特別注意的地方，因為我們的 secret 管理者現在已經搬移到系統外面，所以 pipeline 系統對於你的各種操作都沒有辦 法辨認哪些東西是機密，哪些不是機密，因此 log 檔案會有機會將你的操作內容全部記錄下來，包含你的帳號與密碼。 這部分要特別小 心，避免這些資訊遺漏否則可能會釀成大禍。 之前提到的三個情境再這個架構下的實作方法大同小異，這邊就不再贅述，相關的隱憂基本上也都存在，只有搬移平台部分會是相對順 利，因為所有的存取方式與內容都與 pipeline 系統抽離。  Encrption/Decryption 最後一個架構的做法是透過加解密的方式來記錄這些資訊，過往我們會擔心 Kubernetes Secret 的資訊是因為其本身只有 base64 編碼，並不是加密，因此實際上就是沒有安全性可言。 那如果我們有辦法把它變成加密後的結果，是否就會更有信心的放到 Yaml 上面，直接使用 Git 保存? 這個架構就是基於這樣的想法，其運作流程如下(概念，實作不一定) Kubernetes 中要安裝一個 Controller，這個 Controller 會準備 key，這把 key 會用來加密跟解密Kubernetes 中新增一個全新的物件，譬如叫做 Encrypted Secret，代表被加密後的 secret 資料。叢集管理者使用那把 key 將機密資料加密，得到加密後的結果，並且把這個結果寫到 EncryptedSecret 的 Yaml 中當未來這個 EncryptedSecret 被部署到叢集之中，該 Controller 就會偵測到，並且使用 key 去解密解密成功後就會把解密的內容產生一個 Secret ，這時候應用程式就可以去讀取來使用 簡單來說，上述的流程也是把取得機密內容的時間點延後到 Kubernetes 內部，只是他的做法是一開始加密，直到進入 Kubernetes 後就將其解密，最後得到真正資訊。 接下來我們嘗試將上述三種情境套入到這個架構中，看看會怎麼執行 ","version":"Next","tagName":"h3"},{"title":"Helm​","type":1,"pageTitle":"Secret 的部署問題與參考解法(下)","url":"/docs/techPost/2020/iThome_Challenge/cicd-24#helm-1","content":"一開始 Helm 裡面就要先透過 key 去加密，準備好一個 Encrypted Secret 的物件CI/CD pipeline 運行到後面階段後，沒有什麼事情要特別處理，因此直接透過 helm upgrade --install ... 的方式部署到 kubernetes內部當應用程式部署進去後，該 controller 偵測到 Encrypted Secret 的出現，就會幫其解密，解密後產生對應的 secret 物件，然後應用程式去讀取 ","version":"Next","tagName":"h3"},{"title":"Kustomize/Yaml​","type":1,"pageTitle":"Secret 的部署問題與參考解法(下)","url":"/docs/techPost/2020/iThome_Challenge/cicd-24#kustomizeyaml-1","content":"基本上這兩者的做法一樣，其實也跟 Helm 差不多，因為不需要於 pipeline 系統中動態取得這些機密資訊，所以這個步驟也不太需要客製化。一切的運作邏輯跟 Helm 一樣，只要確保部署進去的資源能夠與 secret 管理者溝通即可。 這種架構帶來的好壞處有 本身也不需要 CD pipeline 的參與，所以 GitOps 的概念可以實現如同上述架構一樣， Controller 的生命尤為重要，要保護好他的生命以及相關的 key機密資訊都直接存放到 Git 上面，每次要修改都要有對應權限的人去產生加密後的結果，有可能會讓工作效率比較低，但是安全性更高  到這邊為止我們探討了數種可能的解決方式與架構，下一篇我們將實際操作一個範例來展示其用法 ","version":"Next","tagName":"h3"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2020/ipvs-3","content":"","keywords":"linux ipvs","version":"Next"},{"title":"Dynamic Option​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-3#dynamic-option","content":"第一種方式是整個 Kernel 所提供的除錯方式, 稱之為 dynamic-debug，所有 kernel function 都可以向 kernel 註冊一組輸出格式與條件，當今天管理員有需求時就可以動態的打開這些開關使得 kernel 開始輸出相關訊息。 系統上支援的 dynamic debug 都存放於 /sys/kernel/debug/dynamic_debug/control 這個路徑，透過 cat 的方式可以看到的內容都是由 filename:line_number, [kernel_module name], topic, output_format 這樣的格式，譬如下列範例 cat /sys/kernel/debug/dynamic_debug/control net/netfilter/xt_LOG.c:60 [xt_LOG]log_tg_check =_ &quot;prefix is not null-terminated\\012&quot; net/netfilter/xt_LOG.c:55 [xt_LOG]log_tg_check =_ &quot;level %u &gt;= 8\\012&quot; net/bridge/netfilter/ebtables.c:2260 [ebtables]compat_do_replace =_ &quot;tmp.entries_size %d, kern off %d, user off %d delta %d\\012&quot; net/bridge/netfilter/ebtables.c:2129 [ebtables]size_entry_mwt =_ &quot;change offset %d to %d\\012&quot; net/bridge/netfilter/ebtables.c:1775 [ebtables]compat_calc_entry =_ &quot;0x%08X -&gt; 0x%08X\\012&quot; net/netfilter/ipvs/ip_vs_proto.c:266 [ip_vs]ip_vs_tcpudp_debug_packet_v6 =_ &quot;%s: %s %s\\012&quot; net/netfilter/ipvs/ip_vs_proto.c:234 [ip_vs]ip_vs_tcpudp_debug_packet_v4 =_ &quot;%s: %s %s\\012&quot;  前面代表的是每個檔案的路徑與行數，再來是對應的 kernel module name 以及標題，最後是其輸出的格式。 對於管理者來說，可以針對 檔案名稱, 行數 或是 kernel module name 來進行開關，譬如我今天想要打開跟 ipvs** 有關的所有輸出，就可以執行 echo -n 'module ip_vs +p' &gt; /sys/kernel/debug/dynamic_debug/control  這樣 kernel 就會知道把 control 裡面全部屬於 ip_vs 的訊息都輸出，譬如 [ 3415.992622] ip_vs_tcpudp_debug_packet_v4:234: IPVS: ip_vs_in: packet continues traversal as normal: TCP 10.0.2.2:58199-&gt;10.0.2.15:22  對於想要細部觀察封包流向來說，也是不無小補，提供一些資訊觀察 ","version":"Next","tagName":"h2"},{"title":"IP_VS_DEBUG​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-3#ip_vs_debug","content":"第二個則是由 IPVS 自己實作的除錯方式，這個除錯方式相關的功能則是在編譯期間根據參數 IP_VS_DEBUG 來定是否要一起編譯, 預設情況下這個功能是關閉的，這也意味各位如果都適用已經建置好的 IPVS 那基本上是沒有辦法打開這個功能的。 為了打開這個功能，你必須要執行下列步驟 準備對應你系統版本的 kernel source code加入相關的參數，重新建置 IP_VS 的 kernel module移除舊有的 kernel module 並安裝新的到系統內 如果你有興趣要嘗試看看的話，這邊可以顯示一下大概的修改過程 (假設你準備好相關的 kernel source code) 之後 先複製系統目前使用的 kernel config (Ubutnu 為範例)根據該設定檔案重新加入 IP_VS_DEBUG 的參數重新建置 Kernel Module移除舊有的 Kernel Module, 並且加入新的 cp /boot/config-`uname -r` .config make menuconfig sudo rmmod ip_vs_wlc sudo rmmod ip_vs sudo insmod net/netfilter/ipvs/ip_vs.ko sudo insmod net/netfilter/ipvs/ip_vs_wlc.ko make -j4  其中第二部會彈出一個灰色畫面，可以透過 f 進行參數的搜尋，找到對應的位置，然後將其打該設定成 Y 即可，畫面如下圖中也可以看到 IPVS 滿滿的參數有哪些，其中標示為 &quot;M&quot; 都代表會建置成獨立的 kernel module，如果有興趣的也可以將全部變成 Build-in 的方式，這意味 kernel 本身就會包含這些功能，不需要額外在那邊 insmod/rmmod。 此外如果你本身已經有建置過 kernel 的話，可以直接輸入 make modules 單純編譯 kernel modules 相關即可 完成上述步驟之後就可以在系統上的位置發現多了一個路徑 /proc/sys/net/ipv4/vs/debug_level 該路徑對應到 kernel 內則是一個整數，預設是 0, 今天如果要打開 debug 功能的話，就輸入數字到該路徑即可，譬如  echo &quot;12&quot; &gt; /proc/sys/net/ipv4/vs/debug_level  一但該功能打開後，就會看到系統噴出滿滿的除錯訊息，譬如 [391968.554844] IPVS: Enter: ip_vs_out, net/netfilter/ipvs/ip_vs_core.c line 1352 [391968.554846] IPVS: Enter: ip_vs_proto_name, net/netfilter/ipvs/ip_vs_core.c line 83 [391968.554847] IPVS: lookup/out TCP 172.17.8.111:37926-&gt;172.17.8.156:80 not hit [391968.554848] IPVS: Enter: sysctl_nat_icmp_send, net/netfilter/ipvs/ip_vs_core.c line 671 [391968.554849] IPVS: Enter: ip_vs_local_request4, net/netfilter/ipvs/ip_vs_core.c line 2074 [391968.554850] IPVS: Enter: ip_vs_in, net/netfilter/ipvs/ip_vs_core.c line 1884 [391968.554851] IPVS: Enter: ip_vs_proto_name, net/netfilter/ipvs/ip_vs_core.c line 83 [391968.554852] IPVS: lookup/in TCP 172.17.8.111:37926-&gt;172.17.8.156:80 hit [391968.554853] IPVS: Enter: is_new_conn, net/netfilter/ipvs/ip_vs_core.c line 1084 [391968.554854] IPVS: Enter: sysctl_expire_nodest_conn, net/netfilter/ipvs/ip_vs_core.c line 677 [391968.554854] IPVS: Enter: is_new_conn_expected, net/netfilter/ipvs/ip_vs_core.c line 1111 [391968.554855] IPVS: Enter: ip_vs_in_stats, net/netfilter/ipvs/ip_vs_core.c line 117 [391968.554856] IPVS: Enter: ip_vs_set_state, net/netfilter/ipvs/ip_vs_core.c line 209 [391968.554858] IPVS: Enter: ip_vs_nat_xmit, net/netfilter/ipvs/ip_vs_xmit.c line 756 [391968.554859] IPVS: Enter: __ip_vs_get_out_rt, net/netfilter/ipvs/ip_vs_xmit.c line 325 [391968.554860] IPVS: Enter: __ip_vs_dst_check, net/netfilter/ipvs/ip_vs_xmit.c line 98 [391968.554861] IPVS: Enter: crosses_local_route_boundary, net/netfilter/ipvs/ip_vs_xmit.c line 179 [391968.554862] IPVS: Enter: decrement_ttl, net/netfilter/ipvs/ip_vs_xmit.c line 269 [391968.554863] IPVS: Enter: ensure_mtu_is_adequate, net/netfilter/ipvs/ip_vs_xmit.c line 226 [391968.554865] IPVS: Enter: ip_vs_nat_send_or_cont, net/netfilter/ipvs/ip_vs_xmit.c line 624 [391968.554868] IPVS: Enter: ip_vs_nat_send_or_cont, net/netfilter/ipvs/ip_vs_xmit.c line 641  Module Init 如上面所述， 我們將從 ip_vs_core.c 來探討整個 ip_vs 的初始化過程 以下的程式碼來自 Linux 4.15 GitHub ip_vs_core.c 每個 Kernel Module 都會從 module_init 這邊開始，傳入的參數都會是一個 function，當 Module 被載入後這個 function 就會被執行，也可以想成 modprobe ip_vs 呼叫後，這個 function 就會先被執行 module_init(ip_vs_init);  根據上述語法，可以觀察到 ip_vs_init 這個 function static int __init ip_vs_init(void) { int ret; ret = ip_vs_control_init(); if (ret &lt; 0) { pr_err(&quot;can't setup control.\\n&quot;); goto exit; } ip_vs_protocol_init(); ret = ip_vs_conn_init(); if (ret &lt; 0) { pr_err(&quot;can't setup connection table.\\n&quot;); goto cleanup_protocol; } ret = register_pernet_subsys(&amp;ipvs_core_ops); /* Alloc ip_vs struct */ if (ret &lt; 0) goto cleanup_conn; ret = register_pernet_device(&amp;ipvs_core_dev_ops); if (ret &lt; 0) goto cleanup_sub; ret = ip_vs_register_nl_ioctl(); if (ret &lt; 0) { pr_err(&quot;can't register netlink/ioctl.\\n&quot;); goto cleanup_dev; } pr_info(&quot;ipvs loaded.\\n&quot;); return ret; cleanup_dev: unregister_pernet_device(&amp;ipvs_core_dev_ops); cleanup_sub: unregister_pernet_subsys(&amp;ipvs_core_ops); cleanup_conn: ip_vs_conn_cleanup(); cleanup_protocol: ip_vs_protocol_cleanup(); ip_vs_control_cleanup(); exit: return ret; } module_init(ip_vs_init); module_exit(ip_vs_cleanup);  通常觀察一個 kernel module 就是先觀察 init function, 看看到底初始化哪些相關的資訊，其中上面呼叫的函示有 ip_vs_control_init 用來幫內部一些資料結構初始化，主要都是 HASH table 相關的資料結構 ip_vs_protocol_init 用來初始化支持的 Protocol，譬如 TCP,UDP,SCTP 等 根據前述的 make menuconfig 我們可以觀察到相關協定的支持也是可以透過參數去開關的。 ip_vs_conn_init 主要是用來初始化跟 connection 有關的資訊，connection 可以想成 Client 的請求與配置 Real Server 的關係表。 也是透過 connection 來確保相同連線的封包都會被轉發到相同的 Real Server 上 register_pernet_subsys/register_pernet_device 上述兩個函式非常有趣，他們的都是用來註冊 pernet 底下相關元件的函式 ，這邊的 pernet 的意思就是 每個 Network Namespace。 等等我們再來仔細看看到底做了什麼 ip_vs_register_nl_ioctl 用來註冊基於 netlink 的 function handler，這也是我們透過 ipvsadm 這個工具用來操作整個 kernel 內部邏輯的入口。所有的操作指令都會透過 netlink 從 userspace 送到 kernel space 並且呼叫起對應的 Function 來處理。 ","version":"Next","tagName":"h2"},{"title":"register_pernet_subsys/device​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-3#register_pernet_subsysdevice","content":"Kernel 對於 network namespace 的創建提供了兩個方法來註冊相關的 handler, 這兩個差異主要在於對背後資料結構操作的位置不同，詳細的可以參閱 Kernel network namespace, 這邊就單純針對 register_pernet_subsys 來研究。 直接看一下該函式的註解，簡單來說就是註冊一組任何 network namespace 被刪除與創建時都會被呼叫的函式，而要特別注意的是當註冊的瞬間，也會對所有已經存在的 network namespace 呼叫一次。  * register_pernet_subsys - register a network namespace subsystem * @ops: pernet operations structure for the subsystem * * Register a subsystem which has init and exit functions * that are called when network namespaces are created and * destroyed respectively. * * When registered all network namespace init functions are * called for every existing network namespace. Allowing kernel * modules to have a race free view of the set of network namespaces. * * When a new network namespace is created all of the init * methods are called in the order in which they were registered. * * When a network namespace is destroyed all of the exit methods * are called in the reverse of the order with which they were * registered.  接下來看一下這個函式的呼叫範例  ret = register_pernet_subsys(&amp;ipvs_core_ops); /* Alloc ip_vs struct */  其傳入的參數是一個名為 ipvs_core_ops 的結構，而該結構內容如下 static struct pernet_operations ipvs_core_ops = { .init = __ip_vs_init, .exit = __ip_vs_cleanup, .id = &amp;ip_vs_net_id, .size = sizeof(struct netns_ipvs), };  這個結構內最重要的就是 .init 這個 function pointer，其意思是 每當有任何一個 namespace 被創建之時，請呼叫對應的 function 來處理，而這邊這個 function 就是 __ip_vs_init. /* * Initialize IP Virtual Server netns mem. */ static int __net_init __ip_vs_init(struct net *net) { struct netns_ipvs *ipvs; int ret; ipvs = net_generic(net, ip_vs_net_id); if (ipvs == NULL) return -ENOMEM; /* Hold the beast until a service is registerd */ ipvs-&gt;enable = 0; ipvs-&gt;net = net; /* Counters used for creating unique names */ ipvs-&gt;gen = atomic_read(&amp;ipvs_netns_cnt); atomic_inc(&amp;ipvs_netns_cnt); net-&gt;ipvs = ipvs; if (ip_vs_estimator_net_init(ipvs) &lt; 0) goto estimator_fail; if (ip_vs_control_net_init(ipvs) &lt; 0) goto control_fail; if (ip_vs_protocol_net_init(ipvs) &lt; 0) goto protocol_fail; if (ip_vs_app_net_init(ipvs) &lt; 0) goto app_fail; if (ip_vs_conn_net_init(ipvs) &lt; 0) goto conn_fail; if (ip_vs_sync_net_init(ipvs) &lt; 0) goto sync_fail; ret = nf_register_net_hooks(net, ip_vs_ops, ARRAY_SIZE(ip_vs_ops)); if (ret &lt; 0) goto hook_fail; return 0; /* * Error handling */ hook_fail: ip_vs_sync_net_cleanup(ipvs); sync_fail: ip_vs_conn_net_cleanup(ipvs); conn_fail: ip_vs_app_net_cleanup(ipvs); app_fail: ip_vs_protocol_net_cleanup(ipvs); protocol_fail: ip_vs_control_net_cleanup(ipvs); control_fail: ip_vs_estimator_net_cleanup(ipvs); estimator_fail: net-&gt;ipvs = NULL; return -ENOMEM; }  該函式的註解直接標示，針對每一個 network namespace 去進行相關的初始化，其中為重要的則是 ip_vs_control_net_init 以及 nf_register_net_hooks 這兩個函式，後者則是與 netfilter 也就是 iptables 相關的互動，下一章節我們再來仔細看一下這個函式。 ","version":"Next","tagName":"h2"},{"title":"ip_vs_control_net_init​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2020/ipvs-3#ip_vs_control_net_init","content":"根據上述的 debug 規則我們知道系統上會有一個路徑 /proc/sys/net/ipv4/vs 可以讓使用者與之互動，而這個介面的初始化其實就是透過 ip_vs_control_net_init 來完成的。 int __net_init ip_vs_control_net_init(struct netns_ipvs *ipvs) { int i, idx; ... ... if (ip_vs_control_net_init_sysctl(ipvs)) goto err; ...  其呼叫了 ip_vs_control_net_init_sysctl #ifdef CONFIG_SYSCTL static int __net_init ip_vs_control_net_init_sysctl(struct netns_ipvs *ipvs) { struct net *net = ipvs-&gt;net; int idx; struct ctl_table *tbl; ... idx = 0; ... tbl[idx++].data = &amp;ipvs-&gt;sysctl_backup_only; ipvs-&gt;sysctl_conn_reuse_mode = 1; tbl[idx++].data = &amp;ipvs-&gt;sysctl_conn_reuse_mode; tbl[idx++].data = &amp;ipvs-&gt;sysctl_schedule_icmp; tbl[idx++].data = &amp;ipvs-&gt;sysctl_ignore_tunneled; ... ipvs-&gt;sysctl_hdr = register_net_sysctl(net, &quot;net/ipv4/vs&quot;, tbl); ... return 0 }  上述的概念就是創建一個 struct ctl_table *tbl 的物件，並且填入相關的資訊後，呼叫 register_net_stsctl 來註冊這一系列的路徑，也可以觀察到其子路徑就是 net/ipv4/vs. 而這些 table 的資料的定義如下 /* * IPVS sysctl table (under the /proc/sys/net/ipv4/vs/) * Do not change order or insert new entries without * align with netns init in ip_vs_control_net_init() */ static struct ctl_table vs_vars[] = { { .procname = &quot;amemthresh&quot;, .maxlen = sizeof(int), .mode = 0644, .proc_handler = proc_dointvec, }, { .procname = &quot;am_droprate&quot;, .maxlen = sizeof(int), .mode = 0644, .proc_handler = proc_dointvec, }, { .procname = &quot;drop_entry&quot;, .maxlen = sizeof(int), .mode = 0644, .proc_handler = proc_do_defense_mode, }, .... #ifdef CONFIG_IP_VS_DEBUG { .procname = &quot;debug_level&quot;, .data = &amp;sysctl_ip_vs_debug_level, .maxlen = sizeof(int), .mode = 0644, .proc_handler = proc_dointvec, }, #endif { } };  這邊用一種格式化的方式去定義每個路徑的 名稱, 類別， 相關的處理函式以及存取模式，特別注意到的是 debug_level 本身則是被 ifdef CONFIG_IP_VS_DEBUG 這個 macro 給包起來，所以如果沒有特別處理的話，預設情況下就不會把 debug_level 給編譯進去。 觀看這些資料還有一個好處就是你可以知道系統中有哪些參數可以餵給 Kernel 去處理，同時也可以搭配觀看原始碼的方式來了解這些參數到底實際上做了什麼。 這邊有一個概念要注意的就是 net 這個物件是 kernel 內網路系統最重要的結構，每個 netowrk namespace 都會有一個自己的副本，基本上只要函數本身有吃 net 參數，就可以猜到這個功能絕對是每個 network namespace 獨立。這邊要特別注意的是 Host 本身也是一個 network namespace，所以當我們註冊這個函式的時候，就會先針對系統本身這個 network namespace 呼叫這一組對應的 init function. 如果你的操作環境是舊版一點的 kernel ，譬如 4.4.0 之類的，你可以在系統觀察到類似下列的訊息 [3534469.163231] IPVS: Creating netns size=2192 id=1342 [3534793.388007] IPVS: Creating netns size=2192 id=1343 [3535052.371673] IPVS: Creating netns size=2192 id=1344 [3535706.550968] IPVS: Creating netns size=2192 id=1345 [3535761.378872] IPVS: Creating netns size=2192 id=1346 [3537083.860486] IPVS: Creating netns size=2192 id=1347  題外話， kernel code 內滿滿的 goto, goto 不是不能用，而是你要會用，可以看看 kernel 這邊的用法，同時你也可以想想不用 goto 的話，這些程式碼你會怎麼修改。 Summary 今天這個章節針對 IPVS 底層的資訊進行了粗略地探索，主要是知道要如何透過 IPVS 提供的方式來進行除錯，藉由這除錯方式之後會更方便地去整個瞭解程式的呼叫脈絡並且我認為也是一個很好的學習方式 對於 IPVS 的概念可以理解成，IPVS 的功能由 Kernel 提供，但是每個 network namespace 互相獨立不影響彼此，所以你到不同的 network namespace 內透過 ipvsadm 看到的結果都互相獨立，不會牽扯彼此。 透過註冊 register_pernet_subsys 這個函式， ipvs 能夠自動對系統上每個已經存在/未來新增的 network namespace 進行處理，幫其初始化 ipvs 相關的物件。 最後，由於 init 函式開始後就會去進行 sys 相關的初始化，來不及將 debug_level 設定而輸出，因此我在修改原始碼之後重新安裝到系統來觀察更細部的 init 流程。 [669447.783857] IPVS: Enter: ip_vs_init, net/netfilter/ipvs/ip_vs_core.c line 2371 [669447.783858] IPVS: Enter: ip_vs_control_init, net/netfilter/ipvs/ip_vs_ctl.c line 4122 [669447.783862] IPVS: Enter: ip_vs_protocol_init, net/netfilter/ipvs/ip_vs_proto.c line 338 [669447.783863] IPVS: Registered protocols (TCP, UDP, SCTP, AH, ESP) [669447.783864] IPVS: Enter: ip_vs_conn_init, net/netfilter/ipvs/ip_vs_conn.c line 1403 [669447.783937] IPVS: Connection hash table configured (size=4096, memory=64Kbytes) [669447.783937] IPVS: Each connection entry needs 288 bytes at least [669447.783944] IPVS: Enter: __ip_vs_init, net/netfilter/ipvs/ip_vs_core.c line 2257 [669447.783945] IPVS: Enter: ip_vs_control_net_init, net/netfilter/ipvs/ip_vs_ctl.c line 4037 [669447.783951] IPVS: Enter: ip_vs_control_net_init_sysctl, net/netfilter/ipvs/ip_vs_ctl.c line 3926 [669447.783963] IPVS: Enter: ip_vs_protocol_net_init, net/netfilter/ipvs/ip_vs_proto.c line 309 [669447.783966] IPVS: Enter: ip_vs_conn_net_init, net/netfilter/ipvs/ip_vs_conn.c line 1384 [669447.783968] IPVS: Enter: __ip_vs_dev_init, net/netfilter/ipvs/ip_vs_core.c line 2329 [669447.981664] IPVS: Enter: ip_vs_register_nl_ioctl, net/netfilter/ipvs/ip_vs_ctl.c line 4091 [669447.981678] IPVS: ipvs loaded.  下一章節我們會主要針對 封包的流向 來進行探討，包含 iptables/netfilter 與之的互動，以及實際由本地端送出一個封包後，實際上背後的運作邏輯是哪些，又會呼叫到哪些 function 來處理。 ","version":"Next","tagName":"h2"},{"title":"自架 Registry - Harbor","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-21","content":"","keywords":"","version":"Next"},{"title":"Harbor Config​","type":1,"pageTitle":"自架 Registry - Harbor","url":"/docs/techPost/2020/iThome_Challenge/cicd-21#harbor-config","content":"設定檔案內有非常多的細部設定可以處理，譬如 HTTPS 憑證的位置， HTTP/HTTPS 要使用的連接埠，預設的 admin 帳號密碼，相關元件的設定，大部分設定都可以使用預設值，唯獨 hostname 這個一定要修正，同時建議都要使用 HTTPS 來提供安全的連結能力 這邊我的範例是創建一個 registry.hwchiu.com 的DNS 紀錄，接下來過 Let's Encrypt 來獲得一個合法憑證，最後將這個合法憑證的路徑設定到 Harbor 的設定檔案內 # Configuration file of Harbor # The IP address or hostname to access admin UI and registry service. # DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients. hostname: registry.hwchiu.com # http related config http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80 # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /etc/letsencrypt/live/registry.hwchiu.com/fullchain.pem private_key: /etc/letsencrypt/live/registry.hwchiu.com/privkey.pem ....  這邊要特別注意， Harbor 本身沒有跟 Let's Encrypt 整合，這意味者如果使用這個方式，要特別處理每三個月的 Renew，最簡單的方式就是寫個當 Let's Encrypt Renew後，重新運行 Harbor 讓其將新的憑證檔案複製到容器內去使用。 如果要使用外部 Load balancer 的方式來處理 HTTPS，請特別注意這些相關 Issuehttps://github.com/goharbor/harbor/issues/12959https://goharbor.io/docs/1.10/install-config/troubleshoot-installation/#using-nginx-or-load-balancing 記得將設定檔案複製一份命名為 harboy.yaml，接者可以呼叫 prepare 來執行，從其說明文件可以看到有四個參數可以用來決定是否要額外支援其他第三方軟體，譬如 安全性掃描，或是 Helm3 使用的 Chart Museum 等 $ ./prepare --help prepare base dir is set to /home/ubuntu/harbor Usage: main.py prepare [OPTIONS] Options: --conf TEXT the path of Harbor configuration file --with-notary the Harbor instance is to be deployed with notary --with-clair the Harbor instance is to be deployed with clair --with-trivy the Harbor instance is to be deployed with Trivy --with-chartmuseum the Harbor instance is to be deployed with chart repository supporting --help Show this message and exit. $ ./prepare --with-trivy --with-chartmuseum prepare base dir is set to /home/ubuntu/harbor Generated configuration file: /config/log/logrotate.conf Generated configuration file: /config/log/rsyslog_docker.conf Generated configuration file: /config/nginx/nginx.conf Generated configuration file: /config/core/env Generated configuration file: /config/core/app.conf Generated configuration file: /config/registry/config.yml Generated configuration file: /config/registryctl/env Generated configuration file: /config/registryctl/config.yml Generated configuration file: /config/db/env Generated configuration file: /config/jobservice/env Generated configuration file: /config/jobservice/config.yml loaded secret from file: /data/secret/keys/secretkey Generated configuration file: /config/trivy-adapter/env Generated configuration file: /config/chartserver/env Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir  這時侯整個資料夾內產生很多資料 $ tree . . ├── common │ └── config │ ├── chartserver │ │ └── env │ ├── core │ │ ├── app.conf │ │ ├── certificates │ │ └── env │ ├── db │ │ └── env │ ├── jobservice │ │ ├── config.yml │ │ └── env │ ├── log │ │ ├── logrotate.conf │ │ └── rsyslog_docker.conf │ ├── nginx │ │ ├── conf.d │ │ └── nginx.conf │ ├── registry │ │ ├── config.yml │ │ └── passwd │ ├── registryctl │ │ ├── config.yml │ │ └── env │ ├── shared │ │ └── trust-certificates │ └── trivy-adapter │ └── env ├── common.sh ├── docker-compose.yml ├── harbor.v2.0.2.tar.gz ├── harbor.yml ├── harbor.yml.tmpl ├── install.sh ├── LICENSE └── prepare  最內層有各種服務會使用到的設定檔案，最外層則是提過的 dokcer-compose， $ docker-compose up -d .... $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------- chartmuseum ./docker-entrypoint.sh Up (healthy) 9999/tcp harbor-core /harbor/entrypoint.sh Up (healthy) harbor-db /docker-entrypoint.sh Up (healthy) 5432/tcp harbor-jobservice /harbor/entrypoint.sh Up (healthy) harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-portal nginx -g daemon off; Up (healthy) 8080/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:80-&gt;8080/tcp, 0.0.0.0:443-&gt;8443/tcp redis redis-server /etc/redis.conf Up (healthy) 6379/tcp registry /home/harbor/entrypoint.sh Up (healthy) 5000/tcp registryctl /home/harbor/start.sh Up (healthy) trivy-adapter /home/scanner/entrypoint.sh Up (healthy) 8080/tcp  這邊可以看到 Harbor 底下有超多的服務在運作，其中內部也有一個自己的 Nginx 再運行，這也是為什麼如果你外部有一個 nginx 或是其他的 load-balancer 的話，你需要針對裡面的 nginx 去進行設定，否則傳送進來的 HTTP Header 會有錯誤。 一切準備就緒後就可以嘗試透過網頁打開 https://registry.hwchiu.com 來存取看看 harbor 的服務 預設的登入密碼如果沒有修改的話都會是 Harbor12345， 所以可以使用 admin/Harbor12345 登入看看網頁   使用 架設好第一個 Harbor 的服務後，接下來我們想要來試試看幾個功能 Push/Pull Container Image安全性掃描功能導覽 ","version":"Next","tagName":"h2"},{"title":"Push/Pull Container Image​","type":1,"pageTitle":"自架 Registry - Harbor","url":"/docs/techPost/2020/iThome_Challenge/cicd-21#pushpull-container-image","content":"為了展示這個功能，我們必須先在 Harbor 上面創造一個全新的專案，基本上 UI 不算複雜，按照提示一步一步即可完成  完成後，進入到該專案頁面，就如同 Docker Hub 一樣，都會提示你該專案要怎麼使用，譬如你的tag, push 指令。 我的範例就會是 docker push registry.hwchiu.com/ithome/REPOSITORY[:TAG] 其中這邊要注意的是，他的 URL 是由 server/project/repository:tag 四個變數決定，所以一個專案底下可以有很多的 image repository, 而每個 repository 內又可以有很多個 tag 版本。  因為這個新架設創建的專案是 Private, 沒有經過登入是不能存取與觀看的，所以我們接下來要 透過 Docker login 到 https://registry.hwchiu.com下載我本來放在 Docker hub 上的 Image，並透過 docker tag 將其重新命名將新命名的 docker image 推向我們建置的 Harbor container registry $ docker login --username admin --password Harbor12345 https://registry.hwchiu.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded $ docker pull hwchiu/netutils [22/9415] Using default tag: latest latest: Pulling from hwchiu/netutils 6aa38bd67045: Pull complete 981ae4862c05: Pull complete 5bad8949dcb1: Pull complete ca9461589e70: Pull complete 58028a0a00a4: Pull complete 869f1b12c2d6: Pull complete Digest: sha256:be44189c4ebb9923e15885eac9cc976c121029789c2ddc7b7862a976a3f752a5 Status: Downloaded newer image for hwchiu/netutils:latest docker.io/hwchiu/netutils:latest $ docker tag hwchiu/netutils registry.hwchiu.com/ithome/netutils:latest $ docker push registry.hwchiu.com/ithome/netutils:latest The push refers to repository [registry.hwchiu.com/ithome/netutils] 33bd8247fba5: Pushed 733e8cbb9402: Pushed 377c01c3f4e3: Pushed 968d3b985bf4: Pushed 631dfaad8559: Pushed d908d9ad6713: Pushed latest: digest: sha256:be44189c4ebb9923e15885eac9cc976c121029789c2ddc7b7862a976a3f752a5 size: 1569  當一切都建置完畢後，回到 Harbor UI就會發現新的 image 已經推上來了  ","version":"Next","tagName":"h2"},{"title":"安全性掃描​","type":1,"pageTitle":"自架 Registry - Harbor","url":"/docs/techPost/2020/iThome_Challenge/cicd-21#安全性掃描","content":"接下來看一下預設的安全性掃描，此功能支援不同的實作方案，預設中我們使用的是 Trivy 這個專案 點選到目標 repository (netutils) 裡面後，選起我們的 tag，並於左上方點選 SCAN 功能，就可以看到右方的進度條正在運行，代表掃描中  掃瞄完畢之後點選 image 名稱進入到該 image 裡面就可以看到滿滿的潛在危險性報告，其中會針對每個 CVE 都有詳細的連結，並且把等級以及相關的軟體都列出來，對於有安全性需求的人可以考慮試試看這個方案。  ","version":"Next","tagName":"h2"},{"title":"功能導覽​","type":1,"pageTitle":"自架 Registry - Harbor","url":"/docs/techPost/2020/iThome_Challenge/cicd-21#功能導覽","content":"最後我們來看一下其他的功能，首先在登入的部分， Harbor 提供多種機制，最簡單的就是讓 Harbor 的資料庫來管理使用者  如果想要跟團隊內已經使用的登入系統整合，可以使用 LDAP/UAA/OIDC 等三種協定進行互動，如果走的是 OIDC 可以考慮使用 DEX 這個開源專案來整合，一旦整合完畢後，使用者登入的頁面就會變成  此外還有一個 Replication 的功能，可以讓你的 Harbor 與其他的 Container Registry 互動，達成同步的功能。這個功能有兩個走法，一個是主動將本地的 image 推到遠方，一種則是定期將遠方的 image 拉回到本地端。 這功能對於有大量 container registry 需求的環境來說非常的好，譬如邊緣運算等，可以讓大部分要使用的 Container Image 都有一份在本地端，加快存取速度，藉由這種多層次的架構來提供更靈活與彈性的管理。  到這邊我們就大概理解了 Harbor 的基本功能，下一章節我們將來看看要如何將 Harbor 與 Kubernetes 整合，讓我們的 Kubernetes 可以存取 Harbor 上的 Container Image。 ","version":"Next","tagName":"h2"},{"title":"自架 Registry 與 Kubernetes 的整合","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-22","content":"自架 Registry 與 Kubernetes 的整合 上篇講述了如何透過 Harbor 來架設屬於自己的 Container Registry，而本篇我們就要將其與之前部署的 Kubernetes 整合 基本上官方文件 Pulling Image Private Registry 有清楚的描述要執行哪些步驟，因此本篇文章就來將這些步驟詳細的跑一次 但是這邊要注意的，如果你的 Container Registry 使用的是自簽的憑證，甚至是根本沒有 HTTPS 保護，那整個步驟會變得非常麻煩。 假設你的 Kubernetes 叢集預設使用的都是 docker container 作為你的容器解決方案，你必須要讓你的 dockerd 信賴這些 不知道能不 能信賴的 container registry。 docker 官方也有頁面 use-self-signed-certificates 專門介紹要如何讓你的 dockerd 去處理這些動作。 如果今天只有一台機器的話，這些步驟都還算簡單，還可以處理，但是當這些機器數量很多，同時有可能是動態創建的，那我們就必須要 想辦法去設定這些機器上的 dockerd，這樣這些機器加入到 Kubernetes 叢集後，才有辦法去連接到你自行架設但是沒有可信賴憑證的 container registry。 接下來的步驟都是基於你的 Container Registry 本身有一個可信賴的憑證，同時所有的容器解決方案都是基於 docker。 Kubernetes 如果今天要從本地端去抓取一個 private container registry，我們第一件要做的事情就是 docekr login，可以參閱 Docker 官方docker login來看更多說明與使用。 對於 Kubernetes 來說，其會使用 secret 的特殊型態 docker-registry 作為登入任何 private container registry 的帳號密碼來源。 這邊有兩種方式可以使用 第一種是先透過 docker login 登入，之後將登入後的設定檔案送給 Kubernetes secret 物件 第二種則是創建 Kubernetes secret 時使用明碼的帳號密碼 接下來的範例會針對(1)去使用，對第二種範例有興趣可以參閱 Create a Secret by providing credentials on the command line $ docker login --username admin --password Harbor12345 https://registry.hwchiu.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded $ cat ~/.docker/config.json { &quot;auths&quot;: { &quot;registry.hwchiu.com&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } }, &quot;HttpHeaders&quot;: { &quot;User-Agent&quot;: &quot;Docker-Client/19.03.12 (linux)&quot; } 接下來把這個檔案，送給 Kubernetes 去使，這邊要注意的是我們使用的是基於 dockerconfigjson 這個類型 kubectl create secret generic regcred \\ --from-file=.dockerconfigjson=~/.docker/config.json&gt; \\ --type=kubernetes.io/dockerconfigjson 如果想要使用 Yaml 去維護的話，可以透過 base64 去編碼該config，譬如 $ cat harbor_secret.yaml apiVersion: v1 kind: Secret metadata: name: myregistrykey data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJyZWdpc3RyeS5od2NoaXUuY29tIjogewoJCQkiYXV0aCI6ICJZV1J0YVc0NlNHRnlZbTl5TVRJek5EVT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE5LjAzLjEyIChsaW51eCkiCgl9Cn0= type: kubernetes.io/dockerconfigjson 如果想要驗證到底自己的 secret 是否正確，我們可以將 secret 的內容抓出來，重新用 base64 解編碼，並且跟本來的 ~/.docker/config.json 進行比較 $ kubectl get secret regcred --output=&quot;jsonpath={.data.\\.dockerconfigjson}&quot; | base64 --decode { &quot;auths&quot;: { &quot;registry.hwchiu.com&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } }, &quot;HttpHeaders&quot;: { &quot;User-Agent&quot;: &quot;Docker-Client/19.03.12 (linux)&quot; } $ kubectl get myregistrykey --output=&quot;jsonpath={.data.\\.dockerconfigjson}&quot; | base64 --decode { &quot;auths&quot;: { &quot;registry.hwchiu.com&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } }, &quot;HttpHeaders&quot;: { &quot;User-Agent&quot;: &quot;Docker-Client/19.03.12 (linux)&quot; } 確認資料都沒有正確後，我們就可以來準備部署的我們的 Pod 運算資源了! $ cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: ithome-private-1 namespace: default labels: name: &quot;ithome-private-1&quot; spec: replicas: 3 selector: matchLabels: app: ithome-private-1 template: metadata: name: ithome-private-1 labels: app: ithome-private-1 spec: containers: - image: registry.hwchiu.com/ithome/netutils:latest name: ithome imagePullSecrets: - name: regcred --- apiVersion: apps/v1 kind: Deployment metadata: name: ithome-private-2 namespace: default labels: name: &quot;ithome-private-2&quot; spec: replicas: 3 selector: matchLabels: app: ithome-private-2 template: metadata: name: ithome-private-2 labels: app: ithome-private-2 spec: containers: - image: registry.hwchiu.com/ithome/netutils:latest name: ithome imagePullSecrets: - name: myregistrykey 這邊只有一個要注意，就是在 imagePullSecrets 這邊指定你要使用的 secret 即可，我們上述的範例有 regcred 以及 myregistrykey 兩個，所以我們就創造兩個 deployment 但是使用不同的 secret 來試試看 $ kubectl apply -f deployment.yaml deployment.apps/ithome-private-1 created deployment.apps/ithome-private-2 created $ kubectl get pods NAME READY STATUS RESTARTS AGE ithome-private-1-765997748-2gttp 1/1 Running 0 4s ithome-private-1-765997748-c8fdz 1/1 Running 0 4s ithome-private-1-765997748-mgpfx 1/1 Running 0 4s ithome-private-2-84c74f8c6d-5tws8 1/1 Running 0 4s ithome-private-2-84c74f8c6d-cz6rq 1/1 Running 0 4s ithome-private-2-84c74f8c6d-xkvjt 1/1 Running 0 4s 到這邊就順利的讓 Kubernetes 連接到剛剛架設的 Harbor Registry 了。 這邊要特別注意，如果今天 secret 有一些問題，要除錯的話除了透過 kubectl describe 去看之外，另外一種方式就是到 Kubernetes 節點上面去看相關的 container log，裡面會有更詳細為什麼會 pull image 失敗，看是憑證問題，帳號密碼認證失敗等。有些太底層的原因 kubectl 是看不到了。 Helm Chart 3 最後我們來示範如何將 Helm Chart (v3) 與 Harbor 整合，並且讓其推向到遠方的 Kubernetes 叢集中，整個流程是 讓 Helm Chart 登入到遠方 Harbor Registry創建一個測試用的 nginx Helm Chart.打包 nginx Helm Chart將 nginx Helm Chart 推到 Harbor Registry砍掉本地的 nginx Helm Chart 資料夾，並且移動到其他資料夾將遠方的 charts 複製一份到本地端並且使用 helm 工具將其安裝到 Kubernetes $ export HELM_EXPERIMENTAL_OCI=1 $ helm registry login -u admin registry.hwchiu.com Password: Login succeeded $ helm create nginx $ cd nginx $ helm chart save . registry.hwchiu.com/ithome/nginx:ithome ref: registry.hwchiu.com/ithome/nginx:ithome digest: 477087f52e48bcba75370928b0895735bc0c3c1d7612d82740dd69c2b70bbba4 size: 3.5 KiB name: nginx version: 0.1.0 $ helm chart push registry.hwchiu.com/ithome/nginx:ithome The push refers to repository [registry.hwchiu.com/ithome/nginx] ref: registry.hwchiu.com/ithome/nginx:ithome digest: 477087f52e48bcba75370928b0895735bc0c3c1d7612d82740dd69c2b70bbba4 size: 3.5 KiB name: nginx version: 0.1.0 ithome: pushed to remote (1 layer, 3.5 KiB total) 當上述指令執行完畢後，可以看到 Harbor 內多出了相關的 repo, 名稱跟我們剛剛透過 Helm 去打包的名稱一致 進去到裡面觀看細節，可以看到裡面現在顯示的資訊包含了 Charts 的資料，還有其相關的 values.yaml 都有，實實在在的透過 Harbor 這套 registry 來保存我們的 Helm Charts。 接下來我們就嘗試把遠方的 charts 給複製到本地，並且用 helm install 來安裝。 $ cd ../ $ rm -rf nginx $ helm chart export registry.hwchiu.com/ithome/nginx:ithome ref: registry.hwchiu.com/ithome/nginx:ithome digest: 477087f52e48bcba75370928b0895735bc0c3c1d7612d82740dd69c2b70bbba4 size: 3.5 KiB name: nginx version: 0.1.0 Exported chart to nginx/ $ helm install ithome nginx/ $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ithome default 1 2020-09-13 23:49:47.200022078 +0000 UTC deployed nginx-0.1.0 1.16.0 到這邊就有一個簡易的展示，如何將 Helm3 &amp; Harbor &amp; Kubernetes 進行整合，透過這個功能我們可以只需要用一個伺服器就滿足 Helm &amp; Container Image。我個人認為這個在未來應該會變成主流，畢竟只要夠穩定，能夠減少要維護的伺服器數量可以更少，和樂不為？","keywords":"","version":"Next"},{"title":"Kubernetes 應用測試","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-11","content":"","keywords":"","version":"Next"},{"title":"Yamllint​","type":1,"pageTitle":"Kubernetes 應用測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-11#yamllint","content":"yamllint 官網介紹如下 A linter for YAML files. yamllint does not only check for syntax validity, but for weirdnesses like key repetition and cosmetic problems such as lines length, trailing spaces, indentation, etc. 這個工具就是幫忙檢查一些寫法，但是並沒有語意的檢查，不過會針對一些 key 重複的問題也指證出來，以下有一些範例 這邊是一個完整沒錯誤的 Yaml 檔案 kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 nodes: - role: control-plane - role: worker - role: worker  接下來我們對其修改，譬如加入一個重複的 Key, 然後讓底下的縮排格式不一致，長這樣 kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 nodes: - role: control-plane - role: worker - role: worker nodes: test  這種情狂下我們使用 yamllint 針對這個檔案檢查 $ yamllint kind.yaml kind.yaml 1:1 warning missing document start &quot;---&quot; (document-start) 6:1 error syntax error: expected &lt;block end&gt;, but found '-' 7:1 error duplication of key &quot;nodes&quot; in mapping (key-duplicates)  第一行主要是警告，提醒要有文件的描述，但是不影響運行。 後面兩行則是不同的錯誤，分別是因為 第六行的縮排有問題，以及第七行產生一個重複 key 而導致的錯誤。 此外譬如字串雙引號/單引號沒有成雙等類型錯誤也都可以找到，有興趣的人可以去玩玩看這個工具 ","version":"Next","tagName":"h2"},{"title":"Kubeeval​","type":1,"pageTitle":"Kubernetes 應用測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-11#kubeeval","content":"kubeval 官方介紹如下 kubeval is a tool for validating a Kubernetes YAML or JSON configuration file. It does so using schemas generated from the Kubernetes OpenAPI specification, and therefore can validate schemas for multiple versions of Kubernetes. 下列一個合法的 Kubernetes Yaml 檔案 apiVersion: v1 kind: Pod metadata: name: getting-started spec: containers: - name: getting-started image: hwchiu/netutils  我們可以先用 kubeeval 跑看看，接下來我們在修改這個檔案來試試看會有什麼樣的錯誤 $ ./kubeval pod.yaml PASS - pod.yaml contains a valid Pod (getting-started)  接下來我們修改 Yaml 檔案，來進行一些修改讓他不合格，譬如少給一些欄位，或是多給一些欄位 apiVersion: v1 kind: Pod metadata: name: getting-started spec: ithome: ironman  以上就是一個不合格的 Pod Yaml, 首先多一個 ithome 的欄位，同時又少了 containers 這個資訊 首先我們透過 kubeeval 去跑一次，發現有得到一個警告，告知我們 containers 這個欄位是必須的，但是卻沒有給。 但是多出來的 ithome 卻沒有警告？ $ ./kubeval pod.yaml WARN - pod.yaml contains an invalid Pod (getting-started) - containers: containers is required $ ./kubeval -h Validate a Kubernetes YAML file against the relevant schema Usage: kubeval &lt;file&gt; [file...] [flags] Flags: --additional-schema-locations strings Comma-seperated list of secondary base URLs used to download schemas -d, --directories strings A comma-separated list of directories to recursively search for YAML documents --exit-on-error Immediately stop execution when the first error is encountered -f, --filename string filename to be displayed when testing manifests read from stdin (default &quot;stdin&quot;) --force-color Force colored output even if stdout is not a TTY -h, --help help for kubeval --ignore-missing-schemas Skip validation for resource definitions without a schema -i, --ignored-filename-patterns strings A comma-separated list of regular expressions specifying filenames to ignore --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kubernetes-version string Version of Kubernetes to validate against (default &quot;master&quot;) --openshift Use OpenShift schemas instead of upstream Kubernetes -o, --output string The format of the output of this script. Options are: [stdout json tap] --quiet Silences any output aside from the direct results --reject-kinds strings Comma-separated list of case-sensitive kinds to prohibit validating against schemas -s, --schema-location string Base URL used to download schemas. Can also be specified with the environment variable KUBEVAL_SCHEMA_LOCATION. --skip-kinds strings Comma-separated list of case-sensitive kinds to skip when validating against schemas --strict Disallow additional properties not in schema --version version for kubeval  從上面可以觀察到我們需要加入 --strict 這個參數，才會去檢查多出來不存在原本 schema 內的欄位，因此我們再跑一次看看 $ ./kubeval --strict pod.yaml WARN - pod.yaml contains an invalid Pod (getting-started) - containers: containers is required WARN - pod.yaml contains an invalid Pod (getting-started) - voa: Additional property voa is not allowed  這時候就可以順利的看到兩個錯誤都被抓出來了！ ","version":"Next","tagName":"h2"},{"title":"Conftest​","type":1,"pageTitle":"Kubernetes 應用測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-11#conftest","content":"conftest 的官網說明如下 Conftest is a utility to help you write tests against structured configuration data. For instance you could write tests for your Kubernetes configurations, or Tekton pipeline definitions, Terraform code, Serverless configs or any other structured data. Conftest 這個工具可以幫助開發者去測試來驗證不同類型的設定檔案，譬如 Kubernetes, Tekton 甚至是 Terraform 的設定。 不過使用上必須要先撰寫相關的 Policy 去描述自己期望的規則，最後會幫你的設定檔案與相關的 Policy 去比對看看你的設定檔案是否破壞你的 Policy。 相對於前面的工具去針對 yaml 格式， kubernetes 資源的 schema 的比較， contest 更像是針對 policy 去比對，舉例來說，我們有一下列一個 pod yaml. apiVersion: v1 kind: Pod metadata: name: getting-started spec: containers: - name: getting-started image: hwchiu/netutils restartPolicy: Always  然後團隊今天有個要求，所有 Pod 的 Yaml 都必須要符合兩個規範 restartPolicy 只能是 NeverrunAsNonRoot 這個欄位必須要設定是 True，希望可以以非 root 執行 只要有符合任何一個條件，我們希望 conftest 能夠找出來，並且告知錯誤，於是我們準備了下列檔案 $ cat policy/pod.rego package main deny[msg] { input.kind = &quot;Pod&quot; not input.spec.securityContext.runAsNonRoot = true msg = &quot;Containers must not run as root&quot; } deny[msg] { input.kind = &quot;Pod&quot; not input.spec.restartPolicy = &quot;Never&quot; msg = &quot;Pod never restart&quot; }  我們使用了 deny 去描述兩個 policy, 只要符合這些 policy 的都會判錯 接下來我們用 conftest 去執行看看 $ conftest test pod.yaml -p policy/ FAIL - pod.yaml - Containers must not run as root FAIL - pod.yaml - Pod never restart 2 tests, 0 passed, 0 warnings, 2 failures, 0 exceptions  可以發現 conftest 認為系統中有兩個測試要跑，而這兩個測試都失敗 接下來我們修改檔案讓他符合我們的規則後再跑一次 $ cat pod.yaml apiVersion: v1 kind: Pod metadata: name: getting-started spec: containers: - name: getting-started image: hwchiu/netutils restartPolicy: Never securityContext: runAsNonRoot: true $ conftest test pod.yaml -p policy/ 2 tests, 2 passed, 0 warnings, 0 failures, 0 exceptions  這時候就可以發現已經通過測試了，所以如果團隊中有這些需求的人可以考慮導入這個工具看看 Helm 測試 Helm 的測試分成幾個面向，分別是 Helm Chart 的撰寫內容是否正確Helm Chart 搭配 Config 後是否安裝會失敗 其中(2)這點不是什麼大問題，因為我們可以先透過 helm template 的方式讓它渲染出最後產生的 Kubernetes Yaml 檔案，而因為現在 是原生的 Kubernetes yaml 檔案了，所以就可以使用上述的三個工具來進行測試。 而 (1) 的部分主要會牽扯到 Helm 本身的資料夾跟架構，這邊我們可以使用原生的工具 helm lint 來進行或是透過 helm install --dry-run 的方式來嘗試裝裝看，一個簡單的範例如下 $ helm create nginx Creating nginx $ cd nginx/ $ helm lint ==&gt; Linting . [INFO] Chart.yaml: icon is recommended 1 chart(s) linted, 0 chart(s) failed  我們透過 helm 指令創建了一個基本範例的結構，這時候用 helm lint 是沒有任何問題的，然後我們嘗試修改 template 裡面的內容，譬如 針對 go template 的格式進行一些修改，讓其錯誤。 $ echo &quot;{}&quot; &gt;&gt; templates/deployment.yaml ubuntu@dex-test:~/nginx$ helm lint ==&gt; Linting . [INFO] Chart.yaml: icon is recommended [ERROR] templates/deployment.yaml: unable to parse YAML: error converting YAML to JSON: yaml: line 45: did not find expected key [ERROR] templates/deployment.yaml: object name does not conform to Kubernetes naming requirements: &quot;&quot; Error: 1 chart(s) linted, 1 chart(s) failed  上述只是一個範例，有興趣的都可以到 Helm 官網去看更多關於 Helm lint 的討論與用法。 結論 本篇介紹了很多關於 Yaml 相關的工具，每個工具都會有自己的極限，沒有一個工具可以檢查出所有問題，這部分就是需要花時間去評估看看每個工具，看看哪些工具適合自己團隊，是否方便導入以及功能是否滿足 除了上述之外還有很多工具，譬如 kube-score, config-lint..等，有興趣的人都可以搜尋來玩耍看看 ","version":"Next","tagName":"h2"},{"title":"GitOps - ArgoCD 介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-18","content":"","keywords":"","version":"Next"},{"title":"應用程式​","type":1,"pageTitle":"GitOps - ArgoCD 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-18#應用程式","content":"之前有提到如何包裝與管理 Kubernetes 的應用程式，實際上有非常多種用法 ArgoCD 支援以下類型 Helmkustomizejsoonetksonnet原生 Yaml客製化的設定檔案 (自行實現相關 Plugin) 支援度非常強大，基本上 Kubernetes 會用到的格式他都支援 ","version":"Next","tagName":"h2"},{"title":"架構​","type":1,"pageTitle":"GitOps - ArgoCD 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-18#架構","content":"下圖節錄自ArgoCD 的架構介紹，我們用這張圖來大概看一下 ArgoCD 的運作模式  ArgoCD 的 API Server 支援多種控制，譬如使用 UI 操作，使用 CLI 操作，甚至可以透過 gRPC/REST 等方式控制 當開發者完成 Git 程式碼的合併後， Git 可以觸發 webhook 的事件通知 ArgoCD Git 有新的版本，可以來準備更新 除了 web hook 外， ArgoCD 也支持定期詢問與手動的方式來更新 ArgoCD 可以用來管理多套 Kubernetes 叢集，對於測試環境來說，是可以用一套 ArgoCD 的服務，控管多套叢集，但是如果有生產環境的時候，基於權限也是可以考慮分開不同的 ArgoCD，這部分就沒有唯一解答。 ArgoCD 也提供相對應的時間 Hook, 譬如當同步完成後就可以觸發不同的事件，譬如通知 Slack 等 接下來我們來看一下要如何使用 ArgoCD 安裝 接下來的操作中我們會使用 ArgoCD 的 UI 與 CLI 兩個介面來操作，因此安裝過程就會包含 ArgoCD 本身以及相關的工具 首先安裝 ArgoCD 的服務，透過 kubectl 給安裝到叢集中即可，非常簡單 $ kubectl create namespace argocd namespace/argocd created $ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml customresourcedefinition.apiextensions.k8s.io/applications.argoproj.io created customresourcedefinition.apiextensions.k8s.io/appprojects.argoproj.io created serviceaccount/argocd-application-controller created serviceaccount/argocd-dex-server created serviceaccount/argocd-server created role.rbac.authorization.k8s.io/argocd-application-controller created role.rbac.authorization.k8s.io/argocd-dex-server created role.rbac.authorization.k8s.io/argocd-server created clusterrole.rbac.authorization.k8s.io/argocd-application-controller created clusterrole.rbac.authorization.k8s.io/argocd-server created rolebinding.rbac.authorization.k8s.io/argocd-application-controller created rolebinding.rbac.authorization.k8s.io/argocd-dex-server created rolebinding.rbac.authorization.k8s.io/argocd-server created clusterrolebinding.rbac.authorization.k8s.io/argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/argocd-server created configmap/argocd-cm created configmap/argocd-gpg-keys-cm created configmap/argocd-rbac-cm created configmap/argocd-ssh-known-hosts-cm created configmap/argocd-tls-certs-cm created secret/argocd-secret created service/argocd-dex-server created service/argocd-metrics created service/argocd-redis created service/argocd-repo-server created service/argocd-server-metrics created service/argocd-server created deployment.apps/argocd-application-controller created deployment.apps/argocd-dex-server created deployment.apps/argocd-redis created deployment.apps/argocd-repo-server created deployment.apps/argocd-server created  可以看到其實 ArgoCD 安裝的東西還不少，前面一大部分都是針對使用者帳號進行控制，因為 ArgoCD 本身要可以對 Kubernetes 進行操控，所以會幫他創立一個 ServiceAccount 並且配上相對應的權限。 後面則是相關的服務，譬如用來管理帳號登入機制的 OIDC 伺服器 (Dex)，跟 Git Repo 連動的 repo-server, 以及最重要的邏輯處理中心 Application-controller。 預設情況下， ArgoCD 會將服務裝成 ClusterIP，這意味者不方便存取，除非你有 Ingress Controller 等，為了方便 Demo 我們可以 將其修改成 NodePort透過 kubectl port-forward 的方式來存取 如果是基於測試的環境，那這兩種方式我覺得都沒有問題，但是如果是正式環境，最好還是有一個 Ingress Controller 在前面幫忙管理。 $ kubectl port-forward svc/argocd-server -n argocd 8080:443  打開瀏覽器後會出現下方畫面  登入帳號是 admin, 預設的登入密碼可以透過下列指令獲得 $ kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d'/' -f 2  接下來我們來安裝 ArgoCD 的 CLI $ wget https://github.com/argoproj/argo-cd/releases/download/v1.7.4/argocd-linux-amd64 $ sudo chmod 755 $ ./argocd-linux-amd64 argocd controls a Argo CD server Usage: argocd [flags] argocd [command] Available Commands: account Manage account settings app Manage applications cert Manage repository certificates and SSH known hosts entries cluster Manage cluster credentials completion output shell completion code for the specified shell (bash or zsh) context Switch between contexts gpg Manage GPG keys used for signature verification help Help about any command login Log in to Argo CD logout Log out from Argo CD proj Manage projects relogin Refresh an expired authenticate token repo Manage repository connection parameters repocreds Manage repository connection parameters version Print version information Flags: --auth-token string Authentication token --client-crt string Client certificate file --client-crt-key string Client certificate key file --config string Path to Argo CD config (default &quot;/home/ubuntu/.argocd/config&quot;) --grpc-web Enables gRPC-web protocol. Useful if Argo CD server is behind proxy which does not support HTTP2. --grpc-web-root-path string Enables gRPC-web protocol. Useful if Argo CD server is behind proxy which does not support HTTP2. Set web root. -H, --header strings Sets additional header to all requests made by Argo CD CLI. (Can be repeated multiple times to add multiple headers, also supports comma separated headers) -h, --help help for argocd --insecure Skip server certificate and domain verification --logformat string Set the logging format. One of: text|json (default &quot;text&quot;) --loglevel string Set the logging level. One of: debug|info|warn|error (default &quot;info&quot;) --plaintext Disable TLS --port-forward Connect to a random argocd-server port using port forwarding --port-forward-namespace string Namespace name which should be used for port forwarding --server string Argo CD server address --server-crt string Server certificate file Use &quot;argocd [command] --help&quot; for more information about a command.  接下來我們使用一樣的帳號密碼來登入 $ ./argocd-linux-amd64 login localhost:8080 WARNING: server certificate had error: x509: certificate signed by unknown authority. Proceed insecurely (y/n)? y Username: admin Password: 'admin' logged in successfully Context 'localhost:8080' updated  這邊準備就緒後，我們就可以開始來使用囉 使用 如同前述提到，GitOps 本身的一個重點是透過 Git 來管理所有部署的檔案，因此這邊我們會使用 ArgoCD 所準備的一個示範 Git Repo，argocd-example-apps 可以看到該 Repo 內有滿滿的使用不同方式管理 Kubernetes 應用的方法  接下來我們就要告訴 ArgoCD，我想要部署一個新的應用程式，這個應用程式的來源是哪個 Git 以及部署上要注意的一些小設定 這部分可以透過 UI 操作，也可以使用 CLI 來操作  我們來嘗試使用 CLI 操作看看 $ ./argocd-linux-amd64 app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default application 'guestbook' created  上述的指令基本上跟 UI 描述的內容差不多，有些沒有填寫的就是預設值，這種情況下我們會創立一個新的 app，目標是上述提到的 GitRepo，並且詳細的安裝檔案請使用 guestbook這個資料夾內的檔案  該資料夾內的資源非常簡單，就是一個 Deployment 配上一個 Service。 這時候就可以回到 UI 去看，就會看到一個全新的 Application 已經產生了  但是可以觀察到畫面中相關的應用程式其實還沒有正式被部署出來，主要是我們只是告訴 ArgoCD 我們要建立一個新的 Application，但是還沒有要同步。 同時透過指令的方式(UI也可以)觀察到，我們的 Application 目前是沒有開啟 Auto Sync (Sync Policy: None) $ ./argocd-linux-amd64 app get guestbook Name: guestbook Project: default Server: https://kubernetes.default.svc Namespace: default URL: https://localhost:8080/applications/guestbook Repo: https://github.com/argoproj/argocd-example-apps.git Target: Path: guestbook SyncWindow: Sync Allowed Sync Policy: &lt;none&gt; Sync Status: OutOfSync from (6bed858) Health Status: Missing GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Service default guestbook-ui OutOfSync Missing apps Deployment default guestbook-ui OutOfSync Missing  接下來我們就透過 CLI 的方式(UI也可以) 要求 ArgoCD 幫我們同步 Guestbook 這個應用程式 $ ./argocd-linux-amd64 app sync guestbook TIMESTAMP GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE 2020-09-13T17:25:41+00:00 Service default guestbook-ui OutOfSync Missing 2020-09-13T17:25:41+00:00 apps Deployment default guestbook-ui OutOfSync Missing 2020-09-13T17:25:41+00:00 Service default guestbook-ui Synced Healthy Name: guestbook Project: default Server: https://kubernetes.default.svc Namespace: default URL: https://localhost:8080/applications/guestbook Repo: https://github.com/argoproj/argocd-example-apps.git Target: Path: guestbook SyncWindow: Sync Allowed Sync Policy: &lt;none&gt; Sync Status: Synced to (6bed858) Health Status: Progressing Operation: Sync Sync Revision: 6bed858de32a0e876ec49dad1a2e3c5840d3fb07 Phase: Succeeded Start: 2020-09-13 17:25:41 +0000 UTC Finished: 2020-09-13 17:25:41 +0000 UTC Duration: 0s Message: successfully synced (all tasks run) GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Service default guestbook-ui Synced Healthy service/guestbook-ui created apps Deployment default guestbook-ui Synced Progressing deployment.apps/guestbook-ui created  這時候 UI 上的呈現就馬上改變  相關的服務都被部署到 Kubernetes 內，透過 kubectl 也可以觀察到部署的結果。 ArgoCD 的 UI 也提供了一些簡單的操作，包含觀察 Log，觀察部署資源的狀態，其中有一個非常好的功能就是幫你比對狀態的差異 。舉例來說，如果今天有人透過指令的方式手動修改正在運行的資源狀態，我們將 deployment 的數量從 1個變成 4個 $ kubectl scale --replicas=4 deployment guestbook-ui $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-ui-65b878495d-7fthl 1/1 Running 0 15s guestbook-ui-65b878495d-hw9mt 1/1 Running 0 15s guestbook-ui-65b878495d-trsmz 1/1 Running 0 15s guestbook-ui-65b878495d-ts8cg 1/1 Running 0 4m39s  這時候可以從 UI 觀察到相關的應用程式被標上了 OutOfSync ，因為我們沒有開啟 auto-sync , 所以不會自動修復回來  同時我們也可以透過 UI 的方式來瞭解到底當前期望狀態與運行狀態 的差異是什麼，我們的範例就是複本數量有差  到這邊我們簡單玩轉了一下 ArgoCD 的功能，實際上其內部有更多有趣且有效率的功能，如果對於 GitOps 有興趣的人都歡迎嘗試看看這個工具，如果還有時間也一定要試試看 Flux 另外一套不同的 GitOps 實現工具。 ","version":"Next","tagName":"h2"},{"title":"Kubernetes 第三方好用工具介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-28","content":"","keywords":"","version":"Next"},{"title":"安裝​","type":1,"pageTitle":"Kubernetes 第三方好用工具介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-28#安裝","content":"直接到官方 Github Release Page 抓去每個平台的 binary 版本 ","version":"Next","tagName":"h2"},{"title":"使用​","type":1,"pageTitle":"Kubernetes 第三方好用工具介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-28#使用","content":"舉例來說，上述範例會有五個 pod，而且這五個pod的名稱都是 ithome開頭，因此我可以直接用 stern ithom 的方式來抓取這些 pod 的資訊，結果如下圖 $ stern ithome ... ithome-6564f65698-zhwlj netutils Hello! 369 secs elapsed... ithome-6564f65698-fglr9 netutils Hello! 369 secs elapsed... ithome-6564f65698-947rv netutils Hello! 367 secs elapsed... ithome-6564f65698-k5wtg netutils Hello! 368 secs elapsed... ithome-6564f65698-rrvk4 netutils Hello! 369 secs elapsed... ithome-6564f65698-zhwlj netutils Hello! 370 secs elapsed... ithome-6564f65698-fglr9 netutils Hello! 370 secs elapsed... ithome-6564f65698-947rv netutils Hello! 368 secs elapsed... ithome-6564f65698-k5wtg netutils Hello! 370 secs elapsed... ithome-6564f65698-rrvk4 netutils Hello! 370 secs elapsed... ithome-6564f65698-zhwlj netutils Hello! 371 secs elapsed... ithome-6564f65698-fglr9 netutils Hello! 371 secs elapsed... ithome-6564f65698-947rv netutils Hello! 369 secs elapsed... ithome-6564f65698-k5wtg netutils Hello! 371 secs elapsed... ithome-6564f65698-rrvk4 netutils Hello! 371 secs elapsed... ithome-6564f65698-zhwlj netutils Hello! 372 secs elapsed... ithome-6564f65698-fglr9 netutils Hello! 372 secs elapsed... ^C  實際上觀看的時候，不同 Pod 的名稱還會有不同的顏色標註，幫助使用者更快的區別這些文字。 K9S 過往總是透過 kubectl 指令於各個資源，各 namespace 間切來切去，特別是要使用 exec, get, describe, logs, delete 等指令時，常常打的手忙腳亂或是覺得心累，有這種困擾的人可以考慮使用看看 k9s 這個工具 K9s 官網介紹 K9s provides a terminal UI to interact with your Kubernetes clusters. The aim of this project is to make it easier to navigate, observe and manage your applications in the wild. K9s continually watches Kubernetes for changes and offers subsequent commands to interact with your observed resources. 基本上就是基於 Terminal 去提供一個友善的操作畫面，讓你可以透過鍵盤來輕鬆的完成上面提到的事情，不論是切換 namespace, 砍掉資源，執行 Shell, 觀看 log 等都可以輕鬆達成。 ","version":"Next","tagName":"h2"},{"title":"使用​","type":1,"pageTitle":"Kubernetes 第三方好用工具介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-28#使用-1","content":"上述五個 pod 的範例透過 k9s 執行後可以得到下面的畫面，畫面中可以清楚地看到 Pod 的名稱有沒有開 Port-Forward當前 Continers's READY 狀態當前 Pod 狀態當前 IP運行節點資訊存活時間 這些指令其實都可以用 kubectl 獲得，但是操作起來可能就相對繁瑣，需要比較多的指令  此外畫面上方還會有一些基本資訊，譬如 Context/Cluster/User 等 Kubeconfig 內的資訊，右邊還有可以使用的快捷鍵，除了上述提到的功能之外，還可以透過 port-forward 來使用，個人覺得相當不錯。  一路往下點選後，還可以看到每個 Pod 裡面每個 Container 各自的 log, 使用上非常方便，過往有多個 containers 的時候都要於 kubectl logs -f $Pod_name -c $container_name 來讀取，特別是沒有仔細去看 Pod 的設定都會忘記 Container Name，這時候又要再跑別的指令查詢一次。  透過 k9s 這工具可以提供一個滿不錯的視窗管理工具，讓你一目了然 kubernetes 當前的狀態，並且提供基本功能讓你進行操作 Ksniff 接下來要介紹的是一個抓取網路封包的工具，過往我們分析封包的時候都會使用 tcpdupm 或是 wireshark 這些工具來輔助，而 Ksniff 就是一個將這些工具整合到 Kubernetes 系統內的工具 Ksniff 的介紹如下 A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. You get the full power of Wireshark with minimal impact on your running pods. 基本上本身也是一個 kubectl 的 plugin ，所以也是可以透過前述的 krew 來安裝管理。這邊就不再贅述其安裝過程 ","version":"Next","tagName":"h2"},{"title":"使用​","type":1,"pageTitle":"Kubernetes 第三方好用工具介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-28#使用-2","content":"其使用上的概念是，選擇一個想要觀察的 Pod，然後 Ksniff 這個工具會嘗試幫你將 tcpdump 的執行檔案給複製到該 Pod的某個 Container 裡面(預設是第一個)，接下來根據你的參數幫你運行 tcpdump，最後將結果複製出來到本機上面的 wireshark 來呈現。 但是假如系統中沒有 wireshark 可以呈現這些結果，可以改用命令列的工具，譬如 tshark 來取代 $ sudo apt install tshark $ kubectl sniff ithome-6564f65698-947rv -o - | tshark -r - $ kubectl sniff ithome-6564f65698-947rv -o - | tshark -r - INFO[0000] sniffing method: upload static tcpdump INFO[0000] using tcpdump path at: '/home/ubuntu/.krew/store/sniff/v1.4.2/static-tcpdump' INFO[0000] no container specified, taking first container we found in pod. INFO[0000] selected container: 'netutils' INFO[0000] sniffing on pod: 'ithome-6564f65698-947rv' [namespace: 'default', container: 'netutils', filter: '', interface: 'any'] INFO[0000] uploading static tcpdump binary from: '/home/ubuntu/.krew/store/sniff/v1.4.2/static-tcpdump' to: '/tmp/static-tcpdump' INFO[0000] uploading file: '/home/ubuntu/.krew/store/sniff/v1.4.2/static-tcpdump' to '/tmp/static-tcpdump' on container: 'netutils' INFO[0000] executing command: '[/bin/sh -c ls -alt /tmp/static-tcpdump]' on container: 'netutils', pod: 'ithome-6564f65698-947rv', namespace: 'default' INFO[0000] command: '[/bin/sh -c ls -alt /tmp/static-tcpdump]' executing successfully exitCode: '0', stdErr :'' INFO[0000] file found: '-rwxr-xr-x 1 root root 2696368 Jan 1 1970 /tmp/static-tcpdump ' INFO[0000] file was already found on remote pod INFO[0000] tcpdump uploaded successfully INFO[0000] output file option specified, storing output in: '-' INFO[0000] start sniffing on remote container INFO[0000] executing command: '[/tmp/static-tcpdump -i any -U -w - ]' on container: 'netutils', pod: 'ithome-6564f65698-947rv', namespace: 'default'  從上面可以觀察到這些資訊就代表系統開始運行了，這時候我們可以開啟第二個視窗，進入到該 Container 內透過 ping 8.8.8.8 往外送封包，並且觀察上述的輸出 $ kubectl exec ithome-6564f65698-947rv -- ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=97 time=9.42 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=97 time=9.44 ms 64 bytes from 8.8.8.8: icmp_seq=3 ttl=97 time=9.36 ms ... ------ $ kubectl sniff ithome-6564f65698-947rv -o - | tshark -r - ... 2 38.393757 10.244.1.8 → 8.8.8.8 ICMP 100 Echo (ping) request id=0x04f5, seq=1/256, ttl=64 3 38.403163 8.8.8.8 → 10.244.1.8 ICMP 100 Echo (ping) reply id=0x04f5, seq=1/256, ttl=97 (request in 2) 4 39.394274 10.244.1.8 → 8.8.8.8 ICMP 100 Echo (ping) request id=0x04f5, seq=2/512, ttl=64 5 39.403697 8.8.8.8 → 10.244.1.8 ICMP 100 Echo (ping) reply id=0x04f5, seq=2/512, ttl=97 (request in 4) 6 40.395882 10.244.1.8 → 8.8.8.8 ICMP 100 Echo (ping) request id=0x04f5, seq=3/768, ttl=64 7 40.405230 8.8.8.8 → 10.244.1.8 ICMP 100 Echo (ping) reply id=0x04f5, seq=3/768, ttl=97 (request in 6) 8 41.397387 10.244.1.8 → 8.8.8.8 ICMP 100 Echo (ping) request id=0x04f5, seq=4/1024, ttl=64 ...  可以看到另外一個視窗很及時地將相關的封包內容都給顯示出來。 我認為這個工具最方便的地方就是幫你上傳 tcpdump 的檔案，因為大部分的 Container 內建都沒有這個執行檔案，甚至也不好安裝，所以要錄製封包的時候都不太方便，然而透過這個工具可以幫忙解決這個問題 除此之外還有很多有趣好用的工具，就留待大家自己挖掘囉 ","version":"Next","tagName":"h2"},{"title":"Summary","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-29","content":"Summary 過去的 28 篇文章我們從頭到尾探討了以下這張圖片的種種議題，包含了 如何管理 Kubernetes 應用程式，Helm/Kustomize/原生 Yaml 本地開發者如果有 Kubernetes 使用的需求，那可以怎麼做 Pipeline 該怎麼選擇， SaaS 與自架各自的優劣 CI Pipeline 可以怎麼做，如果有 Kubernetes 的需求，那可以怎麼設計 CI pipeline 要如何對 Kubenretes 應用程式進行測試， Yaml 可以測試針對語法，語意等進行測試 CD pipeline 有哪一些做法，配上 Kubernetes 之後有哪些參考作法 GitOps 是什麼，相對於過往的部署方式，有什麼優劣 GitOps 與 Kubernetes 的整合，有哪些解決方案可以使用 Container Registry 的選擇，SaaS 與自架各自的優劣 自架的 Container Registry 要怎麼與 Kubernetes 整合，有哪些點要注意 Secret 機密資訊於自動部署上要怎處理 Secret 機密部署與 Kubernetes 要如何處理 事實上，上面每個議題都有跳不完的坑，每個議題都有好多的解決方案，不論是開源解決方案，或是商業付費方案，每個都有不同的場景，以及不同的時機去使用。 踏入一個新技術想要嘗試導入時，往往最困難的就是要如何在包山包海的選擇中，挑出一個最後的答案。 這部分吃的除了是技術的洞察力，透過觀察不同軟體的架構來判斷問題外，還有對於自己團隊工作流程的掌握力，一時之間選不出來的時 候，可能還需要針對不同專案進行嘗試，透過實際操作去觀察實際運用的情況，再加以輔佐來進行判斷。 就如同 CNCF End User Technology Radar 關於 Continuouse Delivery 調查報告中所說，很多人使用 Jenkins 是因為舊系統已經正在使用，實在是沒有什麼理由硬要把它拔掉，優劣權衡之後就決定舊系統繼續使用 Jenkins，但是對於很多全新的專案，因為是全新的環境， 就可以開始嘗試不同的解決方法。 該文章也提到，很多公司都嘗試過至少10個以上的解決方案在評估，最後就收斂到 3-4 個繼續穩定使用的專案，幾乎沒有公司是一個專案打天下，甚至很多大公司發現解決方案解決不了問題的時候，就會自己動手實作符合自己工作情境的軟體，甚至將其開源貢獻。","keywords":"","version":"Next"},{"title":"Secret 使用範例: sealed-secrets","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-25","content":"Secret 使用範例: sealed-secrets 上篇文章中我們介紹了不同參考架構的解決方案，而本篇我們將使用 sealed-secrets 這個開源專案來實現其中一種架構，也就是最後一種基於加解密的解決方案。透過這個方案我們可以將機密資訊加密後存放到 Git 保存，但內容被部署到 Kubernetes 內部後則是會被自動解密 安裝 Sealed-secrets 本身有是由兩個元件組成，一個是 Kubernetes 內的 controller，而另外一個則是操作使用的 CLI。 等等我們會需要借助這兩個工具來處理，其中透過 Controller 來解密，而 CLI 要先跟 Controller 溝通取得憑證，最後加密 安裝 Controller 的方法很多種，可以使用原生 Yaml, Kustomize 或是 Helm 都可以 以下我們使用 Helm 來安裝，我們將服務安裝到 default namespace，並且取名為 ithome。 $ helm repo add stable https://kubernetes-charts.storage.googleapis.com $ helm repo update $ helm install --namespace default ithome stable/sealed-secrets 接下來我們要安裝 CLI 的工具，這部分可以直接安裝編譯好的版本 $ wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.12.5/kubeseal-linux-amd64 $ chmod 755 kubeseal-linux-amd64 $ sudo mv kubeseal-linux-amd64 /usr/local/bin $ kubeseal --help Usage of kubeseal: --add_dir_header If true, adds the file directory to the header --allow-empty-data Allow empty data in the secret object --alsologtostderr log to standard error as well as files --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cert string Certificate / public key file/URL to use for encryption. Overrides --controller-* --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --controller-name string Name of sealed-secrets controller. (default &quot;sealed-secrets-controller&quot;) --controller-namespace string Namespace of sealed-secrets controller. (default &quot;kube-system&quot;) --fetch-cert Write certificate to stdout. Useful for later use with --cert -o, --format string Output format for sealed secret. Either json or yaml (default &quot;json&quot;) --from-file strings (only with --raw) Secret items can be sourced from files. Pro-tip: you can use /dev/stdin to read pipe input. This flag tries to fol low the same syntax as in kubectl ...... $ kubeseal (tty detected: expecting json/yaml k8s resource in stdin) error: cannot fetch certificate: services &quot;sealed-secrets-controller&quot; not found 這邊執行會失敗是因為預設情況下， kubeseal 會嘗試跟 sealed-secrets-controller 這個 service 去溝通，取得相關資訊，但是因為我們透過 helm 安裝的關係，名稱不會一致，所以執行的時候要透過 --controller-name 以及 --controller-namespace 兩個來替換掉到我們安裝的名稱與 namespace。 $ kubeseal --controller-name=ithome-sealed-secrets --controller-namespace=default (tty detected: expecting json/yaml k8s resource in stdin) ^C 改成上述執行就不會有獲取憑證失敗的問題了，這時候可以按下 CTRL+C 給跳出。因為 kubeseal 的工作很簡單，給我 kubernetes secret 檔案，我給你加密後的結果。預設情況是從 STDIN 輸入。 使用 接下來的示範流程如下 準備一個 kubernetes secret，決定使用 docker login 後產生的 login.json將該 kubernetes secret 透過 kubeseal 產生出一個 sealedsecret 的物件，該物件的內容是加密，不是 secret 的編碼將 sealedsecret 這個物件部署到 kubernetes 內，觀察是否有產生全新的 secret 內容檢查該 secret 的內容，與(1)產生的一樣透過反解 base64 編碼，確認內容與 login.json 一致 $ kubectl create secret generic ithome-example --from-file=.dockerconfigjson=/home/ubuntu/.docker/config.json --type=kubernetes.io/dockerconfigjson --dry-run=client -o yaml &gt; secret.yaml $ kubeseal --controller-name=ithome-sealed-secrets --controller-namespace=default -o yaml &lt; secret.yaml &gt; sealedsecret.yaml $ cat sealedsecret.yaml apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: ithome-example namespace: default spec: encryptedData: .dockerconfigjson: AgBOq/FUB4OSIjOfua8vikrosi9R6uFROuAeT0rV3myf4memo+Y3LwP9mDGsswcUhFk5N29BS1V76ycLX31a8IbzON40AWJAnclSn9qWoj+ZDZmD1p+1OSPCdjV5FjDhVnGNwi49DAvr+L+WLREGdD2fgizVWq+Ebk7acFjmI2uGq7J2yoocH+/qpX/13e2kj36J7+Rwd+RBhnkKTImlQJsXjKsBYENxjsRnc+UzNjkXjBcXEYihHq9MIXdtElPG1Kur27pIC+urj9FkWnQ4lO2tUoI3NDIuQFCvKaeAwEP0cu+3wlY0F2Ax2/CT0SQ9WB0VM8iyrNaccFDuItGnqRksya0WtXLV4fYafbxR4+itzCpt8sH0VOUouoDml9FqAgLfWrqld74VnEpSJybdf/Wfea3PYLFTDScHClWDW7qBTvZmkCIWDS44/HNcQdflpnrmLJk2sxO20T6aJPYDK9M7V5iD0b7Ch8OHNmL/8e/kDhaCTVqUcUXw2qtx7LBJhaxalSoYfhzvwFIDG9AbRe95d2oQJpXl6mHviNqJkOqNiU5M6Byt3YXR+YaFV+A9n0aj6Rl0Bw8y4s9+0LoXrTdv2t3opSe26xOJhmgfOxuxELKY+kaATNpLYez3+S3QaTgDZ0n7tgTzFg041brOL3SkUa+UZ9MqUG9XKMPGXQY0lFf5DhB1FIjWiCOWfOS+JAJsG38izjd8iYZ8wIWIoe983exo2AaCcLS+4cB18ftwoDmlYn8Y+WqmEtzhZA8OMsk4KTSsWPakWFc8rbxRt6aHTER0enXof86B2V/TwxDuPzN4OWmcO7mSMUgdXxbAnRLKVfmuVwYEYTW91wZN5+IQWZVTHwZnXS+ahHzV7TS+zFF74F06yz7Tx6YRQUmnWUH8HJiuxPTNeZbKkvcD7Q== template: metadata: creationTimestamp: null name: ithome-example namespace: default type: kubernetes.io/dockerconfigjson 首先透過 kubectl create secret 就如同之前 harbor 的範例一樣，產生出一個 secret.yaml，其內容其實是編碼後的 config.json 接下來，透過 kubeseal 的指令，把剛剛的 secret.yaml 傳進去，然後最後產生出一個 sealedsecret.yaml 檔案。我們可以觀察到這個檔案裡面的內容跟 kubernetes secret 很類似，多了一個 encryptedData 的欄位，下面的資訊都是加密後，並不是編碼。這個物件就是我們可以放在 Git內保存的。 接下來我們把這個物件送到 kubernetes 內，然後我們馬上觀察 SealedSecret 以及 Secret，的確有一個全新的 Secret 產生了，名稱就是我們前面用的 ithome-example。 $ kubectl apply -f sealedsecret.yaml sealedsecret.bitnami.com/ithome-example created $ kubectl get SealedSecret NAME AGE ithome-example 12s $ kubectl get secret ithome-example NAME TYPE DATA AGE ithome-example kubernetes.io/dockerconfigjson 1 16s 現在來觀察產生出來的 secret 跟我們最原始的 secret 內容是否一致，主要觀察 data 內部的資料，可以發現 .dockerconfigjson 的編碼結果是完全一致的 $ kubectl get secret ithome-example -o yaml apiVersion: v1 data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJyZWdpc3RyeS5od2NoaXUuY29tIjogewoJCQkiYXV0aCI6ICJZV1J0YVc0NlNHRnlZbTl5TVRJek5EVT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE5LjAzLjEyIChsaW51eCkiCgl9Cn0= kind: Secret metadata: creationTimestamp: &quot;2020-09-14T05:31:36Z&quot; name: ithome-example namespace: default ownerReferences: - apiVersion: bitnami.com/v1alpha1 controller: true kind: SealedSecret name: ithome-example uid: a6fa91c0-eb90-403b-baea-5aabc979212c resourceVersion: &quot;1025425&quot; selfLink: /api/v1/namespaces/default/secrets/ithome-example uid: 8546ec86-6e51-4a20-883f-f403ac2b450a type: kubernetes.io/dockerconfigjson $ cat secret.yaml apiVersion: v1 data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJyZWdpc3RyeS5od2NoaXUuY29tIjogewoJCQkiYXV0aCI6ICJZV1J0YVc0NlNHRnlZbTl5TVRJek5EVT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE5LjAzLjEyIChsaW51eCkiCgl9Cn0= kind: Secret metadata: creationTimestamp: null name: ithome-example type: kubernetes.io/dockerconfigjson 最後再來檢查反編碼後的結果，這邊我使用了 view-secret 這個 kubectl plugin 來自動幫忙反編碼，同時也比對最原始的 ~/.docker/config.json，最後確認兩者內容一致。 $ kubectl view-secret ithome-example Choosing key: .dockerconfigjson { &quot;auths&quot;: { &quot;registry.hwchiu.com&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } }, &quot;HttpHeaders&quot;: { &quot;User-Agent&quot;: &quot;Docker-Client/19.03.12 (linux)&quot; } } $ cat ~/.docker/config.json { &quot;auths&quot;: { &quot;registry.hwchiu.com&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } }, &quot;HttpHeaders&quot;: { &quot;User-Agent&quot;: &quot;Docker-Client/19.03.12 (linux)&quot; } } 到這邊我們的 Demo 就告了一個段落，我們透過 kubeseal 來幫忙加密，加密後的結果是一個名為 SealedSecret 的物件，其內容都是加密後的樣式，我們可以直接存放於 Git 裡面，這樣的話 GitOps 的模式也可以套用上去。 SealedSecret 官網上面還有更多關於 Key 的操作，包含 Renew, 更新等各種進階用法，如果對這個開源軟體有興趣的人歡迎玩耍看看","keywords":"","version":"Next"},{"title":"Helm 介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-3","content":"","keywords":"","version":"Next"},{"title":"Helm​","type":1,"pageTitle":"Helm 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-3#helm","content":"根據官方敘述, Helm 是一個管理 Kubernetes 應用程式的套件，透過 Helm Charts 這套系統，可以幫助開發者打包，安裝，升級相關的 Kubernetes 應用程式。 此外， Helm Charts 本身也被設計得很容易去創造，版本控制，分享以及發佈，所以透過 Helm Charts 就可以避免到處 Copy-and-Paste 各式各樣的 Yaml。 Helm 本身也是一個開源專案，而且也是 CNCF 內的畢業專案，目前是由 Helm 社群 進行維護 Helm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. Charts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste. Helm is a graduated project in the CNCF and is maintained by the Helm community. Helm 的架構概念非常簡單，就是將整包 Kubernetes 的所有資源物件再疊加一層抽象層，這個抽象層是給 Helm 工具使用的，Helm 的工具會有自己的方式去解讀這個抽象層，最後產生出最後的 Kubernetes 資源物件然後安裝到 Kubernetes 裡面 ","version":"Next","tagName":"h2"},{"title":"Purpose​","type":1,"pageTitle":"Helm 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-3#purpose","content":"Helm 將所有 Kubernetes 的應用程式都統稱為 Charts. Helm 的工具會將這些 Charts 打包成 tgz 的檔案，接下來可以可以透過 Helm Charts Server 的方式將這個 tgz 的檔案給散佈出去，讓其 他使用者可以方便地取得這些已經打包好的應用程式(Charts)。 此外， Helm 的工具也可以直接針對這些 Charts 所描述的應用程式去安裝到/解除於 Kubernetes 叢集中 對於安裝到 Kubernetes 中的應用程式， Helm 稱其為 Release 而 Chart 到 Release 中間有一個客製化的概念，稱為 Config，透過這個 config 可以產生出適應不同環境的 Kubernetes Yaml 這三者如下圖所示，每個 Charts 搭配不同環境的設定檔案最後會產生出一個唯一的 Release 物件，而該物件就代表者該應用程式於 Kubernetes 內的實體  ","version":"Next","tagName":"h2"},{"title":"客製化​","type":1,"pageTitle":"Helm 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-3#客製化","content":"為了滿足客製化的需求，希望開發者可以簡單的設計 Charts，使用者又可以簡單的客製化使用，這部分 Helm 採用的是 Go Template 的方式來進行 Yaml 的客製化，舉例來說 下面一個常見的 Service Yaml 檔案，內容全部都寫死 apiVersion: v1 kind: Service metadata: name: example labels: app: example spec: type: ClusterIP ports: - port: 80 targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: example app.kubernetes.io/instance: example  這種情況下使用者就沒有辦法客製化需求，譬如需要的 Port(80)，或是不同類型 (ClusterIP/NodePort) Helm 針對這種情況引入了 Go Template，使得 Yaml 檔案的樣子可能會變成如下圖 apiVersion: v1 kind: Service metadata: name: {{ include &quot;example.fullname&quot; . }} labels: {{ include &quot;example.labels&quot; . | indent 4 }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: {{ include &quot;example.name&quot; . }} app.kubernetes.io/instance: {{ .Release.Name }}  可以看到上述的採用大量的 大括號 的格式來進行變數的替換，使用者再使用該 Charts 的時候會對上述的變數進行設定，而這些變數最後在渲染這些 Template 檔案的時候就會給替換掉最後產生出真正的 Yaml 檔案。 舉例來說，第一個使用者安裝的時候輸入 service.type: ClusterIP 就會產生出一個使用 ClusterIP 的 Service，而若輸入的是 service.type:NodePort 則會產生使用 NodePort 的 Service. 為了方便使用者去使用，開發者設計的時候可以準備一套預設值放到一個名為 values.yaml 的檔案裡面，使用者可以直接修改該檔案或是使用別的檔案來替換所有的變數 這種 Go Template 的方式的確可以讓 Yaml 變得很彈性，可以讓使用者針對不同情境傳入不同的數值，但是我認為他也帶來的更多的複雜性，因為這些 Template 的用法十分多元，從基本的變數替換，到 FOR 迴圈， IF 判斷條件等都可以使用。 對於 Helm 用法不理解的人初次看到這些滿滿被 大括號 入侵的 Yaml加上一堆不確定是幹嘛用的關鍵字，其實會難以入手，沒有花更多時間去理解的情況下，可能就只會使用而沒有辦法成為一個開發者去設計一個好的 Helm Chart ","version":"Next","tagName":"h2"},{"title":"散播與發佈​","type":1,"pageTitle":"Helm 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-3#散播與發佈","content":"當開發者準備好一個 Helm Charts 的檔案時候，就可以透過打包的方式將其上傳到官方或是自行維護的 Helm Chart 伺服器 一個使用範例如下(參考自官網) $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ $ helm search repo stable NAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.2 2.1.1 DEPRECATED Scales worker nodes within agent pools stable/aerospike 0.2.8 v4.5.0.5 A Helm chart for Aerospike in Kubernetes stable/airflow 4.1.0 1.10.4 Airflow is a platform to programmatically autho... stable/ambassador 4.1.0 0.81.0 A Helm chart for Datawire Ambassador ...  上述指令代表的意思是我想要將 https://kubernetes-charts.storage.googleapis.com/ 這個 Helm Charts 的伺服器加入到本地 Helm 指令的來源之一，並且嘗試搜尋上面任何有 stable 字眼的 Helm Chart 下列指令則可以嘗試安裝 stable/mysql 這個 Helm Chart 到 Kubernetes 中，產生的 Release 名稱為 smiling-penguin 這邊要注意的是 Helm 本身會需要存取 Kubernetes 叢集，所以也是使用 KUBECONFIG 等方式來設定存取權限 $ helm install stable/mysql --generate-name Released smiling-penguin  最後可以透過 Helm ls 的指令來觀看目前安裝於叢集內的 Helm Release. $ helm ls NAME VERSION UPDATED STATUS CHART smiling-penguin 1 Wed Sep 28 12:59:46 2016 DEPLOYED mysql-0.1.0  ","version":"Next","tagName":"h2"},{"title":"Helm v2 v.s Helm v3​","type":1,"pageTitle":"Helm 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-3#helm-v2-vs-helm-v3","content":"Helm 目前流通的版本有 Helm v2 以及 Helm v3，使用起來差別不會非常誇張，但是如果是新上手的朋友強烈建議直接上 Helm v3，而不要使用 Helm v2，否則後來還要處理更新搬移的問題。 官方網站就有專門一個頁面在介紹如何從 Helm2 搬移至 Helm3, Migrating Helm v2 to v3, 有興趣的人可以點進去看更多詳細的介紹。 下面來列一下 v3 以及 v2 最大的差異 Tiller 的移除，過往使用 Helm v2 的時候，還要在系統內先行安裝一個叫做 Tiller 的伺服器，同時也要對其設定一些權限，安裝起來麻煩，同時也有潛在的安全性問題。 Helm v3 基本上整個架構變得更乾淨，只需要一個 Helm 指令即可Helm Chart 裡面相關的 apiVersion 需要跳號，從 v1 跳到 v2，才會宣告該 Helm Chart 是屬於 Helm v3.更新應用程式的策略， v3 使用的是三方比對來進行測試，將會使用 過往狀態, 當前運作狀態 以及 期望狀態 來比對，最後產生更新後的內容OCI 的支援，這個是我覺得最有趣的功能，未來 Helm Chart 打包後的格式可以遵循 OCI (Open Contaianer Initiative) 的格式，這意味者我們未來將有機會使用 Container Registry 來存放 Helm Chart, 只需要一個伺服器就可以同時滿足 Container Image 以及 Helm Chart，如果有興趣的人可以嘗試使用 Harbor 這套 Contaienr Registry 的解決方案來體驗看看這個功能 想要知道更多關於 OCI 的介紹，可以參考這篇文章 Helm 一些子指令的新增與移除 基本上修改的細部內容非常多，有興趣的建議參考上述官方連結去看看修改細節，可以更加瞭解 Helm3. ","version":"Next","tagName":"h2"},{"title":"Kubernetes plugin 範例","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-27","content":"Kubernetes plugin 範例 上篇文章中我們介紹了 kubectl plugin 的系統與生態系，後來我們使用 krew 這個工具來管理各式各樣的 kubectl plugin 因此本篇就從裡面挑選一些 plugin 試試看。 View Allocations 我們這邊可以隨便挑一些 plugin 來玩看看 $ kubectl krew install view-allocations Updated the local copy of plugin index. Installing plugin: view-allocations Installed plugin: view-allocations \\ | Use this plugin: | kubectl view-allocations | Documentation: | https://github.com/davidB/kubectl-view-allocations / WARNING: You installed plugin &quot;view-allocations&quot; from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. 這邊要注意，因為我們安裝的都是 kubectl plugin 所以最後執行的時候不需要補上 krew ，譬如我們上面安裝 view-allocations，安裝完畢後直接執行 kubectl view-allocations來看 View-allocations 是一個用來顯示系統上所有 有設定 resource 限定的資源 數量都列出來，可以幫助你評估當前每個節點上總共有多少 CPU/Memory，然後上面運行的資源目前總共要求多少，百分比多少。 要注意的是，如果你的 Pod 沒有用 resource limited 去限制，就不會出現在系統上 $ kubectl view-allocations Resource Requested %Requested Limit %Limit Allocatable Free cpu 1050.0m 9% 300.0m 2% 12.0 10.9 ├─ kind-control-plane 850.0m 21% 100.0m 2% 4.0 3.1 │ ├─ coredns-6955765f44-l4z47 100.0m 0.0 │ ├─ coredns-6955765f44-zb5xx 100.0m 0.0 │ ├─ kindnet-czpsv 100.0m 100.0m │ ├─ kube-apiserver-kind-control-plane 250.0m 0.0 │ ├─ kube-controller-manager-kind-control-plane 200.0m 0.0 │ └─ kube-scheduler-kind-control-plane 100.0m 0.0 ├─ kind-worker 100.0m 2% 100.0m 2% 4.0 3.9 │ └─ kindnet-sbqxd 100.0m 100.0m └─ kind-worker2 100.0m 2% 100.0m 2% 4.0 3.9 └─ kindnet-sw5mq 100.0m 100.0m ephemeral-storage 0.0 0% 0.0 0% 581.5Gi 581.5Gi ├─ kind-control-plane 0.0 0% 0.0 0% 193.8Gi 193.8Gi ├─ kind-worker 0.0 0% 0.0 0% 193.8Gi 193.8Gi └─ kind-worker2 0.0 0% 0.0 0% 193.8Gi 193.8Gi memory 290.0Mi 1% 490.0Mi 1% 46.9Gi 46.4Gi ├─ kind-control-plane 190.0Mi 1% 390.0Mi 2% 15.6Gi 15.3Gi │ ├─ coredns-6955765f44-l4z47 70.0Mi 170.0Mi │ ├─ coredns-6955765f44-zb5xx 70.0Mi 170.0Mi │ └─ kindnet-czpsv 50.0Mi 50.0Mi ├─ kind-worker 50.0Mi 0% 50.0Mi 0% 15.6Gi 15.6Gi │ └─ kindnet-sbqxd 50.0Mi 50.0Mi └─ kind-worker2 50.0Mi 0% 50.0Mi 0% 15.6Gi 15.6Gi └─ kindnet-sw5mq 50.0Mi 50.0Mi pods 0.0 0% 0.0 0% 330.0 330.0 ├─ kind-control-plane 0.0 0% 0.0 0% 110.0 110.0 ├─ kind-worker 0.0 0% 0.0 0% 110.0 110.0 └─ kind-worker2 0.0 0% 0.0 0% 110.0 110.0 這個工具我個人認為還滿好用的，畢竟可以幫你顯示出當前系統上運算資源所使用的 CPU/Memory 等使用量，這些使用量可以用來幫助開發者判斷要如何設定相關的資源限制。 change-ns 這套工具相對簡單，就是幫你切換預設的 namespace，減少每次輸入指令的時候都要一直透過 -n|--namespace 來指定特定的 namespace。 $ kubectl krew install change-ns Updated the local copy of plugin index. Installing plugin: change-ns Installed plugin: change-ns \\ | Use this plugin: | kubectl change-ns | Documentation: | https://github.com/juanvallejo/kubectl-ns / WARNING: You installed plugin &quot;change-ns&quot; from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. $ kubectl change-ns kube-system namespace changed to &quot;kube-system&quot; $ kubectl get pods NAME READY STATUS RESTARTS AGE coredns-6955765f44-l4z47 1/1 Running 0 2d13h coredns-6955765f44-zb5xx 1/1 Running 0 2d13h etcd-kind-control-plane 1/1 Running 0 2d13h kindnet-czpsv 1/1 Running 0 2d13h kindnet-sbqxd 1/1 Running 0 2d13h kindnet-sw5mq 1/1 Running 0 2d13h kube-apiserver-kind-control-plane 1/1 Running 0 2d13h kube-controller-manager-kind-control-plane 1/1 Running 0 2d13h kube-proxy-4b5rl 1/1 Running 0 2d13h kube-proxy-nrspx 1/1 Running 0 2d13h kube-proxy-skfm5 1/1 Running 0 2d13h kube-scheduler-kind-control-plane 1/1 Running 0 2d13h 類似的工具還有ctx ，可以幫切換不同的 kubeconfig context，讓你更方便的於多個 Kubernetes Cluster 中切換 Status 這個工具算是幫你把 description 的資訊再次整理，舉例來說我們準備了一個 pull image 會失敗的案例，這時候我們用 status 這個指令來試試看 $ kubectl krew install status Updated the local copy of plugin index. Installing plugin: status Installed plugin: status \\ | Use this plugin: | kubectl status | Documentation: | https://github.com/bergerx/kubectl-status / WARNING: You installed plugin &quot;status&quot; from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. 安裝完畢後我們針對一個失敗的 pod 來使用 kubectl status pod xxxx $ kubectl status pod pull-fail Pod/pull-fail -n default, created 2m ago Pending Burstable PodScheduled -&gt; Initialized -&gt; Not ContainersReady -&gt; Not Ready Ready ContainersNotReady, containers with unready status: [getting-started] for 2m ContainersReady ContainersNotReady, containers with unready status: [getting-started] for 2m Standalone POD. Containers: getting-started (hwchiu/netutils-qq) Waiting ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image &quot;docker.io/hwchiu/netutils-qq:latest&quot;: failed to resolve reference &quot;docker.io/hwchiu/netutils-qq:latest&quot;: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed Events: Scheduled 2m ago from default-scheduler: Successfully assigned default/pull-fail to kind-worker Pulling 28s ago (x4 over 1m) from kubelet,kind-worker: Pulling image &quot;hwchiu/netutils-qq&quot; Failed 28s ago (x4 over 1m) from kubelet,kind-worker: Failed to pull image &quot;hwchiu/netutils-qq&quot;: rpc error: code = Unknown desc = failed to pull and unpack image &quot;docker.io/hwchiu/netutils-qq:latest&quot;: failed to resolve reference &quot;docker.io/hwchiu/netutils-qq:latest&quot;: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed Failed 28s ago (x4 over 1m) from kubelet,kind-worker: Error: ErrImagePull BackOff 13s ago (x6 over 1m) from kubelet,kind-worker: Back-off pulling image &quot;hwchiu/netutils-qq&quot; Failed 13s ago (x6 over 1m) from kubelet,kind-worker: Error: ImagePullBackOff 上面可以看到一些資訊，譬如說 PodScheduled -&gt; Initialized -&gt; Not ContainersReady -&gt; Not Ready Pod 失敗是因為卡在 ContainersReady 這個狀態會失敗，導致最後整個 Pod 沒有成功Standalone POD 這個 Pod 本身沒有任何的 StatefulSet/ReplicaSet，而是獨立的 PodContainers: 底下就是一些詳細訊息，譬如為什麼會失敗Events: 這個 Pod 的一些事件資訊 除了 Pod 之外， Status 也可以用來看其他的資源，有興趣可以玩看看 access-matrix 接下來這個工具主要是用來列出當前使用者對於系統上的全部 Resource的權限資訊，主要是該使用者對於特定資源上的不同動詞 (Get/Update/List/Delete) 等是否可以執行 $ kubectl krew install access-matrix Updated the local copy of plugin index. Installing plugin: access-matrix Installed plugin: access-matrix \\ | Use this plugin: | kubectl access-matrix | Documentation: | https://github.com/corneliusweig/rakkess | Caveats: | \\ | | Usage: | | kubectl access-matrix | | kubectl access-matrix for pods | / / WARNING: You installed plugin &quot;access-matrix&quot; from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. 此外也可以透過 --sa 等指令來切換不同的 service account，所以可以看到下列的範例，用不同的使用者去看權限，我預設的使用者有幾乎無敵的權限，什麼都可以執行。如果是系統上 kube-system:namespace-controller 則只能 LIST/DELETE。 除了這四個動詞之外，其實還有很多動詞可以用，只是預設情況下只會列出這四個 $ kubectl access-matrix --sa kube-system:namespace-controller NAME LIST CREATE UPDATE DELETE apiservices.apiregistration.k8s.io ✔ ✖ ✖ ✔ bindings ✖ certificatesigningrequests.certificates.k8s.io ✔ ✖ ✖ ✔ clusterrolebindings.rbac.authorization.k8s.io ✔ ✖ ✖ ✔ clusterroles.rbac.authorization.k8s.io ✔ ✖ ✖ ✔ componentstatuses ✔ configmaps ✔ ✖ ✖ ✔ controllerrevisions.apps ✔ ✖ ✖ ✔ cronjobs.batch ✔ ✖ ✖ ✔ csidrivers.storage.k8s.io ✔ ✖ ✖ ✔ ..... $ kubectl access-matrix NAME LIST CREATE UPDATE DELETE apiservices.apiregistration.k8s.io ✔ ✔ ✔ ✔ bindings ✔ certificatesigningrequests.certificates.k8s.io ✔ ✔ ✔ ✔ clusterrolebindings.rbac.authorization.k8s.io ✔ ✔ ✔ ✔ clusterroles.rbac.authorization.k8s.io ✔ ✔ ✔ ✔ componentstatuses ✔ configmaps ✔ ✔ ✔ ✔ controllerrevisions.apps ✔ ✔ ✔ ✔ cronjobs.batch ✔ ✔ ✔ ✔ csidrivers.storage.k8s.io ✔ ✔ ✔ ✔ starboard 最後來看一個跟安全性有關的 plugin Starboard integrates security tools into the Kubernetes environment, so that users can find and view the risks that relate to different resources in a Kubernetes-native way. Starboard provides custom security resources definitions and a Go module to work with a range of existing security tools, as well as a kubectl-compatible command-line tool and an Octant plug-in that make security reports available through familiar Kubernetes tools. 接下來示範怎麼用(假設已經安裝完畢) $ kubectl starboard init $ kubectl create deployment nginx --image nginx:1.16 先透過 starboard 去初始化相關資源，接者我們部署一個 nginx:1.16 的容器到系統中 $ kubectl starboard find vulnerabilities deployment/nginx $ kubectl starboard get vulnerabilities deployment/nginx .... summary: criticalCount: 0 highCount: 4 lowCount: 93 mediumCount: 34 noneCount: 0 unknownCount: 0 vulnerabilities: - description: Missing input validation in the ar/tar implementations of APT before version 2.1.2 could result in denial of service when processing specially crafted deb files. fixedVersion: 1.8.2.1 installedVersion: 1.8.2 layerID: &quot;&quot; links: - https://bugs.launchpad.net/bugs/1878177 - https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-3810 - https://github.com/Debian/apt/issues/111 - https://github.com/julian-klode/apt/commit/de4efadc3c92e26d37272fd310be148ec61dcf36 - https://lists.debian.org/debian-security-announce/2020/msg00089.html - https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/U4PEH357MZM2SUGKETMEHMSGQS652QHH/ - https://salsa.debian.org/apt-team/apt/-/commit/dceb1e49e4b8e4dadaf056be34088b415939cda6 - https://salsa.debian.org/jak/apt/-/commit/dceb1e49e4b8e4dadaf056be34088b415939cda6 - https://tracker.debian.org/news/1144109/accepted-apt-212-source-into-unstable/ - https://usn.ubuntu.com/4359-1/ - https://usn.ubuntu.com/4359-2/ - https://usn.ubuntu.com/usn/usn-4359-1 - https://usn.ubuntu.com/usn/usn-4359-2 resource: apt severity: MEDIUM title: &quot;&quot; vulnerabilityID: CVE-2020-3810 ... 可以看到上面有很多訊息，列出當前 image 上有哪些潛在的 CVE，如果覺得這樣看起來實在不討喜，可以使用 starboard-octant-plugin 這個整合專案，把上述的報告用 UI 的方式視覺話呈現出來，譬如說下圖(下圖節錄自 starboard-octant-plugin 官方 Repo) 到這邊為止，我們介紹了一些有趣的 Kubectl plugin，當然這些 plugin 本身也都是一個獨立的執行檔案，所以其實就算不透過 kubectl 來執行也是沒問題的，所有個工具都可以獨立使用。透過 krew 只是我們可以更方便的搜尋到有哪些 plugin 可以用，實務上要怎麼執行都是個人喜歡，方便，操作順暢即可。 Krew 上面的工具非常多，使用上可以都可以嘗試看看，也因為這樣才有辦法找到真的對自己日常工作有幫助的好幫手","keywords":"","version":"Next"},{"title":"kubelet Plugin 介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-26","content":"","keywords":"","version":"Next"},{"title":"探索​","type":1,"pageTitle":"kubelet Plugin 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-26#探索","content":"因為其架構非常簡單，所以只要執行 kubectl plugin list，預設情況下會因為沒有安裝任何的東西，執行結果就不會找到任何東西 $ kubectl plugin list Unable read directory &quot;/home/ubuntu/go/bin&quot; from your PATH: open /home/ubuntu/go/bin: no such file or directory. Skipping... error: unable to find any kubectl plugins in your PATH  但是如果每次要安裝這些 plugin 都要自己想辦法去找這些執行檔，並且放到系統環境下其實滿累的。 所以這時候我們就可以使用 krew 這套系統來幫忙管理整個 kubectl plugin, 等等就來試試看相關功能 ","version":"Next","tagName":"h2"},{"title":"限制​","type":1,"pageTitle":"kubelet Plugin 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-26#限制","content":"要注意的是因為 kubectl 本身已經有很多子指令可以使用了，因此這些 Plugin 不能覆蓋這些子指令，譬如不能創造一個 kubectl-get 來取代 kubectl get 之類的，不過對於非開發者來說，這些限制可以忽略，專注於如何找尋現存的 plugin 並且使用到日常工作中即可 Krew Krew 的官方說明如下 Krew is the package manager for kubectl plugins. Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew. Today, over 70 kubectl plugins are available on Krew. 根據說明目前已經超過 70 個以上的 plugin, 因此我們接下來就從中挑幾個有趣的來玩看看 ","version":"Next","tagName":"h2"},{"title":"安裝​","type":1,"pageTitle":"kubelet Plugin 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-26#安裝","content":"首先我們要先安裝 Krew, 基本上 Krew 也會變成一個 kubectl plugin ，之後會透過 kubectl krew 來管理 $ ( set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp; curl -fsSLO &quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz&quot; &amp;&amp; tar zxvf krew.tar.gz &amp;&amp; KREW=./krew-&quot;$(uname | tr '[:upper:]' '[:lower:]')_amd64&quot; &amp;&amp; &quot;$KREW&quot; install krew ) $ export PATH=&quot;${KREW_ROOT:-$HOME/.krew}/bin:$PATH&quot;  $ kubectl krew krew is the kubectl plugin manager. You can invoke krew through kubectl: &quot;kubectl krew [command]...&quot; Usage: kubectl krew [command] Available Commands: help Help about any command index Manage custom plugin indexes info Show information about an available plugin install Install kubectl plugins list List installed kubectl plugins search Discover kubectl plugins uninstall Uninstall plugins update Update the local copy of the plugin index upgrade Upgrade installed plugins to newer versions version Show krew version and diagnostics Flags: -h, --help help for krew -v, --v Level number for the log level verbosity Use &quot;kubectl krew [command] --help&quot; for more information about a command.  Krew 底下滿多指令的，我們等等會透過 search 來探索全部 plugins 並且透過 install 來安裝 ","version":"Next","tagName":"h2"},{"title":"使用​","type":1,"pageTitle":"kubelet Plugin 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-26#使用","content":"搜尋上非常簡單，透過 kubectl krew search 去列出目前 $ kubectl krew search NAME DESCRIPTION INSTALLED access-matrix Show an RBAC access matrix for server resources no advise-psp Suggests PodSecurityPolicies for cluster. no apparmor-manager Manage AppArmor profiles for cluster. no auth-proxy Authentication proxy to a pod or service no bd-xray Run Black Duck Image Scans no bulk-action Do bulk actions on Kubernetes resources. no ca-cert Print the PEM CA certificate of the current clu... no capture Triggers a Sysdig capture to troubleshoot the r... no cert-manager Manage cert-manager resources inside your cluster no change-ns View or change the current namespace via kubectl. no cilium Easily interact with Cilium agents. no cluster-group Exec commands across a group of contexts. no config-cleanup Automatically clean up your kubeconfig no cssh SSH into Kubernetes nodes no ctx Switch between contexts in your kubeconfig no custom-cols A &quot;kubectl get&quot; replacement with customizable c... no datadog Manage the Datadog Operator no debug Attach ephemeral debug container to running pod no debug-shell Create pod with interactive kube-shell. no deprecations Checks for deprecated objects in a cluster no df-pv Show disk usage (like unix df) for persistent v... no doctor Scans your cluster and reports anomalies. no duck List custom resources with ducktype support no edit-status Edit /status subresources of CRs no eksporter Export resources and removes a pre-defined set ... no emit-event Emit Kubernetes Events for the requested object no evict-pod Evicts the given pod no example Prints out example manifest YAMLs no exec-as Like kubectl exec, but offers a `user` flag to ... no exec-cronjob Run a CronJob immediately as Job no fields Grep resources hierarchy by field name no flame Generate CPU flame graphs from pods no fleet Shows config and resources of a fleet of clusters no fuzzy Fuzzy and partial string search for kubectl no gadget Gadgets for debugging and introspecting apps no get-all Like `kubectl get all` but _really_ everything no gke-credentials Fetch credentials for GKE clusters no gopass Imports secrets from gopass no graph Visualize Kubernetes resources and relationships. no grep Filter Kubernetes resources by matching their n... no gs Handle custom resources with Giant Swarm no iexec Interactive selection tool for `kubectl exec` no images Show container images used in the cluster. no ingress-nginx Interact with ingress-nginx no ipick A kubectl wrapper for interactive resource sele... no konfig Merge, split or import kubeconfig files no krew Package manager for kubectl plugins. yes kubesec-scan Scan Kubernetes resources with kubesec.io. no kudo Declaratively build, install, and run operators... no kuttl Declaratively run and test operators no kyverno Kyverno is a policy engine for kubernetes no match-name Match names of pods and other API objects no modify-secret modify secret with implicit base64 translations no mtail Tail logs from multiple pods matching label sel... no neat Remove clutter from Kubernetes manifests to mak... no net-forward Proxy to arbitrary TCP services on a cluster ne... no node-admin List nodes and run privileged pod with chroot no node-restart Restart cluster nodes sequentially and gracefully no node-shell Spawn a root shell on a node via kubectl no np-viewer Network Policies rules viewer no ns Switch between Kubernetes namespaces no oidc-login Log in to the OpenID Connect provider no open-svc Open the Kubernetes URL(s) for the specified se... no operator Manage operators with Operator Lifecycle Manager no oulogin Login to a cluster via OpenUnison no outdated Finds outdated container images running in a cl... no passman Store kubeconfig credentials in keychains or pa... no pod-dive Shows a pod's workload tree and info inside a node no pod-logs Display a list of pods to get logs from no pod-shell Display a list of pods to execute a shell in no podevents Show events for pods no popeye Scans your clusters for potential resource issues no preflight Executes application preflight tests in a cluster no profefe Gather and manage pprof profiles from running pods no prompt Prompts for user confirmation when executing co... no prune-unused Prune unused resources no psp-util Manage Pod Security Policy(PSP) and the related... no rbac-lookup Reverse lookup for RBAC no rbac-view A tool to visualize your RBAC permissions. no resource-capacity Provides an overview of resource requests, limi... no resource-snapshot Prints a snapshot of nodes, pods and HPAs resou... no restart Restarts a pod with the given name no rm-standalone-pods Remove all pods without owner references no rolesum Summarize RBAC roles for subjects no roll Rolling restart of all persistent pods in a nam... no schemahero Declarative database schema migrations via YAML no score Kubernetes static code analysis. no service-tree Status for ingresses, services, and their backends no sick-pods Find and debug Pods that are &quot;Not Ready&quot; no snap Delete half of the pods in a namespace or cluster no sniff Start a remote packet capture on pods using tcp... no sort-manifests Sort manifest files in a proper order by Kind no split-yaml Split YAML output into one file per resource. no spy pod debugging tool for kubernetes clusters with... no sql Query the cluster via pseudo-SQL no ssh-jump A kubectl plugin to SSH into Kubernetes nodes u... no sshd Run SSH server in a Pod no ssm-secret Import/export secrets from/to AWS SSM param store no starboard Toolkit for finding risks in kubernetes resources no status Show status details of a given resource. no sudo Run Kubernetes commands impersonated as group s... no support-bundle Creates support bundles for off-cluster analysis no tail Stream logs from multiple pods and containers u... no tap Interactively proxy Kubernetes Services with ease no tmux-exec An exec multiplexer using Tmux no topology Explore region topology for nodes or pods no trace bpftrace programs in a cluster no tree Show a tree of object hierarchies through owner... no unused-volumes List unused PVCs no view-allocations List allocations per resources, nodes, pods. no view-secret Decode Kubernetes secrets no view-serviceaccount-kubeconfig Show a kubeconfig setting to access the apiserv... no view-utilization Shows cluster cpu and memory utilization no virt Control KubeVirt virtual machines using virtctl no warp Sync and execute local files in Pod no who-can Shows who has RBAC permissions to access Kubern... no whoami Show the subject that's currently authenticated... no  如果覺得上面資訊太多，也可以傳遞第二個參數來過濾，譬如 $ kubectl krew search pod NAME DESCRIPTION INSTALLED evict-pod Evicts the given pod no pod-dive Shows a pod's workload tree and info inside a node no pod-logs Display a list of pods to get logs from no pod-shell Display a list of pods to execute a shell in no podevents Show events for pods no rm-standalone-pods Remove all pods without owner references no sick-pods Find and debug Pods that are &quot;Not Ready&quot; no support-bundle Creates support bundles for off-cluster analysis no  下一篇我們就來從中挑選一些有趣的 plugin 玩看看 ","version":"Next","tagName":"h2"},{"title":"本文","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-4","content":"本文 上篇文章中我們介紹了 Helm 的概念，包含了 Helm Chart, Config 以及 Released，而要瞭解這些概念最好的方式就是直接參考一個實際的範例， 首先根據官方教學，安裝 Helm 指令到系統中, 多種安裝方法，擇一即可 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 因此我們會使用 Helm create 指令創建一個基本的 Helm Chart，並從中瞭解其架構 $ helm create ithome $ tree ithome ithome ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml 3 directories, 10 files 一個全新產生的 Helm Chart 內總共有 10 個檔案， 3個資料夾 裡面跟 Kubernetes 有關的物件資源有五個，包含 deployment.yaml, hpa.yaml, ingress.yaml, service.yaml, serviceaccount.yaml, 這些 Yaml 內容都含有 Go Template 的內容 $ cat ithome/templates/hpa.yaml {{- if .Values.autoscaling.enabled }} apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: {{ include &quot;ithome.fullname&quot; . }} labels: {{- include &quot;ithome.labels&quot; . | nindent 4 }} spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: {{ include &quot;ithome.fullname&quot; . }} minReplicas: {{ .Values.autoscaling.minReplicas }} maxReplicas: {{ .Values.autoscaling.maxReplicas }} metrics: {{- if .Values.autoscaling.targetCPUUtilizationPercentage }} - type: Resource resource: name: cpu targetAverageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }} {{- end }} {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }} - type: Resource resource: name: memory targetAverageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }} {{- end }} {{- end }} 此外還可以看到最外面有一個 Values.yaml，裡面就包含各式各樣的變數以及預設值， ╰─$ cat values.yaml # Default values for ithome. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: nginx pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: &quot;&quot; imagePullSecrets: [] nameOverride: &quot;&quot; fullnameOverride: &quot;&quot; serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: &quot;&quot; podAnnotations: {} podSecurityContext: {} # fsGroup: 2000 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 service: type: ClusterIP port: 80 ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: &quot;true&quot; hosts: - host: chart-example.local paths: [] tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi autoscaling: enabled: false minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 80 # targetMemoryUtilizationPercentage: 80 nodeSelector: {} tolerations: [] affinity: {} 如果想要安裝這個 Helm Chart 到系統內，依序執行下列指令 創建測試用 namespace將該 Helm Chart 安裝到系統中的 ithome namespace 並且將該 released 命名為 ithome. 來源 Helm Charts 是當前資料夾 . $ kubectl create ns ithome namespace/ithome created $ helm install --namespace ithome ithome . NAME: ithome LAST DEPLOYED: Tue Sep 8 21:54:12 2020 NAMESPACE: ithome STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace ithome -l &quot;app.kubernetes.io/name=ithome,app.kubernetes.io/instance=ithome&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;) echo &quot;Visit http://127.0.0.1:8080 to use your application&quot; kubectl --namespace ithome port-forward $POD_NAME 8080:80 $ kubectl -n ithome get all 1 ↵ NAME READY STATUS RESTARTS AGE pod/ithome-5cc87ff5f4-xnpvh 1/1 Running 0 36s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ithome ClusterIP 10.43.95.165 &lt;none&gt; 80/TCP 36s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ithome 1/1 1 1 36s NAME DESIRED CURRENT READY AGE replicaset.apps/ithome-5cc87ff5f4 1 1 1 36s 再來則是觀看系統上安裝的哪些的 Helm Chart，可以透過 helm ls 的方式來觀看，如果有不同的 namespace 都要透過 -n 來指定 $ helm -n ithome ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ithome ithome 1 2020-09-08 21:54:12.803147 -0700 PDT deployed ithome-0.1.0 1.16.0 最後是一系列好用的指令，helm get 可以取得該 Released 上的各種資料 $ helm get --help Usage: helm get [command] Available Commands: all download all information for a named release hooks download all hooks for a named release manifest download the manifest for a named release notes download the notes for a named release values download the values file for a named release 最簡單的兩個範例就是 manifest 以及 values, 透過 manifest 我們可以直接觀察到最後安裝到系統內的 YAML 檔案長什麼樣子 譬如 $ helm -n ithome get manifest ithome --- # Source: ithome/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ithome labels: helm.sh/chart: ithome-0.1.0 app.kubernetes.io/name: ithome app.kubernetes.io/instance: ithome app.kubernetes.io/version: &quot;1.16.0&quot; app.kubernetes.io/managed-by: Helm --- # Source: ithome/templates/service.yaml apiVersion: v1 kind: Service metadata: name: ithome labels: helm.sh/chart: ithome-0.1.0 app.kubernetes.io/name: ithome app.kubernetes.io/instance: ithome app.kubernetes.io/version: &quot;1.16.0&quot; app.kubernetes.io/managed-by: Helm spec: type: ClusterIP ports: - port: 80 targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: ithome app.kubernetes.io/instance: ithome --- # Source: ithome/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: ithome labels: helm.sh/chart: ithome-0.1.0 app.kubernetes.io/name: ithome app.kubernetes.io/instance: ithome app.kubernetes.io/version: &quot;1.16.0&quot; app.kubernetes.io/managed-by: Helm spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ithome app.kubernetes.io/instance: ithome template: metadata: labels: app.kubernetes.io/name: ithome app.kubernetes.io/instance: ithome spec: serviceAccountName: ithome securityContext: {} containers: - name: ithome securityContext: {} image: &quot;nginx:1.16.0&quot; imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http resources: {} 可以看到這邊就是上述那些充滿 Go Template 格式Yaml渲染後的結果，我們也可以使用 helm get values 來看一次目前是否有任何客製化的設定 helm -n ithome get values ithome USER-SUPPLIED VALUES: null 根據這個指令我們可以觀察到本次安裝沒有任何客製化的變動，採用的是最原生的 Values.yaml. 因此接下來我們嘗試升級該 Release，並且修改裡面的設定值 $ helm -n ithome upgrade ithome --set service.type=NodePort . Release &quot;ithome&quot; has been upgraded. Happy Helming! NAME: ithome LAST DEPLOYED: Tue Sep 8 22:02:49 2020 NAMESPACE: ithome STATUS: deployed REVISION: 2 NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace ithome -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot; services ithome) export NODE_IP=$(kubectl get nodes --namespace ithome -o jsonpath=&quot;{.items[0].status.addresses[0].address}&quot;) echo http://$NODE_IP:$NODE_PORT 指令中我們透過 helm upgrade 的方式來升級已經存在的 Released ithome, 我們透過 --set service.type=NodePort 的方式去覆蓋掉 values.yaml 裡面的預設數值(這邊也可以直接修改 values.yaml, 或是產生一個全新的 Yaml 然後送給 Helm 指令)。最後我們指令來源 Helm Chart 的位置 . (當前目錄)。 可以看到上述指令後來輸出一些部署的資訊，包含該 Relased 是第二個版本，部署的時間，當前狀態，什麼 namespace. 一切完畢之後，我們再度使用 helm get values 的指令來看看是否有什麼變化 $ helm -n ithome get values ithome USER-SUPPLIED VALUES: service: type: NodePort $ kubectl -n ithome get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ithome NodePort 10.43.95.165 &lt;none&gt; 80:30232/TCP 9m20s 這時候可以很明確地看到，當前運行的 ithome released 有一個客製化的選項，就是我們前述所輸入的 service.type ，同時觀察 kuberctl -n ithome get svc 也真的看到 service 的內容變成 NodePort. Helm 可以操作與設定的東西非常多，這邊的設定只是一個非常簡單的範例，實務上有非常多的事情要處理，也有非常多的小麻煩，譬如當你的客製化資訊本身有雙引號或是個 JSON 字串，你的腳本該怎麼處理。 Helm 要如何跟應用程式整理，開發人員跟維護人員誰要負責設計與維護應用程式的 Helm Chart, 基本上都沒有一個完整答案，只要能夠讓你輕鬆上班，簡單部署，達到薪水小偷的境界就是一個好的解決方案。","keywords":"","version":"Next"},{"title":"本文","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-5","content":"","keywords":"","version":"Next"},{"title":"Kubeadm​","type":1,"pageTitle":"本文","url":"/docs/techPost/2020/iThome_Challenge/cicd-5#kubeadm","content":"Kubeadm 是由官方維護的開源專案，我認為是非常簡單的一個測試方式，其本身會透過 systemd 的方式維護 Kubelet 之後再透過 container 的方式叫起 controller/scheduler/kube-proxy 等 Kubernetes 核心元件。 使用方面 Kubeadm 本身不算困難使用，可以透過指令列的方式來創建一切所需資源，唯一要注意的是安裝完畢之後還需要人為手動安裝 CNI 的解決方案整個 Kubernetes 才算是安裝完畢。 Kubeadm 本身也支援架設多節點的叢集，只是在使用上沒有這麼方便，需要先創建 Master 節點，並且產生相對應的 token/key，接下來其他節點使用 kubeadm 的指令加入到已經創建的叢集中。 總體來說， Kubeadm 能夠滿足上述要求，但是實作上會稍嫌麻煩，特別是多節點的情況下還要處理 Token/key 的資訊，此外 CNI 的安裝也需要自己處理，但是作為一個單節點的測試環境也算是容易上手且堪用 ","version":"Next","tagName":"h2"},{"title":"Minikube​","type":1,"pageTitle":"本文","url":"/docs/techPost/2020/iThome_Challenge/cicd-5#minikube","content":"Minikube 也是由官方維護的專案，其本身的架構一開始是依賴於 VM (虛擬機器) 來幫使用者創建一個全新測試的 Kubernetes 叢集，任何平台的開發者都可以輕鬆只用，因為背後都會幫你起一個全新的 VM 。當 VM 起來之後，其會透過 kubeadm 的方式幫忙建立與設定 Kubernetes 叢集，並且幫你把 CNI 等指令都安裝完成。 除了依賴 VM 之外，其也有提供不同底層實作，譬如 none 就可以直接在該機器上透過 kubeadm 來建立，基本上整個架構會變得跟 kubeadm 非常類似，比較大的差異是 CNI 也會一併幫你安裝完成。 此外 Mnikube 本身也有一些屬於自己的套件，可以把一些功能整包裝進去，對於這個功能我的想法是不好也不壞，不壞的地方在於提供一個環境讓使用者去測試功能，著實方便，不好的地方在於可能會讓使用者以為這些功能都是 Kubernetes 本來就有的，反而會有所誤解，甚至對於其背後使用原理都不太清楚就草草學習完畢。 總體來說， Minikube 也可以滿足上述的部分要求，多節點的部分可能就會跑起來多個 VM 來建立，消耗的資源會相對多一點。 ","version":"Next","tagName":"h2"},{"title":"KIND​","type":1,"pageTitle":"本文","url":"/docs/techPost/2020/iThome_Challenge/cicd-5#kind","content":"KIND 的全名是 Kubernetes In Docker，顧名思義就是把 Kubernetes 的節點都用 Docker 的方式叫起來運行，每一個 Docker Container 就是一個 Kubernetes 節點，可以充當 Worker 也可以充當 Master. 使用方面非常簡單，使用 KIND 的指令搭配一個設定檔案就可以輕鬆地建立起 Kubernetes 叢集，由於全部的操作都是由 KIND 完成，所以要建立多節點的方式也非常簡單，只要設定檔案中描述需要多少節點以及各自什麼身份，接下來就一個指令搞定全部，連 CNI 方面都不需要處理， KIND 會自行搞定 總體來說， KIND 可以滿足上述所有需求，多節點的部分則是用 Docker 來管理，因此在資源與啟動速度方面都有良好的效果，搭配 Vagrant 的方式就可以輕鬆打包一個多節點的 VM 環境供測試者開發，著實方便。 ","version":"Next","tagName":"h2"},{"title":"K3D​","type":1,"pageTitle":"本文","url":"/docs/techPost/2020/iThome_Challenge/cicd-5#k3d","content":"K3D 是由 Rancher 所開發 K3S 的 Docker 版本， K3S 是一個輕量級的 Kubernetes 平台，本身適合用在一些低運算資源系統上 而 K3D 直接將 K3S 給移植到 Docker 之中，讓使用者可以更方便的創建一個 K3S 叢集。 使用方便也是很簡單，整個主要架構都在 k3d 這個執行檔案上面，使用該指令搭配不同的參數就可以快速地建立起多節點的 Kubernetes Cluster，此外也可以透過指令動態增加節點，使用上也是非常方便。 與KIND一樣， CNI 的部分也會一併被處理，所以使用者真的只需要一個指令就可以處理好所有的事情，總體來說， K3D 可以滿足上述所有要求，優點基本上跟 KIND 完全類似，搭配上 Vagrant 真的可以輕鬆地建立起多節點的模擬環境。 ","version":"Next","tagName":"h2"},{"title":"K3D","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-6","content":"","keywords":"","version":"Next"},{"title":"安裝​","type":1,"pageTitle":"K3D","url":"/docs/techPost/2020/iThome_Challenge/cicd-6#安裝","content":"安裝過程非常簡單，一行指令就可以 sudo curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash  $ k3d Usage: k3d [flags] k3d [command] Available Commands: cluster Manage cluster(s) completion Generate completion scripts for [bash, zsh, powershell | psh] help Help about any command image Handle container images. kubeconfig Manage kubeconfig(s) node Manage node(s) version Show k3d and default k3s version Flags: -h, --help help for k3d --verbose Enable verbose output (debug logging) --version Show k3d and default k3s version Use &quot;k3d [command] --help&quot; for more information about a command.  整個指令非常簡單，比較常見會使用的就是 cluster, kubeconfig 以及 node ","version":"Next","tagName":"h2"},{"title":"創建 Cluster​","type":1,"pageTitle":"K3D","url":"/docs/techPost/2020/iThome_Challenge/cicd-6#創建-cluster","content":"創建上也是非常簡單，輸入 k3d cluster 可以看到一些跟 cluster 相關的指令，實際上使用的時候都要描述你希望的 cluster 名稱，這邊我就不輸入，一律採用預設值 k3s-default $ k3d cluster Manage cluster(s) Usage: k3d cluster [flags] k3d cluster [command] Available Commands: create Create a new cluster delete Delete cluster(s). list List cluster(s) start Start existing k3d cluster(s) stop Stop existing k3d cluster(s) Flags: -h, --help help for cluster Global Flags: --verbose Enable verbose output (debug logging) Use &quot;k3d cluster [command] --help&quot; for more information about a command  我們可以透過 k3d cluster create 來創建一個 k3s 的叢集，預設情況下是一個節點，我們可以透過 -s 的方式來指定要有多少個 node. $ k3d cluster create -s 3 INFO[0000] Created network 'k3d-k3s-default' INFO[0000] Created volume 'k3d-k3s-default-images' INFO[0000] Creating initializing server node INFO[0000] Creating node 'k3d-k3s-default-server-0' INFO[0009] Creating node 'k3d-k3s-default-server-1' INFO[0010] Creating node 'k3d-k3s-default-server-2' INFO[0011] Creating LoadBalancer 'k3d-k3s-default-serverlb' INFO[0018] Cluster 'k3s-default' created successfully! INFO[0018] You can now use it like this: kubectl cluster-info  創建完畢後，我們馬上透過 docker 指令來觀察，可以觀察到的確有 docker container 被創立起來，不過數量卻是比 server 還要多一個，主要是用來當作 load-balancer 使用 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b5903d159c73 rancher/k3d-proxy:v3.0.1 &quot;/bin/sh -c nginx-pr…&quot; 42 minutes ago Up 42 minutes 80/tcp, 0.0.0.0:44429-&gt;6443/tcp k3d-k3s-default-serverlb aaa0cd077a51 rancher/k3s:v1.18.6-k3s1 &quot;/bin/k3s server --t…&quot; 42 minutes ago Up 42 minutes k3d-k3s-default-server-2 636968375fd2 rancher/k3s:v1.18.6-k3s1 &quot;/bin/k3s server --t…&quot; 42 minutes ago Up 42 minutes k3d-k3s-default-server-1 5bfb8b1c64bb rancher/k3s:v1.18.6-k3s1 &quot;/bin/k3s server --c…&quot; 43 minutes ago Up 43 minutes k3d-k3s-default-server-0  ","version":"Next","tagName":"h2"},{"title":"存取 Kubernetes​","type":1,"pageTitle":"K3D","url":"/docs/techPost/2020/iThome_Challenge/cicd-6#存取-kubernetes","content":"為了存取 Kubernetes，我們都會需要準備一份 KUBECONFIG 裡面描述 API Server 的位置，以及使用到的 Username 等資訊，這部分 k3d 也有提供相關的指令來處理 KUBECONFIG $ k3d kubeconfig Manage kubeconfig(s) Usage: k3d kubeconfig [flags] k3d kubeconfig [command] Available Commands: get Print kubeconfig(s) from cluster(s). merge Write/Merge kubeconfig(s) from cluster(s) into new or existing kubeconfig/file. Flags: -h, --help help for kubeconfig Global Flags: --verbose Enable verbose output (debug logging) Use &quot;k3d kubeconfig [command] --help&quot; for more information about a command.  為了簡單測試，我們可以直接使用 k3d kubeconfig merge 讓他產生一個全新的檔案 $ k3d kubeconfig merge /home/ubuntu/.k3d/kubeconfig-k3s-default.yaml $ KUBECONFIG=~/.k3d/kubeconfig-k3s-default.yaml kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-k3s-default-server-2 Ready master 50m v1.18.6+k3s1 k3d-k3s-default-server-1 Ready master 50m v1.18.6+k3s1 k3d-k3s-default-server-0 Ready master 50m v1.18.6+k3s1  創建完畢後透過 KUBECONFIG 這個環境變數指向該檔案，就可以利用 kubectl 指令來操作創建起來的 k3s 叢集 ","version":"Next","tagName":"h2"},{"title":"動態新增節點​","type":1,"pageTitle":"K3D","url":"/docs/techPost/2020/iThome_Challenge/cicd-6#動態新增節點","content":"如果今天想要動態新增節點，也可以透過 k3d node create 指令來操作 $ k3d node create --role server hwchiu-test $ k3d node list NAME ROLE CLUSTER STATUS k3d-hwchiu-test-0 server k3s-default running k3d-k3s-default-server-0 server k3s-default running k3d-k3s-default-server-1 server k3s-default running k3d-k3s-default-server-2 server k3s-default running k3d-k3s-default-serverlb loadbalancer k3s-default running $ KUBECONFIG=~/.k3d/kubeconfig-k3s-default.yaml kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-k3s-default-server-0 Ready master 51m v1.18.6+k3s1 k3d-k3s-default-server-2 Ready master 51m v1.18.6+k3s1 k3d-k3s-default-server-1 Ready master 51m v1.18.6+k3s1 k3d-hwchiu-test-0 Ready master 9s v1.18.6+k3s1  整個使用上的介紹就到這邊，基本上不會太困難，而且指令簡單，想要快速架起多節點的 Kubernetes，可以嘗試使用看看這套軟體 KIND 接下來我們來看另外一套也是基於 Docker 為基礎的多節點建置工具 KIND, 相對於 K3D， KIND 是完整版本的 Kubernetes，由 Kubernetes 社群維護，使用上也是非常簡單，詳細的介紹可以參閱 官方Repo 安裝 安裝過程非常簡單，也是一些 script 的行為就可以處理完畢，跟 k3d 一樣，所有的操作過程都是在本地的 binary 完成的 curl -Lo ./kind &quot;https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-$(uname)-amd64&quot; chmod a+x ./kind sudo mv ./kind /usr/local/bin/kind  $ kind kind creates and manages local Kubernetes clusters using Docker container 'nodes' Usage: kind [command] Available Commands: build Build one of [base-image, node-image] completion Output shell completion code for the specified shell (bash or zsh) create Creates one of [cluster] delete Deletes one of [cluster] export Exports one of [kubeconfig, logs] get Gets one of [clusters, nodes, kubeconfig] help Help about any command load Loads images into nodes version Prints the kind CLI version Flags: -h, --help help for kind --loglevel string DEPRECATED: see -v instead -q, --quiet silence all stderr output -v, --verbosity int32 info log verbosity --version version for kind Use &quot;kind [command] --help&quot; for more information about a command.  測試上最常用到的指令就是 create, delete 以及 load ，這兩者可以幫忙創建與刪除 kubernetes cluster, 後者則可以將一些 container image 送到 docker container 中，這樣你的 kubernetes cluster 如果要抓取 image 就可以直接從本地抓取。 ","version":"Next","tagName":"h2"},{"title":"創建 Cluster​","type":1,"pageTitle":"K3D","url":"/docs/techPost/2020/iThome_Challenge/cicd-6#創建-cluster-1","content":"接下來我們要用 kind create cluster 來創建一個基於 docker 的 Kubernetes 叢集，預設情況下只會創建出一個單一節點，如果想要創建更多節點，我們要透過 config 的方式告知 KIND 我們需要的拓墣形狀 因此事先準備好下列檔案 kind.yaml kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 nodes: - role: control-plane - role: worker - role: worker  裡面描述我們需要三個 node, 其中一個代表 control-plane, 另外兩個則是單純的 worker, 然後將該 config 傳入 KIND 一起使用 $ kind create cluster --config kind.yaml Creating cluster &quot;kind&quot; ... ✓ Ensuring node image (kindest/node:v1.17.0) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to &quot;kind-kind&quot; You can now use your cluster with: kubectl cluster-info --context kind-kind Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂  創建完畢後，直接使用 docker ps 來觀察結果 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 97d7d804ea75 kindest/node:v1.17.0 &quot;/usr/local/bin/entr…&quot; 4 minutes ago Up 4 minutes kind-worker2 9085118d47b3 kindest/node:v1.17.0 &quot;/usr/local/bin/entr…&quot; 4 minutes ago Up 4 minutes 127.0.0.1:32768-&gt;6443/tcp kind-control-plane b9eedb6d5f38 kindest/node:v1.17.0 &quot;/usr/local/bin/entr…&quot; 4 minutes ago Up 4 minutes kind-worker  可以觀察到的確有相對應數量的 docker container 被叫起來，不同於 k3d， kind 並不會幫忙準備額外的 load-balancer，所以數量就是我們指定的數量 不同於 k3d, kind 本身創建完畢後就會直接把相關的 KUBECONFIG 給寫入到 $home/.kube/config 裡面，因此使用者可以直接使用預設的位置來進行使用 $ kubectl get nodes NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 13m v1.17.0 kind-worker Ready &lt;none&gt; 12m v1.17.0 kind-worker2 Ready &lt;none&gt; 12m v1.17.0  KIND 本身並沒有辦法動態增加節點，這個是使用上的限制，不過我認為這個功能不會影響太多，畢竟作為一個本地測試的節點，有任何問題就砍掉重建就好，花費的時間也不會太長。 ","version":"Next","tagName":"h2"},{"title":"本地開發 Kubernetes 應用程式流程","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-7","content":"","keywords":"","version":"Next"},{"title":"Kubeadm​","type":1,"pageTitle":"本地開發 Kubernetes 應用程式流程","url":"/docs/techPost/2020/iThome_Challenge/cicd-7#kubeadm","content":"如果今天採用的是 Kubeadm 這個部署方式，由於 Kubadm 預設創立的是一個單節點的 Kubernetes 叢集，這種情況下只要開發者跟 Kubernetes 是同一台機器，那基本上 Docker Container Image 就可以共用。 架構如下圖所示  開發者產生的 Container Image 可以直接給同台機器上面的 Kubernetes 使用，開發者唯一要處理的就只有部署過程中 (Yaml/Helm) 所描述的 Image Name 而已。 上述的便利性是建立在開發者使用的環境與 Kubeadm 架設的環境是同個機器上，如果 Kubeadm 本身也建立多節點叢集，那這種便利性就不存在，必須要用額外的方法來處理。 ","version":"Next","tagName":"h2"},{"title":"KIND/K3D​","type":1,"pageTitle":"本地開發 Kubernetes 應用程式流程","url":"/docs/techPost/2020/iThome_Challenge/cicd-7#kindk3d","content":"如果今天採用的是 KIND/K3D 這類型基於 Docker 而部署的 Kubernetes 架構，那整個架構就會有點不同，如下圖。  當開發者建立起 Container Image 後，這些 Image 是屬於本地端，然而 KIND/K3D 的環境都是基於 Docker，這意味者如果要在 KIND/K3D 的環境中跑起開發者的 Container Image, 勢必要把這些 Contaienr Image 給複製到 Kubernetes Node 中，也就是那些 Docker，所以其實背後使用到的是 Docker in Docker 的技術，基於 Docker 所創建立的 Kubernests 裡面再根據 Docker Image 去創建 Pod(Containers)。 這部分如果使用的是 KIND 指令的話，其本身有特別提供一個功能來幫助使用者把本地端的 Image 給快速地送到 KIND 建立的叢集裡面 $ kind load Loads images into node from an archive or image on host Usage: kind load [command] Available Commands: docker-image Loads docker image from host into nodes image-archive Loads docker image from archive into nodes Flags: -h, --help help for load Global Flags: --loglevel string DEPRECATED: see -v instead -q, --quiet silence all stderr output -v, --verbosity int32 info log verbosity Use &quot;kind load [command] --help&quot; for more information about a command.  可以看到 kind 支援兩種格式的 container image, 一種是直接從當前節點已知的 comtainer image，另外一種則是從被打包壓縮過的 image 格式。 KIND 可以將這兩種格式的 container 給送到 KIND 裡面。 首先，我們先來觀察一下預設情況下， KIND 架構中的 docker 有哪些 contaienr image $ docker exec -it kind-worker crictl image IMAGE TAG IMAGE ID SIZE docker.io/kindest/kindnetd 0.5.4 2186a1a396deb 113MB docker.io/rancher/local-path-provisioner v0.0.11 9d12f9848b99f 36.5MB k8s.gcr.io/coredns 1.6.5 70f311871ae12 41.7MB k8s.gcr.io/debian-base v2.0.0 9bd6154724425 53.9MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90d 290MB k8s.gcr.io/kube-apiserver v1.17.0 134ad2332e042 144MB k8s.gcr.io/kube-controller-manager v1.17.0 7818d75a7d002 131MB k8s.gcr.io/kube-proxy v1.17.0 551eaeb500fda 132MB k8s.gcr.io/kube-scheduler v1.17.0 09a204f38b41d 112MB k8s.gcr.io/pause 3.1 da86e6ba6ca19 746kB  這邊要特別注意的是， KIND 其實並不是在 docker 內使用 dockerd 作為 Kubernetes 的 container runtime，而是採用 containerd ，因此系統上並沒有 docker 指令可以使用，取而代之的是我們要使用 crictl (container runtime interface control) 這個指令來觀察 container 的資訊。 透過 crictl image 可以觀察到預設情況下有的都是 kubernetes 會使用到的 container image 以及二個由 KIND 所安裝的 image, kindnetd(CNI) 以及 local-path-provisioner (storageclass for hostpath). 接下來假設本機上面有一個 postgres:10.8 的 container image, 我們透過 kind load 的指令將其傳送到 KIND 叢集裡面 $ kind load docker-image postgres:10.8 Image: &quot;postgres:10.8&quot; with ID &quot;sha256:83986f6d271a23ee6200ee7857d1c1c8504febdb3550ea31be2cc387e200055e&quot; not present on node &quot;kind-worker2&quot; Image: &quot;postgres:10.8&quot; with ID &quot;sha256:83986f6d271a23ee6200ee7857d1c1c8504febdb3550ea31be2cc387e200055e&quot; not present on node &quot;kind-control-plane&quot; Image: &quot;postgres:10.8&quot; with ID &quot;sha256:83986f6d271a23ee6200ee7857d1c1c8504febdb3550ea31be2cc387e200055e&quot; not present on node &quot;kind-worker&quot;  上述的指令描述說，因為系統上目前不存在，所以要開始複製，當一切就緒後再次透過 crictl 指令來觀察，就可以看到這時候 postgres:10.8 這個 container image 已經放進去了。 $ docker exec -it kind-worker crictl image IMAGE TAG IMAGE ID SIZE docker.io/kindest/kindnetd 0.5.4 2186a1a396deb 113MB docker.io/library/postgres 10.8 83986f6d271a2 237MB docker.io/rancher/local-path-provisioner v0.0.11 9d12f9848b99f 36.5MB k8s.gcr.io/coredns 1.6.5 70f311871ae12 41.7MB k8s.gcr.io/debian-base v2.0.0 9bd6154724425 53.9MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90d 290MB k8s.gcr.io/kube-apiserver v1.17.0 134ad2332e042 144MB k8s.gcr.io/kube-controller-manager v1.17.0 7818d75a7d002 131MB k8s.gcr.io/kube-proxy v1.17.0 551eaeb500fda 132MB k8s.gcr.io/kube-scheduler v1.17.0 09a204f38b41d 112MB k8s.gcr.io/pause 3.1 da86e6ba6ca19 746kB  透過上述的流程我們就可以很順利的將本地開發的 Image 給快速的載入到 KIND 建立的 Kubernetes 叢集中，又不需要將 Container Image 給傳送到遠方 Registry 花費如此冗長的傳輸時間，整個開發效率上會提升不少。 ","version":"Next","tagName":"h2"},{"title":"Skaffold 本地開發與測試","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-8","content":"","keywords":"","version":"Next"},{"title":"Detecting Source Code​","type":1,"pageTitle":"Skaffold 本地開發與測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-8#detecting-source-code","content":"如同前言所述， Skaffold 希望開發者可以專注於程式碼的開發，而後續的流程都讓其來幫忙搞定，因此其內建一個偵測系統，當目標目錄內的程式碼有所更動時，就會自動地執行相關工作流程，這樣對於使用者來說，只需要存擋，等待一點時間就可以於 Kubernetes 叢集中看到最新的程式碼 ","version":"Next","tagName":"h2"},{"title":"Bulding Artifacts​","type":1,"pageTitle":"Skaffold 本地開發與測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-8#bulding-artifacts","content":"當程式碼被偵測到更動後， Skaffold 就會開始建置相關產物，這邊支援多種類型，譬如 Dockerfile, Bazel, Jib Maven 甚至是其他自定義的腳本。除了本地的產物產生之外， Skaffold 也有跟 Google Cloud Build 有所整合，這部分我認為跟 Skaffold 是 Google 開源有很大的關係，所以目前只有 Google 家的服務有支援。 ","version":"Next","tagName":"h2"},{"title":"Test Artifacts​","type":1,"pageTitle":"Skaffold 本地開發與測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-8#test-artifacts","content":"當產物產生後，會對這個產物進行測試，這個階段能做的選擇比較少，目前是基於 Container-structure-test 這套開源軟體來進行測試，有興趣瞭解這個專案做什麼的可以點選前述連結或是到官方頁面瞭解更多 ","version":"Next","tagName":"h2"},{"title":"Tagging Artifacts​","type":1,"pageTitle":"Skaffold 本地開發與測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-8#tagging-artifacts","content":"當產物產生也測試完畢之後，接下來會對產物進行 Tag 的動作，該 Tag 會打到 Container Image 上，目前有支援四種選項，包含 Git Commit IDsSha256 HashGo Tempate with Environment Variable SupportDate &amp; Time 四者詳細的差異可以觀看官方頁面 來瞭解更多，基本上就是讓你選擇不同的 image tag ","version":"Next","tagName":"h2"},{"title":"Pushing Artifac​","type":1,"pageTitle":"Skaffold 本地開發與測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-8#pushing-artifac","content":"這個步驟就是想辦法將上述的產物給送到 Kubernetes 裡面，這部分如果 Kubernetes 是本地機器，可以忽略這個步驟直接使用，就如同前述的 Kubeadm 的環境一樣。 如果是遠方的環境的話，這邊就會根據遠方 Kubernetes Cluster 不同種類而採用的方式來處理，其判斷準則則是依據 KUBECONFIG CURRENT-CONTEXT 的名稱，就以最上面的支援環境來說 Kubernetes context\tLocal cluster type\tNotesdocker-desktop\tDocker Desktop docker-for-desktop\tDocker Desktop\tThis context name is deprecated minikube\tminikube kind-(.*)\tkind\tThis pattern is used by kind &gt;= v0.6.0 (.*)@kind\tkind\tThis pattern was used by kind &lt; v0.6.0 k3d-(.*)\tk3d\tThis pattern is used by k3d &gt;= v3.0.0 可以看到不同版本的 KIND 產生的 kubernetes context 名稱不同，但是只要有符合這兩個規則， Skaffold 都會視為是 KIND 並且用 KIND 的方式幫你推上 KIND 叢集 ","version":"Next","tagName":"h2"},{"title":"Deploying Artifacts​","type":1,"pageTitle":"Skaffold 本地開發與測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-8#deploying-artifacts","content":"最後則是將應用程式部署到 Kuberentes 裡面，這邊支援三種工具來部署，分別是 kubectlhelmkustomize 我認為這三種基本上已經涵蓋了大部分人的使用情境， Skaffold 會將檔案內的 ImageTag 換成前面步驟產生的 Tag 並且將內容推到 Kubernetes 內部去更新 想要瞭解更多關於 Skaffold 的介紹可以參閱官網 安裝 安裝指令也非常簡單，整個 Skaffold 的運作核心都在其 Binary，所以也只有一個軟體需要下載與安裝 curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 &amp;&amp; \\ sudo install skaffold /usr/local/bin/  安裝完畢後可以看到該指令有非常多的用法可以使用，接下來將會介紹本地開發時可能會使用的指令及相關用法。 $ skaffold A tool that facilitates continuous development for Kubernetes applications. Find more information at: https://skaffold.dev/docs/getting-started/ End-to-end pipelines: run Run a pipeline dev Run a pipeline in development mode debug [beta] Run a pipeline in debug mode Pipeline building blocks for CI/CD: build Build the artifacts deploy Deploy pre-built artifacts delete Delete the deployed application render [alpha] Perform all image builds, and output rendered Kubernetes manifests Getting started with a new project: init [alpha] Generate configuration for deploying an application fix Update old configuration to a newer schema version Other Commands: completion Output shell completion for the given shell (bash or zsh) config Interact with the Skaffold configuration credits Export third party notices to given path (./skaffold-credits by default) diagnose Run a diagnostic on Skaffold schema List and print json schemas used to validate skaffold.yaml configuration survey Opens a web browser to fill out the Skaffold survey version Print the version information Usage: skaffold [flags] [options] Use &quot;skaffold &lt;command&gt; --help&quot; for more information about a given command. Use &quot;skaffold options&quot; for a list of global command-line options (applies to all commands).  Demo 這邊我們直接使用官方的範例 Repo 來測試 git clone https://github.com/GoogleContainerTools/skaffold cd skaffold/examples/getting-started  此外，我的系統中目前有之前由 KIND 所建立的 Kuberentes 叢集 前述講到整個 Skaffold 的架構，裡面有些階段都會有些不同的選擇，實際上這些選擇都是依賴一個 yaml 的設定檔案來處理，該資料夾內就有一個這樣的檔案 $ cat skaffold.yaml apiVersion: skaffold/v2beta7 kind: Config build: artifacts: - image: skaffold-example deploy: kubectl: manifests: - k8s-*  這裡面設定幾個部分 產物的部分，會把 image 叫做 skafoold-example部署的部分會把所有符合 k8s-* 字眼的檔案都用 kubectl 給部署進去 預設的情況下都會使用 Dockerfile 來建置產物 $ cat Dockerfile FROM golang:1.12.9-alpine3.10 as builder COPY main.go . RUN go build -o /app main.go FROM alpine:3.10 # Define GOTRACEBACK to mark this container as using the Go language runtime # for `skaffold debug` (https://skaffold.dev/docs/workflows/debug/). ENV GOTRACEBACK=single CMD [&quot;./app&quot;] COPY --from=builder /app .  下方則是相關的 Kubernetes yaml, 非常乾淨與單純 $ cat k8s-pod.yaml apiVersion: v1 kind: Pod metadata: name: getting-started spec: containers: - name: getting-started image: skaffold-example  接下來我們可以呼叫 skaffold 這個指令來執行 一次完整的 workflow, 包含建置 image, push image 以及 deploy 到 kubernetes 裡面。 $ skaffold dev Listing files to watch... - skaffold-example Generating tags... - skaffold-example -&gt; skaffold-example:v1.14.0-7-g677d665c3 Checking cache... - skaffold-example: Not found. Building Found [kind-kind] context, using local docker daemon. Building [skaffold-example]... Sending build context to Docker daemon 3.072kB Step 1/7 : FROM golang:1.12.9-alpine3.10 as builder 1.12.9-alpine3.10: Pulling from library/golang 9d48c3bd43c5: Pull complete 7f94eaf8af20: Pull complete 9fe9984849c1: Pull complete cf0db633a67d: Pull complete 0f7136d71739: Pull complete Digest: sha256:e0660b4f1e68e0d408420acb874b396fc6dd25e7c1d03ad36e7d6d1155a4dff6 Status: Downloaded newer image for golang:1.12.9-alpine3.10 ---&gt; e0d646523991 Step 2/7 : COPY main.go . ---&gt; afab364bca27 Step 3/7 : RUN go build -o /app main.go ---&gt; Running in 7ac080c720c1 ---&gt; cbcc0f655527 Step 4/7 : FROM alpine:3.10 3.10: Pulling from library/alpine 21c83c524219: Already exists Digest: sha256:f0e9534a598e501320957059cb2a23774b4d4072e37c7b2cf7e95b241f019e35 Status: Downloaded newer image for alpine:3.10 ---&gt; be4e4bea2c2e Step 5/7 : ENV GOTRACEBACK=single ---&gt; Running in 3336c2434250 ---&gt; f7da9bb5a8f4 Step 6/7 : CMD [&quot;./app&quot;] ---&gt; Running in ad83b9fb99e8 ---&gt; c18d1a41c91d Step 7/7 : COPY --from=builder /app . ---&gt; 4dec7885d19b Successfully built 4dec7885d19b Successfully tagged skaffold-example:v1.14.0-7-g677d665c3 Tags used in deployment: - skaffold-example -&gt; skaffold-example:4dec7885d19bcf6a6fef2bc62c609390787a73be61501ad0bdaffd3b229fd9a5 Loading images into kind cluster nodes... - skaffold-example:4dec7885d19bcf6a6fef2bc62c609390787a73be61501ad0bdaffd3b229fd9a5 -&gt; Loaded Images loaded in 1.629866454s Starting deploy... - pod/getting-started created Waiting for deployments to stabilize... Deployments stabilized in 13.655262ms Press Ctrl+C to exit Watching for changes... [getting-started] Hello world! [getting-started] Hello world! [getting-started] Hello world! [getting-started] Hello world! [getting-started] Hello world! [getting-started] Hello world  上述的範例可以觀察到 Push 的規則, 偵測到使用的是 KIND，所以就呼叫 KIND 的方式把 Image 送進去 Loading images into kind cluster nodes... - skaffold-example:4dec7885d19bcf6a6fef2bc62c609390787a73be61501ad0bdaffd3b229fd9a5 -&gt; Loaded  Deploy 的部分則是用 Kubectl 的方式將 Yaml 送進去，然後自動輸出相關的 Log. ","version":"Next","tagName":"h2"},{"title":"修改程式碼​","type":1,"pageTitle":"Skaffold 本地開發與測試","url":"/docs/techPost/2020/iThome_Challenge/cicd-8#修改程式碼","content":"接下來我們開兩個視窗，一個視窗透過 skaffold dev 來偵測並處理整個流程，另一個視窗則是用來修改 main.go 接下來我們修改 main.go 改成下列內容 package main import ( &quot;fmt&quot; &quot;time&quot; ) func main() { for { fmt.Println(&quot;Hello world!-hwchiu-ithome&quot;) time.Sleep(time.Second * 1) } }  當檔案存下去之後，馬上觀察另外一個視窗，會發現很快就偵測到程式碼的更動，並且馬上將修改的內容直接送到 Kubernetes 裡面 [getting-started] Hello world!-hwchiu [getting-started] Hello world!-hwchiu [getting-started] Hello world!-hwchiu [getting-started] Hello world!-hwchiu Generating tags... - skaffold-example -&gt; skaffold-example:v1.14.0-7-g677d665c3-dirty Checking cache... - skaffold-example: Not found. Building Found [kind-kind] context, using local docker daemon. Building [skaffold-example]... Sending build context to Docker daemon 3.072kB Step 1/7 : FROM golang:1.12.9-alpine3.10 as builder ---&gt; e0d646523991 Step 2/7 : COPY main.go . ---&gt; 5a9d1bded1b1 Step 3/7 : RUN go build -o /app main.go ---&gt; Running in 0b71f1abe4e7 ---&gt; bcc350de6d46 Step 4/7 : FROM alpine:3.10 ---&gt; be4e4bea2c2e Step 5/7 : ENV GOTRACEBACK=single ---&gt; Using cache ---&gt; f7da9bb5a8f4 Step 6/7 : CMD [&quot;./app&quot;] ---&gt; Using cache ---&gt; c18d1a41c91d Step 7/7 : COPY --from=builder /app . ---&gt; a73f3a1b761b Successfully built a73f3a1b761b Successfully tagged skaffold-example:v1.14.0-7-g677d665c3-dirty Tags used in deployment: - skaffold-example -&gt; skaffold-example:a73f3a1b761b040dfab47ba89b145da88c517ec7d031c32e5d61cb5e3bf205d3 Loading images into kind cluster nodes... - skaffold-example:a73f3a1b761b040dfab47ba89b145da88c517ec7d031c32e5d61cb5e3bf205d3 -&gt; Loaded Images loaded in 1.327803742s Starting deploy... - pod/getting-started configured Waiting for deployments to stabilize... Deployments stabilized in 2.156854ms Watching for changes... [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome [getting-started] Hello world!-hwchiu-ithome  這時候用 kubectl 去觀察系統上的資源 $ kubectl get pods NAME READY STATUS RESTARTS AGE getting-started 1/1 Running 1 111s  可以發現這個 Pod 的 Restart 次數有增加，這是因為 Container Image 更新後，Pod 重啟，載入新的 Image 最後顯示出新的 log 資訊 Hello world!-hwchiu-ithome 到這邊我們就基本介紹了 Skaffold 的操作流程跟一個簡單 Demo, 如果對於這個工具有興趣的話可以嘗試玩玩看，將其整合到 Helm 或是 Kustomize 等不同部署方式，看看是否真的能夠提升自己的開發效率。 ","version":"Next","tagName":"h2"},{"title":"Pipeline System 介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2020/iThome_Challenge/cicd-9","content":"","keywords":"","version":"Next"},{"title":"部署模式​","type":1,"pageTitle":"Pipeline System 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-9#部署模式","content":"部署模式上基本上就是兩大塊，自架服務或是SaaS服務，這兩種類型我認為他們的好壞優點有 ","version":"Next","tagName":"h2"},{"title":"自架服務​","type":1,"pageTitle":"Pipeline System 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-9#自架服務","content":"優點: 彈性，擴展性佳，可以根據各種需求去修正，甚至有機會透過修改原始碼來滿足客製化需求使用上限制比較少 缺點: 要自己維護伺服器，包含了運算資源，儲存資源，網路資源甚至可能連硬體都要處理。 第三方整合不一定有，需要自己研究跟處理，甚至還要自己撰寫程式碼來完成 發現問題時不一定有太多支援可以尋求，會變成要花更多時間在處理這些問題而非賺錢的商業邏輯 ","version":"Next","tagName":"h3"},{"title":"SaaS​","type":1,"pageTitle":"Pipeline System 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-9#saas","content":"優點: 使用起來簡單，不用擔心底層基礎架設，譬如硬體資源，網路資源，儲存資源以及運算資源付費情況下會有比較好的支援服務可以尋求，發生問題時可以讓對方幫忙處理 缺點: 限制多，需要花更多的錢來獲得更好的服務與使用條件支援的平台與支援的語言完全受限於廠商，沒有辦法擴充彈性與擴充性比較低，一切都依賴廠商去開發 大部分的 SaaS 都會提供免費版本的功能讓使用者使用，但是部分功能都會有所限制，想要解除這些限制就要付費，透過付錢來取得更好的使用品質，至些功能可能有 可以有多少個並行的工作一段時間內可以跑多少時間的工作，譬如每個月只能跑 10 小時的工作每個 job 能夠支援的 Timeout 上限支援哪些平台與機器類型，譬如是否可以支援 Docker 或是 VM, 平台除了常見的 Linux 之外是否也支援 Winodws/OSX/iOS/Andorid 等平台。 此外，對於這些 Piepline 系統來說，自架跟 SaaS 並不是二擇一，很多情況下，這些系統除了提供 SaaS 的服務外，也有提供自架服務 這種情況下就可以讓使用者決定到底要使用自架服務或是 SaaS，譬如先透過 SaaS 去使用評估看看，覺得喜歡看考慮自架或是反過來 一開始先自架來用看看，如果喜歡但是覺得維護麻煩，覺得改用 SaaS 更可以省時省力。 ","version":"Next","tagName":"h3"},{"title":"特色​","type":1,"pageTitle":"Pipeline System 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-9#特色","content":"每個不同的 Piepline 系統都會有不同的特性，這些特性不一定每個環境都需要，所以選擇上還是根據自己的需求去選擇 我個人的經驗下，可能會有這些特性(包含但不侷限) 通知系統。當工作成功或是失敗的時候，能不能把這些結果通知出去，讓管理員有辦法被動知道這些工作的結果 專案的追蹤與問題管理，譬如是否該系統能夠把這些每個工作都跟一些 Issue Tracking 整合，譬如 Jira 使用者權限整合，是否可以跟已知常用的系統整合，譬如 LDAP/Windows AD/Google Suite/Crowd/OpenID 流水線的工作內容是否可以用程式碼的方式來保存，類似 Pipeline as a Code 使用工具與閱覽工具有哪些，是否有好用的 UI 或是工具可以使用 除錯與文件的完整性，使用上是否能夠找到詳細的文件來使用，發生問題時是否容易找到詢問的管道 Secret 這種機密資訊的管理與使用是否有支援，譬如 db password 等 要找到一套系統完全支持上述所有功能且都要好用穩定不會出錯其實不可能，最困難的還是去評估每個系統以及其特性，看看有哪些特性 是你們一定要有，哪些可以妥協不一定要有的 ","version":"Next","tagName":"h2"},{"title":"工具的選擇​","type":1,"pageTitle":"Pipeline System 介紹","url":"/docs/techPost/2020/iThome_Challenge/cicd-9#工具的選擇","content":"就如同最上面所述的，市面上有非常多 Piepline 系統可以選擇，每個都有各自的優點與使用，接下來的文章為了讓整體操作簡單與順利，會採取使用 SaaS 服務的 Piepline 系統，並且基於免費版本來使用 這些選擇中有 CircieCI, TravisCI 甚至是 Github 本身的 Github Action. 由於我們的專案都很習慣放在 GitHub 上，我們就來使用 GitHub Action 作為後續的操作環境! ","version":"Next","tagName":"h2"},{"title":"CNCF Secrets 使用者調查報告","type":0,"sectionRef":"#","url":"/docs/techPost/2021/cncf-tech-radar-secrets","content":"CNCF Secrets 使用者調查報告 CNCF Storage 使用者調查報告 前言 今天要分享的 CNCF Radar 是 2021/02 所公布的報告，該報告所瞄準的範圍是 Secret Management 。 就如同前篇文章所述 CNCF Continuous Delivery 使用者調查報告， CNCF 雷達主要是針對 CNCF 會員的使用經驗進行調查，根據這些經驗回饋來統計當前 CNCF 會員對於各項解決方案的推薦程度。 本篇文章翻譯自 Secret Management, February 2021，並且加上個人心得 詳細的訪談影片可以參閱 CNCF end user technology radar, February 2021 - Secrets Management Secret Management 對於要導入自動部署的團隊來說是個不可避免的挑戰，就算使用 GitOps 流程也有一樣的問題。 原文所提及的 Secret Management 不單單只是應用程式的機密資訊，也包含如何管理系統中使用的大量憑證。 Radar Technology Radar 旨在成為特定領域的一個意見參考指南，因此 CNCF End User Techonlogy Radar 就是一個針對科技領域受眾所建立的一個專案參考指南，這些專案領域都聚焦於 Clou Native 上，透過這個報告可以知道 CNCF End User Community 內這些公司他們實際上都使用哪些解決方案，對於這些方案保持什麼樣的看法 Level 為了簡單量化這些調查報告，所有的調查都會要求使用者對於是否推薦這個專案給予下列答案之一 Adopt 這個答案代表該使用者(通常是廠商)是明確的推薦這個技術，使用者已經使用這個專案一段時間，而且也被團隊內證實的確是穩定且有幫助的Trail 這個答案代表使用者有成功的使用過這些技術且推薦大家要多關注這些技術的發展Assess 這個答案代表使用者有嘗試使用過且認為他們是有未來的，推薦大家當你專案內有特別需求的時候可以去看看這些專案 基本上我的認知就是信心程度，由上到下遞減。 除了上述三個答案之外，還有一個選項就是 HOLD，顧名思義就是可以停一下，不要太執著這個專案甚至不要使用。 關於這個專案的一些運作，譬如題目跟專案的選擇，甚至一些概念的介紹都可以參閱官方網站 資料來源 這次的報告總共有來自 29 個 CNCF 會員參與，全部票數有 79 票，參與的廠商規模有大小，領域也不同，下圖節錄自官方報告 從人數規模來看，基本上每個公司都是百人規模以上，甚至一半以上都是千人等級，還有六家公司是萬人等級。 報告 下圖節錄自官方的結論報告 該報告就是根據上面的標準，讓參與的 CNCF 會員來回報對這些專案的推薦程度 這邊要注意，這邊的結果是粗略的統計結果，沒有太多明確的數學定義到底什麼樣的等級可以歸類為 ADOPT，所以觀看時就當做一個參考看看即可 上述的統計報表中，可以觀察到一些資訊 Hashicorp Vault 可以說是自架解決方案的霸主，第二名 Sealed Secrets 可能算看不到車尾燈管理憑證方面幾乎是 Cert-manager 的天下AWS 提供的兩個工具, AWS Secrets Manager 以及 AWS KMS 也都上榜，是公有雲廠商中最受青睞的。GCP Secrets Management 可以看到很多團隊使用後就保持觀望 結論 Vault has the broadest adoption across many companies and industries HashiCorp Vault 被廣泛地使用於各公司以及各領域中 Vault 過往總是被認為是一個複雜且難以駕馭維運的工具，但是對於很多小組織與團隊來說，與其外包 Secret Managment 服務或是自己設計相關服務，使用 Vault 反而是一個折衷的方式，其解決了自行設計解決方案的困難處同時又可以自行維護。 Vault 可能會這麼受歡迎的原因是因為其解決 secret management 的方式不會很綁死雲端環境，使用各種雲端服務的團隊都可以導入 Vault 到其環境中。 After Vault, groups tend to use the native solutions provided by their public cloud provider. 這個結果是顯而易見的，對於很多團隊來說，如果已經大量採用某雲端技術，通常也會考慮直接使用整合好的 secret management 服務 Radar 的資料報告中被提及的雲端解決方案有 AWS Secrets Manager, AWS Key Management Service, AWS Certificate Manager, Azure Key Vault, GCP Secrets Management. 不過最後 Azure Key Vault 以及 AWS Certificate Manager 都沒有上榜 根據 Radar 的報告顯示，基礎建設的規模程度以及使用者數量都會大大的去影響團隊如何選擇這些 secret management 的解決方案。特別是當要考慮使用多雲環境時，如果將使用特定雲端廠商的服務很容易陷入 vendor lock-in 的狀態，導致未來要搬遷轉移都很困難。 Certificate manager has become a popular choice in the Kubernetes ecosystem. Certificate manager 於 certificate 這方面的處理獲得極高的使用率 其本身的使用情境不是廣泛使用，專注於 certificate 方面的處理，同時與 Kubernetres 有極好的整合性 Other solutions in the space are fragmented across various levels of maturity and complexity. 除了上述幾個大方向外， CNCF 團隊也觀察到 Radar 中有非常多小量零星的專案，這些專案都被設計成更明確的使用方向，譬如開發框架，Git 專案中加密或是已經跟 Kubernetes 生態系中有大量重疊的使用情境。 這些工具因為沒有收到足夠數量的回應所以都沒有上榜","keywords":"","version":"Next"},{"title":"CNCF Storage 使用者調查報告","type":0,"sectionRef":"#","url":"/docs/techPost/2021/cncf-tech-radar-storage","content":"CNCF Storage 使用者調查報告 CNCF Storage 使用者調查報告 前言 今天要分享的 CNCF Radar 是 2020/11 所公布的報告，該報告所瞄準的範圍是 Database Storage。 就如同前篇文章所述 CNCF Continuous Delivery 使用者調查報告， CNCF 雷達主要是針對 CNCF 會員的使用經驗進行調查，根據這些經驗回饋來統計當前 CNCF 會員對於各項解決方案的推薦程度。 本篇文章翻譯自 Database Storage, November 2020，並且加上個人心得 詳細的訪談影片可以參閱 CNCF End User Technology Radar: Database Storage, November 2020 Radar Technology Radar 旨在成為特定領域的一個意見參考指南，因此 CNCF End User Techonlogy Radar 就是一個針對科技領域受眾所建立的一個專案參考指南，這些專案領域都聚焦於 Clou Native 上，透過這個報告可以知道 CNCF End User Community 內這些公司他們實際上都使用哪些解決方案，對於這些方案保持什麼樣的看法 Level 為了簡單量化這些調查報告，所有的調查都會要求使用者對於是否推薦這個專案給予下列答案之一 Adopt 這個答案代表該使用者(通常是廠商)是明確的推薦這個技術，使用者已經使用這個專案一段時間，而且也被團隊內證實的確是穩定且有幫助的Trail 這個答案代表使用者有成功的使用過這些技術且推薦大家要多關注這些技術的發展Assess 這個答案代表使用者有嘗試使用過且認為他們是有未來的，推薦大家當你專案內有特別需求的時候可以去看看這些專案 基本上我的認知就是信心程度，由上到下遞減。 除了上述三個答案之外，還有一個選項就是 HOLD，顧名思義就是可以停一下，不要太執著這個專案甚至不要使用。 關於這個專案的一些運作，譬如題目跟專案的選擇，甚至一些概念的介紹都可以參閱官方網站 資料來源 這次的報告總共有來自 26 個 CNCF 會員參與，全部票數有 273 票，參與的廠商規模有大小，領域也不同，下圖節錄自官方報告 從人數規模來看，基本上每個公司都是百人規模以上，甚至一半以上都是千人等級，還有六家公司是萬人等級。 報告 下圖節錄自官方的結論報告 該報告就是根據上面的標準，讓參與的 CNCF 會員來回報對這些專案的推薦程度 這邊要注意，這邊的結果是粗略的統計結果，沒有太多明確的數學定義到底什麼樣的等級可以歸類為 ADOPT，所以觀看時就當做一個參考看看即可 上述的統計報表中，可以觀察到一些資訊 Redis, Elasticsearch, PostgreSQL 這三個專案的推薦程度都是一致性的高，幾乎是有使用過的團隊都會推薦MySQL, MongoDB, Cassandra 這三個專案獲得較多的 Assess 票數，也就是有不少團隊使用後並沒有強烈推崇繼續使用，反而保持觀望的角度。Kafka, Memcached 等知名專案也都有很高的關注度及支持度AWS 底下的 DynamoDB 以及 Aurora 都有上榜，算是雲端廠商內提供這塊服務做得最好的 結論 Companies are cautious with their data and slow to adopt newer technologies. 團隊傾向謹慎且小心地去導入全新的儲存技術來處理資料，調查報告中的新專案如 CockroachDB, TiDB, Vitess 都沒有於調查報告中獲得廣大的迴響。 有很多理由使得團隊會謹慎小心的去採用新儲存技術，其中最主要的原因就是太難去管理。當團隊需要轉移大量資料，從數 TB 到 PB 這種層級時，這中間花費的成本非常巨大，因此轉移帶來的好處要是不能夠蓋掉轉移的成本的話，很難說服團隊去進行儲存專案的轉移 其他導致謹慎選擇的可能原因是很難找到一個新專案技術的專家 令人感到興趣的是知名的 etcd 竟然沒有出現在報告中。大部分 etcd 的使用都是基於 Kubernetes 的需求。看起來很少團隊會單獨使用 etcd 來管理其資料。 Choosing a managed database service depends heavily on use cases. 報告中檢視使用雲端管理的儲存技術得票數並不高，這令人感到驚訝。一種解讀方式是雲端管理的資料技術非常仰賴使用情境，譬如應用程式最終部署的位置，資料的使用量以及團隊是否本來就使用該雲端產品。 如果應用程式產生的資料非常大量，使用這些雲端管理的儲存技術可能會帶來非常巨大的花費 是否使用雲端管理的儲存設備一個很大的影響原因是該團隊是否已經使用該雲端服務。假如一個團隊所有服務都是地端自行架設，不太可能單獨將資料放到雲端去保存。 團隊如果處理的是非常敏感機密的資料的話，更有可能將資料給部署到自己的機器上而非使用雲端管理的儲存設備。 訪談中談到 AWS RDS 這個服務，其獲得的票數也非常的少所以最終被移除出報告之中。 3. Keep an open mind! Storage 領域依然處於一個開發演化的階段，可以發現到很多專案都是非常久遠且知名的大專案。這些專案藉由其穩定且運作來好的特性帶來了很高量的 ADOPT 票數 愈來愈多針對 Cloud Native 環境下所開發的儲存專案，而這些專案都有專屬的使用情境使得很難落入使用者雷達這種比較全面性的調查報告中。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/cncf-tech-rader-multicaluster","content":"","keywords":"","version":"Next"},{"title":"Level​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/cncf-tech-rader-multicaluster#level","content":"為了簡單量化這些調查報告，所有的調查都會要求使用者對於是否推薦這個專案給予下列答案之一 Adopt 這個答案代表該使用者(通常是廠商)是明確的推薦這個技術，使用者已經使用這個專案一段時間，而且也被團隊內證實的確是穩定且有幫助的Trail 這個答案代表使用者有成功的使用過這些技術且推薦大家要多關注這些技術的發展Assess 這個答案代表使用者有嘗試使用過且認為他們是有未來的，推薦大家當你專案內有特別需求的時候可以去看看這些專案 基本上我的認知就是信心程度，由上到下遞減。 除了上述三個答案之外，還有一個選項就是 HOLD，顧名思義就是可以停一下，不要太執著這個專案甚至不要使用。 關於這個專案的一些運作，譬如題目跟專案的選擇，甚至一些概念的介紹都可以參閱官方網站 Multi Cluster 這次題目為 Multicluster Management，主要想要探討 CNCF 團隊是如何管理多套 Kubernetes 叢集的。 不過 RADAR 團隊將結果分成兩類，分別是 Cluster Deployment 以及 Core Services/Add-ons，前者主要探討如何去管理與部署 Kubernetes Cluster，後者則是探討當前述的 Cluster 搭建完畢後，接下來會部署哪些核心服務來提供更上層的使用者去使用。  從上述結果來看，可以看到 Cluster Management 的問券調查中，按照人數投票排名下來 公有雲的 Kubernetes 服務(AKS,EKS...etc)客製化的部署工具Terraform私有雲的 Kubernetes 管理服務(地端管理平台) 而 kOps 以及 Cluster API 目前都是屬於有被嘗試使用，但是並沒有獲得大多數使用者強烈推薦於正式生產環境使用。 當叢集架設完畢後，有哪些相關服務與共識是團隊會使用與安裝的，按照人數投票排名下來 各種 Operators 的服務，譬如 Prometheus-Operator, Kafka-Operator使用 Helm 來部署與管理應用程式使用 Kustomize 來部署與管理應用程式使用 ArgoCD 來完成 GitOps 部署使用 Flux 來完成 GitOps 部署 Jsonnet 也有出現於投票清單中，但是並沒有獲得大部分使用者認同適合放到生產環境上。 從數據圖來看的結果如下  公有雲的 Kubernetes 服務沒什麼意外的獲得很多使用者的青睞，畢竟能夠幫忙將叢集管理與升級的煩惱用錢花掉，的確可以讓團隊省下很多麻煩，特別是這些步驟又麻煩，一旦出錯還要有能力可以復原，所以大部分團隊會希望能夠更專注於上層應用程式的部署是完全可以預料的。內部的客製化部署工具影片中就沒有探討太多到底有哪些類型，私有雲與資料中心的環境很多時候還是需要團隊自己動手去維護與撰寫相關的部署工具，此外 Terraform 的熱門程度一直居高不下，不知道之後有沒有機會看到 Pulumi 衝上來的一天。RADAR Team 有觀察到當管理叢集數量不多時，有些團隊會使用 kOps, Kubeadm 等工具進行簡單管理與維護，但是當叢集數量更多時就會改用 Kubernetes 管理服務，雖然影片中沒有特別提到，不過我認為 Rancher 應該也算一種 Kubernetes 管理服務。Operator 這個框架的流行讓愈來愈多複雜的應用程式能夠用簡單的方式去管理與部署，譬如 Prometheus，管理人員透過易懂的 CRD 內容與 YAML 格式就能夠輕鬆的設定 Prometheus，或是 Kafka 可以自己去管理 Kafka Cluster，管理者不太需要知道管理這些叢集的底層設定與方式，一切都讓 Operator 處理，結論 Operator 真的太棒了Helm/Kustomize 還是主流的應用程式管理方式，偷偷提一下新版的 Kustomize 已經支援 Helm Chart 了，可以直接用 Kustomize 去部署一個 Helm Chart 的應用程式，當然背後一定還是轉成純 Kubernetes YAML。現成 GitOps 解決方案的使用者也是愈來愈多， ArgoCD/Flux 兩位老牌解決方案還是相對出名 RADAR Team 想法 多叢集管理目前沒有一個銀色子彈可以一統江山，不同環境與需求都有各自的一片天社群目前很期待 ClusterAPI 的茁壯發展，希望能夠減少更多客製化的需求與複雜度。眾多社群工具一起結合來解決問題，特別觀察到 GitOps 最常搭配 Helm 使用，而 Operator 的解決方案也很常透過 GitOps/Helm 的方式給部署到叢集中Operator 真的很棒 ","version":"Next","tagName":"h2"},{"title":"Kubernetes - IP 重複奇遇記","type":0,"sectionRef":"#","url":"/docs/techPost/2020/kubernetes-duplicate-pod-ip","content":"","keywords":"kubernetes ip conflict","version":"Next"},{"title":"部署 Pods​","type":1,"pageTitle":"Kubernetes - IP 重複奇遇記","url":"/docs/techPost/2020/kubernetes-duplicate-pod-ip#部署-pods","content":"→ kubectl apply -f debug.yaml deployment.apps/debug-pod created  這邊我們專注觀察 kind-worker3 上面所有 Pod 的 IP 即可  → kubectl get pods -o wide | grep kind-worker3 debug-pod-7f9c756577-6nv7b 1/1 Running 0 3m38s 10.244.2.6 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-9l766 1/1 Running 0 3m38s 10.244.2.8 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-bg89r 1/1 Running 0 3m39s 10.244.2.9 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-hb2cr 1/1 Running 0 3m38s 10.244.2.7 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-kzkjr 1/1 Running 0 3m39s 10.244.2.3 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-ncqg8 1/1 Running 0 3m38s 10.244.2.5 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-pwk9v 1/1 Running 0 3m39s 10.244.2.2 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-twwpn 1/1 Running 0 3m39s 10.244.2.4 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-z7kmq 1/1 Running 0 3m39s 10.244.2.11 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-zgs7k 1/1 Running 0 3m38s 10.244.2.10 kind-worker3 &lt;none&gt; &lt;none&gt;  目前上面總共有 10 個 pod，所有 IP 都不一樣，非常正常 ","version":"Next","tagName":"h2"},{"title":"停止節點上的 kubelet 並刪除節點上特定資料夾​","type":1,"pageTitle":"Kubernetes - IP 重複奇遇記","url":"/docs/techPost/2020/kubernetes-duplicate-pod-ip#停止節點上的-kubelet-並刪除節點上特定資料夾","content":"如同前面所述，我們針對 kind-worker3 進行模擬，因此執行下列指令來停止 kubelet， ○ → sudo docker exec kind-worker3 systemctl stop kubelet ○ → sudo docker exec kind-worker3 systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) ...  確認停止後，接者刪除相關資料夾 sudo docker exec kind-worker3 rm -rf /run/cni-ipam-state  ","version":"Next","tagName":"h2"},{"title":"重啟節點上的 kubelet 並部署更多 pod 到該節點上​","type":1,"pageTitle":"Kubernetes - IP 重複奇遇記","url":"/docs/techPost/2020/kubernetes-duplicate-pod-ip#重啟節點上的-kubelet-並部署更多-pod-到該節點上","content":"上述都完成後，我們重啟 kubelet  ○ → sudo docker exec kind-worker3 systemctl start kubelet ○ → sudo docker exec kind-worker3 systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) (thawing) since Fri 2020-11-27 04:04:28 UTC; 3s ago Docs: http://kubernetes.io/docs/  接下來我們要部署新的 pod 到 kind-worker3 身上，這邊我採取的方式是透過 drain 的概念，將 kind-worker 以及 kind-worker2 上面的 pod 都移除掉，然後重新部署到 kind-worker3 上，並且觀察這些 IP ○ → kubectl drain kind-worker --ignore-daemonsets ○ → kubectl drain kind-worker2 --ignore-daemonsets  接者透過相同指令觀察 kind-worker3 上面的所有 pod IP → kubectl get pods -o wide | grep kind-worker3 | sort -k 6 debug-pod-7f9c756577-hkhx9 1/1 Running 0 54s 10.244.2.10 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-zgs7k 1/1 Running 0 11m 10.244.2.10 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-qmjqk 1/1 Running 0 54s 10.244.2.11 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-z7kmq 1/1 Running 0 11m 10.244.2.11 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-pd6hl 1/1 Running 0 54s 10.244.2.12 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-ssmsv 1/1 Running 0 54s 10.244.2.13 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-bbx6j 1/1 Running 0 54s 10.244.2.14 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-9rn4j 1/1 Running 0 54s 10.244.2.15 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-sfwhs 1/1 Running 0 54s 10.244.2.16 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-jc8nw 1/1 Running 0 54s 10.244.2.17 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-5sb28 1/1 Running 0 54s 10.244.2.18 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-mgtzx 1/1 Running 0 54s 10.244.2.19 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-64xlq 1/1 Running 0 53s 10.244.2.20 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-jpmfk 1/1 Running 0 53s 10.244.2.21 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-pwk9v 1/1 Running 0 11m 10.244.2.2 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-rhfk4 1/1 Running 0 71s 10.244.2.2 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-kzkjr 1/1 Running 0 11m 10.244.2.3 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-njmw6 1/1 Running 0 71s 10.244.2.3 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-l9fl8 1/1 Running 0 71s 10.244.2.4 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-twwpn 1/1 Running 0 11m 10.244.2.4 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-ncqg8 1/1 Running 0 11m 10.244.2.5 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-rshx5 1/1 Running 0 71s 10.244.2.5 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-6nv7b 1/1 Running 0 11m 10.244.2.6 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-vklgs 1/1 Running 0 71s 10.244.2.6 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-hb2cr 1/1 Running 0 11m 10.244.2.7 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-jkpbd 1/1 Running 0 54s 10.244.2.7 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-9l766 1/1 Running 0 11m 10.244.2.8 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-lr8t5 1/1 Running 0 54s 10.244.2.8 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-bg89r 1/1 Running 0 11m 10.244.2.9 kind-worker3 &lt;none&gt; &lt;none&gt; debug-pod-7f9c756577-wx4s6 1/1 Running 0 54s 10.244.2.9 kind-worker3 &lt;none&gt; &lt;none&gt;  重上述的結果可以觀察到 10.244.2.11 以及更以前的(.2~.10) 地址全部都重複了。 到這邊為止，我們透過一個奇怪的操作流程來製造了 IP 重複的現象，但是目前還有兩個問題 為什麼這個過程會導致 IP 重複?這個過程看起來很妙，什麼樣的實際情況會發生? 探究問題 從這邊開始，我們就認真地去探討這個問題的本質，一旦理解整個 kubernetes 運作流程後，面對這個問題就會非常有想法知道該從何下手，以及可以很快的縮小問題的發生點 問題開始前，我們要先有一個基本的概念，到底 IP 地址是怎麼來的？ 實際上 Kubernetes 本身並不會去幫忙分配這些 IP 地址，唯二會做就兩件事情 Controller 會根據參數設定 Node 上的 NodeCIDR 等數值(本文不談)Kubelet 創建 Pod 時，最後會呼叫起 CNI 來幫忙處理 Container Network Interface(CNI) 這邊不談太多，想要瞭解更多可以閱讀我之前關於 CNI 的介紹文 接下來我們用圖表看一下 CNI 的運作流程(大概流程，實際上底層的呼叫有一點點不一樣) 一開始， kubelet 會創造一個 Pod 的沙盒，這時候該 Pod 還沒有任何 IP 地址 CNI 本身會要負責給予 Pod 對應的網路能力，這邊也包含 IP 的分配，預設情況下 CNI 的設定檔案會放在 /etc/cni/net.d。 kubelet 會去讀取 /etc/cni/net.d 來判斷當前系統使用的 CNI 是誰 → docker exec kind-worker3 cat /etc/cni/net.d/10-kindnet.conflist { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;kindnet&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;ptp&quot;, &quot;ipMasq&quot;: false, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;dataDir&quot;: &quot;/run/cni-ipam-state&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;ranges&quot;: [ [ { &quot;subnet&quot;: &quot;10.244.2.0/24&quot; } ] ] } , &quot;mtu&quot;: 1500 }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ] }  上面範例是 kind 裡面的 CNI 的設定檔案，我們注意 IPAM 欄位，他使用的是 host-local 這個服務，並且將資料放到 /run/cni-ipam-state。 所以 kubelet 就會根據找到的結果，叫起 kind-net 這套 CNI 來幫忙處理，而 kind-net 找上 host-local 來幫忙處理 IP 分配問題 host-local 會使用本地檔案 /run/cni-ipam-state/kindnet 作為本地資料庫，紀錄哪些 IP 已經使用過。最後根據搜尋從裡面結果挑出一個還沒有被用過的 IP 地址，並且把該結果送回給 kind-net CNI ○ → docker exec kind-worker3 ls /run/cni-ipam-state/kindnet 10.244.2.10 10.244.2.11 10.244.2.12 10.244.2.13 10.244.2.14 10.244.2.15 10.244.2.16 10.244.2.17 10.244.2.18 10.244.2.19 10.244.2.2 10.244.2.20 10.244.2.21 10.244.2.3 10.244.2.4 10.244.2.5 10.244.2.6 10.244.2.7 10.244.2.8 10.244.2.9 last_reserved_ip.0 lock  可以看到這邊會記錄當前使用的所有 CNI 狀態，這邊要特別注意的是，不是每個 CNI/IPAM 都是走 host-local 這種方式。 host-local 如其名稱一樣，就是每個節點獨立自行處理，而背後就是依賴一個資料夾搭配眾多檔案來記住當前到底分配哪些 IP 地址 一切完畢之後，kind-net 就會把相關的 IP 給設定到 Pod 上面。 ","version":"Next","tagName":"h2"},{"title":"再看一次​","type":1,"pageTitle":"Kubernetes - IP 重複奇遇記","url":"/docs/techPost/2020/kubernetes-duplicate-pod-ip#再看一次","content":"所以回到前面的模擬環境，現在要理解整個問題流程就非常清楚明瞭了。 一開始，我們部署大量的 Pod 到 kind-worker3 上面， host-local 開始針對每個分配的 IP 去寫檔案，並且記錄到 /run/cni-ipam-state/kindnet。 接者我們停掉 kubelet，讓 CNI 暫時不會被呼叫，然後把維護 host-local 的所有狀態都移除了。 到這個階段，其實系統已經出現了狀態不一致的現象， CNI 用來維護 IP 的狀態資訊被移除，但是使用那些 IP 的容器都還活者。 最後，我們透過重新部署 Pod 到 kind-worker3 上面，這時候 host-local 會嘗試重新產生 IP 地址，由於 /run/cni-ipam-state/kindnet 裡面是空的，所以又會重新開始計算，因此分配出來的 IP 就會跟以前重複，產生 IP 衝突問題。 真實情境 看到這邊，對於整個問題的發生心理都已經有了個底，但是我們真實情況下會發生這類型的問題嗎? 要滿足這個問題有幾個條件 採用的 CNI 最後會使用 host-local 這種本地資料庫來記錄使用的 Pod IP 地址系統上 host-local 使用的資料夾被刪除，導致裡面的資料全部消失，而 host-local 根本不知道 要踩到條件一其實很簡單，預設情抗下 Flannel 就是使用這個方式來記錄的 至於條件二，如果系統檔案沒有遇到不如預期的修改的話，通常不會踩到這個點。 但是今天有一個使用情境會完全滿足上述條件，就是透過 Rancher 安裝 Kubernetes 並且使用 Flannel 作為 CNI。 然後透過 Restore 的功能來復原 Kubernetes，這種情況下就會踩到這個問題。 Rancher 再進行 Restore 的過程中，會先關掉 kubelet，並且對當節點進行一番清除，譬如  const ( ToCleanEtcdDir = &quot;/var/lib/etcd/&quot; ToCleanSSLDir = &quot;/etc/kubernetes/&quot; ToCleanCNIConf = &quot;/etc/cni/&quot; ToCleanCNIBin = &quot;/opt/cni/&quot; ToCleanCNILib = &quot;/var/lib/cni/&quot; ToCleanCalicoRun = &quot;/var/run/calico/&quot; ... ) func (h *Host) CleanUpAll(ctx context.Context, cleanerImage string, prsMap map[string]v3.PrivateRegistry, externalEtcd bool) error { log.Infof(ctx, &quot;[hosts] Cleaning up host [%s]&quot;, h.Address) toCleanPaths := []string{ path.Join(h.PrefixPath, ToCleanSSLDir), ToCleanCNIConf, ToCleanCNIBin, ToCleanCalicoRun, path.Join(h.PrefixPath, ToCleanTempCertPath), path.Join(h.PrefixPath, ToCleanCNILib), } if !externalEtcd { toCleanPaths = append(toCleanPaths, path.Join(h.PrefixPath, ToCleanEtcdDir)) } return h.CleanUp(ctx, toCleanPaths, cleanerImage, prsMap) }  該函式就會把 ToCleanCNILib 這個路徑給移除掉，而 ToCleanCNILib 指向的位置則是 /var/lib/cni/。 非常巧的是 host-local 預設的路徑也是 /var/lib/cni var defaultDataDir = &quot;/var/lib/cni/networks&quot; // Store is a simple disk-backed store that creates one file per IP // address in a given directory. The contents of the file are the container ID. type Store struct { *FileLock dataDir string }  兩個事情很巧的撞再一起之後，就會產生 IP 衝突的問題了。 解決方式 這個問題從兩個人的角度來看，其實都不覺得自己有錯，都很盡忠職守。 但是問題出在，對於 CNI/IPAM 來說，我用來維護整個 IP 狀態的檔案被移除了，可是使用那些 IP 的容器卻沒有被移除。 所以一種做法就是把該節點上的所有 Pod 都重啟一次，讓 CNI 重新跑過。 或是我們透過 Rancher 安裝 Flannel 的時候，修改設定檔案讓 IPAM(host-local) 使用不同的位置。 畢竟 Rancher (2.5以前)是基於 etcd 的概念去還原與備份整個 Kubernetes Cluster，因此這個過程將節點上的資料都清除是非常合理。 但是 etcd 的資料並不能完整呈現整個 Kubernetes Cluster，畢竟還有一些資料並不存在 etcd 內。 譬如本文章的 CNI，或是其他的 PV/PVC 等，想要做到 k8s 完全備份與還原幾乎是不太可能的，這部分除了技術問題外，我覺得更多是哲學問題，要先定義你想要的備份與還原。 ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2020/kubernetes-static-pod","content":"Preface 本文想要跟大家來分享討論一下另外一種部署 Kubernetes Pod 的方式，稱之為 Static Pod，這個部署方式最大的示範情境就是 Kubeadm 的使用。 當部署完 Kubeadm 後，透過 kubectl -n kube-system get pods，是不是會看到 kube-scheduler, kube-apiserver 以及 kube-controller-manager. 那..這些核心元件組成了 Kubernetes Control-Plane，但是本身卻又是被 Kubernetes 所管理，那到底是這中間是怎麼運作的? 這個問題就要從 Static Pod 的部署來談起 Environment 本文觀察環境基於下列版本 kubeadm: v1.17.3kubectl: v1.17.3Kubernetes: v1.17.3 如果有使用 Vagrant 的人，可以用下列的檔案建置相關環境 # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;bento/ubuntu-18.04&quot; config.vm.box_version ='201912.14.0' config.vm.hostname = 'k8s-dev' config.vm.define vm_name = 'k8s' config.vm.provision &quot;shell&quot;, privileged: false, inline: &lt;&lt;-SHELL set -e -x -u export DEBIAN_FRONTEND=noninteractive #change the source.list sudo apt-get update sudo apt-get install -y vim git cmake build-essential tcpdump tig jq socat bash-completion # Install Docker export DOCKER_VERSION=&quot;5:19.03.5~3-0~ubuntu-bionic&quot; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; sudo apt-get update sudo apt-get install -y docker-ce=${DOCKER_VERSION} sudo usermod -aG docker $USER #Disable swap #https://github.com/kubernetes/kubernetes/issues/53533 sudo swapoff -a &amp;&amp; sudo sysctl -w vm.swappiness=0 sudo sed '/vagrant--vg-swap/d' -i /etc/fstab sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee --append /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo kubeadm init --pod-network-cidr=10.244.0.0/16 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml echo 'source &lt;(kubectl completion bash)' &gt;&gt;~/.bashrc SHELL config.vm.network :private_network, ip: &quot;172.17.8.111&quot; config.vm.provider :virtualbox do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, 2] v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, 4096] v.customize ['modifyvm', :id, '--nicpromisc1', 'allow-all'] end end How Kubeadm Works 為了部署一個 Pod 到 Kubernetes 節點上，其中牽扯了多個元件，從 API Server, Scheduler, Controller 到節點上的 kubelet, Container Runtime。 然而對於 Scheduler/Controller/API Server 這三個核心元件說，到底該怎麼建制以及維護？ Native Application, 直接運行三個不同的 Binary 執行檔案 可透過 systemd 來包裝這些應用程式 透過已經包裝好的 Container Image 來執行這三個服務 以 Kubeadm 為範例，其先透過 systemd 的方式來管理 kubelet，確保 kubelet 這個 daemon 本身可以被監控，如果有問題會自動重新起動，甚至重新開機後都可以滿足叫起來提供服務。 vagrant@k8s-dev:~$ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Tue 2020-03-10 04:42:30 UTC; 1h 20min ago Docs: https://kubernetes.io/docs/home/ Main PID: 646 (kubelet) Tasks: 21 (limit: 4659) CGroup: /system.slice/kubelet.service └─646 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kub 至於上述三個元件本身都有相關的容器映像檔，其實最簡單的方式就是於節點上直接透過 Container Runtime 去運行這三個節點即可。 可以看到安裝完畢 kubeadm 的環境後，系統上都有這些相關的 container image. vagrant@k8s-dev:~$ docker images | grep k8s.gcr k8s.gcr.io/kube-proxy v1.17.3 ae853e93800d 3 weeks ago 116MB k8s.gcr.io/kube-controller-manager v1.17.3 b0f1517c1f4b 3 weeks ago 161MB k8s.gcr.io/kube-apiserver v1.17.3 90d27391b780 3 weeks ago 171MB k8s.gcr.io/kube-scheduler v1.17.3 d109c0821a2b 3 weeks ago 94.4MB k8s.gcr.io/coredns 1.6.5 70f311871ae1 4 months ago 41.6MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 4 months ago 288MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 2 years ago 742kB 為了能夠運行起整個 Kubernetes, 只要透過(以範例來說) docker start 的方式並且給予相關的參數，就能夠將整個 API-Server/Controller/Schduler 運行起來搭建一個 Kubernetes 叢集。 然而實際上你透過下列指令你卻會發現 kube-system 中卻有這三個容器的存在 vagrant@k8s-dev:~$ kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE coredns-6955765f44-gdvrd 1/1 Running 1 15d coredns-6955765f44-p4wj2 1/1 Running 1 15d etcd-k8s-dev 1/1 Running 1 15d kube-apiserver-k8s-dev 1/1 Running 1 15d kube-controller-manager-k8s-dev 1/1 Running 1 15d kube-flannel-ds-amd64-k2w8g 1/1 Running 1 15d kube-proxy-6nnrt 1/1 Running 1 15d kube-scheduler-k8s-dev 1/1 Running 1 15d 更特別的是，由下列幾種方式可以推論出基本上沒有使用更上層的管理 Deployment, ReplicaSet, DaemonSet, StatefulSet, ReplicatController Pod 的命名規則Pod 裡面的 ownerReference vagrant@k8s-dev:~$ kubectl -n kube-system get pod kube-scheduler-k8s-dev -o json | jq '.metadata.ownerReferences' [ { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;controller&quot;: true, &quot;kind&quot;: &quot;Node&quot;, &quot;name&quot;: &quot;k8s-dev&quot;, &quot;uid&quot;: &quot;b8755102-968b-41ac-a923-0e2cceacaf03&quot; } ] kubectl -n kube-system get all 對一個沒有任何更高階的 Controller 管理的 Pod 來說，你如果嘗試將這些 Pod 移除，你會發現這些 Pod 都會自己重生 vagrant@k8s-dev:~$ kubectl -n kube-system get pods -l component=kube-controller-manager NAME READY STATUS RESTARTS AGE kube-controller-manager-k8s-dev 1/1 Running 1 15d vagrant@k8s-dev:~$ kubectl -n kube-system delete pod kube-controller-manager-k8s-dev pod &quot;kube-controller-manager-k8s-dev&quot; deleted vagrant@k8s-dev:~$ kubectl -n kube-system get pods -l component=kube-controller-manager NAME READY STATUS RESTARTS AGE kube-controller-manager-k8s-dev 0/1 Pending 0 3s 但是如果你自己創立一個 pod 並且刪除，就真的完全刪除不會重啟。 vagrant@k8s-dev:~$ kubectl get pods No resources found in default namespace. vagrant@k8s-dev:~$ kubectl run --generator=run-pod/v1 nginx --image=nginx pod/nginx created vagrant@k8s-dev:~$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx 0/1 ContainerCreating 0 3s vagrant@k8s-dev:~$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 6s vagrant@k8s-dev:~$ kubectl get pods nginx -o json | jq '.metadata.ownerReferences' null vagrant@k8s-dev:~$ kubectl delete pod nginx pod &quot;nginx&quot; deleted vagrant@k8s-dev:~$ kubectl get pods No resources found in default namespace. vagrant@k8s-dev:~$ vagrant@k8s-dev:~$ vagrant@k8s-dev:~$ kubectl get pods No resources found in default namespace. vagrant@k8s-dev:~$ 這其中的奧妙就在於 ownerReferences，自行創立的 pod 是完全空的，但是 kubeadm 裡面的 API Server/Controller/Scheduler 卻是有資料，並且資料是 [ { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;controller&quot;: true, &quot;kind&quot;: &quot;Node&quot;, &quot;name&quot;: &quot;k8s-dev&quot;, &quot;uid&quot;: &quot;b8755102-968b-41ac-a923-0e2cceacaf03&quot; } ] 其實這邊已經透漏出了玄機，這些 Pod 是由 節點Node 本身去維護的，本身不依賴任何我們到的 workload 型態。 節點取代了過往的 Kubernetes Controller 去確保三個核心功能的 Pod 必須活者 而這個用法就是所謂的 Static Pod Static Pod 相對於透過 Kubernetess 控制平面來管理這些 Pod, Static Pod 有一些特性 沒有 Schedule 的概念，就是固定於該節點運行由 Kubelet 去進行監控並且管理，一旦該 Pod 結束則會重新啟動Kubelet 本身會 Mirror 該 Pod 的資訊，所以才可以透過 kubectl 等相關資訊去看到 其中如果(3)的部分可以參閱 kubelet 原始碼 // Create Mirror Pod for Static Pod if it doesn't already exist if kubetypes.IsStaticPod(pod) { podFullName := kubecontainer.GetPodFullName(pod) deleted := false if mirrorPod != nil { if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) { // The mirror pod is semantically different from the static pod. Remove // it. The mirror pod will get recreated later. klog.Infof(&quot;Trying to delete pod %s %v&quot;, podFullName, mirrorPod.ObjectMeta.UID) var err error deleted, err = kl.podManager.DeleteMirrorPod(podFullName, &amp;mirrorPod.ObjectMeta.UID) if deleted { klog.Warningf(&quot;Deleted mirror pod %q because it is outdated&quot;, format.Pod(mirrorPod)) } else if err != nil { klog.Errorf(&quot;Failed deleting mirror pod %q: %v&quot;, format.Pod(mirrorPod), err) } } } if mirrorPod == nil || deleted { node, err := kl.GetNode() if err != nil || node.DeletionTimestamp != nil { klog.V(4).Infof(&quot;No need to create a mirror pod, since node %q has been removed from the cluster&quot;, kl.nodeName) } else { klog.V(4).Infof(&quot;Creating a mirror pod for static pod %q&quot;, format.Pod(pod)) if err := kl.podManager.CreateMirrorPod(pod); err != nil { klog.Errorf(&quot;Failed creating a mirror pod for %q: %v&quot;, format.Pod(pod), err) } } } } 對於 Kubelet 來說，其本身有一個設定叫做 staticPodPath, 這是一個資料夾，只要放到該資料夾下的檔案都會被 kubelet 用來創立 static pod. 至於 Kubelet 創造 Pod 的方式其實還是遵循 Kubernetes 的走法，並非直接使用 docker start(舉例) 來創立。這部分是為了讓所有的 Container 操作全部都經由 Container Runtime Interface 來管理，進而提供更好的相容性。 Hands-on 接下來我們就動手觀察一下相關的參數，首先根據剛剛 systemd 的提示，我們知道用來控管 kubelet 的啟動檔案於 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf vagrant@k8s-dev:~$ sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot; Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot; # This is a file that &quot;kubeadm init&quot; and &quot;kubeadm join&quot; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 裡面要注意的是 KUBELET_CONFIG_ARGS 這個變數，裡面使用了 --config=/var/lib/kubelet/config.yaml 來指名 kubelet 的參數檔案 因此接下來看一下其內容 vagrant@k8s-dev:~$ sudo cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s clusterDNS: - 10.96.0.10 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s kind: KubeletConfiguration nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s rotateCertificates: true runtimeRequestTimeout: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s 其中吸引我們注意的是 staticPodPath 這個參數，接下來看一下該資料夾的位置 vagrant@k8s-dev:~$ sudo ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml 可以看到裡面放了四個 yaml 檔案(包含 etcd)，如果打開這些 yaml 就會是我們所熟悉的 Kubernetes 格式了。 如果這時候隨便放入一個 yaml 到該資料夾中，會發生什麼事情? cat &lt;&lt;EOF | sudo tee /etc/kubernetes/manifests/static-debug.yaml apiVersion: v1 kind: Pod metadata: name: static-debug spec: containers: - name: hwchiu image: hwchiu/netutils EOF vagrant@k8s-dev:~$ kubectl get pods NAME READY STATUS RESTARTS AGE static-debug-k8s-dev 1/1 Running 0 27s vagrant@k8s-dev:~$ kubectl delete pod static-debug-k8s-dev pod &quot;static-debug-k8s-dev&quot; deleted vagrant@k8s-dev:~$ kubectl get pods NAME READY STATUS RESTARTS AGE static-debug-k8s-dev 0/1 Pending 0 1s vagrant@k8s-dev:~$ kubectl get pods NAME READY STATUS RESTARTS AGE static-debug-k8s-dev 1/1 Running 0 3s 這邊可以看到馬上就會產生對應的 Pod 並且也獲得了自動重啓的能力。 最後！ 我們透過 docker ps 觀察一下 vagrant@k8s-dev:~$ docker ps | grep kube-controller 5f16cb76460f b0f1517c1f4b &quot;kube-controller-man…&quot; 2 hours ago Up 2 hours k8s_kube-controller-manager_kube-controller-manager-k8s-dev_kube-system_25245994bd78f09602b6f5c3e5d2246c_1 a94f104316af k8s.gcr.io/pause:3.1 &quot;/pause&quot; 2 hours ago Up 2 hours k8s_POD_kube-controller-manager-k8s-dev_kube-system_25245994bd78f09602b6f5c3e5d2246c_1vagrant@k8s-dev:~$ docker ps | grep kube-controller 5f16cb76460f b0f1517c1f4b &quot;kube-controller-man…&quot; 2 hours ago Up 2 hours k8s_kube-controller-manager_kube-controller-manager-k8s-dev_kube-system_25245994bd78f09602b6f5c3e5d2246c_1 a94f104316af k8s.gcr.io/pause:3.1 &quot;/pause&quot; 2 hours ago Up 2 hours k8s_POD_kube-controller-manager-k8s-dev_kube-system_25245994bd78f09602b6f5c3e5d2246c_1 可以看到針對 kube-controller-manager 這個 Pod 來說，其實背後也是有 Pause Container 作為整個 Pod 的沙盒，這也證明了這些 Static Pod 的創建也是基於 CRI 的標準所創立的，並非是直接透過 docker command 來創立。 CRI 內的基本單位都是 Pod, 而非 Container, 有興趣的可以參考他們的 gRPC 介面 另外，其實 kubeadm 的安裝過程就已經透露出相關資訊 [control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot; [control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot; [control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot; W0310 06:57:46.902862 2798 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot; [control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot; W0310 06:57:46.903651 2798 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot; [etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifes ts&quot;. This can take up to 4m0s Summary 這次我們探討了關於 Static Pod 的概念，雖然實際部署上比較少這樣使用，但是透過這次的探討我們可以更了解 Kubeadm 是如何安裝 Kubernetes 環境， 並且也學習到了一些 Kubernetes 本身的特性。 課程分享 最後，我目前於 Hiskio 上面有開設一門 Kubernetes 入門篇的課程，裡面會探討運算/網路/儲存三個最重要的平台資源，此外對於 CRI/CNI/CSI 也都有簡單的介紹，主要會基於 Kubernetes 本身的設計原理及各資源的用法與情境去介紹。 如果本身已經很熟練的使用 Kubernetes 於環境中就不太適合這門課程，主要是給想要踏入到 Kubernetes 世界中的朋友，有興趣的幫忙捧場或推廣 線上課程詳細資訊: https://course.hwchiu.com/ Reference https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/https://github.com/kubernetes/kubernetes/blob/v1.17.3/pkg/kubelet/kubelet.go#L1637-L1638","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day12","content":"前言 前幾篇文章探討了如何透過 Rancher 操作與管理 Kubernetes 叢集，不論是直接抓取 Kubeconfig 或是使用網頁上的 web terminal 來操作，此外也探討了 Rancher 整合的應用程式，特別是最重要的 Monitoring 該如何使用。 有了上述的概念後，使用者已經可以順利的操作 Kubernetes 來部署各種服務。 不過 Rancher 想做的事情可沒有這麼簡單， Rancher 希望能夠強化 Kubernetes 讓其更佳適合給多位使用者共同使用了。 對於這種多租戶的概念， Kubernetes 提供了 namespace 的機制來達到資源隔離，不過 namespace 普遍上被認為是個輕量級的隔離技術，畢竟 namespace 主要是邏輯層面的隔離，底層的運算與網路資源基本上還是共用的。 就算是 RKE 也沒有辦法完全顛覆 namespace 讓其變成真正的隔離技術，畢竟 CPU/Memory/Network 等相關資源因為 Container 的關係本來就很難切割，要達到如 VM 般真正隔離還不是這麼容易。 不過 Rancher 還是有別的方向可以去發展與強化，就是如何讓 Kubernetes 變得更適合一個團隊使用，如果該團隊內有數個不同的專案，這些專案要如何共同的使用一套 Kubernetes 叢集同時又可以有一個清楚且清晰的管理方式。 Project Rancher 提出了一個名為 project 的概念，Project 是基於 Kubernetes Namespace 的實作的抽象管理概念，每個 Project 可以包含多個 namespace，同時 project 也會與 Rancher 內部的使用者權限機制整合。 從架構層面來看 Rancher 管理了多套 Kubernetes 叢集Kubernetes 叢集管理多個 ProjectProject 擁有多個 namespace. 如同前面探討的, Kubernetes 原生提供的 namespace 機制是個輕量級虛擬化概念，所有 kubernetes 內的機制也都是以 namespace 為基礎去設計的，這意味者如果你今天要透過 RBAC 設定權限等操作你都需要針對 namespace 去仔細設計。但是 Rancher 認為一個產品專案可能不會只使用一個 namespace，而是會使用多個 namespace 來區隔不同的應用程式。 這種情況下你就必須要要針對每個 namespace 一個一個的去重複設定，從結果來說一樣可以達到效果，但是操作起來就是不停重複相同的動作。 透過 Rancher Project 的整合，叢集管理者可以達到 整合使用者群組權限，一口氣讓特定群組/使用者的人針對多個 namespace 去設定 RBAC針對 Project 為單位去進行資源控管，一口氣設定多個 namespace 內 CPU/Memory 的使用量套用 Pod Security Policy 到多個 namespace 中 因此實際上管理 Kubernetes 叢集就變成有兩種方式 完全忽略 Rancher 提供的 Project 功能，直接就如同其他 Kubernetes 版本一樣去操作使用 Rancher 所設計的 Project 來管理 Rancher 會開發 Project 勢必有其好處，但是要不要使用就是另外一回事情，因為這個技術與概念是只有 Rancher 才有的，如果今天團隊同時擁有多套不同的 Kubernetes 叢集，有些用 Rancher 管理，有些沒有。 這種情況下也許不要使用 Rancher 工具而是採用盡可能原生統一的工具來管理會更好，因為可以避免團隊中使用的工具有太多的客製化行為，造成開發與維護都不容易。相反的使用所有 Kubernetes 發行版本都有的工具與管理方式反而有機會降低工具的複雜性。 所以到底要不要使用這類型的工具反而是見仁見智，請依據每個團隊需求去思考。 接下來就來看一下到底如何使用 Rancher Project 概念。 操作 Project 因為是用來簡化同時操作多個 namespace 的一種概念，因此管理上會跟 namespace 放在一起。 Rancher 畫面上方的 Projects/Namespaces 就是用來管理這類型概念的，點選進去會看到類似下圖的版面。 因為 Project 包含多個 namespace，所以版面中都是以 Project 為主，列出該 Project 底下有哪些 namespace， Rancher 內的任何 Kubernetes 叢集預設都會有兩個 Projects，System 與 Default System 內會放置任何跟 Rancher 以及 Kubernetes 有關的 namespace，譬如 cattle-system, fleet-system, kube-system, kube-public 等 Default 是預設的 Project，預設對應到 default 這個 namespace。 任何不是透過 Rancher 創立的 namespace 都不會加入到任何已知的 project 底下，因此圖片中最上方可以看到一堆 namespace，而這些 namespace 都不屬於任何一個 project。 因此要使用 project 的話就需要把這些 namespace 給搬移到對應的 project 底下。 圖中右上方有一個按鈕可以創立新的 Project，點下去可以看到如下畫面 創立一個 Project 有四個資訊需要輸入，分別是 使用者權限Project 資源控管Container 資源控管Labels/Annotation. 使用者權限可以控制屬於什麼樣的使用者/群組可以對這個 Project 有什麼樣的操作。 Project 與 Container 的資源控管之後會有一篇來介紹 創立完 Project 之後就可以回到最外層的介面，將已經存在的 namespace 給掛到 project 底下 譬如上述範例就將 cis-operator-system, longhorn-system 這兩個 namespace 給分配到剛剛創立的 project 底下。 之後重新進入到該 Project 去編輯，嘗試將 QA 使用群組加入到該 Project 底下，讓其變成 Project Owner，代表擁有完整權限。 創造完畢後，就可以透過 UI 切換到不同的 project，如上圖所示，可以看到 ithome-dev 叢集底下有三個 Project，其中有兩個是預設的，一個是剛剛前述創立的。 切換到該 Project 之後，觀察當前的 URL 可以觀察到兩個有趣的ID，c-xxxx/p-xxxxx 會分別對應到 clusterID 以及 project ID，因此之後只要看到任何 ID 是 c-xxx 開頭的，基本上都是 Rancher 所創立的，跟 Cluster 有關，而 p-xxxx 開頭的則是跟 Project 有關，每個 Project 都勢必屬於某個 Cluster。 有了 ProjectID 之後，仿造之前透過 kubectl 去觀察使用者權限的方式，這次繼續觀察前述加入的 QA 群組會有什麼變化。 從上述指令中可以看到 QA 群組對應到一個新的 ClusterRole，叫做 p-p6xrd-namespaces-edit，其中 p-p6xrd 就是對應到前述創立的 project，而 edit 代表則是擁有 owner 般的權限，能夠去編輯任何資源。 接者更詳細的去看一下該 ClusterRole 的內容 可以看到該 ClusterRole 針對設定的兩個 namespace 都給予了 &quot;*&quot; 的動詞權限，基本上就是讓該使用者能夠如管理者般去使用這兩個 namespace。 除了上述的權限外，當切換到 Project 的頁面時，就可以看到從 Rancher 中去看到該 Project 底下 namespaces 內的相關 Kubernetes 物件資源，譬如下圖 Workloads 就是最基本的運算單元，譬如 Pod, Deployment, Job, DaemonSet..等 而 Config(ConfigMap), Secrets 可以看到整個叢集內的相關資源，此外 secret 透過網頁的可以直接看到透過 base64 解密後的結果。 註: Pipelines 請忽略他， v2.5 之後 Rancher 會主推使用 GitOps 的方式來部署，因此過往 pipeline 的方式這邊就不介紹了。 以 workloads 為範例，點進去後可以更詳細的去看當前系統中有哪些 workloads。 每個 workloads 旁邊都有一個選項可以打開，打開後會看到如上的選擇，這邊就有很多功能可以使用，譬如 Add a Sidecar，可以幫忙加入一個 sidecar contaienr 進去Rollback 到之前版本Redeploy 重新部署取得該 Pod 的 shell (如果該 workloads 底下有多種 pod，則不建議這邊使用這個功能) 基本上這些功能都可以透過 kubectl 來達到，網頁只是把 kubectl 要用的指令給簡化，讓他更輕鬆操作。 上述的範例 test 是使用 deployment 去部署的，點進去該 deployment 可以看到更詳細 pods 的資訊，如下 該畫面中就可以看到每個 Pod 的資訊，包含 Pod 的名稱，部署到哪個節點，同時也可以透過 UI 去執行該 Pod 的 shell 或是觀看相關 log。 以上就介紹了關於 Project 的基本概念。 Project 是 Rancher 內的最基本單位，因此要透過 Rancher 的 UI 去管理叢集內的各種部署資源則必須要先準備好相關的 Project，並且設定好每個 Project 對應的 namespace 以及使用者權限。 當然 Rancher Project 不是一個一定要使用的功能，因為也是有團隊單純只是依賴 Rancher 去部署 Kubernetes 叢集，而繼續使用本來的方式來管理與部署 Kubernetes 叢集，畢竟現在有很多種不同的專案可以提供 Kubernetes 內的資源狀況。一切都還是以團隊需求為主。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day13","content":"前言 前篇文章探討為什麼需要 Project 這樣的概念，透過 Project 能夠帶來什麼樣的好處，然而前篇文章只有帶到簡單的操作以及如何使用透過 Rancher 的 UI 來檢視 Project 內的各種 Kubernetes 物件。 本篇文章將介紹我認為 Project 最好也最方便的功能， Resource Quotas 與 Container Default Resource Limit 到底是什麼以及如何使用。 資源控管介紹 熟悉 Kubernetes 的讀者應該都知道資源控管是一個非常困難的問題，其根本原因是 Container 本身的實作方式導致資源控管不太容器。 很多人使用資源控管最常遇到的問題有 不知道該怎麼設定 Resources Limit， CPU/Memory 到底要用哪種? 三種內建的 QoS 型態有哪些? 有哪些影響?設定好了 Limit/Request 後結果運作不如預期，或是某些情況下應用程式效能大幅度降低等 第一點是最容易遇到的，畢竟要如何有效地去分配容器使用的 CPU/Memory 是個很困難的問題，特別是第一次踏入到容器化的團隊對於這個問題會有更大的疑惑，不確定該怎麼用。 第二個問題則是部分的 Linux Kernel 版本實作 Container 的資源控管與限制上會有一些 bug，可能會導致你的應用程式被不預期的 throttle，導致效能變得很低。 本篇文章不太探討這兩個問題，反而是探討最基本的概念，畢竟上述兩個概念跟 Rancher 沒太大關係，反而是比較進階使用與除錯的內容。 Kubernetes 中針對 CPU/Memory 等系統資源有兩種限制，稱為 Request 與 Limit。 Request 代表的是要求多少，而 Limit 代表的是最多可以使用多少。 這些資源是以 Container 為基本單位，而 Pod 本身是由多個 Container 組成的，所以 CPU/Memory 的計算上就相對繁瑣。 Kubernetes 本身有一個特別的物件稱為 ResourceQuota，透過該物件可以針對特定 namespace 去限定該 namespace 內所有 Container 的資源上限。譬如可以設定 default namespace 最多只能用 10顆 vCPU，超過的話就沒有辦法繼續部署。 Rancher 的 Project 本身就是一個管理多 namespace 的抽象概念，接下來看一下 Project 中有哪些關於 Resource 的管理。 操作 為了方便操作，先將 default namespace 給加入到之前創立的 Project 中，加進去後當前 project 中有三個 namespace，如下圖。 接者編輯該 Project 去設定 Resource 相關的資訊，如下圖 Project 中有兩種概念要設定，第一種是 Resource Quota，第二個是 Container Default Resource Limit. Resource Quota 是更高階層的概念，是用來控管整個 Project 能夠使用的 CPU/Memory 用量。 由於 Project 是由多個 namespaces 所組成的，所以設定上還要去設定每個 namespace 的用量，如上述範例就是設定 整個 Project 可以使用 100個 vCPU，而每個 namespace 最多可以使用 10 vCPU。 但是因為 namespace 本身就是使用 kubernetes ResourceQuota 來實作，而這個功能本身會有一個限制就是。 一但該 namespace 本身設定了 ResourceQuota，則所有部署到該 namespace 的容器都必須要明確的寫出 CPU/Memory 用量。 這個概念也滿容易理解的，畢竟你要去計算 namespace 的使用上限，那 namespace 內的每個 container 都需要有 CPU/Memory 等相關設定，否則不能計算。 如果你的容器沒有去設定的話，你的服務會沒有辦法部署，會卡到 Scheduler 那個層級，連 Pending 都不會有。 但是如果要求每個容器部署的時候都要設定 CPU/Memory 其實會有點煩人，為了讓這個操作更簡單，Project 底下還有 Container Default Resource Limit 的設定。 該設定只要打開，所有部署到該 namespace 內的 Container 都會自動的補上這些設定。 如上圖的概念就是，每個 Container 部署時就會被補上 CPU(Request): 3顆, CPU(Limit): 6顆 這邊有一個東西要特別注意，Project 設定的 Container Default Resource Limit 本身有一個使用限制，如果 namespace 是再設定 Resource Quota 前就已經加入到 Project 的話，設定的數字並不會自動地套用到所有的 namespace 上。 反過來說，設定好這些資訊後，所有新創立的 namespace 都會自動沿用這些設定，但是設定前的 namespace 需要手動設定。 所以這時候必須要回到 namespace 上去重新設定，如下圖 namespace 的編輯頁面就可以重新設定該 namespace 上的資訊，特別是 Container Default Resource Limit。 當這邊重新設定完畢後，就可以到系統中去看相關的物件 首先 Project 設定好 Resource Quota 後，Kubernetes 就會針對每個 namespace 都產生一個對應的 Quota 來設定，如下 因為設定每個 namespace 的 CPU 上限是 10顆，而該 project 總共有三個 namespace，所以系統中這三個 namespace 都產生了對應的 quota，而這些 quota 的設定都是 10顆 CPU。 其中 default namespace 的標示是 5025m/10 代表目前已經用了 5.025顆 CPU，而系統上限是 10顆。 這時候將 default namespace 內的 pod 都清空，接者重新再看一次該 quota 物件就會發現 used 的數值從 5025m 到 0。 由於上述 default namespace 中設定 CPU 預設補上 0.1顆 CPU (Request/Limit)，所以 Kubernetes 會創造相關的物件 Limits 從上述物件可以觀察到該 LimitRange 設定了 100m 的 CPU。 最後嘗試部署一個簡單的 deployment 來測試此功能看看，使用一個完全沒有標示任何 Resource 的 deployment，內容如下。 該物件部署到叢集後，透過 kubectl describe 去查看一下這些 Pod 的狀態，可以看到其 Resource 被自動的補上 Limits/Requests: 100m。 Resource 的管理一直以來都不容易， Rancher 透過 Project 的管理方式讓團隊可以更容易的去管理多 namespace 之間的資源用量，同時也可以透過這個機制要求所有要部署的 container 都要去設定資源用量來確保不會有容器使用過多的資源。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day14","content":"前言 過去十多篇文章從從三個不同角度來探討如何使用 Rancher，包含系統管理員，叢集管理員到專案管理員，不同層級專注的角度不同，因此使用上也有不同的功能可以用。 本篇文章將探討 Rancher 一些其他的注意事項。 清除節點 之前文章探討過三種不同安裝 Kubernetes 的方式，其中一種方式是運行 docker command 在現有的節點上將該節點加入到 RKE 叢集中。 但是如果今天有需求想將該節點從 RKE 中移除該怎麼辦? Cluster Manager 中可以直接到 Cluster 頁面將該節點從 RKE 中移除，但是要注意的是，這邊的移除代表的只是將該節點從 RKE 移除，該節點上可能會有一些因為加入 RKE 而產生的檔案依然存在節點上。 假設今天有需求又要將該節點重新加入回到 RKE 中的話，如果上次移除時沒有妥善地去刪除那些檔案的話，第二次運行 docker command 去加入 RKE 叢集有非常大的機率會失敗，因為節點中有太多之前的產物存在。 官網有特別撰寫一篇文章探討如果要清除這些產物的話，有哪些資源要處理，詳細版本可以參閱 Removing Kubernetes Components from Nodes 這邊列舉一下一個清除節點正確步驟 從 Rancher UI 移除該節點重啟該節點，確保所有放到暫存資料夾的檔案都會消失Docker 相關資料Mount 相關資訊都要 umount移除資料夾移除多的網卡移除多的 iptables 規則再次重開機 第三點移除 docker 相關資料，官方列出三個指令，分別移除 container, volume 以及 image。 docker rm -f $(docker ps -qa) docker rmi -f $(docker images -q) docker volume rm $(docker volume ls -q) 如果該節點接下來又要重新加入到 Rancher 中，建議不需要執行 docker rmi 的步驟，之前的 image 可以重新使用不需要重新抓取，這樣可以省一些時間。 第四點的 mount 部分要注意的是，官文文件沒有特別使用 sudo 的指令於範例中，代表其假設你會使用 root 身份執行，因此如果不是使用 root 的話記得要在 umount 指令中補上 sudo for mount in $(mount | grep tmpfs | grep '/var/lib/kubelet' | awk '{ print $3 }') /var/lib/kubelet /var/lib/rancher; do sudo umount $mount; done 第五點跟第四點一樣，但是第五點非常重要，因為系統上有太多的資料夾都含有過往 RKE 叢集的資料，所以第五步一定要確保需要執行才可以將資料清除。 sudo rm -rf /etc/ceph \\ /etc/cni \\ /etc/kubernetes \\ /opt/cni \\ /opt/rke \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/etcd \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/lib/rancher/rke/log \\ /var/log/containers \\ /var/log/kube-audit \\ /var/log/pods \\ /var/run/calico 第六跟第七這兩步驟並不一定要處理，因為這些資訊都是節點加入到 Kubernetes 後被動態創建的，基本上重開機就不會有這些資訊，只要確保節點重新開機後沒有繼續成為 Kubernetes 的節點，那相關的虛擬網卡跟 iptables 規則也就不會被產生。 要注意的是官方文件中的所有步驟不一定都會有東西可以刪除，主要會取決於叢集內的設定，不同的設定可能會有不同的結果，譬如採用不同的 CNI，其產生的 iptables 規則與虛擬網卡就會有所不同。 離線安裝 雖然雲端環境方便存取，但是很多產業與環境可能會需要於一個沒有對外網路的環境下去安裝 Kubernetes 叢集，這種情況下如果想要使用 Rancher 的話就要探討如何達到離線安裝。 Rancher 講到離線安裝有兩種含義，一種是 Rancher 本身的離線安裝Rancher 以離線安裝的方式幫忙創建 RKE 叢集 上述兩種方式其實都還是仰賴各式各樣的 container image 來處理，所以處理的方法一致，就是要安裝一個 container registry 並且將會需要使用的 container image 都事先匯入到該 container registry 中，接者安裝時要讓系統知道去哪下載相關的 container image 即可。 官網有數篇文章探討這種類型下的安裝該怎麼處理，有興趣的也可以參考 Air Gapped Helm CLI Install 由於安裝 Rancher 本身有很多方式，譬如多節點的 RKE 或是單節的 Docker 安裝，以下簡述一下如何用 Docker 達成單節點的離線安裝。 架設一個 Private Container Registry透過 Rancher 準備好的腳本去下載並打包 Rancher 會用到的所有 Container Image把第二步驟產生 Container Image 檔案給匯入到 Private Container Registry運行修改過後的 Docker 來安裝 Rancher. 第一點這邊有幾點要注意 a. 可以使用 container registry v2 或是使用 harbor b. 一定要幫該 container registry 準備好一個憑證，這樣使用上會比較方便，不用太多地方要去處理 invalid certificate 的用法。憑證的部分可以自簽 CA 或是由一個已知信任 CA 簽署的。 c. image 的容量大概需要 28 GB 左右，因此準備環境時要注意空間 第二跟第三點直接參閱官網的方式，先到 GitHub 的 Release Page 找到目標版本，接者下載下列三個檔案 rancher-images.txtrancher-save.images.shrancher-load-images.sh 第二個腳本會負責去下載 rancher-images.txt 中描述的檔案並且打包成一個 tar 檔案，系統中會同時存放 container image 以及 tar 檔，所以最好確保空間有 60GB 以上的足夠空間。 第三個腳本會將該 tar 檔案的內容上傳到目標 container registry。 這一切都準備完畢後，就可以執行 docker 指令，可以參閱Docker Install Commands 一個範例如下 docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -v /mysite/fullchain.pem:/etc/rancher/ssl/cert.pem \\ -v /mysite/previkey.pem&gt;:/etc/rancher/ssl/key.pem \\ -e CATTLE_SYSTEM_DEFAULT_REGISTRY=test.hwchiu.com \\ # Set a default private registry to be used in Rancher -e CATTLE_SYSTEM_CATALOG=bundled \\ # Use the packaged Rancher system charts --privileged registry.hwchiu.com/rancher/rancher:v2.5.9 \\ --no-cacerts 請特別注意上述的參數，不同的憑證方式會傳入的資訊不同，自簽的方式還要額外把 CA_CERTS 給丟進去。","keywords":"","version":"Next"},{"title":"Kubernetes","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day1","content":"Kubernetes 近年來 Kubernetes 的聲勢水漲船高，愈來愈多的產業與團隊都在思考是否要引入 Kubernetes 來取代舊有的部署平台。 導入 Kubernetes 自然而來就會產生兩個需要討論的問題 團隊目前的環境需要使用 Kubernetes 嗎?如果需要使用 Kubernetes，該使用哪一套 Kubernetes ? 第一個問題看似簡單，其實非常困難，每個團隊原先的部署流程都是獨一無二且完全不同的，因此導入 Kubernetes 到底能夠帶來什麼樣的改變？ 這類型的改變可能有 團隊是否已經透過容器部署應用程式，如果沒有，那想要直接導入 Kubernetes 會是一個非常痛苦的過程，畢竟 Kubernetes 跟單純 Docker Container 的使用方式有非常大的差異，部署及管理的方式也困難很多團隊目前是採用公有雲的環境來部署還是自行維護機房? 三大公有雲基本上都有提供非常多的功能來提供使用者去部署應用程式，有些團隊單純依賴這些服務而不使用 Kubernetes，結果來看整體的運作流程也非常順暢團隊人員是否有足夠的技術與知識來使用 Kubernetes？ 如果沒有則導入 Kubernetes 也會是一個很大的過渡期，訓練既有人員或是招聘新員工對公司來說都會有成本增加的考量。既有工作流程再導入 Kubernetes 之後是否會變輕鬆? 這部分可以再細分多個小節 a. 團隊的服務是否已經有針對不同流量的水平擴展計畫? b. 網路流量要如何有效地處理，是否有 Load-Balancer 之類的可以自動地將流量導向後方服務? c. 開發跟維運人員要如何更新版本測試 所以作為團隊的領導者，切記不要跟風，千萬不要因為 kubernetes 是潮流就貿然採用 Kubernetes，我認為一個可以參考的做法是 讓團隊中一個熟悉既有運作模式跟架構的員工去學習 Kubernetes找出團隊目前維運上的痛點比較 Kubernetes 如何解決維運上的痛點，這些痛點帶來的改善是否直得期待如果評估後認為轉換有價值，從小服務開始導入來測試，不要一口氣直接轉換也要注意混合過程中，部分服務是k8s，部分服務是原先架構的情況下會不會有什麼問題出現 假設今天決定想要導入 Kubernetes，則接下來談談如何管理與架設團隊的 Kubernetes 叢集 如何管理 Kubernetes Kubernetes 本身是個開源專案，既然是個開源專案就意味使用者是有機會直接使用其開源版本的內容來架設屬於自己的 Kubernetes 叢集。 但是 Kubernetes 本身架構不算簡單，部署雖然容易但是長期的維護與除錯這部分需要仰賴對於 Kubernetes 的理解與經驗，特別是要與眾多服務進行整合時，整個難度又更高。 因此自然而然也會衍生出系統整合商的生意模式，提供一個更好使用且有技術支援的 Kubernetes 平台。 除此之外，公有雲本身也都有基於 Kubernetes 提供託管 Kubernetes 叢集的服務，譬如 Azure(AKS), GCP(GKE), AWS(EKS) 等，這類型的服務也都是要額外收費的。 我認為 Kubernetes 平台如何部署與架設，可以用下列的方式去分類 如果團隊打算使用地端(on-premises)環境 a. 直接於 bare-metal 的機器上使用開源專案來架設所有環境，譬如 K8s，必要時還可以先架設 VM b. 找尋相關的系統整合商，請對方提供整體的解決方案，從機器到 k8s 叢集等如果團隊打算使用雲端環境 a. 直接使用雲端的 Kubernetes 服務 b. 使用雲端的 VM 作為主體，上面使用開源專案幫忙架設 Kubernetes 並管理 c. 找尋相關的系統整合商，請對方提供整體的解決方案，包含使用哪套雲端環境，如何架設 k8s 叢集等 不考慮人力的情況下，基本上1(a),2(b)的價格會是相對低的，畢竟你付錢取得機器，後續的架設與管理都要自行處理，其餘三個選項都會有額外的金錢成本來購買相對應的服務。 除了成本外還需要考慮到所謂的技術支援。技術支援本身也是需要錢的，團隊如果本身沒有辦法培養熟悉 Kubernetes 的人才，也許用錢買服務是相對簡單的方式。 這也是採用開源專案的一個痛點，畢竟純開源專案的情況下，遇到問題都需要仰賴團隊的工程師自行想辦法解決。 談了這麼多種變化，本次系列文沒有辦法針對所有可能都去探討與分析，處而代之的則是從開源的角度出發，去探討如果想要自行管理與維護整個 Kubernetes 叢集的話會有什麼選擇。 接下來將使用開源專案 Rancher 作為管理多套 Kubernetes 叢集的平台，Rancher 能夠針對上述的 1(a), 2(a,b) 等三個類別去處理，提供了一個友善且強大的管理介面，讓團隊可以輕鬆的去架設與管理多套 Kubernetes 叢集。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day10","content":"前言 前述文章透過不同的安裝方式讓 Rancher 管理三套不同的 Kubernetes 叢集，其中有兩套叢集是基於 RKE 的版本，而另外一個則是 Azure AKS。 這三個叢集除了安裝的方式不同外，實際上因為底層的限制與安裝方式的不同，網頁操作上會有一些功能有些許不同，而本章節就會開始來探討要如何使用 Rancher 的介面來管理這些 Kubernetes 叢集。 叢集存取 對於大部分的 Kubernetes 工程師來說， kubectl 是個必須要會的使用工具，而 dashboard 更多時候是作為一個輔助的工具，提供更友善的視覺化方式來有效的提供資訊。 Rancher 本身提供非常友善的 Dashboard，可以讓非工程人員也可以快速地瀏覽與理解當前 Kubernetes 叢集的狀態，舉例來說隨便點進一個之前創立的 Kubernetes 叢集，會看到如下的畫面。 畫面中有幾個點可以注意 Rancher 內部有兩種瀏覽介面，分別是 Cluster Manager 以及 Cluster Explorer，預設情況下都是使用 Cluster Manager 來瀏覽Rancher v2.5 之後將慢慢的轉往 Cluster Explorer 去使用，所以可以觀察到畫面上都有提示，告知使用者可以嘗試使用看看 Cluster Explorer 來管理與瀏覽 Kubernetes 叢集畫面中間大大的顯示了三個關於 Kubernetes 資源的資訊，CPU, Memory 以及 Pod 的數量。Kubernetes 預設情況下每個節點最多只能部署110個 Pod，所以畫面中顯示的是 18/330，代表說目前已經有 18 個Pod 部署了。而 CPU/Memory 代表的則是有多少系統資源已經被預先保留，這部分是透過 Pod 裏面的 Resource.Request 來處理的最下面還有四個健康狀態，代表整個叢集中的 Etcd, Control Plane(Controller,Scheduler) 以及節點之間的健康狀態最下面 Events 展開則是可以看到 Kubernetes 內的相關 Event 上述的 Portal 簡單地呈現了當前 Kubernetes 叢集是否健康，特別是當叢集有任何問題時，下方的四個狀態都會變成紅色醒目的提醒使用者叢集有問題。 有了基本介面後，接下來把注意力移動到右上方兩個選項，分別是 Launch kubectl 以及 Kubeconfig File。 點選 Kubeconfig File，則會看到類似下列的畫面，該畫面中呈現的就是完整的 Kubeconfig file 內容。 這意味你可以把該檔案抓到你的電腦，直接於本地端使用 Kubectl 指令去存取目標叢集，示範中使用的是給 DEV 人員操作的叢集，所以該 Kubectl 本身對應的使用者其實也就是我當前用來登入 Rancher 系統的使用者。 如果熟悉 Kubeconfig 格式的讀者會觀察到, Rancher 本身會針對 Clusters 這個欄位填入多個組合，這些組合分兩大類，分別是 叢集中的 API Server 位置Rancher Server 本身 假如今天你有辦法直接存取到目標節點，譬如節點本身有 Public IP 且也有開啟 6443 Port，那就可以使用這個方式直接存取該 Kubernetes 叢集。 但是如果該節點今天是一個封閉的環境，沒有任何 Public IP 可以直接存取，那可以採取第二種方式，把任何 API 請求都打向 Rancher 服務，如圖中的 &quot;https://rancher.hwchiu.com/k8s/clusters/c-z8j6q&quot; 這個位置，然後 Rancher 就會幫忙把請求給轉發到目標叢集內，可以想像成是一個 Proxy By Pass 的概念。 補充一下: 因為目標 Kubernetes 叢集內都會安裝 Rancher 相關的服務，這些服務都會主動的跟 Rancher 進行連線，所以 Rancher 才有辦法把這些 API 請求給轉發到這些不能被外界主動存取的 Kubernetes 叢集。 以下是個範例，將上述檔案存成一個名為 ithome 的檔案，接者執行 kubectl 的時候可以透過 --kubeconfig 的參數來指定當前 kubectl 要使用哪個檔案 上述指令就呈現了當前 DEV 叢集中的相關 Pod 資訊，其中可以看到 Flannel CNI 符合之前 RKE Template 的選擇RKE 叢集有滿多相關的服務cattle-system 有所謂的 cattle-node-agent，這些角色就是會負責跟 Rancher 溝通。 基本上只有擁有了 KUBECONFIG 的檔案，管理者就有辦法透過 kubectl,helm 等指令直接管理該叢集。 如果系統上剛好沒有安裝這些指令，但是又想要使用 kubectl 來操作怎麼辦？ Rancher 也想到了這一塊，所以叢集畫面右上方的 Launch Kubectl 按鈕給他點下去， 該功能會開啟一個 web-based 的終端機，裡面提供了 kubectl 的指令，同時 kubeconfig 也都設定完畢了。 所以可以直接於該環境中使用 kubectl 去操作叢集，範例如下 基本上掌握這兩個功能的用法，就等於掌握了直接操作當前 Kubernetes 叢集的能力，習慣使用 kubectl 的使用者也可以開始透過 kubectl 來管理與部署該 Rancher 上的各種應用，當然 Rancher 本身也有自己的架構能夠讓使用者去部署應用程式，好壞沒有絕對，都要進行評估與比較。 Kubectl 與 Kubecfongi File 旁邊有一個按鈕，該按鈕點下去後可以看到一些關於 Kubernetes 叢集的選項，而不同的搭建的叢集顯示的功能都不同，譬如 如果節點是透過 AKS 搭建的，可以看到選項非常少，只有編輯與刪除是常見會使用的功能，編輯頁面中可以針對叢集名稱，叢集的使用權限，甚至針對 K8S 叢集的選項進行調整。不過由於該叢集是由 AKS 維護的，所以修改的內容也都是跟 AKS 有關。 第二個看到的是透過 Docker 指令於現存節點上安裝的 RKE 叢集，這種狀況下可以選擇的操作非常多，譬如 Rotate Certificates，該功能主要是針對 Kubernetes 內各元件溝通用的憑證，譬如 API Server, Controller..等Snapshot 主要會針對 etcd 進行備份與還原，該備份並沒有辦法針對使用者部署的應用程式去處理備份跟還原，之後可以細談一下這塊Registration Cmd，由於該叢集是透過讓節點運行 Docker 指令將其加入到 RKE 叢集中，因此如果今天有新的節點要使用時，就可以直接點選該指令取得相關的 docker 指令，介面中也可以重新選擇身份與相關的標籤/Taint等。Run CIS Scan，這個功能會慢慢被淘汰，v2.5 後 Cluster Explorer 內關於 App 的管理方式有更好的處理方式，建議使用那邊的 CIS 處理。 最後一個則是透過 API 請求 Azure 創造 VM 的 RKE 叢集，基本上差異就只是沒有 Docker 指令可以處理。 從上述三個叢集的觀察到可以發現， Rancher 很多功能都跟 RKE 叢集有關，所以如果今天是讓 Rancher 管理並非是由 Rancher 創造的叢集，功能上都會有所限制，並不能完全發揮 Rancher 的功能。 看完叢集相關的狀態後，切換到節點頁面，節點頁面也會因為不同安裝方式會有不同的呈現方式 下圖是基於 AKS 所創造的叢集，該叢集顯示了三個節點，這些節點因為 AKS 的關係被打上了非常多的標籤。 如果該叢集是透過 API 要求 Azure 動態新增 VM 所創造的叢集，則該頁面是完全不同的類型 上述畫面中有幾個點可以注意 每個 Node Pool 都是獨立顯示，可以看到該 Node Pool 下目前有多少節點，每個節點的 IP 等資訊每個 Node Pool 右方都有 +- 兩個按鈕，可以讓你動態的調整節點數量由於這些節點都是動態創立的，因此如果今天有需求想要透過 SSH 去存取這些節點的話，實際上可以到每個節點旁邊的選項去下載該節點的 SSH Keys，這個功能是只有這種創造方式的叢集才擁有的。其他創造方式的叢集節點沒有辦法讓你下載相關的 SSH Key。 上述畫面除了 Cluster, Nodes 外還有其他選項，Member 頁面可以重新設定到底該叢集的擁有者與會員有誰，譬如最初 DEV 叢集只有 DEV 群組的使用者可以操作，目前嘗試將 QA 群組的使用者加入進去，並且設定權限為 Cluster Member，設定完後的畫面如下。 這種情況下，如果使用 QA 使用者登入，就可以看到這個 DEV 叢集，接者使用該 QA 使用者嘗試去存取該 DEV 叢集並且獲取該 Kubeconfig 就可以順利的使用了。 如果熟悉 Kubernetes RBAC 的讀者，可以嘗試挖掘一下到底 Rancher 是如何把設定的這些權限給對應到 Kubernetes 內的權限。下圖是一個範例。 下圖是 QA 使用者存取 DEV 叢集用的 Kubeconfig，可以看到 User 部分使用的 Token 進行驗證，該 Token 中有一個資訊代表的是該 User 的 ID，u-dc5fezjbyi 擁有該資訊後，可以到該 Kubernetes 叢集內去找尋 cattle-system 底下 ClusterUserAttribute 這個物件，看看是否有符合這個名稱的物件，找到後可以看到該物件描述了這使用者本身有一個 Group 的屬性。 該 Group 很明顯跟 Azure 有關，其值為 azuread_group://ec55ce9e-dbd4-427c-905c-d8063b19f150. 這個 Group 就會被用到 ClusterRoleBinding 中的 Subject 因此透過 kubectl 搭配 jq 的一些語法去找，看看有沒有哪些 ClusterRoleBinding 裏面是對應到這個 Group 群組，可以發現系統中有四個物件符合這個情況，而這四個物件對應到的 Role 分別是 read-only-promoted, cluster-member, p-**-namespaces-readonly，後面那個包含 &quot;p-**&quot; 字串的物件會跟之後探討的 Project 概念有關。 接者有興趣的可以再繼續看這些不同的 Roles 實際上被賦予什麼樣的權限與操作。 除了 Member 可以操作外， Cluster 還有一個 Tools 的清單可以玩 裡面有很多第三方整合工具可以安裝，但是如果是 v2.5 的使用者，非常建議直接忽略這個頁面，因為這邊都是舊版安裝與設 定行為，v2.5 後這些整合工具除了 Catalog 外基本上都已經搬移到 Cluster Explorer 頁面去安裝與操作。 因此接下來就嘗試進入到 Cluster Explorer 來看看這個 Rancher 想要推廣的新操作介面。 Cluster Explore 的介面跟 Cluster Manager 是截然不同的，這邊列出幾個重點 右上方可以選擇當前是觀看哪個叢集，可以快速切換，同時提供一個按鈕返回 Cluster Manager.中間簡單呈現當前 Kubernetes 叢集的資訊，版本，提供者與節點資訊跟之前一樣呈現系統的資源使用量，不過 CPU/Memory 本身同時提供當前使用量已經當前被預約使用量，這兩個數字可以更好的去幫助管理員去設計當前相關資源的 request/limit 要多少左上方是重要的功能選單，不少功能都可以點選該處來處理左下方呈現 Kubernetes 中的各項資源，每個資源都可以點進去觀看，譬如 ClusterRoleBinding 就會更友善的呈現每個物件對應到的到底是 User ， Group 還是 Service Account。 點選左上方 Cluster Explorer 後切換到 Apps &amp; Marketplace，可以看到類似下方的畫面 該畫面中呈現了可以讓使用者輕鬆安裝的各類應用程式，這些應用程式分成兩大類，由 Rancher 自行維護整合的或是由合作夥伴提供的。 如果安裝的是由 Rancher 整合的 Application，那安裝完畢左上方都會出現一個針對該 App 專屬的介面，譬如我們可以嘗試安裝 CIS Benchmark。 安裝過程中，畫面下方會彈出一個類似終端機的視窗告知使用者安裝過程，待一切安裝完畢後可以透過畫面中間的 &quot;X&quot; 來關閉這個視窗。 接者重新點選左上方的清單就會看到這時候有 CIS Benchmark 這個應用程式可以使用，該應用程式可以用來幫助管理去掃描 Kubernetes 叢集內是否有一些安全性的疑慮，該專案背後是依靠 kube-bench 來完成的，基本上 Rancher 有提供不同的 Profile 可以使用，所以對於安全性有需求的管理員可以安裝這個應用程式並且定期掃描。 一個掃描的示範如上，該圖片中顯示了使用的是 rke-profile-permissions-1.6 這個 profile，然後跑出來的結果有 62 個通過， 24 個警告， 36 測試不需要跑。 如果拿 RKE 的 profile 去跑 AKS 的叢集就會得到失敗，因為 RKE 的 profile 是針對 RKE 的環境去設計的，因此可能會有一些功能跟 AKS 的設計不同，會失敗也是可以預料的。 Rancher 本身提供的 Application 非常多，下篇文章就來仔細看看其中最好用的 Monitoring 套件到底能夠提供什麼功能，使用者安裝可以如何使用這個套件來完成 Promethues + Grafana 的基本功能。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day11","content":"前言 前篇文章探討了 Rancher 叢集的基本使用方式，包含透過取得 KUBECONFIG 以及使用 Rancher 提供的網頁來觀察與操作 Rancher 叢集。 除了基本叢集的狀態顯示外， Rancher 也有整合一些常用的應用程式到 Rancher 管理的叢集中，而其中有一個功能基本上是所有 Kubernetes 叢集都需要的功能，也就是 Monitoring。 Monitoring 可以使用的專案非常多，有免費開源也有付費服務，免費開源最知名的組合技大概就屬 Prometheus + Grafana 兩套功能。 Rancher 所提供的 Monitoring 功能就是基於 Prometheus + Grafana 來使用的，本篇文章就來介紹一下這個功能到底該怎麼使用。 Rancher Rancher 從 v2.5 開始正式推行 Monitoring v2 的架構，並且淘汰過往的 Monitoring v1 的版本。 從使用者的角度來看，從 Cluster Manager 頁面中透過 Monitoring 安裝的都會是 Monitoring v1 的架構，而透過 Cluster Explorer 中 App &amp; Marketplace 安裝的 Monitoring 則會是 Monitoring v2 的架構。 探討 Monitoring v1與v2 的差異前，先來瞭解一下 Rancher 希望提供什麼樣的 Monitoring 功能給使用者。 從使用者角度出發來看，大部分人都會希望可以有下列的功能 安裝 Prometheus 到叢集中，能夠有機會去聽取所有資訊安裝 Grafana 到叢集中，同時該 Grafana 能夠跟 Prometheus 直接整合，能夠透過 Grafana 去打造出一個適合團隊用的監控面板能夠使用 Alert 的相關功能，不論是由 Prometheus 或是 Grafana 提供的。對於使用者所部署的應用程式也能夠整合到 Prometheus 中，一旦能夠整合到 Prometheus，就有辦法透過 Grafana 去處理。對於 Prometheus 與 Grafana，能夠提供一個有效簡單的方式去存取這兩個服務的網頁 上述五點的前兩點比較偏向安裝 Prometheus/Grafana 的問題，第三與第四點相對麻煩，畢竟使用者如果已經習慣 Prometheus/Grafana 本來的設定與玩法，這時候要是 Rancher 本身的介面弄得太複雜，可能會讓使用者要重新學習如何修改，這點對使用者體驗來說是一個很大的考量。 最後一點也是最麻煩的一點，因為 Rancher 所管理的 Kubernetes 叢集不一定都有對外 IP 可以被直接存取，同時每個環境也不一定有合法的 SSL 憑證可以讓使用者以 HTTPS 的方式去存取這些網頁。 但是對於使用者來說，如果安裝完畢後還要去擔心煩惱這些 IP, Domain Name, SSL 等相關問題，這樣的使用者體驗就不會太好，為了解決這個問題， Rancher 特別針對這一塊進行了所謂的 Proxy 存取。 由於 Rancher 有辦法跟管理的 Kubernetes 叢集溝通，而通常 Rancher 本身安裝時都有準備好 HTTPS 與相關的 domain。所以使用方式就會變成，使用者存取 Rancher 本身， Rancher 作為一個 Proxy 幫忙\b轉發所有跟 Prometheus/Grafana 網頁有關的存取，讓使用者可以更為輕鬆地去存取封閉式網路的 Monitoring 相關頁面。 這部分之後的實驗就可以更加清楚理解到底是什麼意思。 V1/V2 上述提到的五個概念中，v1 的版本會於 Rancher 中安裝相關的 Controller，如果使用者想要自己的應用程式可以被 Prometheus \b去抓資料的話，就要於自己的部署 YAML 中去撰寫是先定義好的 Annotation，Controller 判別到有這個 Annotation 後就會將自動產生一個關於 Prometheus 的物件來提供此功能。 v1 的這種設計對於不熟悉 Prometheus 的使用者來說很便利，可以輕鬆地處理，但是其提供的變數過少，沒有辦法太有效的客製化，因此如果是熟悉 Prometheus 的使用者則會覺得綁手綁腳，沒有辦法發揮全部功能。 再來其實 Monitoring v1 底層也是基於 Prometheus Operator 這套框架去實作的，Rancher 基於這個框架再去實作一個 Controller 幫助使用者轉換各種規則，這層規則對於已經習慣使用 Prometheus Operator 的使用者來說也是綁手綁腳，因為本來就很習慣直接操作 Prometheus Operator 的物件去操作。 因此 Monitoring v2 的最大進展就是， Rancher 將讓 Prometheus Operator 盡可能地浮出來，減少 Rancher 的抽象層。使用者有任何的客製化需求都直接使用 Prometheus Operator 的方式去管理，譬如可以直接創造如 ServiceMonitor, PrometheusRule 等物件來管理叢集中的 Prometheus。 接下來我們就直接使用 DEV 叢集作為示範，如何安裝 Monitoring v2，並且最後使用 https://github.com/bashofmann/rancher-2.5-monitoring 這個專案內的介紹來嘗試部署應用程式以及相關的 Prometheus/Grafana 資訊到叢集中。 環境 前述提到，要安裝 Monitoring v2 要切換到 Cluster Explorer 中的 App &amp; Marketplace 去安裝，切換到該頁面找到相關的 App 就點選安裝。 結果示範的叢集顯示下列警告，告知叢集內可被預訂的 CPU 數量低於需求，該 App 需要 4.5 顆 CPU而系統內不夠 由於 DEV 叢集是透過 Azure 動態創建 VM 而搭建出來的叢集，所以切換到 Cluster Manager 去修改節點數量，將 worker 節點從一個增加到三個，如下 這邊等待數分鐘，讓 Rancher 去處理 VM 的創建並且將這兩個節點安裝到 Rancher 中。一切準備後就緒後就可以回到 Cluster Explorer 去安裝 Monitoring v2 整合功能。 安裝完畢後，可以從左上方的清單中找到 Monitoring 的頁面，點擊進去會看到類似下面的畫面。 該畫面中呈現了五個不同的功能，熟悉 Prometheus Operator 功能的讀者一定對這些名稱不陌生，隨便點選一個 Grafana 試試看。 點選 Grafana 後會得到一個新的頁面，效果如下。 該畫面呈現的是一個 Grafana 的資訊面板，值得注意的是其 URL 的組成。 前面是由 Rancher Server 本身的位置，後面緊接者該 DEV Cluster 於 Rancher 中的 ID，最後就是對應服務的 namespace 與 service。 透過這種方式使用者就可以繼續使用 Rancher Server 的 HTTPS 與名稱來順利的存取不同叢集上的 Prometheus/Grafana 服務。 而這個服務實際上並不是全部都由 Rancher 所完成的，而是 Kubernetes API Server 本身就有提供這樣的功能，詳細的可以參閱官方的教學文件Access Services Running on Clusters 預設情況下，該 Grafana 內會已經創造好非常多的 dashboard，譬如下圖所示 除了 Grafana 之外， Prometheus 的相關網頁也都有，譬如點選 Prometheus Targets 就可以看到如下的畫面 此外當系統安裝了 Monitoring 的整合功能後， Cluster Explorer 的首頁也會自動地被加上相關監控資訊，如下所示 可以直接於首頁觀察到基本資訊的過往狀態，這邊提供的是非常基礎的效能指標，如果想要看到詳細的指標甚至是客製化，都還是要到 Grafana 的頁面去存取。 實驗 透過上述的介紹，基本上已經有一個簡單的 Prometheus + Grafana 的 Monitoring 功能到目標叢集中，接下來要示範如何透過 https://github.com/bashofmann/rancher-2.5-monitoring 這個開源專案來幫我們自己的應用程式加上 Prometheus 與 Grafana 的設定，最重要的是這些設定都是由 YAML 去組成的，意味者這些設定都可以透過 Git 保存與控管，可以避免任何線上修改會因為重啟而消失。 該專案的介面頁面有提供非常清楚的使用流程，這邊針對這些流程重新介紹 安裝一個示範的 Redis 應用程式幫該 Redis 應用程式安裝一個 sidecar 服務，該服務有實作 Prometheus 介面，可以讓 Prometheus 來抓不同的指標安裝 ServiceMonitor 到叢集中，讓 Prometheus Operator 知道要怎麼去跟(2)安裝的服務去要 Redis 的資料安裝一個事先準備好的 Grfana json 描述檔案，能夠讓 Grafana 自動地去產生一個針對 Redis 的監控面板 第一點非常簡單，就是基本的 Kubernetes 服務，這邊就不探討這個 Redis 應用程式到底如何組成，其安裝指令也非常簡單 $ kubectl -n default apply -f scrape-custom-service/01-demo-shop.yaml 安裝完畢後可以透過下列指令打開 port-forward 並且於瀏覽器打開 http://localhost:8000 $ kubectl port-forward svc/frontend 8000:80 可以看到畫面基本上就代表這個示範用的應用程式已經順利安裝完成。 接下來幫 Redis 安裝一個 sidecar 的服務來提供 Prometheus 的介面 $ kubectl -n default apply -f scrape-custom-service/02-redis-prometheus-exporter.yaml 最後則是最重要的兩點，這兩點是最主要跟 Prometheus/Grafana 溝通用的物件。 ╰─$ kubectl -n default apply -f scrape-custom-service/03-redis-servicemonitor.yaml ╰─$ cat scrape-custom-service/03-redis-servicemonitor.yaml 130 ↵ apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: redis-cart spec: endpoints: - interval: 30s scrapeTimeout: 20s path: &quot;/metrics&quot; targetPort: metrics namespaceSelector: matchNames: - default selector: matchLabels: app: redis-cart ServiceMonitor 是 Prometheus Operator 中定義的物件，透過這個方式就可以讓 Prometheus 幫忙產生對應的物件與自定義的應用程式溝通，接者就可以到 Prometheus 的網頁中找到這個新增資訊，如下圖。 有了上述資源後，我們就可以透過 Prometheus 去問到 Redis 的相關資訊，為了讓這些資訊更方便處理，接下來部署 Grafana 的相關物件 ╰─$ kubectl apply -f scrape-custom-service/04-redis-grafana-dashboard.yaml ╰─$ cat scrape-custom-service/04-redis-grafana-dashboard.yaml apiVersion: v1 kind: ConfigMap metadata: name: grafana-redis-cart namespace: cattle-dashboards labels: grafana_dashboard: &quot;1&quot; data: redis.json: | { &quot;__inputs&quot;: [ ], &quot;__requires&quot;: [ { &quot;type&quot;: &quot;grafana&quot;, &quot;id&quot;: &quot;grafana&quot;, &quot;name&quot;: &quot;Grafana&quot;, &quot;version&quot;: &quot;3.1.1&quot; 熟悉 Grafana 的讀者都知道，每個 Grafana 的 dashboard 都可以透過 JSON 物件來描述，所以要新增一個 Grafana dashboard 就是準備一個相對應的 json 物件，使用 configMap 來描述，將該物件給部署到 cattle-dashboards 的 namespace 內即可。 ╰─$ kc -n cattle-dashboards get cm NAME DATA AGE grafana-redis-cart 1 51m kube-root-ca.crt 1 144m rancher-default-dashboards-cluster 2 144m rancher-default-dashboards-home 1 144m rancher-default-dashboards-k8s 4 144m rancher-default-dashboards-nodes 2 144m rancher-default-dashboards-pods 2 144m rancher-default-dashboards-workloads 2 144m rancher-monitoring-apiserver 1 144m rancher-monitoring-cluster-total 1 144m rancher-monitoring-controller-manager 1 144m rancher-monitoring-etcd 1 144m rancher-monitoring-ingress-nginx 2 144m rancher-monitoring-k8s-coredns 1 144m rancher-monitoring-k8s-resources-cluster 1 144m rancher-monitoring-k8s-resources-namespace 1 144m rancher-monitoring-k8s-resources-node 1 144m rancher-monitoring-k8s-resources-pod 1 144m rancher-monitoring-k8s-resources-workload 1 144m rancher-monitoring-k8s-resources-workloads-namespace 1 144m rancher-monitoring-kubelet 1 144m rancher-monitoring-namespace-by-pod 1 144m rancher-monitoring-namespace-by-workload 1 144m rancher-monitoring-node-cluster-rsrc-use 1 144m rancher-monitoring-node-rsrc-use 1 144m rancher-monitoring-nodes 1 144m rancher-monitoring-persistentvolumesusage 1 144m rancher-monitoring-pod-total 1 144m rancher-monitoring-prometheus 1 144m rancher-monitoring-proxy 1 144m rancher-monitoring-scheduler 1 144m rancher-monitoring-statefulset 1 144m rancher-monitoring-workload-total 1 144m 事實上也可觀察到該 namespace 內有滿滿的 configmap，而每個 configmap 內的內容都會對應到一個專屬的 Grafana Dashboard，因此如果想要客製化 Grafana 的資訊，常見的做法都是透過 UI 創造，創造完畢後複製 JSON 的格式，並且將該格式用 ConfigMap 給包裝起來。 上述物件創建完畢後，就可以到 Grafana 的介面去重新整理，順利的話可以看到一個名為 &quot;Redis Dashboard for Prometheus Redis Exporter 1.x&quot; 的 dashboard，如果沒有看到的話就等待一點時間即可。 最後預設情況下， Grafana 都是基於匿名的唯獨模式去存取的，想要擁有編輯權利的話可以嘗試使用預設的帳號密碼 admin/prom-operator 去登入這個系統來編輯，編輯後記得將 JSON 物件給匯出保存，透過這樣的機制就可以方便的管理 Grafana。 本章簡單探討了一下關於 Rancher Monitoring v2 的用法，如果有需求的人甚至可以不需要到 Cluster Explorer 去安裝，而是可以直接使用 Helm 的方式去安裝相關物件，主要的物件內容是由 rancher-monitoring 這個 Helm Charts 去安裝的，有興趣嘗試可以參考這個官方檔案 rancher/charts","keywords":"","version":"Next"},{"title":"概念探討","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day17","content":"概念探討 Rancher 作為一個 Kubernetes 管理平台，提供不同的方式將 Kubernetes 叢集給匯入到 Rancher 管理平台中，不論是已經創立的 Kubernetes 或是先透過 Rancher 創造 RKE 接者匯入到 Rancher 中。 但是 Kubernetes 終究只是一個容器管理平台，前述介紹的各種機制或是 Rancher 整合的功能都是輔助 Kubernetes 的維護，對於團隊最重要的還是產品本身，產品可能是由數個應用程式所組合而成，而每個應用程式可能對應到 Kubernetes 內又是多種不同的物件，譬如 Deployment, Service, StorageClass 等。 接下來會使用應用程式這個詞來代表多個 Kubernetes 內的資源集合。 過往探討到部署應用程式到 Kubernetes 叢集內基本上會分成兩個方向來探討 如何定義與管理應用程式供團隊使用部署應用程式給到 Kubernetes 叢集的流程 定義與管理應用程式 Kubernetes 的物件基本上可以透過兩種格式來表達，分別是 JSON 與 YAML，不過目前主流還是以 YAML 為主。 這意味這一個最簡單管理應用程式的方式就是使用一堆 YAML 檔案，檔案內則是各種 Kubernetes 的物件。 這些應用程式本身還需要考慮到下列使用情境 該應用程式會不會需要跨團隊使用該應用程式是否需要針對不同環境有不同的參數該應用程式本身有沒有其他相依性，譬如部署 A 應用程式會需要先部署 B 應用程式...等 上述的這些使用情境是真實存在的，而為了解決這些問題，大部分情況下都不會使用純 YAML 檔案來管理應用程式，譬如想要讓一個 Service 針對不同環境有不同設定就不太好處理，除了準備多個幾乎一樣的檔案外幾乎沒有辦法。 目前主流的管理方式有 Helm, Kustomize，其餘的還有 ksonnet 等。 不同解決方案都採用不同的形式來管理與部署應用程式，舉例來說 使用 Helm 的使用者可以採用下列不同方式來安裝應用程式 helm installhelm template | kubectl apply - 而使用 kustomize 的使用者則可以使用 kustomize ...kubectl -k ... 因為 kubectl 目前已經內建 kustomize 的功能，所以直接使用 kustomize 指定或是 kubectl 都可以。 當團隊選擇好如何管理與部署這些應用程式後，下一個問題就是如何部署這些 Helm/Kustomize 物件到 Kubernetes 叢集。 部署流程 基本上所有的部署都以自動化為目標去探討，當然這並不代表手動部署就沒有其價值，畢竟在自動化部署有足夠的信心前，團隊也必定會經歷過各式各樣的手動部署，甚至很多自動化的撰寫與開發也是都仰賴手動部署的經驗。 從 Rancher 的角度來看，自動化部署有三種不同的方式 KubeconfigRancher CatalogRancher Fleet 下面稍微探討一下這三者的概念與差異。 Kubeconfig 一個操作 Kubernetes 最簡單的概念就是直接使用 kubectl/helm 等指令進行控制，而 Rancher 也有針對每個帳戶提供可存取 Kubernetes 叢集所要使用的 KUBECONFIG。 假設團隊已經完成 CI/CD 的相關流程，就可以於該流程中透過該 KUBECONFIG 來得到存取該 Kubernetes 的權限，接者使用 Helm/Kubectl 等功能來部署應用程式到叢集中。 基本上使用這種方式沒有什麼大問題，畢竟 RKE 也是一個 Kubernetes 叢集，所以如果團隊已經有現存的解決方案是透過這種類型部署的話，繼續使用這種方式沒有任何問題。 Rancher Catalog Rancher 本身有一個名為 catalog 的機制用來管理要部署到 Rancher 內的應用程式，這些應用程式必須要基於 Helm 來管理。 其底層背後也是將 Helm 與 Helm values 轉換為 YAML 檔案然後送到 Kubernetes 中。 這種作法跟第一種最大的差異就是，所有的安裝與管理中間都多了一層 Rancher Catalog 的管理。 CI/CD 流程要存取時就不是針對 Kubernetes 叢集去使用，也不需要取得 KUBECONFIG。 相反的需要取得 Rancher API Token，讓你 CI/CD 內的腳本有能力去呼叫 Rancher，要求 Rancher 去幫忙創建，管理，刪除不同的 Catalog。 這種方式只限定於 Rancher 管理的叢集，所以如果團隊中不是每個叢集都用 Rancher 管理，那這種方式就不推薦使用，否則只會讓系統混亂。 Rancher Fleet Rancher Fleet 是 Rancher v2.5 正式推出的功能，其替代了過往的 Rancher pipeline(前述文章沒有探討，因為基本上要被淘汰了)的部署方式。 Fleet 是一個基於 GitOps 策略的大規模 Kubernetes 應用部署解決方案，基於 Rancher 的架構使得 Fleet 可以很輕鬆的存取所有 Rancher 控管的 Kubernetes 叢集，同時 GitOps 的方式讓開發者可以簡單的一口氣將應用程式更新到多個 Kubernetes 叢集。 接下來的文章就會從 Rancher Catalog 出發，接者探討 GitOps 與 Rancher Fleet 的使用方式。","keywords":"","version":"Next"},{"title":"Rancher Application","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day18","content":"Rancher Application Rancher v2.5 是一個非常重要的里程碑，有很多功能於這個版本進入了 v2 下一個里程碑，本章節要探討的應用程式部署實際上也有這個轉變。 Rancher Catalog 是 Rancher v2.0 ~ v2.4 版本最主要的部署方式，而 v2.5 則改成 Cluster Explorer 內的 App&amp;Marketplace 的方式。 也可以將這個差異說成由 Cluster Manager 轉換成 Cluster Explorer。 那為什麼這個已經要被廢除的功能還需要來介紹？ 主要是我自己針對 v2.5 的使用經驗來看，我認為部署應用程式用 Cluster Manager 看起來還是比較簡潔有力，相反的 Cluster Explorer 內的機制沒有好到會讓人覺得替換過去有加分效果。 所以接下來就針對這兩個機制分享一下使用方式。 Rancher Catalog Rancher Catalog 的核心概念分成兩個 如何取得 Kubernetes 應用程式，這部分的資訊狀態就稱為 Catalog將 Catalog 中描述的應用程式給實體化安裝到 Kubernetes 中 Catalog 的核心精神就是要去哪邊取得 Kubernetes 應用程式，Catalog 支援兩種格式 Git 專案，底層概念就是能夠透過 git clone 執行的專案都可以Helm Server，說到底 Helm Server 就是一個 HTTP Server，這部分可以自行實作或是使用 chartmuseum 等專案來實作。 由於 Helm 本身還有版本差別， Helm v2 或是 Helm v3，因此使用上需要標注到底使用哪版本。 Catalog 也支援 Private Server，不過這邊只支援使用帳號密碼的方式去存取。使用權限方面 Catalog 也分成全 Rancher 系統或是每個 Kubernetes 叢集獨立設定。 首先如下圖，切換到 Global 這個範圍，接者可以於 Tools 中找到 Catalog 這個選項。 或是如下圖，切換到 ithome-dev 這個叢集中，也可以看到 Tools 中有 Catalog 的範圍。 這邊我們使用 Kubernetes Dashboard 這個專案作為一個示範，該專案的 Helm Chart 可以經由 https://kubernetes.github.io/dashboard 這個 Helm Server 去存取。 這類型的伺服器預設都沒有 index.html，所以存取會得到 404 是正常的，想要存取相關內容可以使用下列方式去存取 https://kubernetes.github.io/dashboard/index.yaml，這也是 helm 指令去抓取相關資源的辦法，可以知道該 Server 上會有多少 Helm Charts 以及對應的版本有哪些。 點選右上方的 Add Catalog 就可以看到如下的設定視窗 該畫面中我們填入上述資訊，如果是 Git 專案的位置還可以輸入 branch，但是因為我們是 Helm Server，所以 Branch 的資訊就沒有設定的意義。 最後順便設定該 Helm Server 是基於 Helm v3 來使用的。 創建完畢後就意味 Rancher 已經可以透過這個 Catalog 去得到遠方當前有哪些 應用程式以及擁有哪些版本，但是這並不代表 Rancher 已經知道。 一種做法就是等 Rancher 預設的同步機制慢慢等或是直接點選 Refresh 讓 Rancher 直接同步該 Catalog 的資訊。 一種常見的情境就是你的 CI/CD 流程更新了 Helm Chart，推進一個版本，結果 Rancher 還不知道，這時候就可以 refresh 強制更新。 創建 Catalog 完畢後，下一件事情就是要從 Catalog 中找到一個可以用的應用程式，並且選擇該應用程式的版本，如果是 Helm 描述的應用程式還可以透過 values.yaml 的概念去客製化應用程式。 應用程式的安裝是屬於最底層架構的，因此是跟 Project 綁定，從左上方切換到之前創立的 myApplication project，並且切換到到畫面上方的 app 頁面中。 該頁面的右上方有兩個按鈕，其中 Manage Catalog 會切回到該專案專屬的 Catalog 頁面，因此 Catalog 本身實際上有三種權限，(Global, Cluster, Project). 右邊的 Launch 意味者要創立一個應用程式。 點進去後會看到如下方的圖 圖中最上方顯示的就是前述創立的 Catalog，該 Helm Server 中只有一個應用程式名為 kubernetes-dashboard 下面則是一些系統預設的 catalog，譬如 helm3-library，該 helm server 中則有非常多不同的應用程式。 其中這些預設提供的 helm chart 還會被標上 partner 的字樣。 點選 kubernetes-dashboard 後就會進入到設定該應用程式的畫面。 畫面上會先根據 Helm Chart 本身的描述設定去介紹該 Helm Charts 的使用方式 接下來就要針對該應用程式去設定，該設定包含了 該應用程式安裝的名稱該 Helm Chart 要用什麼版本，範例中選擇了 4.5.0該服務要安裝到哪個 Kubernetes namespace 中最下面稱為 Answer 的概念其實就是 Helm Chart values，這邊可以透過 key/value 的方式一個一個輸入，或是使用 Edit as YAML 直接輸入都可以 預設情況下我們不進行任何調整，然後直接安裝即可。 安裝完畢後就可以於外面的 App 頁面看到應用程式的樣子，其包含了 應用程式的名稱當前使用版本，如果有新版則會提示可以更新狀態是否正常有多少運行的 Pod是否有透過 service 需要被外部存取的服務 點選該名稱可以切換到更詳細的列表去看看到底該應用程式包含的 Kubernetes 資源狀態，譬如 Deployment, Service, Configmap 等 如果該資源有透過 Service 提供存取的話， Rancher 會自動的幫該物件創建一個 Endpoint，就如同 Grafana/Monitoring 那樣，可以使用 API Server 的轉發來往內部存取。 譬如途中可以看到有產生一個 Endpoint，該位置就是基於 Rancher 的位置後面補上 cluster/namespace/service 等相關資訊來進行處理。 這類型的資訊也會於最外層的 App 介面中直接呈現，所以如果直接點選的話就可以很順利地打該 Kubernetes Dashboard 這個應用程式。 最後也可以透過 Kubectl 等工具觀察一下目標 namespace 是否有相關的資源，可以看到有 deployment/service 等資源 透過 Rancher Catalog 的機制就可以使用 Rancher 的介面來管理與存取這些服務，使用上會稍微簡單一些。既然都可以透過 UI 點選那就有很大的機會可以透過 Terraform 來實現上述的操作。 接下來示範如何透過 Terraform 來完成上述的所有操作，整個操作會分成幾個步驟 先透過 data 資料取得已經創立的 Project ID創立 Catalog創立 Namespace創立 App data &quot;rancher2_project&quot; &quot;system&quot; { cluster_id = &quot;c-z8j6q&quot; name = &quot;myApplication&quot; } resource &quot;rancher2_catalog&quot; &quot;dashboard-global&quot; { name = &quot;dashboard-terraform&quot; url = &quot;https://kubernetes.github.io/dashboard/&quot; version = &quot;helm_v3&quot; } resource &quot;rancher2_namespace&quot; &quot;dashboard&quot; { name = &quot;dashboard-terraform&quot; project_id = data.rancher2_project.system.id } resource &quot;rancher2_app&quot; &quot;dashboard&quot; { catalog_name = &quot;dashboard-terraform&quot; name = &quot;dashboard-terraform&quot; project_id = data.rancher2_project.system.id template_name = &quot;kubernetes-dashboard&quot; template_version = &quot;4.5.0&quot; target_namespace = rancher2_namespace.dashboard.id depends_on = [rancher2_namespace.dashboard, ncher2_catalog.dashboard-global] } 上述做的事情基本上跟 UI 是完全一樣，創造一個 Catalog，輸入對應的 URL 並且指名為 Helm v3 的版本。 然後接者創立 Namespace，因為使用 namespace，所以要先利用前述的 data 取得目標 project 的 ID，這樣就可以把這個 namespace 掛到特定的 project 底下。 最後透過 catalog 名稱, Template 的名稱與版本來創造 App。 準備完畢後透過 Terraform Apply 就可以於網頁看到 App 被創造完畢了。 透過 Terraform 的整合，其實可以更有效率的用 CI/CD 系統來管理 Rancher 上的應用程式，如果應用程式本身需要透過 Helm 來進行客製化，這部分也都可以透過 Terraform 內的參數來達成，所以可以更容易的來管理 K8s 內的應用程式，有任何需求想要離開時，就修改 Terraform 上的設定，然後部署即可。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day19","content":"前言 前篇文章介紹了 Rancher(v2.0 ~ v2.4) 中主打的應用程式管理系統， Catalog，所有的 Catalog 都必須要於 Cluster Manager 的介面中去管理。 而 Rancher v2.5 開始主打 Cluster Explorer 的介面，該介面中又推行了另外一套應用程式管理系統，稱為 App &amp; Marketplace。 事實上前述的章節就已經有透過這個新的系統來安裝 Monitoring 的相關資源，因此現在 Rancher 都已經使用這個新的機制來提供各種整合服務，因此後續的新功能與維護也都會基於這個新的機制。 使用上我認為兩者還是有些差異，因此對於一個 Rancher 的使用者來說最好兩者都有碰過，稍微理解一下其之間的差異，這樣使用上會更有能力去判斷到底要使用哪一種。 Rancher App 新版的應用程式架構基本上跟前述的沒有不同，不過名詞上整個大改，相關名詞變得與 Helm 生態系更佳貼近，譬如舊版使用 Catalog 來描述去哪邊抓取 Helm 相關的應用程式，新版本則是直接貼切的稱為 Helm Repo。 Cluster Explorer 的狀態下，左上方點選到 Apps &amp; Marketplace 就可以進入到新版的系統架構，\b 該架構中左方有三個類別，其中第二個 Charts Repositories 就是新版的 Catalog，畫面如下。 預設系統上有兩個 Chart Repo，其中之前安裝的 Monitoring 就是來自於這邊。 接者點選右上方的 Create 就會看到新版的創立畫面 新版本的創建畫面更加簡潔與簡單，首先可以透過 target 來選擇到底要使用 Git 還是 HTTP 來存取，針對 Private Helm Repo 這次則提供了兩種驗證方式，譬如 SSH Key 與 HTTP Basic 兩種方式。 創造完畢後就可以於系統上看到新建立的 Helm Repo，系統預設的兩個 Helm Repo 都是基於 Git 去處理，而本篇文章新創立的則是 HTTP。 有了 Helm Repo 後，下一步就是要創造應用程式，切換到 Charts 的介面就可以看到如下的畫面。 畫面中上方顯示了目前擁有的 Helm Repo 有哪些，這邊可以透過勾選的方式來過濾想要顯示的 Helm Repo 只有單純勾選 dashboard 後就可以看到 kubernetes-dashboard 這個 Helm Chart。 點選該 kubernetes-dashboard 後進入到新版的安裝設定介面，該畫面中相對於舊版的 Catalog 來說畫面更為簡潔有力。 畫面中，最上方包含了 App 的名稱，該使用的 Helm Chart 版本，範例使用了 4.5.0。 接者下方則是安裝的 namespace ，這些選擇都與舊版的介面差不多。 最下方則列出不同的類別，包含 ValuesHelm READMEHelm Deploy Options Rancher 新版 App 捨棄了舊版 Answer 的叫法，同時也完全使用 YAML 的格式來設定 values，而不是透過 UI 一行一行慢慢設定。 註: 事實上舊版的 UI 的設定方式其實有滿多問題，某些情況還真的不能設定，透過檔案還是相對簡單與方便。 下面的 Helm Deploy Options 有不同的部署選項，譬如 要不要執行 Helm Hooks部署 Helm 時要不要設定 Timeout，多久的時間沒有成功部署就會判定失敗 一切設定完畢後就可以開始安裝，安裝畫面跟 Monitoring 的經驗類似，都會彈出一個 Terminal 畫面來顯示安裝過程。 畫面最下方則是顯示了到底系統是使用什麼指令來安裝 Helm Chart，安裝完畢可以用左上的按鈕離開畫面。 接者移動到 Installed Charts 可以找到前述安裝的 App，外面提供的 Active 資源數量則是代表所有 Kubernetes 的資源，不單單只是舊版所顯示的 Pod 而已。 新版跟舊版的 App 最大的差異我認為就是 Endpoint 的顯示，舊版的 Catalog 會很好心地將 Endpoint 呈現出來讓使用者可以輕鬆存取這些服務，但是新版卻不會。 要注意的是這些存取實際上是透過 Kubernetes API 去轉發的，所以其實這項功能並不需要 Rancher 特別幫你的應用程式處理什麼，因此如果知道相關的規則，還是可以透過自行撰寫 URL 來存取相關服務網頁，如下。 透過 UI 觀察新版應用程式後，接下來就示範如何透過 Terraform 來管理這種新版本的 Application。 Rancher 於 Terraform 中的實作是將 Catalog 與 App 的概念分開，新的概念都會補上 _v2 於相關的資源類型後面，譬如 catalog_v2, app_v2。 這個範例中的作法跟前述一樣 取得 project 的 ID(此處省略)透過 catalog_v2 創造 Helm Repo創造要使用的 namespace接者使用 app_v2 創造 App Terraform 的程式碼非常簡單，如下 resource &quot;rancher2_catalog_v2&quot; &quot;dashboard-global-app&quot; { name = &quot;dashboard-terraform&quot; cluster_id = &quot;c-z8j6q&quot; url = &quot;https://kubernetes.github.io/dashboard/&quot; } resource &quot;rancher2_namespace&quot; &quot;dashboard-app&quot; { name = &quot;dashboard-terraform-app&quot; project_id = data.rancher2_project.system.id } resource &quot;rancher2_app_v2&quot; &quot;dashboard-app&quot; { cluster_id = &quot;c-z8j6q&quot; name = &quot;k8s-dashboard-app-terraform&quot; namespace = rancher2_namespace.dashboard-app.id repo_name = &quot;dashboard-terraform&quot; chart_name = &quot;kubernetes-dashboard&quot; chart_version = &quot;4.5.0&quot; depends_on = [rancher2_namespace.dashboard-app, rancher2_catalog_v2.dashboard-global-app] } 其實透過觀察 v2 版本的 API 就可以觀察出來 v2 的改動很多，譬如 catalog_v2 (Helm Repo) 移除了關於 Scope 的選項，現在所有的 Helm Repo 都是以 Cluster 為單位，不再細分 Global, Cluster, Project.app_v2 (App) 安裝部分差異最多，特別是 Key 的部分跟貼近 Helm Chart 使用的名詞，使用上會更容易理解每個名詞的使用。 譬如使用 chart_name, chart_version 取代過往的 template, template_version，同時使用 repo_name 取代 catalog_name。 不過如果都要使用 repo_name 了，其實直接捨棄 catalog_v2 直接創造一個新的物件 helm_repo 我認為會更佳直覺一些。 另外 App 移除了對於 Project 的使用，反而是跟 Cluster 有關，變成 App 都是以 Cluster 為基本單位。 當 Terraform 順利執行後，就可以於 App 頁面觀察到前述描述的應用程式被順利的部署起來了，如下圖。 到這邊可能會感覺到有點混淆，似乎使用 Cluster Explorer 就再也沒有 Project 的概念了，因此我認為 Rancher v2.6 後續還有很多東西要等，短時間內 Cluster Explorer 沒有辦法完全取代 Cluster Manager 的介面操作，但是部分功能 (Monitoring) 又已經完全轉移到 Cluster Explorer，這會造就管理者可能會兩個功能 (Cluster Explorer/Manager) 都會各自使用一部分的功能。 期許 Rancher 能夠將這些概念都同步過去才有辦法真正的移除 Cluster Manager，或是更可以直接的說過往的某些概念於新版後都不再需要。","keywords":"","version":"Next"},{"title":"Rancher","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day2","content":"Rancher Rancher 是一個由 Rancher Labs 的公司所維護的 Kubernetes 相關開源專案，Rancher Lab 於 2020 年底被 Suse 據傳已 600萬 ~ 700萬美金左右收購，因此如果目前搜尋 Rancher 相關的資源有時候會看到跟 Suse 這間公司有關的消息就不要太意外。 \b簡單來說，Rancher 是一個 Kubernetes 管理平台，希望能夠讓團隊用更簡單及有效率的方式去管理各式各樣的 Kubernetes 叢集，其支援幾種不同方式 Rancher 自行維護的 Kubernetes 版本，Rancher Kubernetes Engine(RKE)各大公有雲所提供的 Kubernetes 服務，如 AKS, EKS 以及 GKE任何使用者自己創建的 Kubernetes 叢集 除了上述 Kubernetes 叢集外， Rancher 也支援眾多公有雲平台來簡化整個部署流程，譬如可以讓公有雲自動創建 VM 並且於 VM 上創建 RKE 叢集，而且這些 VM 還可以根據不同的需求設定不同的能力，譬如某些節點設定 4c8g(4vCPU, 8G Memory)，某些給予 16c32g，同時有些專門當 worker，有些可以當 etcd/control plan等不同角色。 註: 不同來源的 Kubernetes 叢集功能上會有些許差異，詳細可以參閱官網介紹，RKE 跟 EKS/GKE 於 2.5.8 版本則擁有全部的操作能力，但是 AKS 或是其他使用者自行架設的 Kubernetes 叢集會有些功能沒辦法使用。 有些人會好奇，如果自己都已經有方式去架設跟管理自己的 Kubernetes 叢集，那為什麼還需要使用 Rancher 的管理平台? 就如同 Kubernetes 一樣，要不要導入 Rancher 也是要評估的，我認為符合下列情況的團隊其實並不一定要使用 Rancher，譬如 雲端環境直接採用 Kubernetes 服務，如 EKS/AKS/GKE直接尋找系統整合商購買 Kubernetes 服務沒有地端(On-premises)環境需求公司不太想要使用開源專案，希望專案都要有人員提供技術服務 如果團隊都沒有符合上述需求時，其實可以評估看看是否要導入 Rancher 導入的第一個問題就是導入 Rancher 能夠帶來什麼好處?，為什麼要使用 Rancher? 我個人認為 Rancher 對於團隊帶來的好處有 很輕鬆地去架設一套 RKE 的環境，雖然本身是 Rancher 所維護的版本，但是大部分情況跟原生 Kubernetes 使用起來沒有差異。如果團隊同時有地端跟雲端的混合環境，可以透過 Rancher 方便管理多套 Kubernetes如果今天地端環境本身擁有網路防火牆限制，導致想要從外部使用 Kubectl 來存取與管理該地端上的 Kubernetes 叢集會有困難時，使用 Rancher 能夠輕鬆地處理這個問題。Rancher 提供的 Dashboard 提供滿多訊息，可以一目明瞭目前所有 Kubernetes 叢集的健康狀態，非工程人員也可以容易閱讀Rancher 本身支援不同的認證機制，可以跟團隊本身使用的認證服務整合，直接透過現有的狀態來認證與授權，管理上非常方便 有了上述功能後，來看一下從官方所節錄的架構圖，來看看導入 Rancher 後對於整個團隊有什麼變化? 上圖分成三個部分，左邊代表 DevOps Team，中間是 Rancher 管理平台，右邊則是公司的 IT Team. Rancher平台(中間) Rancher 本身管理多套 Kubernetes 叢集，譬如圖中的 GKE/EKS，甚至可以跟 VMware 整合，將 RKE 安裝到產生的 VM 上如果已經跟公有雲平台串接完畢(API)，則可以透過 Rancher 的介面自動創立相關 VM 並且直接再上面創建 RKE 叢集，因此可以很方便根據需求創立 Dev/Staging/QA/Prod 等不同用途的 Kubernetes 叢集 IT Team(右邊) IT Team 對於公司內的環境會有比較不同的需求，譬如帳號認證授權，安全政策等IT 直接將 Rancher 與團隊內的身份機制整合，可以讓每個不同的 Kubernetes 都擁有不同的存取權限，譬如 QA Team 的人只擁有 QA 叢集的完全存取權限，而 Dev Team 的人可以存取 Dev 叢集，DevOps Team 的人則可以對所有叢集都有權限。可以直接於 Rancher 本身設定相關的安全政策，這些安全政策會直接套用到所有託管的 Kubernetes 叢集內。Rancher 其實也有實作 Terraform 的介面，所以 IT Team 是可以直接透過 Terraform 使用 Infrastructure as Code 的概念來維護 Rancher，這樣就可以很簡單與快速的維護與創建各種叢集。 DevOps Team(左邊) DevOps Team 使用 IT Team 設定好的身份帳號來存取相關 Kubernetes 叢集Rancher 也提供 KUBECONFIG 供使用者透過 kubectl/helm 等工具使用，也可以將此資訊整合到 CI/CD 流程來達成自動部署。Rancher 也提供應用程式部署的相關機制讓使用者可以方便地管理 Kubernetes 上的應用Rancher 整合的 Monitoring/Logging/Alert 功能讓使用者用起來很簡單。Rancher Fleet 使用 GitOps 的方式簡化了部署流程，使用者只需要更新 Git Repo 就可以順利更新自己的應用程式，甚至本身對於 Kubernetes 底層不太熟悉都能夠順利部署進去 當然上述架構只是一個範例，實際上更有可能是 DevOps Team 而非 IT Team 需要維護 Rancher 本身，這部分完全是取決於團隊的分工與組成。 版本選擇 目前主流的 Rancher 版本是 v2.5 系列，如果還沒有使用過 Rancher 的讀者建議都直接使用 v2.5 系列版本，主要是 v2.5 相對於前版有很多重大修改，譬如 Monitoring 功能的改進，v2.5 以前是用 Rancher 自行整合的 Prometheus/Grafana，所以使用者要客製化上會相對麻煩。 v2.5 整個架構都改成基於 Prometheus Operator 的做法，因此如果本來就熟悉 Prometheus Operator 的使用者可以更容易的使用 Rancher Monitoring 來加上自己想要的功能。Rancher 的 UI 也有大幅度的改動，過往瀏覽觀察 Cluster 的介面稱為 Cluster Manager，而新版的 Cluster Explorer 將會是未來維護的主要功能整合 Rancher Fleet 來提供基於 GitOps 的部署方式，之後的章節會詳細介紹如何使用 Rancher Fleet 來管理多叢集的應用程式提升與 AWS EKS 的整合，可以將已經創立的 EKS 直接整合到 Rancher 讓管理員用一個 Rancher 去管理多個 Kubernetes 目前 v2.6 版本還在積極開發中，目前已知 2.6 也在努力提升與 AKS/GKE 的整合。 同時 Rancher v2.5 之後 Rancher 本身的安裝方式也都轉移到 Helm3，因此如果需要從舊版 Rancher 轉移到新版 Rancher 時，有可能會遇到 Helm 轉移的問題 所以新的使用者都強烈建議直上 v2.5，而不要再嘗試舊版了。 下篇文章將詳細介紹 Rancher 的架構，看完該架構會更加理解到底 Rancher 扮演何種角色。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day20","content":"前言 前述文章探討了應用程式部署的基本思路，從 Rancher 管理的叢集出發有至少三種不同的部署方式，分別為 直接取得 Kubeconfig 獲得對 Kubernetes 叢集操作的權限使用 Rancher 內的應用程式機制(Catalog or App &amp; Marketplace) 來安，並可透過 Terraform 來達到 Infrastructure as Code 的狀態。使用 GitOps 的方式來管理 Kubernetes 應用程式 本篇文章開始將探討何謂 GitOps\b 以及 GitOps 能夠帶來的好處，並且最後將基於 Rancher FLeet 去進行一系列 GitOps 解決方案的 示範。 GitOps 就如同 DevOps 是由 DEV + OPS 兩種概念結合而成， GitOps 的原意來自於 Git 以及 OPS，目的是希望以 Git 上的資料為基底去驅動 Ops 相關的操作。 該詞源自於 2017 年由 Weave Works 所提出，GitOps 本身並沒有一個非常標準的定義與實作方式，就如同 DevOps 的文化一樣， 不同人使用 GitOps 的方式都不同，但是基本上都會遵循一個大致上的文化。 GitOps 的精神就是以 Git 作為唯一的資料來源，所有的應用程式部署都只能依賴這份 Git 上內容去變化。 基於這種精神，下列行為都希望盡量減少甚至避免。 直接透過 KUBECONNFIG 對叢集直接使用 Helm/Kubectl 去進行操作透過其他機制(Rancher Catalog/App) 去對叢集進行應用程式的管理 當 Git 作為一個唯一的資料來源時，整個部署可以帶來下列的好處 Git 本身的管理控制提供了應用程式的稽核機制，透過 Git 機制可以知道誰於什麼時間點什麼時間點帶來了什麼樣的改變。需要退版的時候，可以使用 Git Revert 的方式來退版 Git 內容，因此應用程式也會退版可以透過 Git 的方式(Branch, tag) 等本身機制來管理不同環境的應用程式由於 Git 本身都會使用 Pull Request/Git Review 等機制來管理程式碼管理，因此該機制可以套用到應用程式管理上。 這邊要注意的是， GitOps 本身的並沒有特別限制只能使用於 Kubernetes 環境之中，只是當初 Weave work 講出這名詞時是基於 Kubernetes 的環境來探討，因此後續比較多的解決方案也都是跟 Kubernetes 有關，但是這並不代表 GitOps 只能使用於 Kubernetes 內，任何的使用環境只要有基於 Git/Ops 的理念，基本上都可以想辦法實作 GitOps. 但是 GitOps 到底要如何實作? 要如何將 Git 的更動給同步到應用程式的部署則沒有任何規範與標準，目前主要有兩種主流，以下都是一種示範介紹，實務上實作時可以有更多不同的變化。 專屬 CI/CD 流水線獨立 Controller 接下來以 Kubernetes 為背景來探討一下可能的解法。 專屬 CI/CD 流水線 這種架構下會創立一個專屬的 CI/CD Pipeline, 該 Pipeline 的觸發條件就是 Git 專案發生變化之時。 所以 Pipeline 中會去抓取觸發當下的 Git 內容，接者從該內容中判別當前有哪些檔案被修改，從這些被修改的檔案去判別是哪些應用程式有修改，接者針對被影響的應用程式去進行更新。 以 Kubernetes 來說，通常就是指 CI/CD Pipeline 中要先獲得 KUBECONFIG 的權限，如果使用的是 Rancher，則可以使用 Rancher API Token。 當系統要更新應用程式時，就可以透過這些權限將 Kubernetes 內的應用程式進行更新。 這種架構基本上跟傳統大家熟悉的 CD 流程自動化看起來沒有什麼不同，不過 GitOps 會更加強調以 Git 為本，所以會希望只有該 CI/CD Pipeline 能夠有機會去更新應用程式，這也意味任何使用者直接透過 KUBECONFIG 對 Kubernetes 操作這件事情是不被允許的。 所以 GitOps 不單單是一個工具與解決方案，也是一個文化。 獨立 Controller 第二個解決方式是目前 Kubernetes 生態中的常見作法，該作法必須要於 Kubernetes 內部署一個 Controller，該 Controller 本身基於一種狀態檢查的無限迴圈去運行，一個簡單的運作邏輯如下。 檢查目標 Git 專案內的檔案狀態檢查當前 Kubernetes 叢集內的應用程式狀態如果(2)的狀態與(1)不同，就更新叢集內的狀態讓其與(1)相同 一句話來說的話，該 Controller 就是用來確保 Git 專案所描述的狀態與目標環境的現行狀態一致。 為了完成上述流程，該 Controller 需要有一些相關權限 能夠讀取 Git 專案的權限能夠讀取 Kubernetes 內部狀態的權限能夠更新 Kubernetes 應用程式的權限 由於該 Controller 會部署到 Kubernetes 內部，所以(2+3)的權限問題不會太困難，可以透過 RBAC 下的 Service Account 來處理。 (1)的部分如果是公開 Git 專案則沒有太多問題，私人的話就要有存取的 Credential 資訊。 以下是一個基於 Controller 架構的部署示範 1) 先行部署 Controller 到 Kubernetes 叢集內 2) 設定目標 Git 專案與目標 k8s 叢集/namespace 等資訊。 3) 開發者針對 Git 專案進行修改。 4) Controller 偵測到 Git 專案有變動 5) 獲取目前 Git 狀態 6) 獲取目前 叢集內的應用程式狀態 7) 如果(5),(6)不一樣，則將(5)的內容更新到叢集中 8) 反覆執行 (4~7) 步驟。 到這邊為止探討了關於 GitOps 的基本概念，接下來就會數個知名的開源專案去進行探討","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day21","content":"前言 前篇文章探討了基本的 GitOps 概念，GitOps 本身沒有嚴謹明確的實作與定義，所以任何宣稱符合 GitOps 工作流程的解決方案其實作方式與使用方有可能並不相同。 \b本文將探討數個常見的 GitOps 解決方案，針對其基本概念進行研究，一旦對這些解決方案都有了基本認知後，就可以更快的理解 Rancher Fleet 這套由 Rancher v2.5 後主推的 GitOps 解決方案是什麼，該怎麼使用。 KubeStack GitOps 並不是專屬於 Kubernetes 的產物，任何架構與專案都有機會採用 GitOps 的概念來實作。 KubeStack 是目前極為少數非 Kubernetes 應用程式的 GitOps 解決方案，官網宣稱是一個專注於 Infrastructure 的 GitOps 框架。該架構基於 Terraform 去發展，因此 KubeStack 的使用者實際上還是撰寫 Terraform ，使用 Terraform 的語言。 KubeStack 針對 Terraform 發展了兩套不同的 Terraform Module，分別是 Cluster Module 以及 Cluster Service Module。 Cluster Module 讓使用者可以方便的去管理 Kubernetes 叢集，該叢集可以很輕鬆的去指定想要建立於哪種雲端架構上，透過 KubeStack 使用者也可以很容易的針對不同地區不管雲端架構來搭建多套的 Kubernetes 叢集。 其實整體概念滿類似 Rancher 的，只不過這邊是依賴 Terraform 來管理與多個雲端架構的整合，同時 Kubernetes 叢集也會採用原生版本或是 Kubernetes 管理服務的版本。 Cluster Service Module 目的是用來創造 Kubernetes 相關資源，所以使用上會先透過 Cluster Module 創建 Kubernetes 叢集，接者透過 Cluster Service Module 部署相關服務。 Cluster Service Module 的目的並不是部署各種團隊的商業邏輯服務，相反的，其目的是則是部署前置作業，任何真正部署前需要用到的服務都會透過這個 Module 來處理。預設情況下 KubeStack 有提供 Catalog 清單來提供預設提供的服務，包含了 ArgoCD/FluxCert-ManagerSealed SecretsNginx IngressTektonPostgreSQL OperatorPrometheus Operator 而前述兩個則是針對 kubernetes 應用程式的 GitOps 解決方案。 KubeStack 的使用方式是採用前述探討的第一種實作，團隊需要準備一個專屬的 CI/CD Pipeline，其內透過呼叫 Terraform 的方式來完成整個更新的流程，對於 KubeStack 有興趣的可以參閱其官網。 ArgoCD/Flux 探討到開源且針對 Kubernetes 應用程式部署的解決方案時，目前最知名的莫過於 ArgoCD 以及 Flux。 ArgoCD 本身的生態系非常豐富，該品牌底下有各式各樣不同的專案，專注於不同功能，而這些功能又有機會彼此互相整合，譬如 ArgoCDArgo WorkflowArgo RollOut ArgoCD 是專注於 GitOps 的解決方案， Argo Workflow 是套 Multi-Stage 的 pipeline 解決方案，而 Argo Rollout 則是希望能夠針對 Kubernetes 提供不同策略的部署方式，譬如藍綠部署，金絲雀部署等，這些都是 Kubernetes 原生不方便實作的策略。 ArgoCD 採用的是第二種實作方式，需要於 Kubernetes 內安裝 ArgoCD 解決方案，該解決方案大致上會於叢集內安裝 Argo API ServerArgo ControllerDex ServerRepository Service 以下架構圖來自於官方網站 Argo Controller/Repository Service 是整個 GitOps 的核心功能，能夠偵測 Git 專案的變動並且基於這些變動去比較當前 Kubernetes 內的即時狀態是否符合 Git 內的期望狀態，並且嘗試更新以符合需求。 Argo API Server 則是提供一層 API 介面，讓外界使用者可以使用不同方式來操作 ArgoCD 解決方案，譬如 CLI, WebUI 等。 ArgoCD 安裝完畢後就會提供一個方式去存取其管理網頁，大部分的使用者都會透過該管理網頁來操作整個 ArgoCD，該介面的操作符合不同需求的使用者，譬如 PM 想要理解當前專案部署狀態或是開發者想要透過網頁來進行一些部署操作都可以透過該網頁完成。 為了讓 ArgoCD 可以更容易的支援不同帳戶的登入與權限管理，其底層會預先安裝 Dex 這套 OpenID Connector 的解決方案，使用者可以滿容易地將 LDAP/OAuth/Github 等帳號群組與 ArgoCD 整合，接者透過群組的方式來進行權限控管。 應用程式的客製化也支援不少，譬如原生的 YAML，Helm, Kustomize 等，這意味者大部分的 kubernetes 應用程式都可以透過 ArgoCD 來部署。 ArgoCD 大部分的使用者一開始都會使用其 UI 進行操作與設定，但是這種方式基本上與 Rancher 有一樣的問題 UI 提供的功能遠少於 API 本身，UI 不能 100% 發揮 ArgoCD 的功能設定不易保存，不容易快速複製一份一樣的 ArgoCD 解決方案，特別是當有災難還原需求時。 舉例來說，ArgoCD 可以管理多套 Kubernetes 叢集，這意味你可以於叢集(A)中安裝 ArgoCD，透過其管理叢集B,C,D。 管理的功能都可以透過網頁的方式來操作，但是要如何讓 ArgoCD 有能力去存取叢集 B,C,D，相關設定則沒辨法透過網頁操作，必須要透過 CLI 或是修改最初部署 ArgoCD時的 YAML 檔案。 ArgoCD 實際上於 Kubernetes 內新增了不少 CRD(Custom Resource Definition)，使用者於網頁上的所有設定都會被轉換為一個又一個的 Kubernetes 物件，而且 ArgoCD 本身的部署也是一個又一個 YAML 檔案，因此實務上解決設定不易保存的方式就是 「讓 ArgoCD 透過 GitOps 的方式來管理 ArgoCD」 該工作流程如下(範例) 將所有對 ArgoCD 的設定與操作以 YAML 的形式保存於一個 Git 專案中使用官方 Helm 的方式去安裝最乾淨的 ArgoCD於 ArgoCD 的網頁上新增一個應用程式，該應用程式目標是來自(1)的 Git 專案ArgoCD 會將(1)內的 Git 內容都部署到 Kubernetes 中ArgoCD 網頁上就會慢慢看到所有之前設定的內容 如果對於 ArgoCD 有興趣的讀者可以參考我開設的線上課程kubernetes 實作手冊： GitOps 版控整合篇，該課程中會實際走過一次 ArgoCD 內的各種操作與注意事項，並且最後也會探討 ArgoCD 與 Argo Rollout 如何整合讓部署團隊可以用金絲雀等方式來部署應用程式。 下篇文章就會回到 Rancher 專案身上，來探討 Rancher Fleet 是什麼，其基本元件有哪些，接者會詳細的介紹 Rancher Fleet 的用法。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day15","content":"前言 前篇探討了各式各樣透過 Rancher UI 來管理 Rancher, Kubernetes 的各種方式，所有的操作基本上都是基於 UI 點擊而完成的。 試想下列情境，是否會覺得某些情況還是有點卡? 公司想要有針對不同環境有不同的 Rancher 叢集，譬如 Production 跟其他要分開希望能夠減少管理員透過 UI 管理的頻率，畢竟透過 UI 點選創建有時並不會有太完善的稽核性有快速重複部署 RKE 叢集的需求，某些情況還需要刪除重建 上述這些要求全部都可以用之前分享的方式慢慢處理，不過會不會有更好的方式處理? 這幾年流行的 IaC 架構， Infrastructure as Code 的概念能不能套用到 Rancher 身上? 試想一下如果可以透過程式碼的方式定義 Rancher，對於開發者與管理者來說可以達到什麼樣的好處 當 Rancher 整個損毀，需要重新安裝或是需要部署類似環境時，可以非常快的部署，不需要重新透過手動的方式去重新設定所有細節所有重大操作都以程式碼為基礎去設定，減少任何人為操作的可能性，同時有任何出錯時可以基於程式碼重新部署來修復環境。\b複製程式碼就可以複製環境，修改一些變數就可以創建出類似的叢集 更重要的是，如果將這些 IaC 的概念與 CI/CD 流程整合，還可以透過 Code Review 的方式來合作檢視所有 Rancher 上的修改，同時透過自動化的方式去維護 Rancher 服務。 有任何不適當的修改想要復原也可以透過 Git Revert 的方式來回復到之前的狀態。 這種狀況下 Rancher 會變得更加容易維護與管理。 IaC IaC 的工具非常的多，當人們講到跟 Cloud Infrastructure 有關時，大部分人都會提到 Terraform 這套解決方案，而近年 Pulumi 的聲勢也漸漸提昇，愈來愈多人嘗試使用 Pulumi 來取代 Terraform，兩者最大的差別在於撰寫方式。 Terraform 有自己設計一套語法，意味使用 Terraform 就要使用該語法，而 Pulumi 則是基於不同的程式語言提供不同的 API 來使用ㄓ，所以開發者可以使用自己習慣的程式語言去撰寫。 本篇文章將介紹如何透過 Terraform 來管理我們的 Rancher，之後的章節有機會的話也會順便展示一下使用 Terraform 的寫法。 Terraform 關於 Terraform 的使用方式推薦參閱我好朋友 [David 所撰寫的 Terraform 系列文章](xxx) 本篇文章就不會探討太多 Terraform 的基本概念與使用方式，會更加專注於如何透過 Terraform 來管理 Rancher。 Terraform 官網中有非常詳細的資訊探討 Rancher 所有 API 的使用方式，有興趣可以參閱Rancher2 Provider 為了要能夠跟 Rancher 溝通，必須要先獲得一組 Access/Secrey Key 來存取 Rancher，這組 Key 可以從 Rancher 的使用者帳號去取得。 首先用一個可以管理 Rancher 的帳號登入到 Rancher UI，接者於右上方使用者那邊去點選 API &amp; Keys，如下圖。 進去之後可以看到系統預設有一些 Key，這些忽略即可。 要注意的是，每組 Key 產生後都會得到一組對應的 Secret Key，該 Key 是沒有辦法透過 UI 找回來的，這意味如果你當下忘了儲存或是之後不見了，那這把 secret key 就再也沒有辦法找回。 點選右上方的 Add Key 可以看到如下的畫面，該畫面可以先設定該 Key 會不會自動過期的時間，以及使用範圍。 設定名稱後就會看到如下圖的畫面，畫面中有四種相關不同的資訊，分別是 Access Endpoint: 存取的 API 位置Access KeySecret Key(只有這邊會出現，一但按下 Close 就再也拿不回來了)針對 HTTP 需求譬如 kubectl 是有機會直接使用最後一個 Bearer Token 使用 這次的 Terraform 要使用前三組資訊，這邊不考慮任何 Terraform 的撰寫技巧與 Style，單純用最簡單的風格來介紹如何將 Terraform 與 Rancher 整合。 以下示範是基於 Terraform 1.0 與 Rancher Provider 1.17.0 的版本 首先準備一個 main.tf 的檔案，內容如下 ╰─$ cat main.tf terraform { required_providers { rancher2 = { source = &quot;rancher/rancher2&quot; version = &quot;1.17.0&quot; } } } provider &quot;rancher2&quot; { api_url = &quot;https://rancher.hwchiu.com&quot; access_key = &quot;token-ng6df&quot; secret_key = &quot;l8kjh7w5mdb5s5nzmp56c5rctpt59p9bcq9wbw2g8b66wsdchrkdv2&quot; } 接者透過 Terraform init 先初始化相關模組 ╰─$ terraform init Initializing the backend... Initializing provider plugins... - Finding rancher/rancher2 versions matching &quot;1.17.0&quot;... - Installing rancher/rancher2 v1.17.0... - Installed rancher/rancher2 v1.17.0 (signed by a HashiCorp partner, key ID 2EEB0F9AD44A135C) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run &quot;terraform init&quot; in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running &quot;terraform plan&quot; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. 一切都準備完畢後，接下來示範一下如何透過 Rancher 來創造一些 Rancher 的資源，譬如說來創造一個 RKE Template。 這邊直接參考範例將下列內容寫入到 main.tf 中 該範例會創建一個 RKE Template 並且針對 etcd 以及一些更新策略進行設定。 $ cat main.tf terraform { required_providers { rancher2 = { source = &quot;rancher/rancher2&quot; version = &quot;1.17.0&quot; } } } provider &quot;rancher2&quot; { api_url = &quot;https://rancher.hwchiu.com&quot; access_key = &quot;token-ng6df&quot; secret_key = &quot;l8kjh7w5mdb5s5nzmp56c5rctpt59p9bcq9wbw2g8b66wsdchrkdv2&quot; } resource &quot;rancher2_cluster_template&quot; &quot;foo&quot; { name = &quot;ithome_terraforn&quot; template_revisions { name = &quot;V1&quot; cluster_config { rke_config { network { plugin = &quot;canal&quot; } services { etcd { creation = &quot;6h&quot; retention = &quot;24h&quot; } } upgrade_strategy { drain = true max_unavailable_worker = &quot;20%&quot; } } } default = true } description = &quot;Terraform cluster template foo&quot; } 接者透過 terraform apply 去更新 $ terraform apply ... + rke_config { + addon_job_timeout = 0 + ignore_docker_version = true + kubernetes_version = (known after apply) + prefix_path = (known after apply) + ssh_agent_auth = false + ssh_cert_path = (known after apply) + ssh_key_path = (known after apply) + win_prefix_path = (known after apply) + network { + mtu = 0 + options = (known after apply) + plugin = &quot;canal&quot; } + services { + etcd { + ca_cert = (known after apply) + cert = (sensitive value) + creation = &quot;6h&quot; + extra_args = (known after apply) + gid = 0 + image = (known after apply) + key = (sensitive value) + path = (known after apply) + retention = &quot;24h&quot; + snapshot = false + uid = 0 } } + upgrade_strategy { + drain = true + max_unavailable_controlplane = &quot;1&quot; + max_unavailable_worker = &quot;20%&quot; } } } } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes rancher2_cluster_template.foo: Creating... rancher2_cluster_template.foo: Creation complete after 2s [id=cattle-global-data:ct-rtd4f] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. 一切創造完畢後就可以移動到 Rancher UI 中，從 RKE Template 可以看到多出了一個新的 RKE Template，名稱為 ithome_terraform，與我們前述 main.tf 中描述的一樣。 點進去該 RKE Template 就可以看到詳細的設定，這邊要補充一下， Rancher 大部分的物件都提供兩種閱覽模式，一種是友善的 UI 介面另一種則是純 YAML 的描述檔案。 按照下方的方式點選 View as a Form 來看看基於 YAML 的內容。 從 YAML 內就可以看到 Terraform 描述的設定都有正確的寫進來。 透過這樣簡單的方式，就可以使用程式碼的方式來管理 Rancher，除了 RKE Template 之外， Cloud Credential, Node Template, Cluster 也都可以透過 Terraform 的方式來管理，這樣有另外一個好處就是系統管理員只要撰寫好這些資源後，接下來的使用者就不需要接觸到這些太細節的機密資訊，能夠專心的目標資源與邏輯去描述與創造即可。 最後這邊要再補充一個使用 API 溝通 Rancher 的好處，事實上， Rancher UI 沒有辦法展現 Rancher 100% 的能力， RKE 內有非常多的設定可以處理，但是有些處理實際上沒有辦法透過 UI 去設定，譬如說想要針對 Kubelet 給一些額外參數的話，這些設定是沒有辦法從 Rancher UI 完成的，但是如果是透過 Rancher API 來設定的話就沒有問題。 Rancher API 有很多方式可以處理，不論是直接撰寫應用程式溝通 Rancher 或是使用 Terraform/Pulumi 等工具都可以，透過這類型工具去描述 Rancher 實際上可以將 Rancher 使用的更靈活與更強大，設定的東西也更多元化。 如果團隊有意願長期使用 Rancher，會非常推薦使用 IaC 的工具來維護與管理 Rancher，期帶來得好非常的多。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day16","content":"前言 之前曾經探討過如何透過 Terraform 來管理 Rancher，為了完成這個步驟必須要於 Rancher UI 中去取得相關的 Access Token/Secret Key。實際上該 Access Token 除了給 Terraform 去使用外，也可以讓 Rancher 自行開發維護的 CLI 工具來使用。 本篇文章就來介紹一下 Rancher CLI 可以怎麼用，裡面有什麼好用值得注意的功能 CLI 基本上 Rancher CLI 的功用跟網頁沒差多少，最主要的目的是讓使用者可以透過指令列的方式去操作 Rancher 而非透過網頁操作，這個概念跟 Terraform 是完全一致的。 因此實務上我會推薦都使用 Terraform 工具來管理 Rancher 而非使用 CLI 這個工具，那這樣還有必要學習 CLI 的用法嗎? 答案是肯定的，因為學得愈廣，當問題出現時腦中就會有更多的候選工具供你選擇去思考該如何解決面前的問題。 Rancher CLI 有一個好用的功能我認為是 Terraform 比不上的，這點稍後會來探討。 首先如同先前操作一樣，到 Rancher UI 去取得相關的 Access Token/Secret Key，不過這一次因爲 CLI 會透過 HTTP 進行授權存取，所以會用到的是下方的 Bearer Token，其實就是把 Access Key 跟 Secret Key 給合併而已。 取得這些資訊之後就來去 Rancher CLI 官網下載相對應的 CLI 版本，這邊要注意的是 Rancher CLI 的版本不會完全跟 Rancher 對齊。 Rancher 本身的 Release Note 都會描述當前版本對應的 CLI 與 RKE 的版本。 對應到 Rancher v2.5.9 的 CLI 版本是 v.2.4.11 安裝完畢後可以執行看看確認版本是否符合 ╰─$ rancher --version rancher version v2.4.11 ╰─$ rancher --help Rancher CLI, managing containers one UTF-8 character at a time Usage: rancher [OPTIONS] COMMAND [arg...] Version: v2.4.11 Options: --debug Debug logging --config value, -c value Path to rancher config (default: &quot;/Users/hwchiu/.rancher&quot;) [$RANCHER_CONFIG_DIR] --help, -h show help --version, -v print the version Commands: apps, [app] Operations with apps. Uses helm. Flags prepended with &quot;helm&quot; can also be accurately described by helm documentation. catalog Operations with catalogs clusters, [cluster] Operations on clusters context Operations for the context globaldns Operations on global DNS providers and entries inspect View details of resources kubectl Run kubectl commands login, [l] Login to a Rancher server multiclusterapps, [multiclusterapp mcapps mcapp] Operations with multi-cluster apps namespaces, [namespace] Operations on namespaces nodes, [node] Operations on nodes projects, [project] Operations on projects ps Show workloads in a project server Operations for the server settings, [setting] Show settings for the current server ssh SSH into a node up apply compose config wait Wait for resources cluster, app, project, multiClusterApp token Authenticate and generate new kubeconfig token help, [h] Shows a list of commands or help for one command Run 'rancher COMMAND --help' for more information on a command. 從上述的 Help 可以看到該 CLI 有滿多子指令可以使用的，包含了 clusters, context, nodes, projects, ssh 等各種功能。 為了使用這些功能，必須要使用 login 來獲得與目標 Rancher 溝通的能力，這時候前述獲得的 Bearer Token 就派上用場了 ╰─$ rancher login --name test -t token-8s72l:b425shbg49l7rs9mwlqzk89z6tr472qj94wx6vrm9pwh5r6mklsxf6 https://rancher.hwchiu.com/v3 130 ↵ NUMBER CLUSTER NAME PROJECT ID PROJECT NAME PROJECT DESCRIPTION 1 rke-it c-9z2kx:p-5gdg9 System System project created for the cluster 2 rke-it c-9z2kx:p-lxsz6 Default Default project created for the cluster 3 rke-qa c-p4fmz:p-fccdb System System project created for the cluster 4 rke-qa c-p4fmz:p-r8wvz Default Default project created for the cluster 5 ithome-dev c-z8j6q:p-p6xrd myApplication 6 ithome-dev c-z8j6q:p-q46q5 System System project created for the cluster 7 ithome-dev c-z8j6q:p-vblmb Default Default project created for the cluster 8 local local:p-6knqb System System project created for the cluster 9 local local:p-hgjqp Default Default project created for the cluster Select a Project:5 INFO[0121] Saving config to /Users/hwchiu/.rancher/cli2.json 登入完畢後，系統會要求你選擇一個 Project 做為預設操作的 Project，選擇完畢後就可以透過 CLI 進行操作了。 CLI 基本上可以完成 UI 所能達到的功能，譬如可以使用 cluster 子指令來觀察 Cluster 的狀態，知道目前有哪些 Cluster，上面的名稱與資源又分別有多少。 ╰─$ rancher clusters CURRENT ID STATE NAME PROVIDER NODES CPU RAM PODS c-9z2kx active rke-it Azure Container Service 3 1.25/5.70 1.61/13.38 GB 18/330 c-p4fmz active rke-qa Rancher Kubernetes Engine 2 0.42/4 0.24/7.49 GB 14/220 * c-z8j6q active ithome-dev Rancher Kubernetes Engine 5 5.97/10 3.37/38.39 GB 110/550 local active local Imported 3 0.53/6 0.31/11.24 GB 22/330 如果採用的是舊版本的 catalog 的安裝方式的話，也可以透過 apps 子指令觀察安裝的所有資源，當然也可以透過 Rancher CLI 來安裝 application， 所以也會有團隊嘗試使用 Rancher CLI 搭配 CI/CD 流程來安裝 Rancher 服務，不過實務上會推薦使用 Terraform, 因為更有結構同時使用更為容易。 ╰─$ rancher apps ID NAME STATE CATALOG TEMPLATE VERSION p-p6xrd:dashboard-terraform dashboard-terraform active dashboard-terraform kubernetes-dashboard 4.5.0 那到底有什麼功能是值得使用 Rancher CLI 的? 我認為有兩個，分別是 Node SSHKubernetes KUBECONFIG 前述安裝的 Kubernetes 叢集有一個是採用動態節點的方式，Rancher 透過 Azure 創造這些節點的時候都會準備一把連接用的 SSH Key，這把 Key 是可以透過 UI 的方式下載，不過使用上我認為不太方便。而 Rancher CLI 就有實作這功能，可以讓使用者很方便的透過 CLI 進入到節點中。 指令的使用非常簡單，透過 rancher ssh 搭配節點名稱即可使用。 如果節點本身有兩個 IP 時，透過 -e 可以選擇使用 external 的 IP 地址來使用，否則預設會使用 internal 的 IP 地址。 ╰─$ rancher ssh -e node1 The authenticity of host '40.112.223.2 (40.112.223.2)' can't be established. ECDSA key fingerprint is SHA256:dqMCUUC4iZk/gZealQ+Ck3VhG/KaLaCVdkuLYwZfgsE. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '40.112.223.2' (ECDSA) to the list of known hosts. Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1055-azure x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Wed Aug 25 22:00:22 UTC 2021 System load: 0.39 Users logged in: 0 Usage of /: 33.9% of 28.90GB IP address for eth0: 192.168.0.5 Memory usage: 44% IP address for docker0: 172.17.0.1 Swap usage: 0% IP address for cni0: 10.42.2.1 Processes: 244 * Super-optimized for small spaces - read how we shrank the memory footprint of MicroK8s to make it the smallest full K8s around. https://ubuntu.com/blog/microk8s-memory-optimisation 13 updates can be applied immediately. To see these additional updates run: apt list --upgradable *** System restart required *** Last login: Wed Aug 11 07:28:47 2021 from 52.250.127.84 docker-user@node1:~$ 因此如果今天有需求想要進入到這些節點進行除錯時，透過 CLI 可以大大的簡化整個過程。 另外一個好用的功能就是 kubeconfig 的存取，試想今天一個系統管理員想要透過指令的方式去管理數十個由 Rancher 維護的 Kubernetes 叢集，最簡單的做法透過網頁的方式將每個叢集的 Kubeconfig 一個又一個的抓下來並且自行處理 kubeconfig 的格式。 透過 CLI 的方式可以讓上述的行為更加簡單甚至自動化。 ╰─$ rancher cluster kf Return the kube config used to access the cluster Usage: rancher clusters kubeconfig [CLUSTERID CLUSTERNAME] 透過 rancher clusters kf 的指令加上 cluster 名稱就可以取得該叢集的 KUBECONFIG 內容，譬如 ╰─$ rancher cluster kf rke-it apiVersion: v1 kind: Config clusters: - name: &quot;rke-it&quot; cluster: server: &quot;https://rancher.hwchiu.com/k8s/clusters/c-9z2kx&quot; users: - name: &quot;rke-it&quot; user: token: &quot;kubeconfig-user-qr5lq:v7htf5kcz2s5nv7b5fzjz68ntlxf2978d5rrgxbrjhz2zv7vjhq9h7&quot; contexts: - name: &quot;rke-it&quot; context: user: &quot;rke-it&quot; cluster: &quot;rke-it&quot; current-context: &quot;rke-it&quot; 同時搭配 rancher cluster ls 的指令，我們就可以撰寫一個 for 迴圈來依序取得這些內容，並且將這些內容抓下來處理，譬如 ╰─$ for c in $(rancher clusters ls --format '{{.Cluster.Name}}'); do rancher cluster kf $c ; done 上述功能如果與 kubectl 的 plugin, kconfig 整合就可以更順利的將多個 KUBECONFIG 整合成一個檔案，並且將此功能撰寫成一個 shell function, 這樣就可以隨時隨地的去更新當前環境的 Kubeconfig. 譬如 function update_k8s_config { mv ~/.kube/configs ~/.kube/configs-`date +%Y-%m-%d-%H%M%S` mkdir ~/.kube/configs for c in $(rancher clusters ls --format '{{.Cluster.Name}}'); do rancher cluster kf $c &gt; ~/.kube/configs/$c done kubectl konfig merge ~/.kube/configs/* &gt; ~/.kube/config } 剩下 CLI 的功能就留給大家自己去嘗試看看囉。","keywords":"","version":"Next"},{"title":"Rancher Fleet 架構介紹","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day22","content":"Rancher Fleet 架構介紹 #前言 前篇文章探討了 ArgoCD 與 KubeStack 這兩套截然不同的 GitOps 解決方案，可以觀察到 GitOps 就是一個文化與精神，實際的操作與部署方式取決於不同解決方案。 Rancher 本身作為一個 Kubernetes 管理平台，不但可以管理已經存在的 Kubernetes 叢集更可以動態的於不同架構上創建 RKE 叢集。 基於這些功能的基礎上，Rancher 要實作 GitOps 似乎會簡單一些，畢竟連 Kubernetes 都可以創建了，要部署一些 Controller 到叢集內也不是什麼困難的事情，因此本篇文章就來仔細探討 Rancher Fleet 這套 Rancher 推出的 GitOps 解決方案。 Rancher Fleet Rancher Fleet 是 Rancher 於 v2.5 後正式推出的應用程式安裝功能，該安裝方式不同於 Catalog 以及 v2.5 的 App 專注於單一應用安裝，而是更強調如何透過 GitOps 來進行大規模部署。 Fleet 的設計初衷就是希望提供一個大規模的 GitOps 解決方案，大規模可以是大量的 Kubernetes 叢集或是大量的應用程式部署。為了滿足這個目標， Fleet 架構設計上就是追求輕量與簡單，畢竟 Rancher 擁有另外一套針對物聯網環境的輕量級 Kubernetes 叢集，K3s。 因此 Fleet 也希望能夠針對 K3s 這種輕量級環境來使用。 Fleet 支援三種不同的格式，分別是原生YAML, Helm 以及 Kustomize，其中最特別的是這些格式還可以互些組合，這意味者使用者可以透過 Helm + Kustomize 來客製化你的應用程式，之後會有文章針對這些使用情境來介紹這類型的用途及好處。 Fleet 的內部邏輯會將所有的應用程式動態的轉為使用 Helm 去安裝與部署，因此使用者除了透過 Rancher Fleet 之外也可以透過 Helm 的方式去觀察與管理這些應用程式，簡單來說 Fleet 希望可以讓透過使用者簡單的安裝大規模的應用程式，同時又提供一個良好的介面讓使用者可以管理這些應用程式。 下圖來自於官方網站，該圖呈現了 Fleet 的基本架構與使用概念。圖中有非常多的專有名詞，瞭解這些名詞會對我們使用 Fleet 有非常大的幫助，因此接下來針對這張圖進行詳細介紹。 Fleet 與大部分的 Operator 實作方式一樣，都是透過 Kubernetes CRD 來自定義相關資源，並且搭配一個主要的 Controller 來處理這些資源的變化，最終提供 GitOPs 的功能，因此圖上看到的大部分名詞實際上都可以到 Kubernetes 內找到一個對應的 CRD 資源。 Fleet Manager/Fleet Controller: 由於 Fleet 是一個可以管理多個 Kubernetes 叢集的解決方案，其採取的是 Manager/Agent 的架構，所以架構中會有一個 Kubernetes 叢集其扮演者 Fleet Manager 的概念，而被管理的 Kubernetes 叢集則是所謂的 Fleet Agent 上圖中的 Fleet Controller Cluster 就是一個擁有 Fleet Manager 的 Kubernetes 叢集，底下三個 Cluster Group 代表的是其裡面的所有 Kubernetes 叢集都是 Fleet Agent Fleet Manager 的概念中，實際上會部署一個名為 Fleet Controller 的 Kubernetes Pod，該服務要負責處理 Fleet Agent 註冊的資訊，同時也要協調多個 Fleet Agent 當前的部署狀態最後呈現到 UI 中供管理者使用。 Fleet Agent: 每一個想要被管理的 Kubernetes 叢集都被視為 Fleet Agent，實際上需要安裝一個名為 Fleet Agent 的 Kubernetes Pod 到叢集中，該 Agent 會負責跟 Fleet Manager 溝通並且註冊，確保該叢集之後可以順利地被 Fleet Manager 給管理。 Single/Multi Cluster Style: Fleet 的官方網站提及兩種不同的部署模式，分別是 Single Cluster Style 以及 Multi Cluster Style Single Cluster Style 主要是測試使用，該架構下會於一個 Kubernetes 叢集中同時安裝 Fleet Agent 與 Fleet Controller，這樣就可以於一個 Kubernetes 叢集中去體驗看看 Rancher Fleet 帶來的基本部署功能。 不過實務上因為會有更多的叢集要管，因此都會採用 Multi Cluster Style，該架構如同上圖所示，會有一個集中的 Kubernetes 叢集作為 Fleet Manager，而所有要被管理的 Kubernetes 叢集都會作為 Fleet Agent. GitRepo: Fleet 中會有一個名為 GitRepo 的物件專門用來代表各種 Git 的存取資訊，Fleet Manager 會負責去監控欲部署的 Git 專案，接者將這些專案的內容與差異性給部署到被視為 Fleet Agent 的 Kubernetes 叢集。 Bundle Bundle 可以說是整個 Fleet 中最重要也是最基本的資源，其代表的是一個又一個要被部署的應用程式。 當 Fleet Manager 去掃描 GitRepo 時，就會針對該 GitRepo 中的各種檔案(YAML, Helm, Kustomize) 等 產生多個 Bundle 物件。 Bundle 是由一堆 Kubernetes 物件組成的，基本上也就是前篇所探討的應用程式。舉例來說，今天 Git 專案中透過 Helm 的方式描述了三種應用程式，Fleet Manager 掃描該 GitRepo 後就會產生出對應的三個 Bundle 物件。接者 Fleet Manager 就會將該 Bundle 給轉送到要部署該應用程式的 Fleet Agent 叢集，最後 Fleet Agent 就會將這些 Bundle 動態的轉成 Helm Chart 並且部署到 Kubernetes 叢集。 從上方的架構圖來看，可以看到中間的 Fleet Cluster 本身會連接 Git 專案，並且針對這些專案產生出一個又一個 Bundle 資源(Bundle Definition)，接者這些 Bundle 就會被傳送到需要部署的 Kubernetes 叢集，該叢集上的 Fleet Agent 就會負責處理這些 Bundle，譬如補上針對自身叢集的客製化設定，最後部署到叢集內。 所以可以看到上圖左下方的 Kubernetes 叢集內使用的是 (Bundle with Cluster Specific Configuration) 的字眼，代表這些真正部署到該叢集內的 Bundle 都是由最基本的 Bundle 檔案配上每個叢集的客製化內容。 為了讓 Fleet 能夠盡可能地去管理不同架構的 Kubernetes 叢集， Fleet 跟 Rancher 本身的設計非常類似，都是採取 Agent Pull 的方式。該模式代表的是 Fleet Controller 不會主動的去跟 Fleet Agent 進行連線，而是由 Fleet Agent 主動的去建立連線。 這種架構的好處就是被管理的 Kubernetes 叢集可以將整個網路給隱藏到 NAT 後面，只要確保底層環境有 SNAT 的功能網路可以對外即可。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day23","content":"前言 前述文章探討了關於 Rancher Fleet 的基本架構與概念介紹，瞭解到 Rancher Fleet 本身是個主從式的架構。環境中會有一個 Kubernetes 叢集專門用來部署 Fleet Controller 作為 Fleet Manager，而所有要被託管的 Kubernetes 叢集都必須要部署一個 Fleet Agent 來連接 Fleet Manager。 本篇文章將針對實際部署情況去觀察。 安裝 官方網站 有列出非常詳細的安裝步驟，針對 Fleet Manager 安裝要用到的憑證與除錯方式，如何使用 Helm 安裝 CRD 與 Fleet Controller 都有介紹。 當 Fleet Manager 安裝與設定完畢後，接下來就可以參考 Cluster Registration Overview 這篇官方文章來學習如何將一個 Kubernetes 叢集作為 Fleet Agent 加入到 Fleet Manager 的管理中。 不過這邊要特別注意的是，以上所提的安裝方式都是針對純 Fleet 解決方案時才需要考慮的部分，因為 Fleet 是 Rancher 開發與維護的，因此任何由 Rancher 管理與創建的 Kubernetes 叢集都已經內建 Fleet Agent，大幅度簡化使用者的安裝方式。 當初安裝 Rancher 時會先準備一個 Kubernetes 叢集專門用來負責 Rancher 本身的維運，其他所有的叢集都會透過這個 Rancher 去創建與維護，Rancher Fleet 會採用相同的架構與方式去處理。專門用來部署 Rancher 的 Kubernetes 叢集會被安裝 Fleet Controller。 基於以上兩點，只要所有叢集都是由 Rancher 去創建與管理的， Fleet 就不需要自己手動安裝，Fleet Manager 與 Fleet Agent 都會自動的被安裝與部署到相關叢集中。 觀察 前述所述，用來安裝 Rancher 服務的 Kubernetes 叢集本身也會安裝 Fleet Controller，這部分可以到 Apps 的頁面去看到底有哪些應用程式被安裝到叢集中。同時透過畫面中提供的 kubectl 指令去觀察 fleet-system namespace 中安裝的 Pod，如下圖。 可以觀察到 local 也就是部署 Rancher 的 Kubernetes 叢集有部署三個 Pod，其中 fleet-controller 以及 gitjob 兩個 pod 是針對 fleet manager 而部署的 Pod，而 fleet-agent 則是給 fleet-agent 使用的。 此架構意味者管理者可以透過該 Fleet 來管理 local 這套 Kubernetes 叢集。 之前提到 Fleet 與大部分的 Operator 有相同的開發流程，所以會使用一個 Kubernetes Controller 配上很多預先設定的 CRD 物件，所以透過 kubectl get crd 就可以看到 local 叢集上有各式各樣關於 Fleet 的 CRD。 接下來觀察要被管理的 Kubernetes 叢集，譬如給 Dev 使用的叢集，這時候使用相同的方式去觀察該叢集內安裝的資源，可以觀察到 fleet-system 內只有安裝一個 Pod，該 Pod 就是扮演 Fleet Agent 的角色，讓該叢集能夠順利的 Fleet Manager 溝通，範例如下。 確認完畢之後就可以移動到 Rancher Fleet 的專屬頁面，移動的方式很簡單，不論當前是處於哪個叢集，點選左上方就可以找到一個名為 Continuous Delivery 的選項，點進去就會進入到 Rancher Fleet 的畫面。 進去畫面中後會看到如下的畫面，畫面中有非常多的新東西，接下來針對這些新東西慢慢探索 首先畫面最上方有一個下拉式選單可以選擇，該選單會列出所有可以使用的 Fleet Workspace，那到底什麼是 workspace 呢? Fleet workspace 是一個管理單位，就如同大部分專案的 workspace 概念一樣，每個 workspace 都會有自己獨立的 GitRepo, Group, Bundle 等概念。 一個實務上的上作法會創立多種不同的 workspace，譬如 dev, qa 及 prod。 每個 workspace 內都可包含多個不同的 cluster 與其他的資源。 預設的情況下有兩個 workspace，分別是 fleet-local 以及 fleet-default. 所有剛加入到 Rancher 的叢集都會被加入到 fleet-default 這個 workspace 中。 畫面左邊有六個不同的資源 Git Repos: Git Repos 內的資源是告訴 Fleet 希望追蹤哪些 Git 專案，該 Git 專案中哪些資料夾的哪些檔案要讓 Fleet 幫忙管理與安裝。 因為 Fleet 什麼都還沒有安裝與設定，所以 Git Repo 內目前是空空的，沒有任何要被安裝的應用程式。 Clusters/Cluster Groups: 這邊顯示的是該 workspace 中有多少個 cluster，目前的環境中先前創立三個不同的 Kubernetes 叢集，而這些叢集預設都會被放入到 fleet-default workspace 內的 group。 假設 cluster 數量過多，還可以透過 Cluster Group 的概念來簡化操作，將相容用途的 cluster 用群組的方式來簡化之後的操作。 Workspace: Workspace 可以看到目前系統有多少個 workspace，可以看到系統中有 fleet-default 以及 fleet-local，同時也會顯示這些 workspace 中目前管理多少個 cluster 。 Bundles: 之前提過 Bundle 是 Fleet controller 掃過 GitRepo 專案後會產生的安裝資源檔案。 可以當前的範例非常奇妙，沒有任何 Git Repo 的內容卻擁有這些 Bundle 檔案，主要是因為這三個 Bundle 是非常奇妙與特殊的 Bundle，仔細看的話可以觀察到這些 Bundle 的名稱都是 Fleet-agent-c-xxxx，這些 bundle 是用來安裝 fleet-agent 到目前 Rancher 下的所有 Kubernetes 叢集。 這些安裝是 Rancher 內建強迫的，所以使用上會稍微跟正常用法有點不同。 ClusterRegistrationTokens: 最後一個則是 Cluster(Fleet Agent) 要加入到 Fleet Manager 使用的 Token，不過因為目前的叢集全部都是由 Rancher 去管理與創造的，所以不需要參考官網自行安裝，因此 Rancher 會走比較特別的方式來將 Rancher 管理的叢集給加入到 Fleet manager 中，因此這邊就是空的。 此外透過 Cluster 內的操作，可以將該 workspace 下的 cluster 給移轉到其他的 workspace，所以之後根據需求創立不同的 workspace 後，就可以透過這個方式將 cluster 給移轉到屬於該用途的 workspace。 到這邊為止，稍微看了一下關於 Fleet 介面的操作與基本概念，下一篇就會正式嘗試透過 Git Repo 這個物件來管理試試看 GitOps 的玩法，此外因為 Fleet 內的操作基本上都可以轉化為 Kubernetes 的 CRD 物件，所以很多 UI 的設定都可以使用 YAML 來管理，透過這個概念就可以達到跟 ArgoCD 一樣的想法，用 GitOps 來管理 Fleet 本身。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day25","content":"前言 前篇文章用很簡易的方式去探討如何使用 Fleet 的 GitOps 概念來管理資源，最後面用一個非常簡易的 Deployment 物件來展示如何讓 Fleet 將該資源部署到三個叢集中。 實務上使用的情境會更加複雜，譬如說 應用程式來源不同，有的是純 YAML 檔案，有的是透過 Helm 包裝，有的是透過 Kustomize 去包裝希望針對不同叢集有不同客製化，以 Helm 來說可能不同的叢集要給不同的 values.yaml 因此接下來的文章就會針對上述兩個概念來探討，到底於 Fleet 中要如何滿足上述要求。 GitRepo 掃描方式 Fleet 中要先準備 GitRepo 的物件，該物件中會描述 Git URL檔案的路徑來源 準備好該 GitRepo 物件後， Fleet 就會去掃描該 Git 專案並且掃描底下的路徑，接者從裡面找出可以使用的 Kubernetes 資源檔案。 實際上 Fleet 的運作邏輯更加複雜，因為 Fleet 支援下列幾種變化 原生 YAML 檔案Helm ChartKustomize 這三種變化大抵上可以分成五種不同的檔案來源，分別是 Chart.yamlkustomization.yamlfleet.yaml*.yamloverlays/{name} Helm Chart 本身又分成兩種部署方式，分別是 使用遠方的 Helm Server將 Helm Chart 直接放到 Git 專案中，所以專案內會有 charts.yaml, templates 等檔案。 將上述的概念整合請來大抵上就是 當路徑上有 Chart.yaml 檔案時，Fleet 就會認為要使用 Helm Chart 的概念去部署當路徑上有 kustomization.yaml 檔案時， Fleet 就會認為要使用 Kustomize 的方式來部署應用程式當路徑上有 fleet.yaml 檔案時， Fleet 會依照該檔案中的作法去部署，實務上都會使用 fleet.yaml 去描述部署的策略當路徑上沒有 Chart.yaml 與 kustomization.yaml 時，Fleet 就會使用最直覺的 Kubernetes 資源去部署overlays/{name} 這是針對(4)情況使用的客製化部署，是專門針對純 Kubernetes 資源的客製化。 前篇文章只有準備一個 deployment.yaml，所以就會踩到 (4) 這種部署方式。實務上最常使用的就是 fleet.yaml，fleet.yaml 可以直覺去呈現每個應用程式針對不同叢集的客製化設定，同時還可以做到混合的效果，譬如單純使用 Helm Chart, 單獨使用 Kustomize，或是先 Helm Chart 再 Kustomize 的混合部署。 所以可以知道 Fleet.yaml 可以是整個 Fleet 部署的重要精靈與靈魂，所有的應用程式都需要準備一個 fleet.yaml Fleet.yaml Fleet.yaml 是一個用來控制 Fleet 如何去處理當前資料夾下的 YAML 檔案，該用什麼方式處理以及不同的叢集應該要如何客製化。 每一個 fleet.yaml 都會被產生一個對應的 Fleet Bundle 物件，所以通常會將 fleet.yaml 放到每個應用程式的最上層路徑。 Fleet.Yaml 的詳細內容如下，接下來根據每個欄位介紹一下 defaultNamespace: default namespace: default kustomize: dir: ./kustomize helm: chart: ./chart repo: https://charts.rancher.io releaseName: my-release constraint version: 0.1.0 during values: any-custom: value valuesFiles: - values1.yaml - values2.yaml force: false paused: false rolloutStrategy: maxUnavailable: 15% maxUnavailablePartitions: 20% autoPartitionSize: 10% partitions: - name: canary maxUnavailable: 10% clusterSelector: matchLabels: env: prod clusterGroup: agroup clusterGroupSelector: agroup targetCustomizations: - name: prod namespace: newvalue kustomize: {} helm: {} yaml: overlays: - custom2 - custom3 specified, clusterSelector: matchLabels: env: prod clusterGroupSelector: matchLabels: region: us-east clusterGroup: group1 defaultNamespace/namespace defaultNamespace 代表的是如果 Kubernetes YAML 資源沒有標示 namespace 的話，會自動的被部署到這個 defaultNamespace 所指向的位置。 namespace 則是強迫將所有資源給安裝到某個 namespace 中，要特別注意的是如果目標 namespace 中已經有重複的資源的話，安裝可能會失敗，這點跟正常的 kubernetes 資源一樣。 kustomize: 熟悉 kustomize 的朋友一定都知道 kustomize 習慣上都會透過一個又一個資料夾搭配 kustomization.yaml 來客製化資源，對於 Fleet 來說給予一個相對的資料夾位置， Fleet 就會嘗試尋找該資料夾底下的 kustomization.yaml 並且客製化。 helm: Helm Chart 本身有兩種使用方式，一種是讀取遠方 Helm Server 上面的物件，一種是本地的 Helm 物件，因此 helm 格式內就有 chart/repo 等不同欄位要交互使用。 之後的文章都會有這些的使用範例，所以這邊就不詳細列出使用方式。 如果採用的是 helm server 的話，還可以指名想要安裝的版本，同時可以透過兩種不同的方式來客製化，一種是直接使用 values:.... 的格式來撰寫，這種方式適合少量客製化的需求，當客製化的數量過多時就推薦使用第二種 valuesFiles 的方式來載入客製化內容。 pause: Pause 的用途是讓 Fleet 單純做版本掃描確認有新版本，但是不會幫忙更新 Kubernetes 內的資源，管理人員需要自己手動從 UI 去點選 force update 來更新。 預設情況下都是 false，就代表 Fleet 不但會確認新舊差異也會幫忙更新資源。 rolloutStrategy: Fleet 的用途是管理大量叢集的部署，因此其提供的 rolloutStrategy 的選項來客製化叢集間的更新策略，基本上跟 Kubernetes Deployment 的更新策略非常雷同，同時間可以有多少個 Cluster 可以處於更新的狀態， 這個欄位中主要分成兩大類 如何將 Group 分類，稱為 Partition如何針對所有的 Group/Partition 去設定更新的比率 targetCustomizations: 這個欄位是整個 Fleet.yaml 最重要的部分 前述的 Helm/Kustomization 代表的是如何渲染當前路徑底下的 Kubernetes YAML 檔案。 而 targetCustomizations 則是要如何針對不同的 Kubernetes 叢集進行二次客製化 重要的是下方三個選項，如何選擇一個 Cluster，有三種不同方式 clusterSelectorclusterGroupSelectorclusterGroup 其中(2)/(3)兩個都是針對 Cluster Group 直接處理，所以如果有相同類似的 Cluster 就可以直接群組起來進行處理，不然就要使用第一種方式透過 selector 的方式去處理。 clusterSelector 的方式跟 Kubernetes 內大部分的資源處理一樣，都是透過 Label 的方式去處理。 Label 可以於 UI 方面透過點選的方式去加入這些 label，當然也可以透過 Terraform 去創建 RKE Cluster 的時候一起給予 Label。 從網頁要給予 Label 的話，就點選到 Cluster 頁面，找到目標的 Cluster ，點選 Edit Config/Edit YAML 都可以。 接者於畫面中去填寫想要使用的 Label，這些 Label 就可以於 fleet.yaml 去客製化選擇。 如果想要嘗試 Cluster Group 的話也可以嘗試將不同的 Cluster 給群組起來，之後的範例都可以嘗試看看。 簡單範例 以下範例節錄自官網範例 namespace: fleet-mc-helm-external-example helm: chart: https://github.com/rancher/fleet-examples/releases/download/example/guestbook-0.0.0.tgz targetCustomizations: - name: dev helm: values: replication: false clusterSelector: matchLabels: env: dev - name: test helm: values: replicas: 3 clusterSelector: matchLabels: env: test - name: prod helm: values: serviceType: LoadBalancer replicas: 3 clusterSelector: matchLabels: env: prod 上述的 fleet.yaml 非常直覺 使用 遠方的 Helm Chart 檔案作為目標來源使用 env 作為 label 來挑選三個不同的 cluster每個 cluster 使用時都會設定不同的 value 內容。 下篇文章就會嘗試使用這些概念來實際部署應用程式","keywords":"","version":"Next"},{"title":"Summary","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day30","content":"Summary Rancher x Fleet 系列文到此告一個段落，這系列文中探討四大概念，包含 Rancher 基本知識Rancher 管理指南Rancher 應用程式部署GitOps 部署 Rancher 基本知識 Kubernetes 作為一個容器管理平台，這幾年的聲勢不減反升，愈來愈多的團隊想要嘗試導入 Kubernetes 來替換應用程式部署的底層架構。Kubernetes 不是萬靈丹，並不是所有的情境與環境都適合使用 Kubernetes，但是一旦經過評估確認想要使用 Kubernetes 後就會面臨到下一個重大問題，就是該 Kubernetes 叢集要怎麼安裝與管理? 從 On-Premise 到雲端環境，從手工架設到使用付費 Kubernetes 服務都是選項之一，這種情況下團隊需要花更多心力與時間去思考到底要走哪一個方式，畢竟每個方式都有不同的優缺點。 Rancher Labs 是一個針對 Kubernetes 生態系開發許多工具的強大團隊，譬如 Rancher Kubernetes Engine (RKE): 客製化的 Kubernetes 環境K3s: 輕量級的 Kubernetes 叢集，適合物聯網環境Fleet: 針對 Kubernetes 的 GitOps 解決方案Longhorn: 持久性儲存的解決方案 除了可以將現存的 Kubernetes 叢集讓 Rancher 託管，更多的使用方式是讓 Rancher 直接創造基於 RKE 版本的 Kubernetes 叢集，因為 RKE 叢集才可以真正發揮 Rancher 內的所有功能。 官方文章有專門的文章 Best Practices Guide 探討如何針對生產環境部署一個最適當的 Rancher，接者如何透過這套 Rancher 來託管與創建不同的 Kubernetes 叢集。 透過 docker 可以很輕鬆的部署一個 Rancher，該環境非常適合測試與評估使用，但是如果要將 Rancher 給導入到正式環境的話，就會希望能夠透過一套 RKE 叢集來維護 Rancher 服務。 Rancher 管理 Rancher 本身是個 Kubernetes 管理平台，因此其系統架構的設計有非常多層級的概念 管理 Rancher 服務本身的功能管理 Kubernetes 叢集本身的功能管理 Kubernetes Project 的功能 有了這些基本概念後去閱讀官方文件就會更加理解到底官方文件的編排與含義。 此外，Rancher 基於 RBAC 的方式針對不同的使用者可以設定不同的權限。 使用者的認證除了預設的內建資料庫外，也支援不同的外部服務，如 Azure/GSuite/Keycloak 等不同機制 因此使用 Rancher 時也要特別注意 RBAC 的設定，避免所有 Rancher 的使用者都共享一套 admin 的帳號來操作。 IaC Rancher 本身除了透過 UI 大量操作外，也可以透過 Terraform/Pulumi 這些 IaC 工具來設定，因此一個比較好的模式是推薦使用這類型的工具來操作與管理 Rancher 本身，同時將這些操作與系統中的 CI/CD pipeline 給結合，這樣所有的變更可會更加透明且也能夠透過 CI/CD 的入口當作 Single Source of Truth 的概念 應用程式部署 透過 Rancher 準備好一套可用的 Kubernetes(RKE) 叢集後，接下來可以透過很多種方式去管理叢集上的應用程式。 譬如直接取得 Kubernetes 的 KUBECONFIG，擁有該檔案的任何人都可以直接使用 helm/kubectl 等指令進行操作來安裝各種 Kubernetes 的資源到目標叢集內。 如果想要妥善利用 Rancher 的設計的話，就可以考慮使用 Rancher 內的機制 (Catalog/App) 來安裝應用程式，透過 Rancher 的機制來安裝應用程式會於 UI 方面有更好的呈現與整合，同時使用上可以避免 Kubeconfig 的匯出，可以統一都使用 Rancher API Token 進行存取即可。 如果對於這種手動部署感到厭煩的，也可以嘗試看看 Rancher v2.5 正式推出的 GitOps 解決方案， Rancher Fleet。 透過 Rancher Fleet 的幫助，管理者可以講所有要部署的資源都存放到一個 Git 專案，同時 Fleet 支援數種不同的應用程式客製化。 除了常見的 Helm 及 Kustomize 外， Fleet 還支援將 Helm 與 Kustomize 一起使用，針對使用外部 Helm Chart 的情境特別好用。 不過 Rancher Fleet 目前還在茁壯發展中，因此使用上難免會遇到一些 Bug，這部分都非常歡迎直接到官方 Github 去回報問題，透過社群的幫忙官方才更有機會將這些問題給修復。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day24","content":"前言 前篇文章探討了 Fleet 的基法用法與操作介面，而本文將要正式踏入到 Rancher Fleet GitOps 的世界中 為了使用 GitOps 來部署，必須要先準備一個 Git 的專案來放置要部署的資源， 本篇文章使用的範例都會放到我準備的一個公開 Git Repo Fleet Demo Workspace 前述提到大部分 Rancher Fleet 的資源都是基於 Kubernetes CRD 去描述的，因此除了透過網頁操作外也是可以準備一個相關的 YAML 檔案，只要將該 YAML 給部署到 Kubernetes 內， Fleet Controller 就會根據該資源進行對應的操作與更新。 一開始先從簡單的部分開始練習，嘗試透過 GitOps 幫忙創建與管理 Workspace，預設的情況下叢集會有 fleet-default 與 fleet-local 這兩個 workspace，所以目標是想要創造兩個不同的 workspace，分別是 prod 以及 testing. Workspace 的 YAML 非常簡單，一個範例如下 apiVersion: management.cattle.io/v3 kind: FleetWorkspace metadata: name: prod namespace: prod 創建一個 FleetWorkspace 的物件，並且針對 name/ns 給予相對應的資料即可，因此針對兩個 workspace 準備兩個檔案，並且將這兩個檔案放到 git 專案下的 workspace 資料夾底下。示意圖如下。 ╰─$ tree . . └── workspace ├── production.yaml └── testing.yaml 將上述內容給推向遠方 Git 專案中後，下一步就是要讓 Fleet 知道請追蹤這個 Git Repo 並且將相關的內容給部署到 Kubernetes 內。 切換到 Fleet 的介面，選擇到 Fleet-Local 這個 workspace 並且於 GitRepo 的頁面中點選創立，這時候可以看到如下的介面 該介面中我們需要設定幾個資訊 GitRepo 物件的名稱Git 專案的 URLGit 專案的 BranchGit 專案是否需要透過權限去純取。要從 Git 專案中的哪個位置去尋找相關 YAML 檔案。 因為示範專案是公開的，所以(4)可以直接忽略。 第五點要特別設定成 workspace/，因為前述我們將兩個 workspace 的 YAML 放到 workspace 資料夾底下。 創立完畢後就會看到系統上創建了一個名為 workspace 的 Git 物件，該物件的狀態會從 GitUpdating 最後變成 Active。 由於 fleet-local 這個 workspace 中只有一個 cluster，也就是 local，因此剛剛創立的 GitRepo 只會將相關資源給安裝到這個 local cluster 中，所以可以看到圖中顯示的 Clusters Ready 標示為 1。 點選 workspace 這個資源進去可以看到更多關於該 GitRepo 的資訊，譬如相關的資源有哪些。 範例中可以看到底下提供了兩個屬於 FleetWorkspace 的物件，分別為 prod 以及 testing，這兩個物件都安裝到對應的 namespace 中。 之前也有提過針對每個 GitRepo 所掃描出來的物件都會創造出一個最基本的 Bundle 物件，該物件會描述這個應用程式的所有內容。所以切換到 Bundle 介面去尋找 workspace，可以看到如下的範例。 該 bundle 會把所有要安裝的資源都集中起來，同時因為這次的範例非常簡單，沒有要針對任何 Cluster 去客製化與過濾，所以 targets/targetRestrictions 都是空白的。 此時點選 workspace 的介面或是上方的選單，會發現先前描述的 testing 與 prod 這兩個 workspace 已經被自動創立了，這意味者 Fleet 已經自動地從 Git 專案中學習到要部署什麼資源，並且把資源給成功的部署到 Kubernetes 內，最後的結果也如預期一樣。 用 Fleet 管 GitRepo 下一個範例就是希望透過 Fleet 管理 GitRepo 物件，畢竟能夠盡可能減少 UI 操作是追求自動化過程中不可避免的一環。 首先到 GitRepo 中將該 Workspace 的物件移除，移除後可以觀察到 Bundle 中關於 prod/testing 的物件都不見，同時 workspace 中只剩下 fleet-local 以及 fleet-default. 為了讓 Fleet 幫忙管理，我們需要準備一個描述 GitRepo 的 YAML 檔案，將該檔案放到專案中的 repos/local 底下。 kind: GitRepo apiVersion: fleet.cattle.io/v1alpha1 metadata: name: fleet-demo namespace: fleet-local spec: repo: https://github.com/hwchiu/fleet_demo.git branch: master paths: - workspace/ targets: - clusterSelector: {} 該 Yaml 描述的內容跟前述透過 UI 創建 GitRepo 是一致的，當系統中有愈來愈多應用程式要管理的時候，就修改該物件，讓 Paths 指令更多路徑即可。 準備好該物件後，接下來還是要到 Fleet UI 去創建一個 GitRepo 物件，該 GitRepo 物件是用來幫忙管理所有 GitRepo 物件的，因此必須要先手動創建一次，接下來就可以依賴 GitOps 的流程幫忙管理。 這邊先創造一個新的 GitRepo 物件，該物件指向 repos/local。 所以整個流程就會變成 Fleet 去讀取 Git 專案底下 repos/local 內的物件repos/local 內的物件被套用到 Kubernetes 後就會產生另外一個名為 fleet-demo 的 GitRepo 物件fleet-demo 物件會再次的去把專案內的 workspace/ 給抓進來進行後續安裝。 一切準備完畢後，會觀察到 GitRepo 列表呈現的如下圖，會有兩個 GitRepo 這時候如果點進去 fleet-demo 這個 GitRepo，可以看到該 GitRepo 會部署兩個 workspace，同時最上方還有一個額外的 label，該 label 是由 helm 產生的。 前述提過 Fleet 會將所有資源都動態的轉換為 Helm 格式。 轉移 Cluster 創立好兩個不同的 workspace 後，可以嘗試將三個預先創立的 k8s cluster 給搬移過去，舉例來說將 rke-qa 以及 ithome-dev 這兩套叢集搬移到 testing workspace，而 rke-it 則搬移到 prod workspace。 下一步就是真正實務上的需求，部署應用程式。為了讓 Fleet 安裝應用程式，所以也需要幫忙準備一個 GitRepo 的物件。針對 testing 以及 prod 各準備一個，並且依序放到 repos/testing, repos/prod 底下。 ╰─$ cat prod/app-basic.yaml kind: GitRepo apiVersion: fleet.cattle.io/v1alpha1 metadata: name: prod-app namespace: prod spec: repo: https://github.com/hwchiu/fleet_demo.git branch: master paths: - app/basic targets: - clusterSelector: {} ╰─$ cat testing/app-basic.yaml kind: GitRepo apiVersion: fleet.cattle.io/v1alpha1 metadata: name: testing-app namespace: testing spec: repo: https://github.com/hwchiu/fleet_demo.git branch: master paths: - app/basic targets: - clusterSelector: {} 目前系統上還沒有 app/basic 資料夾，可以先不用管它。 上述兩個 GitRepo 的差異處有兩個 名稱不同安裝的 namespace 不同，注意這邊的 namespace 要跟 workspace 的名稱一致。 接者我們要讓最原始的 repos 一起幫忙處理這兩個 GitRepo，將 repos/local 底下的檔案修改為 ╰─$ cat local/repos.yaml kind: GitRepo apiVersion: fleet.cattle.io/v1alpha1 metadata: name: fleet-demo namespace: fleet-local spec: repo: https://github.com/hwchiu/fleet_demo.git branch: master paths: - workspace/ - repos/testing - repos/prod targets: - clusterSelector: {} 這時候整個專案呈現如下 ╰─$ tree -l . ├── repos │ ├── local │ │ └── repos.yaml │ ├── prod │ │ └── app-basic.yaml │ └── testing │ └── app-basic.yaml └── workspace ├── production.yaml └── testing.yaml 當這些修改都推到遠方 Git 專案後就會觀察到 fleet-local 下的兩個 GitRepo 物件都變成 NotReady 的狀態，如下 以 Fleet 的角度來解釋就是 fleet-demo 這個 GitRepo 本身會希望安裝三個路徑底下的物件，分別是 workspace/ repos/testing, repos/prod, 只要其中有一個沒有順利部署完成，身為老爸的 fleet-demo 也就自然不能說自己完成fleet-demo 這個物件是由 repos 這個 GitRepo 去動態產生的，因此 fleet-demo 本身沒有順利完成的話，repos 物件也沒有辦法說自己順利完成。 接者點進去 fleet-demo 看一下到底是什麼物件沒有順利完成，可以看到剛剛創立的 prod-app 以及 testing-app 這兩個物件也都沒有完成，所以跟 workspace 無關。 這時候切換到 testing 的 workspace，可以觀察到系統上的 testing-app GitRepo 是呈現紅色的字眼，叫做 Git Updating。 同時最上方有顯示相關錯誤訊息，告知使用者為什麼該專案目前不能正常運作。 其訊息告知 Fleet 沒有辦法從 專案底下的 app/basic 路徑找到可以用的 Kubernetes 物件，因此沒有辦法順利安裝資源，所以標示為錯誤。 為了解決這個問題，我從官方範例中複製了一個簡單的 Deployment 物件，將該物件給放到 apps/basic 底下。 apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: selector: matchLabels: app: guestbook tier: frontend replicas: 3 template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google-samples/gb-frontend:v4 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 將這個物件推向遠方的 Git 專案後就可以觀察到 testing-app 成功順利的安裝物件到叢集中，由於 testing 的 workspace 底下有兩個不同的 cluster， ithome-dev 以及 rke-qa。 所以這個物件就會自動的安裝到這兩個叢集中。 這時候如果去 cluster explorer 的介面可以看到 deployment 中有一個名為 frontend 的 deployment 物件被創建出來。 本篇文章到這邊為止，我們嘗試透過 Fleet GitOps 的方式來管理 Fleet 本身並且部署了第一個應用程式，下一篇文章將來探討如何針對不同的 Cluster 給予不同的客製化，譬如 ithome-dev 跟 rke-qa 可以使用不同的參數或是檔案來部署。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day26","content":"前言 前篇文章探討了 Fleet.yaml 的基本概念，而本篇文章就會針對各種不同的情境來示範如何使用 fleet.yaml 來達到客製化的需求。 本篇所有 YAML 範例都來自於官方範例。 Overlay 第一個要示範的情境是使用純 Kubernetes YAML 為基礎的客製化，因為純 Kubernetes YAML 沒有辦法達到類似 Helm/Kustomize 的內容客製化，所以能夠提供的變化有限，頂多只能做到不同檔案的資源部署。 假設今天總共有四種資源，該四種資源為 Deployment AService ADeployment BService B 希望達到的客製化為 Dev 叢集安裝 Deployment AService A IT 叢集安裝 Deployment AService ADeployment B QA 叢集安裝 Deployment AService ADeployment BService B 前述有提過，對於純 YAML 來說，Fleet 提供一個名為 overlay 的資料夾來客製化，因此先於專案中的 app 底下創建一個 basic_overlay 的資料夾。 由於 deployment A 這個資源三個叢集都需要，所以可以放到最外層，讓所有叢集共享，只需要針對(2)/(3)/(4) 進行客製化即可。 客製化的作法很簡單，於 overlays 底下創建不同的資料夾，然後於資料夾中放置想要客製化的檔案即可。 這時候的架構應該會長得很類似下圖 ╰─$ tree . . ├── fleet.yaml ├── frontend-deployment.yaml └── overlays ├── dev │ └── frontend-service.yaml ├── it │ ├── frontend-service.yaml │ └── redis-master-deployment.yaml └── qa ├── frontend-service.yaml ├── redis-master-deployment.yaml └── redis-master-service.yaml 這邊先忽略 fleet.yaml 的內容，仔細看剩下的內容。 frontend-deployment 就是 deployment A 的服務，而 overlays 底下的資料夾對應了三個不同的叢集，每個叢集內都放置更多的資源。 譬如 dev 底下多了 frontend-service，就是所謂的 Service A it 相較於 dev 又新增了 redis-master-deployment.yaml, 也就是 Deployment B qa 相較於 it 又新增了 redis-master-service.yaml, 也就是 service B 這些檔案都準備完畢後，接下來要做的就是準備一個 fleet.yaml 的檔案，讓 Fleet 要針對不同叢集讀取不同環境。 前述提到 Fleet.yaml 中會透過 label 的方式來選擇目標叢集，沒有特別設定的情況下，每個叢集會有一些預設的 label 可以使用。切換到該叢集並且以 YAML 方式瀏覽就可以觀察到這些預設 label. 上圖中呈現了三種 label，其中第一種是比較適合人類閱讀的，該 label 呈現了叢集的名稱，key 為 management.cattle.io/cluster-display-name 所以接下來 fleet.yaml 中就可以透過這個 label 來比對不同的 cluster。 namespace: basicoverlay targetCustomizations: - name: dev clusterSelector: matchLabels: management.cattle.io/cluster-display-name: ithome-dev yaml: overlays: - dev - name: it clusterSelector: matchLabels: management.cattle.io/cluster-display-name: rke-it yaml: overlays: - it - name: qa clusterSelector: matchLabels: management.cattle.io/cluster-display-name: rke-qa yaml: overlays: - qa 上述是一個 Fleet.yaml 的範例，該 Fleet.yaml 希望將所有資源都安裝到 basicoverlay 這個 namespace 中。 接者於 targetCustomizations 的物件中，針對三個不同環境撰寫不同的 clusterSelector。 範例中使用 cluster-name 來比對，符合 ithome-dev 使用 overlays 這個語法來讀取特定的環境，將 overlays/dev 中的資料夾一併納入部署。 剩下兩個環境如法炮製，一切都準備完畢之後，最後修改 repo/prod/app-basic.yaml 以及 repo/testing/app/app-basic.yaml 讓其知道要掃描 app/basic_overlay 這個路徑。 ╰─$ cat repos/prod/app-basic.yaml kind: GitRepo apiVersion: fleet.cattle.io/v1alpha1 metadata: name: prod-app namespace: prod spec: repo: https://github.com/hwchiu/fleet_demo.git branch: master paths: - app/basic - app/basic_overlay targets: - clusterSelector: {} ╰─$ cat repos/prod/app-basic.yaml kind: GitRepo apiVersion: fleet.cattle.io/v1alpha1 metadata: name: prod-app namespace: prod spec: repo: https://github.com/hwchiu/fleet_demo.git branch: master paths: - app/basic - app/basic_overlay targets: - clusterSelector: {} 一切準備完畢後就將修改給推到遠方的 Git 專案，然後靜靜等者 Fleet 開始處理。 當 Testing workspace 底下的 GitRepo 呈現 Active 後就代表環境已經部署完畢了。 這時候點選進去可以看到更為詳細的內容，因為 Fleet 還是一個非常嶄新的專案，我認為其還有很多值得改近的地方，譬如當前的 UI 就會將 Fleet.yaml 中描述的所有資源都一起放進來，而不是針對叢集客製化的資源去顯示，這一點會容易混淆人。希望下一個版本 (v0.3.6) 有機會修復。 一切完畢後透過 kubectl 去觀察三個叢集下 basicoverlay 內的資源變化 dev 的叢集可以看到只有 deployment A 配上 service A 的資源 it 叢集除了 dev 叢集的資源外，還多了 deployment B 也就是 redis-master 的 Pod. qa 叢集則最完整，擁有 Deployment(A,B) 以及 Service (A,B) 下篇文章將針對 Kustomize 的範例介紹","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day27","content":"前言 前篇文章探討了基於純 Kubernetes YAML 的客製化行為，因為純 Kubernetes YAML 沒有辦法針對檔案內的 YAML 客製化，只能使用不同的檔案來部署，如果想要針對檔案內容進行客製化，這時候就要使用 Kustomize 或是 Helm 等技術來處理，本篇文章就來看看如何透過 Kustomize 來客製化應用程式。 本篇所有 YAML 範例都來自於官方範例。 Kustomize 這邊簡單說明一下 Kustomize 的概念， Kustomize 是基於 Patch 的概念去客製化 YAML 內容。 Patch 意味者環境中必須要先擁有一個基本檔案，接者還要有一個描述差異的檔案， Patch 就是將這個差異處給蓋到這個基本檔案上。 這樣就可以達到一個客製化。 舉例來說有一個 Deployment 如下，該檔案就是所謂的基本檔案(Base)。 apiVersion: apps/v1 kind: Deployment metadata: name: redis-slave spec: selector: matchLabels: app: redis role: slave tier: backend replicas: 2 template: metadata: labels: app: redis role: slave tier: backend spec: containers: - name: slave image: gcr.io/google_samples/gb-redisslave:v1 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 接者要準備一個描述差異處的檔案(patch) kind: Deployment apiVersion: apps/v1 metadata: name: redis-slave spec: replicas: 0 該差異處希望將 redis-slave 的 replicas 從 2 修改成 0。 Kustomize 基於這種概念去描述所有檔案，環境要先準備一個名為 kustomization.yaml 的檔案，該檔案會告訴 kustomize 要去哪邊尋找 base 檔案，要去哪邊尋找 patch 檔案，最後將這兩者結合產生出最終檔案，譬如 resources: - ../../base patches: - redis-slave-deployment.yaml - redis-slave-service.yaml 上述範例是告知 Kustomize 請到 ../../base 去找尋所有的基本 YAML 檔案(base)，接者使用當前資料夾底下的兩個檔案作為 patch，該 patch 就會嘗試跟 base 中相對應的內容進行合併最後產生出差異化。 有了基本概念之後，接下來就準備來使用 Kustomize 客製化應用程式，這次繼續使用類似上次的應用程式。 假設環境中依然有四個資源，分別是 Deployment AService ADeployment BService B 這次三個叢集都會部署這四個資源，不過會透過 Kustomize 來客製化調整內容，這些參數於 base 環境中的預設值如下 Deployment A: Replica: 1Service A: Type: ClusterIPDeployment B: Replica: 1Service B: Type: ClusterIP dev 叢集 Deployment A -&gt; Replica: 2Deployment B -&gt; Replica: 2 it 叢集 Deployment A -&gt; Replica: 3Deployment B -&gt; Replica: 3Service B -&gt; NodePort qa 叢集 Deployment A -&gt; Replica: 1Deployment B -&gt; Replica: 3Service A -&gt; NodePort 有了基本概念後，就準備來修改 fleet_demo 的專案內容，先於 app 底下創建一個資料夾為 kustomize，並且先準備好基本資料夾。 ╰─$ tree . . ├── base └── overlays ├── dev ├── it └── qa 首先來處理 base 資料夾，該資料夾中總共要放五個檔案，分別是四個 Kubernetes 資源以及一個 kustomization.yaml，該 kustomization.yaml 主要是讓 kustomzie 知道有哪些檔案要載入去部署。 ╰─$ tree . . ├── frontend-deployment.yaml ├── frontend-service.yaml ├── kustomization.yaml ├── redis-master-deployment.yaml └── redis-master-service.yaml 上述內容如下 ╰─$ cat frontend-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: selector: matchLabels: app: guestbook tier: frontend replicas: 1 template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google-samples/gb-frontend:v4 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 ╰─$ cat frontend-service.yaml apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: type: ClusterIP ports: - port: 80 selector: app: guestbook tier: frontend ╰─$ cat redis-master-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: redis-master spec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: labels: app: redis role: master tier: backend spec: containers: - name: master image: redis resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 ╰─$ cat redis-master-service.yaml apiVersion: v1 kind: Service metadata: name: redis-master labels: app: redis role: master tier: backend spec: type: ClusterIP ports: - port: 6379 targetPort: 6379 selector: app: redis ╰─$ cat kustomization.yaml resources: - frontend-deployment.yaml - frontend-service.yaml - redis-master-deployment.yaml - redis-master-service.yaml 準備好五個檔案後，接下來就是針對不同客製化環境去準備相關的 Patch ╰─$ tree overlays 1 ↵ overlays ├── dev │ ├── frontend-deployment.yaml │ ├── kustomization.yaml │ └── redis-master-deployment.yaml ├── it │ ├── frontend-deployment.yaml │ ├── frontend-service.yaml │ ├── kustomization.yaml │ └── redis-master-deployment.yaml └── qa ├── frontend-deployment.yaml ├── frontend-service.yaml ├── kustomization.yaml ├── redis-master-deployment.yaml └── redis-service.yaml 準備好的架構如下，這邊只列出 it 環境底下的客製化內容，其餘兩個的修改都非常類似。 ╰─$ cat kustomization.yaml resources: - ../../base patches: - frontend-deployment.yaml - redis-master-deployment.yaml - frontend-service.yaml ╰─$ cat frontend-deployment.yaml kind: Deployment apiVersion: apps/v1 metadata: name: frontend spec: replicas: 3 ╰─$ cat frontend-service.yaml kind: Service apiVersion: v1 metadata: name: frontend spec: type: NodePort ╰─$ cat redis-master-deployment.yaml kind: Deployment apiVersion: apps/v1 metadata: name: redis-master spec: replicas: 3 該環境中準備了三個不同的 Patch 檔案，並且於 kustomization.yaml 中去描述 base 的來源(../../base)，同時針對當前的環境去使用三個不同的 patch 檔案。 一切準備完畢後接下來就是 fleet.yaml 的環境。 ╰─$ cat fleet.yaml namespace: appkustomize targetCustomizations: - name: dev clusterSelector: matchLabels: management.cattle.io/cluster-display-name: ithome-dev kustomize: dir: overlays/dev - name: it clusterSelector: matchLabels: management.cattle.io/cluster-display-name: rke-it kustomize: dir: overlays/it - name: qa clusterSelector: matchLabels: management.cattle.io/cluster-display-name: rke-qa kustomize: dir: overlays/qa fleet.yaml 的內容跟前述差不多，唯一的差別之前是透過 yaml.overlay 的方式去處理純 Kubernetes YAML，而 Kustomize 則是改成使用 kustomize.dir 來描述目標叢集要以哪個資料夾當作 Kustomize 的起始資料夾。 一切都準備完畢後就將修改的內容推到遠方的 Git，接者就繼續等 Fleet 去處理。 題外話： Fleet 有時候處理上還不夠完善，有可能 UI 跟底層部署沒有同步，底層資源都完畢但是 UI 會顯示 Not-Ready，這種情況下可以嘗試將該 Bundle 給刪除，讓 GitRepo 重新產生一個全新的 Bundle 即可。 部署完畢後透過 kubectl 去觀察三個叢集，是否都如同預期般的部署。 Dev 叢集希望兩種 Deployment 的 Replica 都是 2，且 service 維持預設的 type: ClusterIP. IT 叢集希望兩種 Deployment 的 Replica 都是3，同時將 frontend 的 service type 改成 NodePort. QA 叢集將兩種 deployment 的 replica 改成 1,3 同時兩種 service type 都改成 NodePort. 下篇文章將針對 Helm 的用法繼續探討","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day28","content":"前言 前篇文章探討了基於 Kustomize 的客製化行為，另外一個常見的應用程式處理方式就是 Helm， Helm 採用的是基於 template 的方式來動態渲染出一個可用的 YAML。 本篇文章就會探討兩種不同使用 Helm 的方式。 本篇所有 YAML 範例都來自於官方範例。 本地 Helm Chart Helm Chart 基於 go-template 的方式來客製化 Yaml 內的數值，這意味者 Helm Chart 本身所擁有的 YAML 其實大部分情況下都不是一個合法 YAML，都需要讓 Helm 動態的將相關數值給填入到 Helm YAML 中來產生最後的檔案內容。 下方是一個 Helm YAML 的範例 apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: selector: matchLabels: app: guestbook tier: frontend replicas: {{ .Values.replicas }} template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google-samples/gb-frontend:v4 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 可以看到 replicas 的部分抽出來，變成一個 template 的格式，搭配下方的 values.yaml replicas: 1 Helm 就會動態的將 replicas 的數值給填入到上方的 template 來產生最終要送到 Kubernetes 內的 YAML 物件。 有了基本概念之後，就可以來看看如何透過 Fleet 來管理 Helm 的應用程式。 這次的應用程式會直接使用 Helm 內建的範例應用程式，透過 helm create $name 就可以創建出來 所以移動到 app 資料夾底下，輸入 helm create helm 即可 ╰─$ helm create helm Creating helm ╰─$ tree helm helm ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml 該 Helm 的範例應用程式會部署一個 nginx 的應用程式，並且為其配上一個 service + ingress 的服務。 這次希望透過 Fleet 為兩個不同的 workspace 去部署不同的環境，意味者 testing workspace 底下的兩個叢集(dev/qa) 採用一組設定，而 prod workspace 底下的 it 叢集採用另外一組設定。 修改的部分採取簡單好理解即可 針對 testing 的環境 replica 設定為 2 份， prod 的環境 replica 為 3開啟 Ingress 物件 為了完成這件事情針對 workspace 下多個 cluster 統一設定，必須要先完成下列之一 將群組給 group 起來給叢集有對應的 Label 因此先到 UI 部分將 Prod Workspace 底下的 cluster (rke-it) 給予一個 env=prod 的 Label. 接者到 testing workspace 底下創建一組 Cluster Group，創建 Cluster Group 的時候可以根據條件去抓到符合條件的 Cluster，預設情況下沒有設定的話就是全抓。 系統還會告訴你目前有多少個 Cluster 符合當前的 Selector，以我們的環境來說該 workspace 底下有兩個不同的 cluster。 一切準備就緒後就來準備 Fleet.yaml ╰─$ cat fleet.yaml namespace: helminternal targetCustomizations: - name: prod helm: values: replicaCount: 3 ingress: enabled: true hosts: - host: rancher.hwchiu.com paths: - /testing clusterSelector: matchLabels: env: prod - name: test helm: values: replicaCount: 2 clusterGroup: testing-group 因為 Fleet 會自己偵測若路徑中有 Chart.yaml 檔案的話，就會使用 Helm 的方式去處理，所以不需要特別於 Fleet.yaml 中去描述需要使用 Helm。 這次的範例會安裝到 helminternal 這個 namespace 中，接者底下針對兩種不同的客製化。 如果 cluster 本身含有 env:prod 這種標籤的話，就會將其的複本數量設定為 3 個，並且將 ingress 給設定為 enable，為了讓 Ingress 物件可以順利創立，需要針對 hosts 底下的物件也給予設定，這邊隨便寫就好，目的只是測試 ingress 物件的部署。 另外一個則是直接使用 testing-group 這個 cluster group，對其底下的所有 cluster 都設定副本數為 2 。 記得也要對 repo/*/app-basic.yaml 兩個檔案去增加 app/helm 的路徑，這樣 GitRepo 才知道也要去掃描 app/helm 的路徑。 一切都部署完畢後，使用 kubectl 去觀察部署的資源，可以觀察到 rke-it 這個屬於 prod workspace 的叢集被部署了三個副本的 deployment 外加一個 ingress 資源。 至於 testing workspace 下的兩個叢集的部署資源都一致，都只有兩個副本的 deployment，沒有任何 ingress 物件。 遠方 Helm Chart 實務上並不是所有部署到團隊中的 Helm Chart 都是由團隊自行維護的，更多情況下可能是使用外部別人包裝好的 Helm Chart，譬如 Prometheus-operator 等。 這種情況下專案的路徑內就是透過 fleet.yaml 來描述要使用哪個遠方的 Helm Chart 以及要如何客製化。 這邊直接使用官方 Helm-External的範例 來操作。 首先先於 app 資料夾底下創建一個 helm-external 的資料夾，因為這次不需要準備 Helm Chart 的內容，所以直接準備一個 fleet.yaml 的檔案即可。 fleet.yaml 內容如下 ╰─$ cat fleet.yaml namespace: helmexternal helm: chart: https://github.com/rancher/fleet-examples/releases/download/example/guestbook-0.0.0.tgz targetCustomizations: - name: prod helm: valuesFiles: - prod_values.yaml clusterSelector: matchLabels: env: prod - name: test helm: valuesFiles: - testing_values.yaml clusterGroup: testing-group 如果要使用遠方的 Helm Chart，總共有兩種不同的寫法，一種是參考上述直接於 chart 中描述完整的下載路徑。 針對一般的 Helm Chart Server 來說會更常使用下列這種形式 helm: repo: https://charts.rancher.io chart: rancher-monitoring version: 9.4.202 這種形式更加容易理解要去哪個 Helm Chart 抓取哪個版本的 Helm Chart 應用程式。 接者這次的 fleet.yaml 要採用不同的方式去進行客製化，當 Helm Values 的客製化非常多的時候，有可能會使得 fleet.yaml 變得冗長與複雜，這時候可以透過 valuesFiles 的方式，將不同環境用到的 values 內容給獨立撰寫成檔案，然後於 fleet.yaml 中將該檔案給讀取即可。 ╰─$ cat prod_values.yaml serviceType: NodePort replicas: 3 ╰─$ cat testing_values.yaml serviceType: ClusterIP replicas: 2 上述兩個設定檔案就是針對不同副本去處理，同時 prod 環境下會將 service 給轉換為 NodePort 類型。 一切完畢後記得修改 repo/*/app-basic.yaml 內的路徑。 注意的是這邊的 replica/serviceType 只會影響 Helm 裏面的 frontend deployment/service，純粹是對方 helm chart 的設計。 部署完畢後透過 kubectl 觀察部署的狀況 可以觀察到 prod workspace 底下的 rke-it 叢集內的確將 frontend 的 replica 設定成 3個，同時 frontend 的 service 也變成 NodePort 而剩下兩個叢集也符合預期的是一個兩副本的 deployment 與 ClusterIP 下篇文章將來探討 fleet 最有趣的玩法，Helm + Kustomize 兩者結合一起運行。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day29","content":"前言 前篇文章探討了基於 Helm 的客製化行為，透過兩種不同方式來部署 Helm 的應用程式，透過 Helm Values 的設定可以讓管理人員更容易的去設定與處理不同的設定。 實務上如果採用的是遠方 Helm Chart Server 的部署方式，還是有機會遇到綁手綁腳的問題，譬如想要針對某些欄位客製化，但是該 Helm Chart 卻沒有定義等 這時候就可以補上 Kustomize 來進行二次處理，應用程式先透過 Helm 進行第一層處理，接者透過 Kustomize 進行二次處理，幫忙 Patch 一些不能透過 values.yaml 控制的欄位。 本篇所有 YAML 範例都來自於官方範例。 Helm + Kustomize 使用遠方 Helm Chart 來部署應用程式的人可能都會有下列的經驗 Helm Values 沒有提供自己想要的欄位如果該 Helm Chart 裡面需要 secret 的物件，需要自己額外部署，沒有辦法跟該 Helm Chart 融為一體。 不同問題會有不同解決方法，譬如 嘗試針對該 Helm Charts 提交 PR 去增加更多的 values 欄位可以使用。這種情況的解法比較漂亮，但是要花比較長的時間來處理程式碼的合併與審核。複製遠方的 Helm Chart 到本地環境中，手動修改欄位符合自己需求，沒有將修改推回遠方的 upstream.團隊內創造一個全新的 Helm Chart，該 Helm Chart 透過 requirement 的概念來使用本來要用的 Helm Chart，接者於自己的環境中補上其他資源。 如果 Helm Values 沒有提供自己想要的欄位，那(1)/(2) 這兩種解法都可以處理，畢竟都有能力針對本來的 Helm YAML 進行改寫。 但是如果今天的需求是想要加入一些全新的 YAML 檔案，譬如上述的 Secret 物件，那(1)/(2)/(3) 三種方法都可以採用。 第一個方法需要花時間將修改合併到 upstream 的專案，而第二個方法其實維護起來很麻煩，因為每次遠方有任何版本更新時都要重新檢查。第三個方法又不能針對 values 的方式去客製化。 Fleet 中提供了一個有效的方式來解決這個困境，就是 Helm + Kustomize 的組合技 透過 Helm 進行第一次的渲染，接者透過 Kustomize 的方式可以達到 動態增加不同的 Kubernetes 物件透過 Kustomize 的 Patch 方式可以動態修改欄位 因此上述的情境問題就可以完美解決。 註: Kustomize 今年的新版本也嘗試提供 Helm 的支援，讓你可以透過 Kustomize 的方式去部署 Helm 的應用程式，詳細可以參考 kustomization of a helm chart 本次的範例繼續使用前篇文章的 Helm Chart，使用 Rancher 提供的 guestbook 作為遠方的 Helm Chart 檔案，接者透過 Kustomize 的方式來動態修改欄位與增加資源。 這次的環境部署要求如下。 預設情況下先透過 Helm Values 將 frontend 的副本數調高為 3 dev 叢集: 將 serviceType 改成 LoadBalancer it 叢集: 將 redis-slave 的副本數改成 0 (預設是2) qa 叢集: 新增一個基於 nginx 的 deployment 有了這些概念後，就來準備相關的檔案，這次於 app 底下創建名為 helm_kustomize 的資料夾，並且於裡面先準備一個 fleet.yaml 的檔案。 ╰─$ cat fleet.yaml namespace: helmkustomize helm: chart: https://github.com/rancher/fleet-examples/releases/download/example/guestbook-0.0.0.tgz values: replicas: 3 targetCustomizations: - name: dev clusterSelector: matchLabels: management.cattle.io/cluster-display-name: ithome-dev kustomize: dir: overlays/dev - name: it clusterSelector: matchLabels: management.cattle.io/cluster-display-name: rke-it kustomize: dir: overlays/it - name: qa clusterSelector: matchLabels: management.cattle.io/cluster-display-name: rke-qa kustomize: dir: overlays/qa 上述的 fleet.yaml 中首先透過 helm chart 去抓取遠方的 helm chart server，接者透過 values 的方式將 frontend 的副本數設定為三個。 接下來是叢集客製化的部分，每個叢集這次採用叢集名稱作為比對的方式，接者使用 kustomize 的方式去 overlays 底下的資料夾來客製化。 ╰─$ tree . . ├── dev │ ├── frontend-service.yaml │ └── kustomization.yaml ├── it │ ├── kustomization.yaml │ └── redis-slave-deployment.yaml └── qa ├── deployment.taml └── kustomization.yaml ╰─$ cat dev/frontend-service.yaml kind: Service apiVersion: v1 metadata: name: frontend spec: type: LoadBalancer ╰─$ cat dev/kustomization.yaml patches: - frontend-service.yaml ╰─$ cat it/kustomization.yaml patches: - redis-slave-deployment.yaml ╰─$ cat it/redis-slave-deployment.yaml kind: Deployment apiVersion: apps/v1 metadata: name: redis-slave spec: replicas: 0 ╰─$ cat qa/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-server image: nginx ╰─$ cat qa/kustomization.yaml resources:: - deployment.yaml 準備好這些檔案並且修改 repo/*/app-basic.yaml 後，就可以將修改給推到遠方的 Git 專案，接者等待 Fleet 來幫忙處理部署。 使用 kubectl 工具觀察 Dev 的環境可以觀察到 Frontend 的 replica 是三個副本Service 的類型改成 LoadBalancer IT 的環境可以觀察到 Frontend 的 replica 是三個副本redis-slave 的 replica 變成 0 QA 的環境可以觀察到 Frontend 的 replica 是三個副本新的一個 Deployment 叫做 test，有三個副本。 可以發現環境中的部署條件都有如先前所述，算是成功的透過 Helm + Kustomize 的方式來調整應用程式。 Fleet 本身發展的時間不算久，因此 UI 上有時候會有一些額外的 Bug，這些除了看官方文件外剩下都要看 Github 上的 issue 來找問題。 此外 Fleet 於 08/28/2021 正式推出 v0.3.6 版本，不過因為如果想要單純使用 Rancher 來使用的話，那這樣就必須要等待新版本的 Rancher 一起推出才可以直接享用新版本的整合。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day3","content":"前言 前篇文章探討了 Rancher 的基本概念與 Rancher 帶來的好處，本章節則要探討 Rancher 的架構 對其架構瞭解愈深，未來使用時要除錯就會更知道要從什麼角度去偵錯同時部署時也比較會有些基本概念為什麼官方會有不同的部署方式。 由於 Rancher 本身是一個管理 Kubernetes 的平台，同時又要提供 UI 介面給使用者管理，因此其本身就是由多個內部元件組成的，如下圖(該圖節錄自官方網站) 註: 此為 v2.5 的架構 從官方的架構圖中可以觀察到， Rancher 本身除了 API Server 作為整體邏輯處理之外，還有額外的元件譬如 Cluster ControllerAuthentication Proxyetcd 其中 Cluster Controller 可以用來控制不同類型的 Kubernetes Cluster，不論是透過 Rancher 所架設的 RKE 或是其他如 EKS/AKS 等。 這邊要特別注意的，任何要給 Rancher 給控管的 Kubernetes Cluster 都會必須要於其叢集中安裝一個 Cluster Agent。 Rancher 要透過 Agent 的幫忙才可以達到統一控管的效用。 API Server 方面本身面對的 Client 很多，有使用 UI 瀏覽的，有使用 CLI 操作，甚至連 Kubernetes API 也都是由 API 處理的。 這邊解釋一下為什麼 Kubernetes API 需要走 Rancher API Server，試想一個純地端的網路環境，如果使用者想要透過 kubectl/helm 等指令去存取該 Kubernetes，這意味者該地端環境需要將 API Server 的 6443 port 給放出來，同時還要準備好相關憑證等。如果該 Kubernetes Cluster 是由 Rancher 所創立的，那 Rancher 可以透過與 Agent 的溝通過程來交換這些 Kubernetes API 的操作，這意味者使用者只要對 Rancher API Server 發送 Kubernetes API 等相關的指令，這些最後都會被 Rancher API Server 給轉發到底下 Kubernetes Cluster 的 API Server。這樣地端環境也不需要開啟 6443 port，只要本身叢集內的 Agent 有跟 Rancher API Server 有保持連線即可。使用上大幅度簡化整個操作流程。 最後提醒的是此功能並非一定要使用，針對 RKE 叢集也是有辦法不經由 Rancher 而直接存取 Kubernetes 。 上述的架構圖也清楚的告訴使用者，要架設一個 Rancher 服務要準備上述這些元件，而官方網站本身則提供的數種不同的安裝方式，而這些方式又會分成兩大類，單一節點或是多節點。 單一節點的安裝方式適合測試使用，而生產環境下會建議採用多節點的方式去部署 Rancher Server，畢竟 Rancher 本身是管理多套 Kubernetes 叢集的服務，因此本身最好要有 HA 的機制去確保不會因為單一節點損毀而導致後面一連串的錯誤。 下圖節錄自官方網站 該架構圖呈現了兩種不同模式下的架構，最大的差別就只是 Rancher Server 本身到底如何被外界存取以及 Rancher Server 有無 HA 等特性。 單一節點的安裝非常簡單，只要使用 docker 指令就可以很輕鬆的起一個 Rancher Server，不過要特別注意的是透過這種方法部署的 Rancher 不建議當作生產環境，最好只是拿來測試即可。 其原理其實是透過一個 docker container 起 Rancher 服務，服務內會用 RKE 創建一個單一節點的 Kubernetes 節點，該節點內會把 Rancher 的服務都部署到該 Kubernetes 內。 多節點安裝的安裝概念很簡單，就是把 Rancher 的服務安裝到一個 Kubernetes 叢集內即可， Rancher 本身提供 Helm 的安裝方式，所以熟悉 Helm 指令就可以輕鬆的安裝一套 Rancher 到 Kubernetes 叢集內。 官方文件提供了不同種 Kubernetes 叢集的安裝方式，包含 RKE (使用 RKE 指令先行創建一個 K8S 叢集，再用 Helm 把 Rancher 安裝進去)EKSGKEK3s (輕量級 RKE，針對 IoT 等環境設計的 Kubernetes 版本)RKE2 (針對美國安全相關部門所開發更為安全性的 RKE 版本) 除了上述所描述的一些安裝方式外， Rancher 也跟 AWS 有相關整合，能夠透過 CloudFormation 的方式透過 EKS 部署 Rancher 服務，詳細的可以參閱Rancher on the AWS Cloud Quick Start Reference Deployment 最後為了讓整體的安裝更加簡化，Rancher 於 v2.5.4 後釋出了一個實驗的新安裝方式，稱為 RancherD 該服務會先創建一個 RKE2 的叢集，並且使用 Helm 將相關服務都安裝到該 RKE2 叢集中。 最後要注意的是，不論是哪種安裝方式，都需要針對 SSL 憑證去進行處理，這部分可以用 Rancher 自行簽署，自行準備或是透過 Let's Encrypt 來取得都可以，所以安裝時也需要對 SSL 有點概念會比較好，能的話最好有一個屬於自已的域名來方便測試。 單一節點的 Docker Container 部署方式有可能會遇到 RKE 內部 k8s 服務憑證過期的問題，如果遇到可以參閱下列解決方式處理 Rancher container restarting every 12 seconds, expired certificates 下一篇文章便會嘗試透過 RKE + Helm 的方式來看看如何架設 Rancher","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day5","content":"前言 本篇文章將會示範如何使用 Rke + Helm 來搭建一個基於 RKE 叢集的 Rancher 服務。 該 RKE 叢集會有三個節點，因此環境方面就會準備三台 VM，這三台 VM 前面就會架設一個 Load-Balancer 來幫忙將流量導向後方服務。 同時也會準備一個 rancher.hwchiu.com 的 DNS 紀錄，這樣之後存取時會更為簡單與方便。 環境準備 接下來的環境都會基於 Azure 雲端環境來使用，包含了 VMs 與 LoadBalancer 的設定 本文章不會探討 Azure 雲端該如何使用，事實上讀者可以使用任意的公有雲服務，甚至是地端機器都可。 下述為相關的軟體版本資訊 VM: Azure VMOS: Ubuntu 20.04.2 LTSRke: v1.2.11 整個架構如圖下 Rancher 前篇文章中已經透過 rke 的指令創建了一個基於三節點的 Kubernetes 叢集，接下來我們要透過 Helm 指令將 Rancher 給安裝到我們的 RKE 之中。 再次提醒，針對 Rancher v2.5 與之後的版本請使用 Helm v3 來安裝，官方已經不再支援使用 Helm v2，如果你舊版的 Rancher 是使用 Helm v2 安裝然後想要將其升級到 Rancher v2.5 系列，則必須要先針對 Helm v2 -&gt; Helm v3 進行轉移。 轉移的方式可以參考 [Helm Plugin helm-2to3](https://github.com/helm/helm-2to3)，官方頁面有介紹詳細的用法，注意使用前先對所有 helm 的檔案進行備份以免不熟悉釀成不可恢復的情況 第一步先將 Rancher 官方的 Helm Repo 給加入到 Helm 清單中，官方提供三種不同的 Helm Repo 供使用者使用，包含 Latest: https://releases.rancher.com/server-charts/latestStable: https://releases.rancher.com/server-charts/stableAlpha: https://releases.rancher.com/server-charts/alpha 根據不同的需求採用不同的版本，如果不是 Rancher 開發者的話，我認為 Alpha 沒有使用的需求，而 Latest 則是讓你有機會可以嘗試目前最新的版本，看看一些新功能或是一些舊有 Bug 是否有被移除。大部分情況下推薦穩穩地使用 stable 版本，遇到問題也比較有機會被解決。 本次安裝將採用 stable 作為來源版本 azureuser@rke-management:~$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable &quot;rancher-stable&quot; has been added to your repositories azureuser@rke-management:~$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the &quot;rancher-stable&quot; chart repository Update Complete. ⎈Happy Helming!⎈ 要安裝 Rancher 之前，有一個要處理的東西就是所謂的 SSL 憑證，官網有提供不同選項的教學，主要可以分成 Rancher 自簽憑證自行準備憑證透過 Let's Encrypt 獲得的憑證 為了後續連線順利與方便，大部分都不會考慮使用自簽憑證而是會採用(2)/(3)兩個選項，而 Let's Encrypt 使用上還是相對簡單與輕鬆。 Kubernetes 生態系中針對 Let's Encrypt 來產生憑證的專案也不少，其中目前最熱門且最多人使用的非 cert-manager 莫屬。 同時 Rancher 官方也推薦使用 cert-manager 來使用，因此接下來就會使用 cert-manager 來輔助 Let's Encrypt 的處理。 這邊要注意的是， Rancher 官網有推薦使用的 Cert-Manager 版本，該版本通常都會比 Cert-Manager 慢一些，因此使用上請以 Rancher 推薦的為主，不要自行的升級 cert-manager 到最新版本，以免 Rancher 沒有進行測試整合而發生一些不預期的錯誤，到時候除錯起來也麻煩。 如同前述環境指出，本範例會使用的域名是 rancher.hwchiu.com，該域名會事先指向 Load-Balancer，並且將 HTTP/HTTPS 的連線都導向後方的三台伺服器 server{1,2,3}。 接者透過 helm 安裝 cert-manager 到環境中，整個步驟跟 Rancher 非常雷同，準備相關的 repo 並且透過 helm 安裝。 azureuser@rke-management:~$ helm repo add jetstack https://charts.jetstack.io &quot;jetstack&quot; has been added to your repositories azureuser@rke-management:~$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the &quot;jetstack&quot; chart repository ...Successfully got an update from the &quot;rancher-stable&quot; chart repository Update Complete. ⎈Happy Helming!⎈ azureuser@rke-management:~$ azureuser@rke-management:~$ kubectl create namespace cert-manager azureuser@rke-management:~$ helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.0.4 --set installCRDs=true 確認 cert-manager 的 pod 都起來後，下一步就是繼續安裝 Rancher azureuser@rke-management:~$ kubectl -n cert-manager get pods NAME READY STATUS RESTARTS AGE cert-manager-6d87886d5c-fr5r4 1/1 Running 0 2m cert-manager-cainjector-55db655cd8-2xfhf 1/1 Running 0 2m cert-manager-webhook-6846f844ff-8l299 1/1 Running 0 2m 首先於 rke 叢集中創立個給 rancher 使用的 namespace，接者透過 helm 指令加上一些參數，這些參數主要是告訴 Rancher 我們的 Ingress 想要使用 letsEncrypt 來產生相關的 SSL 憑證\b，希望指向的域名是 rancher.hwchiu.com. Rancher Helm Chart 就會針對這些參數去產生 cert-manager 需要的物件，如 Issuer，接者 cert-manager 就會接替後續行為來透過 ACME 產生一組合法的 TLS 憑證。 azureuser@rke-management:~$ kubectl create namespace cattle-system azureuser@rke-management:~$ helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.hwchiu.com \\ --set replicas=3 \\ --set ingress.tls.source=letsEncrypt \\ --set letsEncrypt.email=hwchiu@hwchiu.com \\ --version 2.5.9 NAME: rancher LAST DEPLOYED: Sun Aug 8 20:14:03 2021 NAMESPACE: cattle-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Rancher Server has been installed. NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued and Ingress comes up. Check out our docs at https://rancher.com/docs/rancher/v2.x/en/ Browse to https://rancher.hwchiu.com Happy Containering! 註: 預設情況下 ACME 會採用 HTTP 挑戰的方式來驗證域名的擁有權，所以 load-balancer 記得要打開 80/443 的 port，將這些服務導向後端的 rke 叢集。Rancher 會使用 cert-manager + ingress 等相關資源自動處理憑證。 安裝完畢後，等待相關的服務被部署，確認 cattle-system 這個 namespace 下的服務都呈現 running 後，打開瀏覽器連上 https://rancher.hwchiu.com 就會看到下述的登入畫面 因為是第一次登入，系統會要求你更新密碼，同時可以選擇預設的瀏覽模式，由於我們想要使用 Rancher 去管理多個 Cluster，因此我們選擇左邊的格式。接者下一步再次確認要存取的 URL 一切順利的話，就可以正式進入到 Rancher 的主要介面，這時候可以看到如下畫面 Rancher Server 安裝完畢後，會把 用來部署 Rancher 的 Kubernetes 叢集 也加入到 Rancher 的管理視角中，並且使用 local 這個名稱來表示這個 cluster。 其中可以注意到的是該叢集的 Provider 是顯示為 Imported，這意味者這個 Kubernetes 叢集並不是由 Rancher 幫你創造，而是把一個已經運行的叢集給匯入到 Rancher 中。 網頁可以順利存取就意味我們第一個 Rancher 服務順利的架設起來，下一篇文章就會來仔細介紹對於一個 IT Team 的管理人員來說，從系統層面來看 Rancher 的設定有哪些，每個設定對團隊有什麼益處與好處。 最後來看一下 kubernetes 的相關服務，觀察一下一個基本的 Rancher 服務有哪些一些 Pod，未來要除錯時才有概念應該要去哪個 namespace 看哪些服務。 cattle-system 與 kube-system 內都有相關的服務，這邊要注意的是 kube-system 放的是我們最初安裝 RKE 時部署的資源，而 cattle-system 則是我們透過 helm 部署 Rancher 用的。所以基本上就是三個 rancher Pod 以及一個 webhook。 azureuser@rke-management:~$ kubectl get pods -A | awk '{print $1&quot;\\t&quot;$2}' NAMESPACE NAME cattle-system helm-operation-56h22 cattle-system helm-operation-bjvmx cattle-system helm-operation-jtwf6 cattle-system helm-operation-stv9x cattle-system helm-operation-ttxt4 cattle-system helm-operation-xtznm cattle-system rancher-745c97799b-fqfsw cattle-system rancher-745c97799b-ls8wc cattle-system rancher-745c97799b-nhlz6 cattle-system rancher-webhook-6cccfd96b5-grd4q cert-manager cert-manager-6d87886d5c-fr5r4 cert-manager cert-manager-cainjector-55db655cd8-2xfhf cert-manager cert-manager-webhook-6846f844ff-8l299 fleet-system fleet-agent-d59db746-hfbcq fleet-system fleet-controller-79554fcbf5-b7ckf fleet-system gitjob-568c57cfb9-ncpf5 ingress-nginx default-http-backend-6977475d9b-hk2br ingress-nginx nginx-ingress-controller-8rtpv ingress-nginx nginx-ingress-controller-bv2lq ingress-nginx nginx-ingress-controller-mhfm6 kube-system coredns-55b58f978-545dx kube-system coredns-55b58f978-qznqj kube-system coredns-autoscaler-76f8869cc9-hrlqq kube-system kube-flannel-44hvn kube-system kube-flannel-rhw7v kube-system kube-flannel-thrln kube-system metrics-server-55fdd84cd4-wqdkw kube-system rke-coredns-addon-deploy-job-pjdln kube-system rke-ingress-controller-deploy-job-m7sj2 kube-system rke-metrics-addon-deploy-job-vtnfk kube-system rke-network-plugin-deploy-job-mv8nr rancher-operator-system rancher-operator-595ddc6db9-tfgp8 一但使用 Helm 安裝 Rancher，未來的升級大部分都可以透過 Helm 這個指令繼續升級，升級的概念也非常簡單 檢查當前版本的 release note，看看有什麼升級需要注意的事項更新 helm repo透過 helm 更新 rancher 這個 release，並且透過 --version 指名使用新的版本如果不想要每次都輸入前述一堆關於 SSL 的參數，可以把哪些參數變成一個 values.yaml 給傳入 詳細資訊建議參閱官網文章來看看更詳細的升級策略。 如果不想要透過 rke 來維護 Rancher 的話，官網也有如何使用 EKS/AKS/GKE 等公有雲 kubernetes 服務維護 Rancher 的相關教學。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day6","content":"前言 前篇文章透過 rke / helm 成功的搭建了一個 Rancher 服務，並且於第一次登入時按照系統要求創建了一組給 admin 使用的密碼，並且使用該 admin 的帳號觀察到了第一組創建被 Rancher 管理的 Kubernetes 叢集。 複習: 該 K8s 叢集並不是 Rancher 創造的，而是我們事先透過 rke 創造用來部署 Rancher 服務的 k8s 叢集。 對於 IT 管理人員來說，看到一個新的服務通常腦中會閃過的就是該服務的使用者管理權限該怎麼處理? 最直觀也簡單的方式就是透過該服務創建眾多的本地使用者，每個使用者給予不同的權限與帳號密碼。但是這種使用方式實務上會有太多問題 團隊內員工通常不喜歡每一個服務都有獨立的密碼，最好能夠用一套密碼去存取公司內所有服務員工數量過多時，通常團隊也很懶得幫每個員工都獨立創造一份帳號密碼，更常發生的事情是一套帳號密碼多人共同使用。多人共同使用的問題就是會喪失了稽核性，沒有辦法知道是誰於什麼時間點進行什麼操作，未來要除錯與找問題時非常困難如果權限還想要用群組來管理時，整個要處理的事情就變得又多又複雜由於帳號密碼都是服務本地管理，這意味團隊內的帳號密碼是分散式的架構，因此有人想要改密碼就需要到所有系統去改密碼，這部分也是非常不人性化，特別如果員工離職時，要是有服務忘了刪除可能會造成離職員工還有能力去存取公司服務。 因此大部分的 IT 都不喜歡使用本地帳號，更喜歡使用混合模式來達到靈活的權限管理。 服務想辦法整合外部的帳號密碼系統，常見的如 Windows AD, LDAP, GSuite, SMAL, OpenID, Crowd 等。每個服務都維持一個本地使用者，該使用者是管理員的身份，作為一個緊急備案，當外部帳號密碼系統出問題導致不能使用時，就必須要用本地使用者來存取。 混合模式的架構下，所有員工的帳號與密碼都採用集中式管理，任何第三方服務都要與該帳號系統整合，因此 員工只需要維護一套帳號密碼即可登入團隊內所有服務，如果員工需要改密碼，也只需要改一個地方即可IT 人員可以統一管理群組，每個第三方服務針對群組去進行權限控管即可。這種架構下不會有共享帳號密碼的問題，每個使用者登入任何系統都會有相關的日誌，未來除錯也方便 因此本篇文章就來探討 Rancher 提供何種使用者登入與權限控管，系統管理員架設維護時可以如何友善的去設定 Rancher Authorization 授權 Rancher 的世界中將權限分成三大塊，由大到小分別是 Global PermissionCluster RoleProject Role 其中 Cluster/Project 這個概念要到後面章節探討如何用 Rancher 去架設與管理 Kubernetes 叢集時才會提到，因此這邊先專注於第一項，也就是 Global Permission。 Global Permission 代表的是 Rancher 服務本身的權限，本身跟任何 Kubernetes 叢集則是沒有關係。 Rancher 本身採用 RBAC (Role-Based Access Control) 的概念來控制使用者的權限，每個使用者會依據其使用者名稱或是所屬的群組被對應到不同Role。 Global Permission 預設提供多種身份，每個身份都有不同的權限，以下圖來看(Security-&gt;Roles) 圖中是預設的不同 Role，每個 Role 都有各自的權限，同時還可以去設定說當一個新的外部使用者登入時，應該要賦予何種 Role 權限部分是採取疊加狀態的，因此設計 Role 的時候都是以 &quot;該 Role 可以針對什麼 API 執行什麼指令&quot;，沒有描述到的就預設當作不允許。 因此 Role 是可以互相疊加來達到更為彈性的狀態，當然預設 Role 也可以有多個。 註: 本圖片並不是最原始的 Rancher 設定，預設狀態有被我修改過，請以自己的環境為主。 Role 這麼多種對於初次接觸 Kubernetes 與 Rancher 的管理員來說實在太複雜與太困難，因此 Rancher 又針對這些 Role 提供了四種好記的名稱，任何使用者與群組都可以基於這四種 Role 為基礎去添加不同的 Role 來達到靈活權限。 這四種好記的 Role 分別為 Administration 超級管理員，基本上什麼都可以操作，第一次登入時所使用的 admin 帳號就屬於這個權限 Restricted Admin 能力近乎於超級管理員，唯一不能管理的就是 Rancher 本身所在的 kubernetes 叢集，也就是前篇文章看到的 local 叢集。 Standard User: 可以透過 Rancher 創建 Kubernetes 叢集 並且使用的使用者，大部分情況下可以讓非管理員角色獲得這個權限，不過因為創建過多的 Kubernetes 叢集有可能會造成成本提高，所以賦予權限時也要注意到底什麼樣的人可以擁有創造 kubernetes 叢集的權限。 User-Base: 基本上就是一個 read-only 的使用者，同時因為本身權限很低，能夠看到的資訊非常少，更精準的來說就是一個只能登入的使用者。 Authentication 認證 前述探討如何分配權限，接下來要探討的就是要如何幫使用者進行帳號密碼的驗證，這部分 Rancher 除了本地使用者之外也支援了各式各樣的第三方服務，譬如 Microsoft Active DirectoryGitHubMicrosoft Azure ADFreeIPAOpenLDAPMicrosoft AD FSPingIdentityKeycloakOktaGoogle OAuthShibboleth Rancher v2.6 的其中一個目標就是支援基於 OIDC 的 Keycloak ，因此如果團隊使用的是基於 OIDC 的 Keycloak 服務，讀者不仿可以期待一下 v2.6 的新功能。 使用者可以於 security-&gt;authentication 頁面看到如下的設定頁面 官方網站中有針對上述每個類別都提供一份詳細的教學文件，要注意的是因為 Rancher 版本過多，所以網頁本身的內容有可能你會找到的是舊的版本，因此閱讀網頁時請確保你當前看到的版本設定方式與你使用的版本一致。 預設情況下，管理者只能針對一個外部的服務進行認證轉移，不過這只是因為 UI 本身的設定與操作限制，如果今天想要導入多套機制的話是可以從 Rancher API 方面去進行設定，對於這功能有需求的可以參考這個 Github Issue Feature Request - enabling multiple authentication methods simultaneously #24323 實戰演練 上述探討完了關於 Rancher 基本的權限管理機制後，接下來我們就來實際試試看到底用起來的感覺如何。 由於整個機器都是使用 Azure 來架設的，因此第三方服務我就選擇了 Azure AD 作為背後的使用者權限，之後的系列文章也都會基於這個設定去控制不同的使用者權限。 下圖是一個想要達到的設定狀況 Rancher 本身擁有一開始設定的本地使用者之外，還要可以跟 Azure AD 銜接 而 Azure AD 中所有使用者都會分為三個群組，分別是 ITQADEV 我希望 IT 群組的使用者可以獲得 Admin 的權限，也就是所謂整個 Rancher 的管理員。 而 QA/DEV 目前都先暫時給予一個 User-Base 的權限，也就是只能單純登入然後實際上什麼都不能做。 這兩個群組必須要等到後面探討如何讓 Rancher 創建叢集時才會再度給予不同的權限，因此本篇文章先專注於 Rancher 與 AD 的整合。 本篇文章不會探討 Azure AD 的使用方式與概念，因此我已經於我的環境中創建了相關的使用者以及相關的群組。 整合方面分成兩大部分處理 Azure AD 與 Rancher 的整合Rancher 內的 Roles 設定 Azure AD 的部分可以參考官方教學，裡面有非常詳細的步驟告知要如何去 Azure 內設定，這邊要特別注意就是千萬不要看錯版本，以及最後填寫 Azure Endpoints 資訊時版本不要寫錯。 下圖是 Rancher 內的設定，其中 Endpoints 部分要特別小心 Graph 要使用 https://graph.windows.net/ 而不是使用 Azure UI 內顯示的 https://graph.microsoft.comToken/Authorization 這兩個要注意使用的是 OAUTH 2.0 (V1) 而不是 V2 下圖是 Azure 方面的設定，所以使用時要使用 V1 的節點而不是 V2，否則整合時候會遇到各種 invalid version 的 internal error. 當這一切整合完畢後重新登入到 Rancher 的畫面，應該要可以看到如下圖的畫面 畫面中告知 Rancher 的登入這時候分成兩種方式，分別是透過 Azure AD 以及使用本地使用者登入。 權限控制 當與 Azure AD 整合完畢後，首先要先透過本地使用者進行權限設定，因為本地使用者本身也是 Admin 的關係，因此可以輕鬆地去修改 Rancher。 如同前面所提，希望整體權限可以是 IT 群組的人為超級使用者DEV/QA 群組的人為只能登入的使用者 (User-Base)。 同時這邊也要注意，因為 Rancher 的使用者與群組兩個權限是可以分別設定且疊加的，因此設定的時候必須要這樣執行 將所有第一次登入的外部使用者的預設使用者都改為 (User-Base)撰寫群組的相關規則，針對 IT/DEV/QA 進行處理。 預設情況下， Rancher 會讓所有第一次登入的使用者都給予 Standard-User 的權限，也就是能夠創建 k8s 叢集，這部分與我們的需求不同。 所以第一步驟，移動到 security-&gt;roles 裡面去修改預設使用者身份，取消 User 並且增加 User-Base 第二步驟則是移動到 security-groups 內去針對不同 Group 進行設定 針對 IT 群組，給予 Administrator 的權限 針對 Dev 群組給予 User-Base 的權限 最後看起來會如下 到這邊為止，我們做了兩件事情 所有新登入的使用者都會被賦予 User-Base 的權限當使用者登入時，會針對其群組添加不同權限 如果是 IT，則會添加 Administrator 的權限，因此 IT 群組內的人就會擁有 User-Base + Administrator 的權限 如果是 DEV/QA 的群組，則會添加 Base-User 的權限，因此該群組內的人就會擁有 User-Base + User-Base 的權限，基本上還是 User-Base。 設定完畢後就可以到登入頁面使用事先創立好的使用者來登入。 當使用 Dev 群組的使用者登入時，沒有辦法看到任何 Cluster 相反的如果使用 IT 群組的使用者登入時，則因為屬於 Administrator 的權限，因此可以看到系統上的 RKE 叢集。 本篇文章探討了基本權限控管的概念並且展示了使用 Azure AD 後的使用範例，一旦瞭解基礎知識後，接下來就是好好研究 Rancher 內有哪些功能會使用到，哪些不會，針對這部分權限去進行設定，如果系統預設的 Role 覺得不夠好用時，可以自行創立不同的 Roles 來符合自己的需求，並且使用使用者與群組的概念來達到靈活的設定。 下篇文章將會使用 IT 的角色來看看到底 Rancher 上還有什麼設定是橫跨所有 Kubernetes 叢集，以及這些設定又能夠對整個系統帶來什麼樣的好處。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day4","content":"前言 本篇文章將會示範如何使用 Rke + Helm 來搭建一個基於 RKE 叢集的 Rancher 服務。 該 RKE 叢集會有三個節點，因此環境方面就會準備三台 VM，這三台 VM 前面就會架設一個 Load-Balancer 來幫忙將流量導向後方服務。 同時也會準備一個 rancher.hwchiu.com 的 DNS 紀錄，這樣之後存取時會更為簡單與方便。 環境準備 接下來的環境都會基於 Azure 雲端環境來使用，包含了 VMs 與 LoadBalancer 的設定 本文章不會探討 Azure 雲端該如何使用，事實上讀者可以使用任意的公有雲服務，甚至是地端機器都可。 下述為相關的軟體版本資訊 VM: Azure VMOS: Ubuntu 20.04.2 LTSRke: v1.2.11 整個架構如圖下 整個環境的概念如下 準備三個 VM，這些 VM 本身都沒有任何 Public IP，同時這三個 VM 會作為 RKE 叢集裡面的節點。準備一個 Load-Balancer，該 Load-Balancer 未來會將流量都導向前述三個 VM，包含 Rancher UI/API 等相關流量為了方便安裝 RKE 到前述三個節點，會額外準備一個 management server，該伺服器可以透過 ssh 存取前述三台 VM我們會於 Management server 上透過 RKE 指令來安裝 RKE 叢集。 建置 Rancher 整個安裝步驟會分成下列步驟，如 環境檢查並且於 Management server 下載安裝 rke 指令於 Management server 透過 rke 指令來安裝 rke 叢集到 Server{1,2,3}透過 Helm 將 Rancher 安裝到該 RKE 叢集中嘗試透過瀏覽器存取 Rancher 服務 環境檢查並且於 Server1 下載安裝 rke 這個步驟一開始我準備了下列環境 Management Server 以及 Server{1,2,3} Management Server 能夠透過 ssh 存取 server{1,2,3} LoadBalancer backend pool 設定 server{1,2,3}轉發 80/443 Domain Name (rancher.hwchiu.com) 該 domain name 指向該 LoadBalancer 的 public IP。 透過 SSH 登入到 Management Server 之後，我們要來安裝 rke 這個指令。 官方 Github 上面有針對不同平台的安裝檔案，我的環境需要使用的 rke_linux-amd64 wget https://github.com/rancher/rke/releases/download/v1.2.11/rke_linux-amd64 sudo install -m755 rke_linux-amd64 /usr/local/bin/rke 安裝完畢後可以直接嘗試使用看看 rke 這個指令 azureuser@server1:~$ rke NAME: rke - Rancher Kubernetes Engine, an extremely simple, lightning fast Kubernetes installer that works everywhere USAGE: rke [global options] command [command options] [arguments...] VERSION: v1.2.11 AUTHOR: Rancher Labs, Inc. COMMANDS: up Bring the cluster up remove Teardown the cluster and clean cluster nodes version Show cluster Kubernetes version config Setup cluster configuration etcd etcd snapshot save/restore operations in k8s cluster cert Certificates management for RKE cluster encrypt Manage cluster encryption provider keys util Various utilities to retrieve cluster related files and troubleshoot help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --debug, -d Debug logging --quiet, -q Quiet mode, disables logging and only critical output will be printed --trace Trace logging --help, -h show help --version, -v print the version 同時確認該 management server 可以使用 ssh 連結到上述 Server{1,2,3} azureuser@rke-management:~$ ssh 10.0.0.10 &quot;hostname&quot; rke-serve000004 azureuser@rke-management:~$ ssh 10.0.0.8 &quot;hostname&quot; rke-serve000002 azureuser@rke-management:~$ ssh 10.0.0.7 &quot;hostname&quot; rke-serve000001 接者也要確認上述 server{1,2,3} 都安裝好 docker 並且當前非 root 使用者可以執行，因為 rke 會需要透過 docker 去創建基本服務。 azureuser@rke-management:~$ ssh 10.0.0.7 &quot;docker ps &quot; CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES azureuser@rke-management:~$ ssh 10.0.0.8 &quot;docker ps &quot; CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES azureuser@rke-management:~$ ssh 10.0.0.10 &quot;docker ps &quot; CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 透過 rke 於 Server1 安裝 rke 叢集 接下來使用 rke 的指令來創建 rke 創建叢集，首先要讓 rke 指令知道我們有三台伺服器，同時這些伺服器要登入的 IP/SSH Uername 以及相關的 k8s 角色，我們需要準備一個 cluster.yaml 作為設定檔案。 首先透過 rke 指令確認當前支援的 kubernetes 版本 azureuser@rke-management:~$ rke config --list-version --all v1.17.17-rancher2-3 v1.19.13-rancher1-1 v1.18.20-rancher1-2 v1.20.9-rancher1-1 當前支援最高的版本是 v1.20.9-rancher1-1，所以準備一個下列的 cluster.yaml，其描述了 三台伺服器的 IP 以及 ssh 角色名稱每個角色都要扮演 k8s controlplane, k8s worker 以及 etcd.network 使用 calico 作為 CNI啟動 etcd 並且啟動自動備份 cluster_name: ithome-rancher kubernetes_version: &quot;v1.20.9-rancher1-1&quot; nodes: - address: 10.0.0.7 user: azureuser role: [controlplane,worker,etcd] - address: 10.0.0.8 user: azureuser role: [controlplane,worker,etcd] - address: 10.0.0.10 user: azureuser role: [controlplane,worker,etcd] services: etcd: backup_config: enabled: true interval_hours: 6 retention: 60 network: plugin: flannel 準備好上述檔案後，透過 rke up 來創建 cluster azureuser@rke-management:~$ rke up --config cluster.yaml INFO[0000] Running RKE version: v1.2.11 INFO[0000] Initiating Kubernetes cluster INFO[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates INFO[0000] [certificates] Generating admin certificates and kubeconfig INFO[0000] Successfully Deployed state file at [./cluster.rkestate] INFO[0000] Building Kubernetes cluster INFO[0000] [dialer] Setup tunnel for host [10.0.0.8] INFO[0000] [dialer] Setup tunnel for host [10.0.0.10] INFO[0000] [dialer] Setup tunnel for host [10.0.0.7] INFO[0000] [network] Deploying port listener containers INFO[0000] Pulling image [rancher/rke-tools:v0.1.77] on host [10.0.0.7], try #1 INFO[0000] Pulling image [rancher/rke-tools:v0.1.77] on host [10.0.0.8], try #1 INFO[0000] Pulling image [rancher/rke-tools:v0.1.77] on host [10.0.0.10], try #1 .... INFO[0284] [dns] DNS provider coredns deployed successfully INFO[0284] [addons] Setting up Metrics Server INFO[0284] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes INFO[0284] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes INFO[0284] [addons] Executing deploy job rke-metrics-addon INFO[0301] [addons] Metrics Server deployed successfully INFO[0301] [ingress] Setting up nginx ingress controller INFO[0301] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes INFO[0301] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes INFO[0301] [addons] Executing deploy job rke-ingress-controller INFO[0306] [ingress] ingress controller nginx deployed successfully INFO[0306] [addons] Setting up user addons INFO[0306] [addons] no user addons defined INFO[0306] Finished building Kubernetes cluster successfully RKE已經正式創建完畢，當前目錄下會產生一個 KUBECONFIG 的目錄，檔案名稱為 &quot;kube_config_cluster.yaml&quot; azureuser@rke-management:~$ mkdir .kube azureuser@rke-management:~$ install -m400 kube_config_cluster.yaml ~/.kube/config azureuser@rke-management:~$ kubectl get nodes azureuser@rke-management:~$ sudo chmod 400 .kube/config azureuser@rke-management:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.0.10 Ready controlplane,etcd,worker 10m v1.20.9 10.0.0.7 Ready controlplane,etcd,worker 10m v1.20.9 10.0.0.8 Ready controlplane,etcd,worker 10m v1.20.9 azureuser@rke-management:~$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx default-http-backend-6977475d9b-xrmv9 1/1 Running 0 9m34s ingress-nginx nginx-ingress-controller-bl7p9 1/1 Running 0 9m34s ingress-nginx nginx-ingress-controller-g476g 1/1 Running 0 9m34s ingress-nginx nginx-ingress-controller-nqlqv 1/1 Running 0 9m34s kube-system calico-kube-controllers-7ddcfb748f-tvnkp 1/1 Running 0 10m kube-system calico-node-f42dt 1/1 Running 0 10m kube-system calico-node-gsn8f 1/1 Running 0 10m kube-system calico-node-p98tx 1/1 Running 0 10m kube-system coredns-55b58f978-7j85f 1/1 Running 0 9m36s kube-system coredns-55b58f978-l4smb 1/1 Running 0 10m kube-system coredns-autoscaler-76f8869cc9-t2s6f 1/1 Running 0 9m58s kube-system metrics-server-55fdd84cd4-m96ql 1/1 Running 0 9m43s kube-system rke-coredns-addon-deploy-job-7l8zs 0/1 Completed 0 10m kube-system rke-ingress-controller-deploy-job-pjvns 0/1 Completed 0 9m36s kube-system rke-metrics-addon-deploy-job-ddct7 0/1 Completed 0 9m55s kube-system rke-network-plugin-deploy-job-gprzz 0/1 Completed 0 10m 到這個環節，我們已經正式的將 RKE 叢集給創建完畢了，下一章節我們就要來透過 Helm 的方式將 Rancher 給安裝到該 RKE 中。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day7","content":"前言 前篇文章探討如何透過 Rancher 整合既有的帳號管理系統，同時如何使用 Rancher 提供的 RBAC 來為不同的使用者與群組設定不同的權限。 本文將繼續探討從 Rancher 管理人員角度來看還有哪些功能是執得注意與使用的。 系統設定 當今天透過認證與RBAC等功能完成權限控管後，下一個部分要探討的就是團隊的運作流程。 Rancher 作為一個 Kubernetes 管理平台，最大的一個特色就是能夠輕鬆地於各種架構下去安裝一個 Kubernetes 叢集並且管理它， 雖然到現在為止我們還沒有正式的示範如何創建叢集，但是可以先由下圖看一下大概 Rancher 支援哪些不同類型的架構 圖中分成四種不同類型，這四種類型又可以分成兩大類 已經存在的 Kubernetes 叢集，請 Rancher 幫忙管理。請 Rancher 幫忙創建一個全新的 Kubernetes 叢集並且順便管理。 圖中第一個類型就屬於第一大類，這部分目前整合比較好的有 EKS 與 GKE 這意味者如果你有已經運行的 EKS/GKE 叢集，是有機會讓 Rancher 幫忙管理，讓團隊可以使用一個共同的介面(Rancher)來管理所有的 Kubernetes 叢集。 圖中剩下的(2,3,4)都屬於第二大類，只是這三大類的安裝方式有些不同，分別是 使用者要事先準備好節點， Rancher 會於這些節點上去創建 RKE 叢集Rancher 會透過 API 要求服務供應商去動態創建 VM，並且創建 VM 後會自動的建立起 RKE 叢集針對部分有提供 Kubernetes 服務的業者， Rancher 也可以直接透過 API 去使用這些 Kubernetes 服務(AKS/EKS/GKE) 並且把 Rancher Agent 安裝進去，接者就可以透過 Rancher 頁面去管理。 這邊點選第三類別的 Azure 作為一個範例來看一下，透過 RKE 安裝 Kubernetes 會有什麼樣的資訊需要填寫 首先上圖看到關於 Cluster Options 下有四大項，這四大項裡面都問題都跟 Kubernetes 叢集，更精準的說是 RKE 有關 首先第一個類別就是 Kubernetes 的基本資訊，包含了 RKE 的版本，版本跟 Kubernetes 版本是一致走向的。CNI 使用哪套，目前有 Flannel, Calico 以及 Canal。CNI 需不需要額外設定 MTU該環境要不要啟用一些 Cloud Provider 的功能，要的話還要填入一些機密資訊 往下看還有更多選項可以用，譬如該 RKE 創建時，所有用到的 Registry 是否要從一個 Private Registry 來抓取，這功能對某些團隊來說會非常有用，因為部分團隊會希望用到的所有 Container Image 都要有一份備份以免哪天 quay, docker.io 等出現問題導致整個安裝失敗。 因此如果團隊事先將 RKE 用到的 Container Image 都複製一份到自己團隊的 private container registry 的話，就可以打開這個功能讓 Rancher 知道去哪邊抓 Image。 後續則是更多的進階選項，譬如説 RKE 中預設要不要安裝 Nginx 作為 Ingress Controller?系統中的 NodePort 用到的範圍多少?是否要導入一組預設的 PodSecurityPolicy 來限制叢集內所有Pod的安全性規則Docker 有沒有特別指定的版本etcd 要如何備份，要本地備份還是要透過 s3 將 etcd 上傳要不要定期透過 CIS 進行安全性相關的掃描？ 可以看到上述的設定其實滿多的，如果每次創建一個叢集都要一直輸入一樣的資訊難免會出錯，同時有一些設定 IT 人員會有不同的顧慮與要求。為了讓團隊內的所有 RKE 叢集都可以符合團隊的需求，Rancher 就有提供基於全面系統地 RKE Template. RKE Template RKE Template 的概念就是讓系統人員與安全人員針對需求去規範 Kubernetes 的要求，所有使用者都必須要使用這個事先創立的 RKE Template 來創立 RKE 叢集。 透過這個方式有幾個好處 使用者創立的所有 RKE 叢集都可以符合團隊需求使用者使用 Template 去創建 RKE 的話就可以省略那些不確定該怎麼填寫的資訊，簡化整個創造步驟Template 本身也是一個物件，所以 Rancher 前述提到的權限控管就可以針對 Template 去進行設定，譬如 DEV/QA 人員只能使用已經創建的 Template 來創立 RKE 叢集 以下是一些常見使用 RKE Template 的使用範例 系統管理人員強迫要求所有新創立的 RKE 叢集都只能使用事先創立好的 RKE Template系統管理人員創建不同限制的 RKE Template，針對不同的使用者與群組給予不同的 RKE TemplateRKE Template 本身是有版本的概念，所以如果今天公司資安團隊希望調整資安方面的使用，只需要更新 RKE Template 即可。所有使用到的使用者再進行 RKE Template 更新的動作即可 此外 RKE Template 內所有的設定都有一個覆蓋的概念存在，創建該 RKE Template 時可以決定該設定是否能夠被使用者覆蓋，這對於某些很重要的設定來說非常有用。 以下是一個 RKE Template 創建的方式 (Tools-&gt;Template 進入) 圖中最上方代表的是該 RKE Template 的名稱與版本，同時每個 RKE Template 有很多個版本，更新的時候可以選擇當前版本是否要作為當前 Template 的預設版本。 中間部分則是到底誰可以使用這個 RKE Template，裡面可以針對使用者與群組去設定身份總共分成兩個身份，分別是 User 以及 Owner。 Owner: 符合這個身份的使用者可以執行 更新/刪除/分享 這些關於 RKE Template 的設定，概念來說就是這個 RKE Template 的擁有者。User: 簡單來說就是使用這個 RKE Template 的人，使用者再創建 RKE 叢集的時候可以從這些 Template 中去選擇想要使用的 Template。 最下面的部分跟前述探討安裝 RKE 要使用的權限都差不多，唯一要注意的是每個選項旁邊都有一個 &quot;Allow user override?&quot; 的選項，只要該選項沒有打開，那使用者(User)使用時就不能覆蓋這些設定。 實驗 接下來針對 RKE Template 進行一個實驗，該實驗想到達到以下目的。 IT 管理員創建一個 RKE Template，並且設定 DEV 群組的使用者可以使用DEV 群組的使用者可以創建 Cluster，但是被強迫只能使用 RKE Template 創造，並不是自己填寫任何資訊。 為了達到這個目的，我們有三個步驟要做，分別是 透過 Rancher 的設定，強迫所有創建 RKE 叢集一定要使用 RKE Template，不得自行填寫資訊。創造一個 RKE Template，並且設定 DEV 群組的人是使用者，同時該 RKE Template 內讓 Kubernetes 版本是一個可以被使用者覆蓋的設定。登入 DEV 使用者嘗試創造 RKE 叢集，看看上述設定是否可以達到我們的需求。 首先到首頁上方的 Setting，進去後搜尋 template-enforcement 就會找到類似下列這張圖片的樣子 預設狀況下，該設定是 False，透過旁邊的選項把它改成 True，該選項一打開後，所有新的 RKE 叢集都只能透過 RKE Template 來創建。 接者用 IT 人員登入作為一個系統管理員，創建一個如下的 RKE Template 中間部分表示任何屬於 DEV 群組的人都可以使用這個 RKE Template，同時該 RKE Template 允許使用者去修改 Kubernetes 版本，其餘部分都採用 RKE Template 的設定。 最後用 DEV 的身份登入 Rancher 並且嘗試創造一個 RKE 叢集。 從下述畫面可以觀察到一些變化 RKE Template 相關選項 &quot;Use an existing RKE Template and revision&quot; 被強迫打勾，這意味使用者一定要使用 RKE Template選擇了前述創造的 RKE Template 後就可以看到之前創造好的設定只有 Kubernetes 的版本是可以調整的，其餘部分如 CNI 等都是不能選擇的。 註: 當透過 RKE Template 賦予權限給予 DEV 群組後， 群組那邊的設定會被修改，這時候的 DEV 群組會被自動加上 &quot;Creating new Clusters&quot; 這個權限，範例如下 透過上述的範例操作成功地達到預期目標的設定，讓團隊內所有需要創建 RKE 叢集的使用者都必須要使用事先創造好的 RKE Template 來確保所有叢集都可以符合團隊內的需求。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day8","content":"前言 前述文章探討的都是基於一個系統管理員的角度下，可以如何使用 Rancher 這個管理平台來符合整個團隊的需求，譬如 RKE Template 以及最重要的使用者登入與權限控管。 當 Rancher 系統有了妥善的規劃與設定後，下一步就是踏入到 Rancher 最重要的功能，也就是 Kubernetes 管理。 前篇文章探討 RKE Template 的時候有介紹過 Rancher 有四種新增 Kubernetes 叢集的方式，第一種是將已經存在的 EKS/GKE 等叢集直接匯入到 Rancher 中，而剩下三種是根據不同架構來產生一個全新的 RKE 叢集並且匯入到 Rancher 中。 這三種架構以目前的環境架構如下圖所示 Rancher 有三種方式可以架設 RKE，從右到左分別是 透過 API 請求 Azure 幫忙創建 AKS，並且把 Rancher 相關的 agent 安裝到該 AKS 中透過 API 請求 Azure 創造 VM 叢集，接者於這些 VM 上安裝 RKE 叢集直接於已經存在的節點上搭建一套 RKE 叢集 接下來就示範這三種用法有何不同，以及當 RKE 叢集創建出來後該如何使用 環境實驗 三種環境中，我認為相對複雜的是第二種，如何請求 Azure 創建 VM 並且安裝 RKE 叢集。 所以先從這個較為複雜的情況開始探討，掌握這個概念後後續兩個(1)(3)都較為簡單，使用上也就不會有太多問題。 第二種的安裝模式將其仔細攤開來看，其實有幾個重點需要完成 選擇一個想要使用的 Service Provider準備好該 Service Provider 溝通用的設定，譬如帳號密碼, Token 等規劃需要準備多少台 VM，每個 VM 要用何種規模(CPU/Memory)，該機器要扮演 Kubernetes 什麼角色規劃 RKE 的設定 Service Provider 第一點是選擇一個想要使用的 Service Provider，由於之前的環境都是基於 Azure 去使用，因此我接下來的範例都會基於 Azure 去架設。 下圖是一個 Rancher 預設支援的 Service Provider，包含了 AWS, Azure, DigitalOcean, Linode 以及 vSphere. 實際上 Rancher 內部有一個名為 Node Driver 的資源專門用來管理目前支援哪些 Service Provider，該資源是屬於系統層級，也就是整個 Rancher 環境共享的。 Driver (Tools-&gt;Driver) 頁面中顯示了兩種不同的 Driver，分別是 Cluster Driver 以及 Node Drivers. 預設的 Node Driver 狀態如下，可以看到 Driver 分成兩種狀態，分別是 Active 以及 Inactive，而上圖中顯示的 AWS/Azure/Digital/Linode/vSphere 都屬於 Active。 嘗試將上述所有 Inactive 的 Drive 都 active 後，這時候重新回去 Cluster 創建頁面看，就可以發現目前支援的 Service Provider 變得超級多。 所以如果團隊使用的 Service Provider 沒有被 Rancher 預設支援的話，別忘記到 Driver 處去看看有沒有，也許只是屬於 Inactive 的狀態而已。 Access Credentials 選擇 Service Provider(Azure) 後的下一個步驟就是要想辦法讓 Rancher 跟 Azure 有辦法溝通，基本上 Service Provider 都會提供相關的資訊供使用者使用。 這邊試想一下，這種帳號密碼資訊的東西如果每次創建時都要一直重複輸入其實也是相對煩人的，所以如果有一個類似 RKE Template 概念的物件，就能夠讓使用者更為方便的去使用。 譬如使用者只需要事先設定一次，接下來每次要使用到的時候都去參考事先設定好的帳號密碼資訊即可。 Rancher 實際上也有提供這類型的機制，稱為 Cloud Credentials，其設定頁面位於個人使用者底下， 接者點選創建一個 Cloud Credential 並且將 Cloud Credential Type 設定為 Azure 後就會出現 Azure 應該要輸入的相關資訊，對於熟悉 Azure 的讀者來說這三個設定應該不會太陌生，基本上 Rancher 官方都有針對這些類別提供簡單的教學文件。 一切準備就緒後就可以創建一個基於 Azure 的 Cloud Credential 了。未來其他操作如果需要 Azure 相關的帳號密碼時，就不需要一直重複輸入，而是可以直接使用這組事先創建好的連接資訊。 VMs 當 Rancher 準備好如何跟 Service Provider(Azure) 溝通後，下一個要做的就是使用者要去思考，希望這個創建的 RKE 叢集有多少個節點以及相關設定。這些節點都會是由 Rancher 要求 Azure 動態創立的，每個節點都需要下列資訊 節點的 VM 規模，多少 CPU，多少 Memory該 VM 要用什麼樣的 Image，什麼樣的版本，登入角色要用什麼名稱，有沒有 Cloud-Init 要運行每個 Service Provider 專屬設定該節點於 Kubernetes 內扮演的角色，角色又可以分成三種 a. etcd: 扮演 etcd 的角色，要注意的是 etcd 的數量必須是奇數 b. control plane: Kubernetes Control Plane 相關的元件，包含 API Server, Controller, Scheduler 等 c. worker: 單純的角色，可以接受 Control Plane 的命令將 Pod 部署到該節點上。 從上述的資訊可以觀察到，要創建一個 RKE 資訊光節點這邊要輸入的資訊就不少，所以如果每次創建 RKE 叢集都要一直重複輸入上列這些資訊，其實帶來的麻煩不下 Credential 與 K8s 本身。 這個問題 Rancher 也有想到，其提供了一個名為 Node Template 的物件讓使用者可以去設定 VM 的資訊，同時為了讓整個操作更加彈性與靈活，上述四個步驟其實分成兩大類 VM 本身的設定 (1~3)該 VM 怎麼被 RKE 使用 (4) Node Template 要解決的是第一大類的問題，讓 VM 本身的設定可以重複利用，不需要每次輸入。 使用者要創立ㄧ個新的 RKE 叢集時，可以直接使用創造好的 NodeTemplate 設定 VM 資訊，接者根據當前需求決定該節點應該要以何種身份於 RKE 叢集中使用。 Node Template 與 Cloud Credential 一樣，都可以於使用者底下的頁面去設定。 進入到頁面後可以看到目前支援的 Service Provider，因為先前有將所有 Node Driver 都打開，所以這邊的選擇就非常的多。 當選擇為 Azure 後，底下的 Account Access 就會出現之前創立的 Cloud Credential。 如果想要創建新的也可以於這個頁面直接創立該 Cloud Credential。 接者下列就是滿滿的 VM 設定，這邊的設定內容都跟該 Service Provider 有關 可以看到 image 預設是 canonical:UbuntuServer:18.04-LTS:latest，Size 是 Standard_D2_v2SSH 的使用者名稱是誰硬碟空間預設是 30GB 上述還有非常多的設定，除非對這些選項都非常熟悉，不然大部分情況下都可以採取預設選項 一切就緒後給予該 Node Template 一個名稱並且儲存。 實務上通常會針對不同大小的機器創建不同的 Node Template 並且給予名稱時有一個區別，這樣之後使用者要使用時就會比較清楚當前的 Node Template 會創造出什麼樣的機器。 這邊示範一下創建兩個 Node Template 並且給予不同的名稱 RKE 一切資訊都準備完畢後，接者就可以回到 Cluster 的頁面去創造一個基於 Azure 的 RKE 叢集。 上圖紅匡處則是本文章重點處理的部分，也就是所謂的 Node Template。 對於 RKE 叢集來說，會先透過 Node Template 來定義一個 Node Pool，每個 Node Pool 需要定義下列資訊 該 Node Pool 名稱Pool 內有多少節點該 Node Pool 要基於哪個 Node Template 來創造本身要扮演什麼身份 同時 UI 也會提醒你每個身份應該要有多少個節點，譬如 etcd 要維持奇數， Control Plane/Worker 至少都要有一個節點。 決定好叢集身份後，下一件事情就是權限，到底誰有權限去使用這個 RKE 叢集，預設情況下有兩種身份，分別是 Cluster Owner: 該使用者擁有對該叢集的所有操控權，包含裡面的各種資源Cluster Member: 可以讀取觀看各種相關資源，寫的部分是針對底下的專案去操作，沒有辦法對 Cluster 本身進行太多操作，這部分之後會探討叢集內的專案概念時會更加理解。 一切準備就緒就給他點選創立，接者就是慢慢等他處理，整個過程會相對漫長，因為需要從 VM 開始創立，接者才去創建 RKE 叢集。 當畫面顯示如下時，就代表者相關叢集已經創建完畢了。 這邊可以看到，跟之前用來存放 Rancher 的 RKE 不同，這個新創的 RKE 叢集其 Provider 標示為 Azure。 同時我的 Azure VM 中也真的產生出三個新的VM，這些 VM 的名稱與規格都與之前設定的完全一樣。 下一章節會繼續介紹另外兩種不同的安裝方式，並且會基於這三種不同方式安裝三個不同類型的叢集，之後探討到 Rancher Fleet 這個 GitOps 的概念時，就會使用 GitOps 來部署應用程式到這三個不同的叢集。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/iThome_Challenge/day9","content":"前言 前篇文章探討了 Rancher 其中一種安裝 Kubernetes(RKE) 的方式，該方式會先透過 API 請求 Service Provider(Azure) 幫忙創建相關的 VM，接者於這些 VM 上面搭建一個符合需求的 RKE 叢集。 為了讓簡化整個設定過程，我們學到了如何透過 Cloud Credential 以及 Node Template 兩種方式來事先解決繁瑣的操作，接者真正創建 RKE 叢集時則使用 Node Template 與 RKE Template 兩個方式讓整個創建過程變得很簡單，不需要填入太多資訊，只需要利用這兩個 Template 的內容加上自行設計要多少個 VM 節點，這些節點要屬於什麼身份以及該叢集最後要給哪個使用者/群組使用即可。 本篇文章將繼續把剩下兩種安裝方式給走一遍，三種安裝方式都玩過後會對 Rancher 的能力有更多的瞭解，同時也會為之後 Rancher Fleet 的使用先行搭建環境。 之前探討使用者管理與部署時於系統中創建了三種不同群組的使用者，包含了 DEV, QA 以及 IT。而這兩個章節探討的三種部署方式其實剛好就會剛好拿來搭配這些不同的使用者，期望透過不同方式搭造出來的三套 RKE 叢集本身權限控管上就有不同的設定。 目標狀況是三套叢集擁有的權限如下 DEV 叢集 -&gt; IT &amp; DEV 可以使用QA 叢集 -&gt; IT &amp; QA 可以使用IT 叢集 -&gt; IT 可以使用 基本上因為 IT 群組的使用者會被視為系統管理員，因此本身就有能力可以存取其他叢集，所以創建時只需要針對 DEV 以及 QA 兩個叢集去設計。 實務上到底會如何設計取決於團隊人數與分工狀況，這邊的設計單純只是一個權限控管的示範，並不代表真實應用就需要這樣做。 Existing Cluster 前篇文章探討的是動態創建 VM 並且於搭建 RKE 叢集，與之相反的另外一種架設方式就是於一個已經存在的節點上去搭建 RKE 叢集。 這個安裝方式大部分都會用於地端環境，部分使用者的地端環境沒有 vSphere/Openstack 等專案可以幫忙自動創建 VM，這種情況下一台又一台的 bare-metal 機器就會採用這種方式來安裝。 我先於我的 Azure 環境中創建兩個 VM，想要用這兩個 VM 搭建一個屬於 QA 群組使用者的 RKE 叢集 上圖中標示為 qa-rke{1,2} 的機器就是為了這個情況手動創建起來的。 準備好了相關 VM 之後，就切換到 Rancher 的介面去創建一個 RKE 叢集。 介面中選擇非常簡單，只有一個 Existing Nodes 可以選擇，點進去後可以看到類似下方頁面 與之前的安裝方式不同，這邊沒有所謂的 Node Pool 的概念，畢竟是要安裝到一個已存在的節點，所以沒有 Node Pool 需要設定也是合理的。 這邊我將該叢集分配給 QA 群組的使用者，令其為 Owner，擁有整個叢集的管理權限，同時下方繼續使用先前設定好的 RKE 叢集。 一切完畢後點選 Next 到下一個頁面，該頁面才是真正安裝的方式 該介面有三個地方要注意 最下方是安裝的指令，實際上是到該節點上透過 Docker 的方式去運行一個 Rancher Agent 的容器，該容器會想辦法跟安裝 RKE 並且跟遠方的 Rancher 註冊以方便被管理最上方兩個區塊都是用來調整該節點到底要於 RKE 叢集中扮演什麼角色，這些變動都會影響下方 Docker 指令就如同先前安裝一樣，這邊也需要選擇當前節點到底要當 ETCD/Control Plane/Worker 等Show advanced options 選項打開可以看到的是 Labels/Taints 等相關設定 我透過上述介面設定了兩種介面，分別是 單純的 worker全包，同時兼任 worker/etcd/controlplane 接者複製這些 docker 指令到事先準備好的 VM 上去執行 到兩個機器上貼上指令後，就慢慢的等 Rancher 將整個 RKE 架設起來。 可以看到透過這種方式創建的叢集，其 Provider 會被設定成 Custom，跟之前創立的 Azure 有所需別。 點選該叢集名稱進去後，切換到節點區塊可以看到兩台機器正在建立中，這邊要特別注意，節點的 hostname 必須要不同，如果 hostname 一致的話會讓 Rancher 搞混，所以使用 bare-metal 機器建立時千萬要注意 hostname 不要衝突。 一切結束後，就可以於最外層的介面看到三個已經建立的叢集，一個是用來維護 Rancher 本身，兩個則是給不同群組的 RKE 叢集。 Managed Kubernetes Service 接者來看最後一個安裝方式，這個方式想要直接透過 Rancher 去創造 AKS 這種 Kubernetes 服務，並且安裝完畢後將 Rancher 的相關服務部署進去，這樣 Rancher 才有辦法控管這類型的叢集。 回到叢集安裝畫面，這時候針對最下面的服務，預設情況下這邊的選擇比較少，但是如果有到 Tools-&gt;Driver 去將 Cluster Driver 給打開的話，這邊就會出現更多的 Service Provider 可以選擇。 根據我的環境，我選擇了 Azure AKS，點選進去後就可以看到如下圖的設定頁面 進到畫面後的第一個選項就是 Azure 相關的認證資訊，這邊我認為是 Rancher 還沒有做得很完善的部分，先前創建 Node Template 時使用的 Cloud Credential 這邊沒有辦法二次使用，變成每次創建一個 AKS 叢集時都要重新輸入一次相關的資訊，這部分我認為還是有點不方便。 不過仔細觀察需要的資訊有些許不同，創造 Node Template 時不需要 Tenant ID，但是使用 AKS 卻需要。有可能因為這些設定不同導致 Cloud Credential 這邊就沒有辦法很輕鬆的共用。 不過作為使用者也是希望未來能夠簡化這些操作，否則每次創建都要翻找算是有點麻煩。 通過存取資訊驗證後，就可以來到設定頁面，因為這個是 AKS 叢集，因此並沒有辦法使用 RKE Template 來客製化內容，所有的設定內容都是跟 AKS 有關，因此不同的 K8S 服務提供的操作選項就不同。 一切準備就緒後，就可以看到最外層的叢集列表多出了一個全新的 Kubernetes 叢集，其 Provider 則是 Azure AKS。 此時觀看 Azure AKS 的頁面可以發現到也多了一個正在創建中的 AKS 叢集，名稱 c-xxxx 就是 Rancher 創造的證明，每個 Rancher 管理的 Kubernetes 叢集都會有一個內部ID，都是 c-xxxx 的形式。 等待一段時間待節點全部產生完畢，就可以看到一個擁有三個節點的 AKS 叢集被創建了 最後到外層叢集介面可以看到目前有四個 K8S 叢集被 Rancher 管理，其中一個是管理 Rancher 本身，剩下三個則是新創立的 K8s 叢集。同時這三個叢集都分配給不同群組的使用者使用。 如果這時候用 QA 使用者登入，就只會看到一個叢集，整個運作的確有符合當初的設計。 叢集都創立完畢後，下一章節將來探討如何使用 Rancher 的介面來管理 Kubernetes，以及 Rancher 介面還提供了哪些好用的功能可以讓叢集管理員更加方便的去操作叢集。","keywords":"","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/k8s-network-issue","content":"","keywords":"","version":"Next"},{"title":"固定 IP/MAC​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#固定-ipmac","content":"過往透過 Bare Metal 或是 VM 來管理服務時，固定 IP/MAC 是個常見的做法與需求。 使用 DHCP 管理 IP 分配與發送的架構會希望 VM 可以綁定 MAC 地址，如此一來才可以確保該服務可能使用相同的 IP 地址(即使 VM 遷移到不同實體機器)。 而採用靜態 IP 部署的則會希望服務 IP 固定。 基於上述的設計，公司團隊通常習慣所有服務都會有個固定 IP，因此內部防火牆或是相關服務都會透過使用這些 IP 來處理流量。 當這些服務準備透過容器來處理時，會發現就算是透過 docker container 都很難處理，何況是 Kubernetes 更為複雜的管理平台。 許許多多的文章都會告訴你，於容器化的世界中不要再執著於固定 IP 了，請改用 DNS 的方式來存取服務，譬如 Kubernetes 內的 Service 服務。 使用這類型的解決方案從邏輯上看可以滿足大部分需求，實務上則沒有想像的這麼強大，以上述的內部防火牆為範例，可能的失敗原因有 這類型的 DNS 紀錄預設都是由叢集內的 CoreDNS 來管理，這意味外部服務譬如公司防火牆根本沒有辦法取得這些資訊。將 DNS 服務串連可以正確存取後，有可能防火牆不支援使用 DNS 設定，只能使用 IP 地址。Kubernetes Service 回傳的 IP 地址是基於 ClusterIP 的設計，其指向的是一個 virtual IP，本身是沒有辦法被外部訪問的，因此本來就不會有任何外部流量使用 ClusterIP 來存取 Kubernetes 內的服務，這種設定本質上反而沒有意義。若透過 NodePort/Load-Balancer 等方式來設定網路存取，則防火牆面對的對象就不是原生應用程式 IP，而是中間層的 LoadBalacner 或是所有節點(NodePort)。如果 Port Number 會隨者部署更新而改變時，防火牆內的設定就過期了。另外一種可能性就是直接打通所有網路，讓 Contaienr Pod 的 IP 直接與現有架構是相同網段，這樣外部服務可以直接透過 PodIP 存取該應用程式，但是問題就會回歸到原本狀態，每次 Pod 重啟產生不同 IP 使得防火牆沒有辦法設定。 除了上述範例外，如果應用程式本身使用的協定不是常見的 TCP/UDP 這種，譬如 SCTP。只要 Kubernetes 內的 Service 沒有提供成熟的支援，有可能發生 SNAT/DNAT 沒辦法正常運作，這種情況下會希望能夠讓該服務有一個可直接存取的固定 IP，不需要透過 ClusterIP/NodePort 等服務存取。 ","version":"Next","tagName":"h2"},{"title":"主從網路架構​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#主從網路架構","content":"第二個要探討的則是主從網路架構，甚至可以說多重網路架構。 舉一個範例，架設 Ceph 這套分散式儲存架構時，會針對流量設計兩種不同的架構，分別處理 Control Plane 以及 Data Plane。 Control 可以想成 Ceph 架構下所有伺服器上元件互相溝通用的網路，而 Data plane 則是專門用來傳送真正使用者需要的資料。 透過這種分層架構的設計，可以讓 Control/Data 兩種流量分開處理 不會因為 Data 流量過大導致整個網路頻寬被塞爆使得 Control 流量沒有辦法正確交換。同時針對兩種不同需求，網路架構也可以分開設計，譬如 Control Plane 採用 1G 網路而 Data Plane 則是使用 100G 網路。 如果團隊內本來的應用環境也有這種需求的話，下一個問題就是，導入 Kubernetes 後該怎麼處理? 熟悉 Linux 網路的人可能會想說這個問題可以透過 Routing Table 的概念來處理，對 k8s 的節點設定不同的 Routing table 規則，讓封包可以根據不同的 destination IP 網段走不同的網卡。但是這個方法其實有一些令人困擾的事情 目前 Kubernetes 並沒有這種機制，要達成就則是管理人員要想辦法去完成這些設定。設定的同時也要考量到新節點加入後也要自動被設定，同時每個節點上的網卡名稱也有可能不同。不同 CNI 如何實作跨節點 Pod 存取的方式不同，譬如使用單純 Routing，透過 Tunnel 技術(VXLAN...etc)等不同技術。也因為這些網路底層的技術不同，因此如何設定這些 Routing rules 使用的策略也都完全不同 ","version":"Next","tagName":"h2"},{"title":"多租戶網路需求​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#多租戶網路需求","content":"第三個要探討的需求是多租戶的隔離政策，熟悉 Kubernetes 的朋友都知道 Kubernetes 透過 namespace 的概念提供一個隔離的用法，然而 namespace 實際上完全只是一個邏輯上的隔離，所有的使用資源(運算網路儲存)彼此之間並非真正隔離。 假設今天想要透過 Kubernetes 打造一個服務多位使用者的平台，使用者透過 Container 來運行所需的工作，這種情況下要如何達到網路的隔離? Kubernetes 設計了 NetworkPolicy 的標準格式，仰賴各種 CNI 的實作來提供 Kubernetes 元件之間的防火牆，然而我認為透過 NetworkPolicy 並沒有辦法達成真正的網路隔離，原因有 採用的 CNI 如果沒有實作這方面的功能，NetworkPolicy 就不能運作基於 IP/Port 的防火牆只能說是防火牆，沒有辦法當作真正的隔離，相同網域下的其他流量封包如廣播風包是否還是有機會竄流到其他的 Pod 中？ 透過這種方式是否有辦法癱瘓其他 Pod 的網路功能，造成其沒有辦法收送真正的流量相對於過往使用 VLAN TAG 等方式來隔離用戶間的流量，甚至透過 Trunk 來設定更為複雜的隔離關係，想要透過 NetworkPolicy 來實現基本上是非常痛苦且困難的 Kubenetes 網路功能 對於三個困境有些許頭緒後，接下來要思考的就是，如何於 Kubernetes 的環境中解決三個困境。 要解決這個問題，必須要先理解到底 Kubernetes 內的網路功能是如何實作的，透過對 Kubernetes 實作的理解，才有辦法仔細的思考可能的解決方案。 就我個人對 Kubernetes 的理解，目前網路功能由兩大塊元件組成，然而這兩個元件的維護是分開的，因為分開也造就整合上通常會有落差，目前能夠同時兼顧這兩塊的解決方案並沒有太多。 Kube-ProxyCNI ","version":"Next","tagName":"h2"},{"title":"Kube-Proxy​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#kube-proxy","content":"Kube-Proxy 最簡單的功用就是 Service 的實作，如何把 ClusterIP,NodePort 這些 IP:Port 給轉換到最終的 PodIP/Port 則是 Kube-Proxy 要完成的事情。 目前內建的實作主流有兩派，分別是基於 iptables 與 ipvs 兩種不同的方式。 這兩種方式都是透過 Kernel 內的 conntrack 配上各種 netfilter 的模組來完成 service 的轉換 註: iptables 與 ipvs 主流上最大的差異是效能，有興趣瞭解差異的可以參考我之前線上演講影片，有詳細介紹兩者的底層架構與實作方式 這邊要注意的， kube-proxy 幫忙設定好的這些規則，目的是幫忙轉換 ClusterIP, NodePort 到 Pod IP(這邊會發生一次的負載平衡的選擇)，至於封包到底如何於多節點之間轉送則不是 kube-proxy 要處理的，而是讓 CNI 幫忙處理。 ","version":"Next","tagName":"h2"},{"title":"CNI​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#cni","content":"CNI(Container Network Interface) 基本上需要負責兩件事情，分別是 IPAM (IP Address Management)Network Connectivity (如何讓 Pods 可以互相溝通) 上述兩個概念真正要做的事情非常簡單，就是 如何讓你節點上的 Pod 可以存取到另外一個節點上的 Pod，所以第一件事情就是幫你的 Pod 分配一個 IP 地址，接者想辦法讓不同節點之間的網路可以打通。 以下顯示了兩種可能的架構模型 第一種模型的情境是，你想要將不同地區的節點創建出一個大 Kubernetes 叢集，如下圖範例。 叢集中有三個節點，分別位於 Taipei,Hsinchu 以及 Taichung。每個節點自己本身所屬的機房都有自己的網段，分別是 192.168.56.0/24, 10.23.45.0/24 以及 10.54.78.0/24，而這三個節點本身的 IP 地址也屬於這三個網段(是節點IP，跟 Pod 沒有關係) 這三個節點的機房本身要先處理好網路的問題，確保這些節點彼此之間可以互通，這些互通的過程我統稱為WAN 網路(Underlay Network)。 三個節點上都要安裝 CNI 的應用程式(k8s CNI 是節點為單位，你可以發現所有CNI都使用 Daemonset 來安裝相關檔案)，這些 CNI 要想辦法幫各自節點上的 Pod 去維護 IP，並且基於已經打通的 underlay network 再去維護一層 overlay network。 而第二個模型則是，假設所有的 k8s 節點物理位置屬於同一個節點，譬如 Hsinchu，因此三個節點的網段都屬於 10.23.45.0/24 這種情況下，三個節點彼此之間的溝通會簡單很多，由於都屬於同一個機房網段中，我使用 LAN(Underlay) 來稱呼 CNI 的工作跟上述架構，不論底層架構長怎樣，都需要幫忙分配 PodIP 地址，並且打通彼此。 就實務上的經驗，大部分的情況都是基於第二種架構，大部分跨地區的叢集都會是獨立一套自己的 k8s 叢集，而不會讓彼此節點跨地區。 針對上述的架構，可以衍伸出至少三種不同的 IPAM 處理方式，分別如下 第一種是最常見的作法，也是目前自架叢集時 CNI 會採取的策略，針對整個 Cluster 給予一個很大的虛擬IP地址，譬如範例中的 10.123.16.0/20，意義是整個 k8s 叢集內的 Pod 都必須要符合這個網段 接者為了方便與管理，同時基於每個 Pod 的 IP 要唯一不衝突，每個 k8s 節點會需要設定能夠容量的 Pod 數量上限。針對這個上限反推出需要使用的網段大小，譬如要用 256 個Pod 就需要 /24 這樣的網段，而這個網段必須符合叢集範圍 10.123.16.0/20。 由於叢集的範圍是/20,每個節點需要/24，因此差距就是(24-20)，換算出來就是 2^4=16。 這意味整個 k8s 叢集只能有 16 個節點，每個節點 256 個 Pod，整個叢集容量就是 4096 個 Pod。  範例中可以看到三個節點分別分配了 10.123.24.4, 10.123.25.4 以及 10.123.26.4 的IP，精準地講可以說這三個節點上的Pod IP 範圍必須屬於 10.123.24.0/24, 10.123.25.0/24 以及 10.123.26.0/24。 由於 10.123.16.0/20 是一個私有的IP，每個使用者的叢集都可以有一個屬於自己的 10.123.16.0/24 網段，因此如何讓節點A上面的 10.123.24.4 跨縣市存取到節點C上面的 10.123.26.4，這部分就是 Overlay 出馬的時候了。 CNI 根據設定與需求，幫忙搭建出 Overlay 網路，最簡單的概念就是透過封裝(Encapsulating)協定，將Pod的網路封包給封裝起來，常見的 Flannel 就會使用 VXLAN 來進行封裝，其他解法也有 IPIP(IP in IP Encapsulation) 等不同協定。 註: 這種 IP 分配方法直覺且簡單，唯一要注意的是 Pod &amp; Node 的數量Pod 如果重啟被分配到不同節點，被分配到的 IP 網段直接不同，因此 IP 一定不同。 第二種架構很類似，也是先定義一個基於 k8s 叢集的網段，所有的 Pod 都要符合這個網段。接者希望這個網段是供所有節點共用，節點之間不再去區分彼此，簡單來說。 不去考慮節點最多能分配多少 Pod所有節點直接共用 /20 個網段 下圖範例中，節點A上的Pod使用 10.123.24.4 而節點 B 上的Pod則使用 10.123.24.7，兩者都屬於 10.123.16.0/20 的網段。 這種架構帶來的好處就是 Pod 不論分配到哪個節點，都可以擁有相同的網段，意味者有可能保留相同的 IP。 但是此架構帶來的壞處就是，非常難實作與管理。 每個節點上的 CNI 都是獨立運作，要如何確保每個 CNI 獨立運作時不會發生 IP 分配衝突的問題。這意味必須要有一個集中管理的資料庫來分配與計畫Overlay Network 的管理必須要以 IP 為單位去設計，不能以網段為單位去設計。 我之前有嘗試使用 k8s 內建的 etcd 作為資料庫來完成這種範例，但是使用情境其實不太多，單純是一種網路架構的探討與研究，同時底層 CNI 也必須要自己重新實作來支援這種分配。 最後一種架構則是雲端業者最喜歡提供的類型，直接將 Pod 能夠使用的網段與節點的網段直接串一起。 就網路是否能通，是否可以實作的概念來下，前述兩個概念都可以與第三種整合，也就是 Pod 與節點共享網段，但是每個節點會在分配自己的一個小網段。Pod 與節點共享網段，但是每個節點彼此不會有自己的小網段，大家直接攤平使用。 假設今天透過 AWS 的服務來架設 VM，並且於 VM 上面部署 Kubernetes 叢集。能夠讓 VM 與 K8S Pod 使用的網段全部都是基於 VPC 的設定，這種架構非常仰賴底層網路運作以及 CNI 的設定 此架構帶來的好壞處 網段共享，管理人員少一層網段需要管理，直接透過一個大網段管理節點與 Pod任何能夠針對該網段處理的服務現在都可以針對 Pod 使用封包經過愈少 NAT的轉換，除錯方面會更簡單(但是因為你使用的是別人的底層架構，其實除錯還是很麻煩，因為自己能夠碰到的地方太少)。CNI 本身都要客製化，針對不同雲端業者去設計，因此 GCP/Azure/AWS 都有設計自己的 CNI。至於這些 CNI 想要如何分配 IP，要攤平共享還是依賴節點去分配，則是依賴每家業者的實作。 困境分析 看完上述分析，對於 Kubernetes 的網路有基本概念後，接下來就要回到主題的三大困境，來聊聊這三大困境為什麼這麼難滿足 ","version":"Next","tagName":"h2"},{"title":"IP/MAC 固定​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#ipmac-固定","content":"IP的分配是由 CNI 提供的，因此 CNI 本身若不提供靜態IP的分配 -&gt; 基本上沒救如果 IP 是基於節點為單位，則固定IP 的Pod不能夠換節點，否則整個網路都不會通目前 CNI 官方維護的三種 IPAM，分別是 DHCP, Host-local 以及 Static DHCP -&gt; 就是 DHCP，但是使用上要先考慮你的 Pod 是否有辦法收到 DHCP 封包Host-local -&gt; 以節點為單位的分配方式， Flannel 實際上是偷偷呼叫 host-local 來完成 IP 分配Static -&gt; 固定 IP，標榜就是測試用。實務上很難整合，原因是當 Pod 要創立時， kubelet 會叫起 CNI 來幫忙分配，而 CNI 要怎麼知道哪些 Pod 要用 Static，哪些要用其他的 IPAM? 沒有客製化 CNI 的話，很難滿足此需求 此外固定 IP 的架構還要考慮到 基本上不可能使用多重副本的概念，否則 IP 必定衝突 ","version":"Next","tagName":"h2"},{"title":"主從網路架構​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#主從網路架構-1","content":"主從網路架構下， Underlay 網路可能會有多條，架構可能變成如下圖 整個底層網路分成 Control Plane 以及 Data Plane，網段分別是 10.23.45.0/24 以及 10.23.44.0/24。 這時候第一個問題就是，什麼流量走 Control Plane，什麼流量走 Data Plane，範例如 Pod 之間的溝通全部走 Data Plane除此之外的流量全部都走 Control Plane，譬如 API-Server, Scheullder, Controller 之間的溝通 如同前述提到的， Pod 與 Pod 之間該怎麼溝通是 CNI 負責搞定的，如果 CNI 本身根本不知道底層有兩條網路可以選，根本沒有辦法處理。 大部分的基本上 CNI 都是仰賴 Linux Kernel 內的 Routing Table 來處理 Underlay 的轉發，因此有些人會思考到利用修改 Routing Table 的方式，讓 Kernel 知道什麼樣的封包要走哪張網卡出去，也就是走哪種網路出去。 上述的思路要考慮的點 如果 CNI 採用的是封裝技術的話，則封裝過程必須要知道目標節點要使用的是屬於 Data Plane 的網段，不能使用 Control Plane 的網段如果 CNI 採用的 Routing 來處理的話，則也必須要知道 Data Plane 使用的網段與節點資訊 簡單來說，如果 CNI 本身安裝時可以設定 Underlay Network 的資訊的話，這個部分不會是太大的問題，而滿多 CNI 的確也都支援這方面的設定。 但是如果今天上述的需求改成 &quot;特定的Pod走 Data Plane，特定的 Pod 走 Control Plane&quot; 或是 &quot;特定的 Protocol:IP:Port 走 Data Plane, 剩下走 Control Plane&quot; 這種很彈性的設計，則 CNI 也很難處理。 就如同前述所講，CNI 就是創立 Pod 時就會被呼叫起來去設定IP與網路，因此架構是以 Pod 為單位，沒有辦法針對應用程式或是特定的 Pod 進行細部顆粒的處理。 ","version":"Next","tagName":"h2"},{"title":"多從租戶需求​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#多從租戶需求","content":"從上述的架構來看， CNI 就是基於 Underlay 的架構下，搭建出一個能夠讓不同節點Pod互通的 overlay 網路。 CNI 本身被呼叫時也沒有太多關於各別 Pod 的資訊，唯一有的只有 Pod Sandbox ID 這種非常粗略的資訊，因此想要 CNI 可以理解 Pod 與使用者的關係基本上只能重寫 CNI，引入這個機制與概念才有可能解決，否則預設的 CNI 根本不理解這種概念。 CNI 有能力知道每個 Pod 分配的IP(因為就是CNI分配的)，同時也有能力去搭建出串連彼此的 Overlay 網路，因此 NetworkPolicy 的引入讓 CNI 能夠針對網路流量的防火牆貢獻一點微薄之力。 Network Policy 部分如何實作完全取決於 &quot;CNI 如何實作 Network Connectivity&quot;，如果是完全仰賴 Linux Kernel 來處理封包流向的，就有可能會透過 iptables 的規則來阻擋封包傳遞。 所以對於使用者來說，假如透過 Network Policy 去設定某兩個 Pod 彼此之間不能互通，這些 Pod 的 IP 會被轉換成對應的 iptables 規則並且設定。一但 Pod 被重新部署導致 IP 改變，則 CNI 也要有能力偵測到變化並重新設新的規則 註:精準的說，CNI 是一個單次呼叫的 Process，而 CNI 解決方案通常會包含 CNI 執行檔案以及對應的 Controller，這個範例中指的是對應的 Controller。 總結來說，從網路的角度來看， CNI 是完全有辦法去做到很強大的網路隔離，但是 CNI 的架構讓其很難理解 Pod 與使用者的關係，所以想要達成 &quot;基於使用者的網路隔離&quot; 是非常困難的。 不過軟體很難說有什麼辦不到的事情，自己撰寫 CNI 就有辦法達成，只是成本效益的取捨而已。 可能解法 上述三個問題各別處理都很麻煩，何況想要同時解決三個問題，根據我過往的經驗與理解，目前架構下要完成這個議題最有效的方式就是下列兩種方式 混搭使用 CNI抽離 Kubernetes 思考的起點很簡單，根據 CNI 實作百百種，每種實作都有自己適合的場景，如果可以根據不同的需求(譬如不同的Pod)而呼叫不同的 CNI，這樣就可以彈性的去滿足 IPAM 以及網路傳輸。同時如果 Kubernetes 目前就是沒有辦法順利處理這種網路問題，那就想辦法將網路的管理給往外延伸。讓 Kubernetes 專心處理 Pod/Container 的處理，而這些特別的網路需求則讓擅長的解決方案來處理。 ","version":"Next","tagName":"h2"},{"title":"混搭 CNI​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#混搭-cni","content":"重新複習一次，根據目前 k8s 的架構，kubelet 會於創建/刪除 Pod 的時候去呼叫 CNI 來處理網路需求。由於沒有辦法很動態的去決定每個 Pod 要呼叫哪種 CNI，那就撰寫一個全新的 CNI 來提供這種功能，這種 CNI 俗稱為 metaplugin CNI。 Metaplugin CNI 可以根據需求針對單一 Pod 呼叫多種不同的 CNI 來設定網路，所以整個架構變化如下。 下圖中忽略底層的程式呼叫細節(CRI)，主要是專注於 CNI 的呼叫邏輯。 上方呈現的是本文一直探討的 CNI 架構，也是幾乎所有 K8s 玩家會使用的架構，透過一套 CNI 來處理 k8s 叢集內的所有 Pod。下方呈現的則是 Metaplugin CNI 架構， Metaplugin CNI 本身有額外的設定檔案，該檔案中描述支援哪些 CNI，以及彼此的呼叫順序。當 Pod 被創建時， Kubelet 就會把 Pod 的資訊傳給 Metaplugin CNI，該 CNI 則根據設定檔依序呼叫多個 CNI 來處理相同的一個 Pod。 舉例來說，實務上的使用流程為 安裝 Metaplugin CNI設定想要使用到的所有 CNI創建 Pod 時於 Annotation 內去描述希望該 Pod 要使用哪些 CNI。Metaplugin 被呼叫時，會去抓取到該 Pod 描述的資訊，接者呼叫要使用的 CNI 常見的 Metaplugin CNI 解決方案有 Multus-CNI 以及 DANM 兩種解決方案。 透過這種架構， K8s 可以獲得更彈性的 CNI 架構，譬如可以讓單一 Pod 裏面創建多個網路介面，同時有不同的 IP 設定。 以下是兩個使用範例(只是範例，不代表真的會這樣用) 系統內安裝兩種不同的 CNI，這兩個 CNI 設定使用不同的 overlay network，創建 Pod 的時候根據需求決定要使用哪套 CNI 就可以達成 &quot;根據 Pod 來決定要讓其流量走哪條網路&quot;。 這種架構下每個 Pod 內都只會有一張網卡，所有從該 Pod 出去的流量就會走特定的網路出去。 PodA 與 PodC 屬於同一個網路，而 PodB 與 PodD 屬於同一個網路，但是 PodA 不一定可以跟 PodB 溝通，這部分主要取決於 Underlay 網路的設計。 系統內安裝兩種不同的 CNI，這兩個 CNI 設定使用不同的 overlay network，創建 Pod 的時候同時呼叫兩種 CNI，接者 Pod 裡面透過 Routing Table 來決定封包該如何轉發。 第二種架構比較常見，讓所有的 Pod 同時都擁有兩種網路能力，Pod 內的應用程式根據 Routing Table 的規則來決定封包該如何轉送，範例如下兩圖。  這種架構下可以組合出一個有趣的玩法，譬如第一個CNI使用常見的 CNI，第二個 CNI 則使用搭配 Static IPAM 的方式來設定固定 IP。 因此該 Pod 本身會有兩個網卡，其中一個是動態取得的IP，該 IP 會走 Control Plane 的網路，而第二個網卡則是設定固定 IP，該 IP 會走 Data Plane Network。 總結來說，引入 Metaplugin CNI 的架構，可以讓 k8s 的 CNI 變得彈性與靈活，不過前提是團隊針對網路有特別需求，否則使用最簡單的 CNI 架構即可。 ","version":"Next","tagName":"h2"},{"title":"抽離 Kubernetes​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-network-issue#抽離-kubernetes","content":"有了這種混雜使用 CNI 的能力後，下一個思考的問題就是如何將 Pod 與現有的網路架構直接整合，能否跳脫 Kubernetes 的架構? 一種最簡單的做法就是使用 SRIOV 這種網路模型，將支援的實體網卡(PF)拆分多個虛擬介面(VF)並且將這些虛擬介面直接送到 Pod 內使用。 這種架構下， Pod 內所有從該網卡出去的流量都會直接從節點上的網卡出去，不會受到節點本身 Linux Kernel Network Stack 眾多功能的影響，常見的 conntrack, iptables 都沒有辦法運作。 相對的該流量會直接流入到該實體網卡所對接的網路中，所有的流量控管與處理就可以於該網路中去處理，譬如 vlan, ACL 等。 下列這張圖展示了一種使用 SR-IOV 的架構 環境中有兩種網路，所有 對使用 data plane network 有興趣的 Pod 都描述要使用 SR-IOV CNI。SR-IOV CNI 可以搭配 固定IP 或是 固定 MAC Address 來使用，如果使用 固定 MAC address 的話也可以搭配 DHCP 來分配固定 IP。Pod 創立後系統中就會產生兩張網卡，分別對應到 Control Plane 以及 Data Plane使用 Data Plane 的網卡由於流量不會經過節點本身的 Linux Kernel 處理，因此可以直接使用跟 data plane network 相同的網段來設定，不需要額外的 NAT 來轉換封包。 此架構下重新檢視最初的三個困境 固定 IP/MAC主從網路多租戶隔離 透過 Metaplugin 的架構，前述兩個議題都有辦法完成，而第三個主要取決於想要隔離的強度，基本上釐清需求後，都有辦法透過實作出來。 但是 Metaplugin CNI 也並非完美，因為 Pod 擁有能力設定多個網卡與 IP，這個作法實際上與 Kubernetes 是完全不合的。 Kubernetes 預設是每個 Pod 只會有一個 IP，所以其內部的資料結構只會記錄一個 IP 地址，同時如果要使用 Kubernetes Service 時，到底該 Service 要使用哪個 IP 來傳輸? 如果使用的是 Mutlus 這套解決方案的話，第二張網卡以後的資訊都沒有辦法讓 K8s Service 使用，的但是如果使用的是 DANM 的話則提供了辦法解決，讓你可以繼續使用 k8s service 來訪問不同的網卡。 結論 Kubernetes 並非萬能，對於大多的應用來說網路一直都不是重點，能通互相存取即可。但是對部分產業與應用程式來說，網路則是一個硬需求，這種情況下就會發覺 Kubernetes 本身的不足。 Metaplugin CNI 的概念我認為大概99%的使用者都沒有這個需求，但是學習這種概念的設計可以幫助理解其他解決方案的設計，譬如 Kubevirt 專案底下就使用 Multus 來設定 VM 間的網路。 最後還是老話一句，網路好難 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/k8s-tcpdump","content":"","keywords":"kubernetes tcpdump","version":"Next"},{"title":"Flannel​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-tcpdump#flannel","content":"Flannel 預設使用 VXLAN 這種透過 UDP 封裝的方式來處理節點間的封包傳輸，整個架構大概如下 如果對於 Flannel 的封包傳輸有興趣的，可以參考我之前撰寫的分析文CNI - Flannel - VXLAN 封包運作篇，該文章中很仔細的介紹整個封包傳輸的過程。 今天這篇文章我們不講太細，改用下列這種簡單的圖片來看架構 Flannel 的世界中，每個節點上面都會產生一個 cni0 的虛擬網卡，其本質是一個 Linux BridgePod 與 Linux Bridge 會透過 veth 的方式串接彼此，一端於 Pod 裡面，通常命名為 eth0，另一端接上 cni0 上，通常命名是 veth 開頭為了滿足 VXLAN 的作法，系統上還會有 flannel.1 的網卡 簡單來說，使用 Flannel 的情況下，創建第一個 Pod 就會讓系統上產生三個網卡(vethxxx,cni0, flannel.1)，之後每創建一個新的Pod，就會產生一個全新的 vethxxxx。 → brctl show bridge name bridge id STP enabled interfaces cni0 8000.f60b1ed977a3 no vetha3a29b21 vethb33e2721 vethedce4e05  所以想要於節點上抓取封包時，其實可以針對 vethxxx 這個虛擬網卡去抓取封包，就可以抓到從容器內送出或是即將送到容器內的封包。 比較難的問題反而是，要如何知道 veth 對應到哪一個 Pod，這邊我分享一下我的作法，直接列出所有步驟 → kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES debug-pod-7bb76865bd-96kt4 1/1 Running 0 5m14s 10.244.0.20 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-7bb76865bd-bwnwx 1/1 Running 0 62m 10.244.0.13 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-7bb76865bd-dctbh 1/1 Running 0 5m14s 10.244.0.21 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-7bb76865bd-kdqsl 1/1 Running 0 5m14s 10.244.0.18 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-7bb76865bd-n8xgr 1/1 Running 0 5m14s 10.244.0.19 k8s-dev &lt;none&gt; &lt;none&gt; → ping 10.244.0.21 -c1 PING 10.244.0.21 (10.244.0.21) 56(84) bytes of data. 64 bytes from 10.244.0.21: icmp_seq=1 ttl=64 time=0.132 ms --- 10.244.0.21 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.132/0.132/0.132/0.000 ms → arp -na | grep 10.244.0.21 ? (10.244.0.21) at e2:d0:10:82:13:e1 [ether] on cni0 → brctl showmacs cni0 | grep e2:d0:10:82:13:e1 7 e2:d0:10:82:13:e1 no 25.35 → brctl showstp cni0 | grep &quot;(7)&quot; vethe281cd54 (7) → sudo tcpdump -vvv -i vethe281cd54 icmp tcpdump: listening on vethe281cd54, link-type EN10MB (Ethernet), capture size 262144 bytes 05:29:19.930095 IP (tos 0x0, ttl 64, id 63397, offset 0, flags [DF], proto ICMP (1), length 84) 10.244.0.21 &gt; dns.google: ICMP echo request, id 490, seq 16, length 64 05:29:19.950787 IP (tos 0x0, ttl 61, id 8561, offset 0, flags [DF], proto ICMP (1), length 84) dns.google &gt; 10.244.0.21: ICMP echo reply, id 490, seq 16, length 64 05:29:20.931567 IP (tos 0x0, ttl 64, id 63476, offset 0, flags [DF], proto ICMP (1), length 84) 10.244.0.21 &gt; dns.google: ICMP echo request, id 490, seq 17, length 64  先找出你目標的 PodIP該節點上先 ping 一次該 PodIP透過 arp 看一下目標 Pod 裡面網卡的 MAC Address 是多少 (e2:d0:10:82:13:e1)透過 brctl showmacs cni0 來觀察，到底該 MAC Address 實際上在 cni0 這個 Linux Bridge 上的 port number 是多少(範例是7)透過 brctl showstep cni0 來觀察到底 7 port 對應到的網卡是誰，範例中可以查到 vethe281cd54最後直接針對該網卡去 tcpdump 即可 註: 這邊不會解釋為什麼我這麼做，畢竟牽扯到一些網路運作原理 這邊抓到的是容器進出後的封包，如果想要抓到 VXLAN 包裝的封包，那步驟就會更複雜，這邊就不探討。 ","version":"Next","tagName":"h2"},{"title":"Calico​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/k8s-tcpdump#calico","content":"Calico 並不使用 VXLAN 這種封包封裝的協定來轉發封包，取得代之的則是透過各種 routing 的方式來轉發，也因為這個特性使得要找到對應的網卡會簡單非常多。 這邊就不介紹 Calico 的架構，直接用一個簡單範例看看如何針對 Calico 的方式找到對應的虛擬網卡 → kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES debug-pod-554f8fb4b4-4wbz5 2/2 Running 0 6m16s 192.168.252.196 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-554f8fb4b4-dg2wd 2/2 Running 0 6m16s 192.168.252.194 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-554f8fb4b4-hfj46 2/2 Running 0 6m16s 192.168.252.193 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-554f8fb4b4-qw9g5 2/2 Running 0 6m16s 192.168.252.195 k8s-dev &lt;none&gt; &lt;none&gt; debug-pod-554f8fb4b4-v5v9t 2/2 Running 0 6m16s 192.168.252.197 k8s-dev &lt;none&gt; &lt;none&gt; → ip route | grep 192.168.252.193 192.168.252.193 dev cali60337b1a8c1 scope link → sudo tcpdump -vvv -i cali60337b1a8c1 icmp tcpdump: listening on cali60337b1a8c1, link-type EN10MB (Ethernet), capture size 262144 bytes 05:52:52.146265 IP (tos 0x0, ttl 64, id 63515, offset 0, flags [DF], proto ICMP (1), length 84) 192.168.252.193 &gt; dns.google: ICMP echo request, id 145, seq 138, length 64 05:52:52.171319 IP (tos 0x0, ttl 61, id 64009, offset 0, flags [DF], proto ICMP (1), length 84) dns.google &gt; 192.168.252.193: ICMP echo reply, id 145, seq 138, length 64 05:52:53.147177 IP (tos 0x0, ttl 64, id 63672, offset 0, flags [DF], proto ICMP (1), length 84) 192.168.252.193 &gt; dns.google: ICMP echo request, id 145, seq 139, length 64 05:52:53.161166 IP (tos 0x0, ttl 61, id 64014, offset 0, flags [DF], proto ICMP (1), length 84) dns.google &gt; 192.168.252.193: ICMP echo reply, id 145, seq 139, length 64  先找到目標的 PodIP透過 ip route 來看看系統上會怎麼轉發這個封包，理論上可以直接找到該 IP 應該要送到哪個網卡直接對該網卡 cali60337b1a8c1 聽封包即可 結論 網路好難 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2021/ping-implementations","content":"","keywords":"Linux Ping","version":"Next"},{"title":"Ubuntu 14.04​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/ping-implementations#ubuntu-1404","content":"第一個實驗環境如下 Ubuntu 14.04.5 LTSLinux network-lab 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linuxping utility, iputils-s20121221 該 Ubuntu 環境底下的 ping 是來自 iputiles 這個套件，版本是 2012/12/21 先透過 ls -l 的指令去觀察，可以發現 ping 這個指令的權限比較特別，是 rwsr-xr-x，其中 s 的部分就是所謂的 setuid，透過 setuid 可以讓執行該程式的使用者短暫提升權限變成該程式的 owner，這意味任何執行 ping 這個應用程式的使用者都會短暫被提權到 root 權限。 因此使用 ping 指令就會非常正常，沒有什麼特別的問題。 vagrant@network-lab:~$ ls -l $(which ping) -rwsr-xr-x 1 root root 44168 May 7 2014 /bin/ping* vagrant@network-lab:~$ ping 8.8.8.8 -c 1 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=20.7 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 20.740/20.740/20.740/0.000 ms  這時候嘗試透過 chmod u-s 的方式將 setuid 給移除，移除後就會看到 ping 指令的權限變回到 rwxr-xr-x 的權限。 這時候如果直接執行 ping 指令就會發現沒有辦法運作，直接得到 icmp open socket: Operation not permitted 錯誤訊息。 但是如果採用 sudo 的方式讓自己提權到 root，則 ping 指令也是可以順利進行。 vagrant@network-lab:~$ sudo chmod u-s $(which ping) vagrant@network-lab:~$ ls -l $(which ping) -rwxr-xr-x 1 root root 44168 May 7 2014 /bin/ping vagrant@network-lab:~$ ping 8.8.8.8 -c 1 ping: icmp open socket: Operation not permitted vagrant@network-lab:~$ vagrant@network-lab:~$ sudo ping 8.8.8.8 -c1 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=14.0 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 14.020/14.020/14.020/0.000 ms vagrant@network-lab:~$  嘗試透過 strace 去觀察一下到底上述的 icmp open socket 跟什麼有關，可以發現是 socket(PF_INET, SOCK_RAW, IPPROTO_ICMP) = -1 EPERM (Operation not permitted) 這個 syscall 造成的，看起來一般使用者是沒有辦法創造基於 ICMP 協定的 RAW Socket，所以才需要借助 setuid 來提權。 vagrant@network-lab:~$ strace ping 8.8.8.8 -c1 ... capget({_LINUX_CAPABILITY_VERSION_3, 0}, NULL) = 0 capget({_LINUX_CAPABILITY_VERSION_3, 0}, {0, 0, 0}) = 0 socket(PF_INET, SOCK_RAW, IPPROTO_ICMP) = -1 EPERM (Operation not permitted) capget({_LINUX_CAPABILITY_VERSION_3, 0}, NULL) = 0 capget({_LINUX_CAPABILITY_VERSION_3, 0}, {0, 0, 0}) = 0 socket(PF_INET, SOCK_DGRAM, IPPROTO_IP) = 3 connect(3, {sa_family=AF_INET, sin_port=htons(1025), sin_addr=inet_addr(&quot;8.8.8.8&quot;)}, 16) = 0 getsockname(3, {sa_family=AF_INET, sin_port=htons(44904), sin_addr=inet_addr(&quot;10.0.2.15&quot;)}, [16]) = 0 close(3) = 0 dup(2) = 3 fcntl(3, F_GETFL) = 0x8002 (flags O_RDWR|O_LARGEFILE) fstat(3, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f845fe4a000 lseek(3, 0, SEEK_CUR) = -1 ESPIPE (Illegal seek) write(3, &quot;ping: icmp open socket: Operatio&quot;..., 48ping: icmp open socket: Operation not permitted ) = 48 close(3) = 0 munmap(0x7f845fe4a000, 4096) = 0 exit_group(2) = ?  ","version":"Next","tagName":"h2"},{"title":"CentOS 7​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/ping-implementations#centos-7","content":"CentOS Linux release 7.9.2009 (Core)3.10.0-1062.18.1.el7.x86_64ping utility, iputils-s20160308 這個環境中首先透過 ls 觀察 ping 指令的權限，可以發現是單純的 rwx-r-xr-x，但是 ping 指令是可以正常運作的 vagrant@network-lab:~$ ls -l $(which ping) -rwxr-xr-x. 1 root root 66176 Aug 4 2017 /usr/bin/ping vagrant@network-lab:~$ ping 8.8.8.8 -c1 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=93 time=7.85 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 7.851/7.851/7.851/0.000 ms  這種情況下 ping 可以運作是因為透過了 Linux Capabilities 這個框架賦予該應用程式額外的權限，可以透過 getcap 這個指令來觀察 vagrant@network-lab:~$ getcap $(which ping) /usr/bin/ping = cap_net_admin,cap_net_raw+p  從上述的指令可以觀察到 ping 這個應用程式被賦予了 net_admin, net_raw+p(permitted) 這兩個主要權限，這時候透過 strace 去觀察該 ping 指令的運行(strace 預設情況都會忽略 setuid/capabilities, 要用 -u 去處理)，可以看到整個過程很順利基於 SOCK_RAW 去開啟一個 ICMP 協定的 socket，該 socket fd 是 3，後續就針對 3 這個 fd 進行 ICMP 封包的讀寫。 vagrant@network-lab:~$ sudo strace -u vagrant ping 8.8.8.8 -c1 .... socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) = 3 .... setsockopt(3, SOL_SOCKET, SO_TIMESTAMP, [1], 4) = 0 setsockopt(3, SOL_SOCKET, SO_SNDTIMEO, &quot;\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0&quot;, 16) = 0 setsockopt(3, SOL_SOCKET, SO_RCVTIMEO, &quot;\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0&quot;, 16) = 0 getpid() = 32035 ... sendto(3, &quot;\\10\\0\\230\\34}#\\0\\1\\271\\0217a\\0\\0\\0\\0(y\\v\\0\\0\\0\\0\\0\\20\\21\\22\\23\\24\\25\\26\\27&quot;..., 64, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(&quot;8.8.8.8&quot;)}, 16) = 64 recvmsg(3, {msg_name={sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(&quot;8.8.8.8&quot;)}, msg_namelen=128-&gt;16, msg_iov=[{iov_base=&quot;E`\\0T\\0\\0\\0\\0]\\1\\247q\\10\\10\\10\\10\\n\\36\\233\\252\\0\\0\\240\\34}#\\0\\1\\271\\0217a&quot;..., iov_len=192}], msg_iovlen=1, msg_control=[{cmsg_len=32, cmsg_level=SOL_SOCKET, cmsg_type=SCM_TIMESTAMP, cmsg_data={tv_sec=1630998969, tv_usec=759874}}], msg_controllen=32, msg_flags=0}, 0) = 84 write(1, &quot;64 bytes from 8.8.8.8: icmp_seq=&quot;..., 5464 bytes from 8.8.8.8: icmp_seq=1 ttl=93 time=7.96 ms ) = 54 write(1, &quot;\\n&quot;, 1 ) = 1 write(1, &quot;--- 8.8.8.8 ping statistics ---\\n&quot;, 32--- 8.8.8.8 ping statistics --- ) = 32 write(1, &quot;1 packets transmitted, 1 receive&quot;..., 601 packets transmitted, 1 received, 0% packet loss, time 0ms  這個時候如果透過 setcap 將 capbility 給拔掉，整個 ping 指令又會不能正常運作了。 vagrant@network-lab:~$ sudo setcap -r $(which ping) vagrant@network-lab:~$ ./ping 8.8.8.8 -c1 ping: socket: Operation not permitted vagrant@network-lab:~$ sudo strace -u vagrant ping 8.8.8.8 -c1 ... socket(AF_INET, SOCK_DGRAM, IPPROTO_ICMP) = -1 EACCES (Permission denied) socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) = -1 EPERM (Operation not permitted) open(&quot;/usr/share/locale/locale.alias&quot;, O_RDONLY|O_CLOEXEC) = 3 fstat(3, {st_mode=S_IFREG|0644, st_size=2502, ...}) = 0 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f088dd51000 read(3, &quot;# Locale name alias data base.\\n#&quot;..., 4096) = 2502 read(3, &quot;&quot;, 4096) = 0 close(3) = 0 ... write(2, &quot;ping: socket: Operation not perm&quot;..., 38ping: socket: Operation not permitted ...  ","version":"Next","tagName":"h2"},{"title":"Ubuntu 20.04​","type":1,"pageTitle":"前言","url":"/docs/techPost/2021/ping-implementations#ubuntu-2004","content":"前述兩個環境分別透過 SetUID 與 Capabilities 讓一般使用者都可以順利的使用 PING ，然而下列的實驗環境卻完全不同了 Ubuntu 20.04.1 LTSLinux network-lab 5.4.0-58-generic #64-Ubuntu SMP Wed Dec 9 08:16:25 UTC 2020 x86_64 x86_64 x86_64 GNU/Linuxping: ping from iputils s20190709 該系統下的 ping 指令也沒有賦予 SetUID 的權限，但是 Capabilities 還是有給予 net_raw 的權限，這時候嘗試將該 Capabilities 給移除並且使用 ping 看看。 vagrant@network-lab:~$ ls -l $(which ping) -rwxr-xr-x 1 root root 72776 Jan 30 2020 /usr/bin/ping vagrant@network-lab:~$ getcap $(which ping) /usr/bin/ping = cap_net_raw+ep vagrant@network-lab:~$ sudo setcap -r $(which ping) vagrant@network-lab:~$ getcap $(which ping) vagrant@network-lab:~$ ping 8.8.8.8 -c1 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=22.2 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 22.150/22.150/22.150/0.000 ms  與前述不同的是，就算不給予 SetUID 與 Capabilities， ping 指令也可以正常運作，就算使用最基本的 strace 去觀察 ping 指令也可以正常運作，似乎除了 SetUID 與 Ccapbilities 外還有其他的機制來幫忙處理 vagrant@network-lab:~$ strace ping 8.8.8.8 -c1 ... socket(AF_INET, SOCK_DGRAM, IPPROTO_ICMP) = 3 socket(AF_INET6, SOCK_DGRAM, IPPROTO_ICMPV6) = 4 ... sendto(3, &quot;\\10\\0\\v\\372\\0\\0\\0\\1`\\0247a\\0\\0\\0\\0\\211\\274\\f\\0\\0\\0\\0\\0\\20\\21\\22\\23\\24\\25\\26\\27&quot;..., 64, 0, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(&quot;8.8.8.8&quot;)}, 16) = 64 setitimer(ITIMER_REAL, {it_interval={tv_sec=0, tv_usec=0}, it_value={tv_sec=10, tv_usec=0}}, NULL) = 0 recvmsg(3, {msg_name={sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(&quot;8.8.8.8&quot;)}, msg_namelen=128-&gt;16, msg_iov=[{iov_base=&quot;\\0\\0\\23\\367\\0\\3\\0\\1`\\0247a\\0\\0\\0\\0\\211\\274\\f\\0\\0\\0\\0\\0\\20\\21\\22\\23\\24\\25\\26\\27&quot;..., iov_len=192}], msg_iovlen=1, msg_control=[{cmsg_len=32, cmsg_level=SOL_SOCKET, cmsg_type=SO_TIMESTAMP_OLD, cmsg_data={tv_sec=1630999648, tv_usec=855986}}, {cmsg_len=20, cmsg_level=SOL_IP, cmsg_type=IP_TTL, cmsg_data=[63]}], msg_controllen=56, msg_flags=0}, 0) = 64 write(1, &quot;64 bytes from 8.8.8.8: icmp_seq=&quot;..., 5464 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=21.3 ms ) = 54 write(1, &quot;\\n&quot;, 1 ) = 1 write(1, &quot;--- 8.8.8.8 ping statistics ---\\n&quot;, 32--- 8.8.8.8 ping statistics --- ) = 32 write(1, &quot;1 packets transmitted, 1 receive&quot;..., 601 packets transmitted, 1 received, 0% packet loss, time 0ms ) = 60 write(1, &quot;rtt min/avg/max/mdev = 21.289/21&quot;..., 53rtt min/avg/max/mdev = 21.289/21.289/21.289/0.000 ms ) = 53 close(1) = 0 close(2) = 0 exit_group(0) = ? +++ exited with 0 +++  透過 strace 的觀察結果，我發現呼叫的 syscall 有些微的不同，之前的系統是透過 socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) = 3 取得一個基於 RAW Socket 的 ICMP Socket，而新版則是呼叫 socket(AF_INET, SOCK_DGRAM, IPPROTO_ICMP) = 3，基於 DGRAM 類型的 ICMP Socket. 針對這個關鍵字去搜尋後可以得到這個最初討論的 Linux Kernel Patch ipv4: add ICMP socket kind，該 Patch 希望於基於 SOCK_DGRAM 去增加一個全新的 IPPROTO_ICMP 的類型封包，讓收送 ICMP 封包可以不需要透過 RAW Socket 來處理，而是讓 Kernel 幫忙處理掉 ICMP 的封包來回，藉此打造出一個不用權限的 ping 指令。 這也是為什麼第三個環境系統上的 ping 指令不需要任何 SetUID 與 Capabilities 也能夠順利的收送 ICMP 封包，原因就是底層 Kernel 使用的機制不同，從過往的 RAW Socket 改成專門提供 ICMP 服務的 socket。 上述的 Patch 最後還是有加上一些門檻，並非所有使用者都可以直接呼叫 socket(AF_INET, SOCK_DGRAM, IPPROTO_ICMP) 來收送 ICMP 封包，只有呼叫者的 GID 符合 net.ipv4.ping_group_range 這個參數的使用者才可以直接呼叫。 以第三個系統來說，可以觀察到 vagrant@network-lab:~$ sysctl net.ipv4.ping_group_range net.ipv4.ping_group_range = 0 2147483647 vagrant@network-lab:~$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),111(lxd),118(lpadmin),119(sambashare),997(docker)  該系統內只要使用者的 GID 是介於 0~2147483647 的就可以呼叫該特殊的 socket，基於實驗精神，將該參數改到無法符合當前 vagrant 使用者的 GID 並且再次執行 ping 看看 vagrant@network-lab:~$ sudo sysctl -w net.ipv4.ping_group_range=&quot;2147483647 2147483647&quot; net.ipv4.ping_group_range = 2147483647 2147483647 vagrant@network-lab:~$ sysctl net.ipv4.ping_group_range net.ipv4.ping_group_range = 2147483647 2147483647 vagrant@network-lab:~$ ping 8.8.8.8 -c1 ping: socket: Operation not permitted  透過 sysctl 的指令將 ping_group_range 的範圍修改成只有 2147483647 可以符合，而 vagrant 這個使用者的 GID 並沒有包含其中，所以這時候的 ping 指令就沒有辦法順利啟動。 最後進行一個小實驗，手動新增一個全新的 group，將其 GID 設定為 2147483647，並命名為 ping_test 接者將 vagrant 使用者加入到該 ping_test 的群組中，再次使用 ping 指令測試看看。 vagrant@network-lab:~$ sudo groupadd ping_test -g 2147483647 vagrant@network-lab:~$ sudo usermod -a -G ping_test vagrant vagrant@network-lab:~$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),111(lxd),118(lpadmin),119(sambashare),997(docker),2147483647(ping_test) vagrant@network-lab:~$ ping 8.8.8.8 -c1 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=15.0 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 15.027/15.027/15.027/0.000 ms  果不其然的 ping 又可以順利啟動了 透過一系列簡單的實驗觀察了不同系統上三種不同的 ping 實作方式，有趣的讀者也可以觀察一下自己的系統目前是採取何種方式的實作 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2022/k8s-capacity-allocatable","content":"前言 玩過 Kubernetes 的玩家大概都知道 Pod 裡面都可以透過 Request/Limit 的方式來限制單一Pod 針對 CPU, Memory 的用量，而 Kubernetes 會根據這些使用量幫忙分發這些 Pod 到符合需求的節點上，不考慮其他各種條件下，一個最基本的條件就是節點上要有足夠的 CPU/Memory 供目標 Pod 使用。 透過 kubectl describe node 可以觀察到每個節點上的資源分配，其中關於資源分配有兩個重要的概念分別是 Capacity 與 Allocatable，而本篇文章就來仔細研究一下這兩個 概念的差異以及實務上要注意的地方有哪些。 對於一個 Kubernetes 的節點來說，資源量通常會包含 CPU, Memory, Ephemeral-Storage 等，而節點本身擁有的總量就會稱為 Capacity，而能夠分配給 Kubernetes Pod 的總量則會稱為 Allocatable。 透過 kubectl describe node 都可以到這些資料，譬如透過 kubeadm 安裝的預設叢集資訊如下 註: 本篇文章所有環境都基於 Kubernetes v1.21.8 Capacity: cpu: 2 ephemeral-storage: 64800356Ki hugepages-2Mi: 0 memory: 4039556Ki pods: 110 Allocatable: cpu: 2 ephemeral-storage: 59720007991 hugepages-2Mi: 0 memory: 3937156Ki pods: 110 上述的資料是由每個節點上的 kubelet 維護而回傳的，以 kubelet 來說， Capacity 與 Applicatable 之間的關係是如下 Capacity = Allocatable + System-Reserved + Kube-Reserved + Eviction-Thresholds System-Reserved 作為一個 Kubernetes 節點，其本身除了幫 Kubernetes 運行各種容器外本身系統上也會有一些服務要運行，譬如 sshd, dhclient 等各種系統服務，以 CPU 的概念來說 如果今天將節點上所有的 CPU 都分配給 Kubernetes Pod 使用，那有沒有可能節點上的系統應用，如 sshd 等就沒有足夠的 CPU 去維持基本運作？ 而上述的 System-Reserved 就是針對這個情境設計的，主要是讓 kubelet 知道請預留一些系統資源給系統相關服務，不要將所有的節點資源都分配給 Pod 使用。 預設值: 除非特別指定，否則預設不開 設定: kubelet 透過 --system-reserved 來設定不同資源量 Kube-Reserved 與 System-Reserved 概念完全一樣，不過 Kube-Reserved 的目的則是針對任何與 Kubernetes 互動的應用，最簡單的範例就是 kubelet 這個應用程式，透過這個參數可以稍微地讓 kubelet 彈性一點的去設計資源用量，針對 system(系統應用)與 kube(k8s 相關應用)等資源控制 預設值: 除非特別指令，否則預設不開 設定: kubelet 透過 --kube-reserved 來設定不同資源量 Evicition-Thresholds 舉例來說，當節點上 Memory 使用過多時，就有可能會產生 OOM 的情況導致系統上的正在運行的 K8s Pod 被 Kernel 給刪除，為了盡量減少這個問題的發生可能性， kubelet 發現節點資源快不夠時就會開始將運行的 Pod 給踢出去，讓 Scheudler 想辦法將該 Pod 給調度到其他資源比較充沛的節點去運行。 所以 Eviction Thresholds 就是一個資源門檻，當系統資源低於該門檻時就會觸發剔除機制 預設值: 預設打開，Memory 是 100MiB, Ephemeral-Storage 是 10% 設定: kubelet 透過 --eviction-hard 來設定不同資源量 有了基本概念後來看一下上述的概念 Capacity: cpu: 2 ephemeral-storage: 64800356Ki hugepages-2Mi: 0 memory: 4039556Ki pods: 110 Allocatable: cpu: 2 ephemeral-storage: 59720007991 hugepages-2Mi: 0 memory: 3937156Ki pods: 110 這邊分析幾個重要的節點資源 CPU: 沒有差異Memory: 4039556Ki-3937156Ki, 差額是 102400Ki, 也就是 100MiEphemeral-Storage: 64800356Ki - 59720007991 Byte, 先將前者轉為 Byte 乘上 12，這時候就是Capacity:66355564544, Allocatable: 59720007991, 所以 59720007991/66355564544 大約是 0.8999999851 也就是 0.9 所以真正能夠配置的容量只有 90% 註: Mi, Ki, Gi 都是基於 1024 前述的 Memory/Ephemeral-Storage 的基本用量都是被 Evicition Threshold 給佔走。 這時候嘗試針對 Kube-Reserved, System-Reserved 以及 evictionHard 三個參數來設定看看 註: Eviction Threshold 的設定實際上透過 evictionHard 的參數 針對 /var/lib/kubelet/config.yaml 加入下列內容 systemReserved: memory: 500Mi cpu: 250m kubeReserved: memory: 1Gi cpu: 500m evictionHard: memory.available: 200Mi nodefs.available: 20Gi 並且透過 sudo systemctl restart kubelet 重啟 kubelet 來載入新設定，一切都完畢後就透過 kubectl describe node 觀察一下變化 Capacity: cpu: 2 ephemeral-storage: 64800356Ki hugepages-2Mi: 0 memory: 4039556Ki pods: 110 Allocatable: cpu: 1250m ephemeral-storage: 43828836Ki hugepages-2Mi: 0 memory: 2274180Ki pods: 110 先計算一下我們的設定會使用多少系統資源如下表 注意的是 Memory 有 Gi 與 Mi，而 1Gi 則是 1024 Mi，所以總額是 1724 Mi 資源類型\tSystemReserved\tKubeReserved\tEvictionHard\t總共CPU\t250m\t500m\t0\t750m Memory\t500Mi\t1Gi\t200Mi\t1724Mi Ephemeral-storage\t0\t0\t20Gi\t20Gi 有個基本概念後就來計算一下實際上的差異吧 CPU 部分 2 代表 2000m, 所以差額很簡單就是 750mMemory 部分先使用 Ki 進行運算，相減得到 1765376 Ki, Ki/1024 會得到 Mi，所以 1765376Ki/1024 = 1724 MiStorage 的話也是先用 Ki 運算會得到 20971520Ki, 將這個 Ki/1024/1024 會得到 Gi，所以 20971520/1024/1024= 20Gi 資源類型\tCapacity\tAllocatable\t差額CPU\t2\t1250m\t750m Memory\t4039556Ki\t2274180Ki\t1724Mi Ephemeral-storage\t64800356Ki\t43828836Ki\t20Gi 經過驗算結果完全符合預期，以一張圖來概括上述的概念 Enfore Node Allocatable 上述瞭解了基本 Capacity 與 Allocatable 的基本概念與計算方式後，下一個來瞭解的就是更為細節的應用程式控管。 實際上 system-reserved 與 kube-reserved 這兩個參數的含義是 「請求 kubelet 根據 system-reserved 與 kube-reserved 的參數幫我預留系統資源，避免 Kubernetes Pod 佔用過多資源。」 這時候先問幾個問題來思考 系統應用程式或是 Kubernetes 相關應用程式如果用超過設定(system-reserved, kube-reserved) 的系統資源，會發生什麼事情？什麼樣的應用程式歸類於 system-reserved? 什麼樣的應用程式歸類於 kube-reserved?自行開發的應用程式可以加入到其中一個類別嗎？ 預設情況下，上述的答案是 沒有事情，什麼都不會發生因為超過也不會發生任何事情，所以到底有誰也沒有意義因為超過也不會發生任何事情，所以自己的應用程式要不要被納管也沒有意義 如果希望(1)可以有所作為，譬如應用程式用太多就把他砍掉，那該怎麼做？ 這時候就要使用 kubelet 的另外一個參數 --enforce-node-allocatable ，這個參數有三個參數可以組合使用，分別是 pods, system-reserved 以及 kube-reserved 。 該參數的意義是「哪些類型的資源超過用量要被系統幹掉」，預設值是 Pods，這也是為什麼 Pod 如果有透過 request/limit 等設定一些用量但是卻超過時可能就會觸發 OOM 然後被系統直接砍掉 而 system-reserved 與 kube-reserved 預設都不會被設定，所以用超過量也沒有任何問題。 而實作上 kubelet 也不參與任何監控與刪除應用程式的決策，只是單純根據設定把一切都交給 cgroup，讓 kernel 來幫忙處理，所以如果你看文件的話會告說如果想要於 enforce-node-allocatable 中設定 system-reserved 與 kube-reserved 的話，你也必須要設定 --kube-reserved-cgroup, --system-reserved-cgroup 這兩個參數。 註: 上述兩個參數預設都是空白，所以要使用一定要設定本文就不介紹 cgroup 的概念，直接假設讀者都有基本概念 加入以下資料到 /var/lib/kubelet/config.yaml systemReservedCgroup: /system.slice enforceNodeAllocatable: - pods - system-reserved 上述範例是告訴 kubelet 請幫我針對 system-reserved 群組的應用程式進行容量控管，另外 system-reserved 的定義就是所以 /system.slice 這個 cgroup 路徑下的應用程式。 我的 Ubuntu 18.04 環境中， /system.slice 關於 CPU 則有下列應用程式 ○ → lscgroup cpu:/system.slice cpu,cpuacct:/system.slice/ cpu,cpuacct:/system.slice/irqbalance.service cpu,cpuacct:/system.slice/systemd-update-utmp.service cpu,cpuacct:/system.slice/vboxadd-service.service cpu,cpuacct:/system.slice/lvm2-monitor.service cpu,cpuacct:/system.slice/systemd-journal-flush.service cpu,cpuacct:/system.slice/containerd.service cpu,cpuacct:/system.slice/systemd-sysctl.service cpu,cpuacct:/system.slice/systemd-networkd.service cpu,cpuacct:/system.slice/systemd-udevd.service cpu,cpuacct:/system.slice/lxd-containers.service cpu,cpuacct:/system.slice/cron.service cpu,cpuacct:/system.slice/sys-fs-fuse-connections.mount cpu,cpuacct:/system.slice/networking.service cpu,cpuacct:/system.slice/sys-kernel-config.mount cpu,cpuacct:/system.slice/docker.service cpu,cpuacct:/system.slice/polkit.service cpu,cpuacct:/system.slice/systemd-remount-fs.service cpu,cpuacct:/system.slice/networkd-dispatcher.service cpu,cpuacct:/system.slice/sys-kernel-debug.mount cpu,cpuacct:/system.slice/accounts-daemon.service cpu,cpuacct:/system.slice/systemd-tmpfiles-setup.service cpu,cpuacct:/system.slice/kubelet.service cpu,cpuacct:/system.slice/console-setup.service cpu,cpuacct:/system.slice/vboxadd.service cpu,cpuacct:/system.slice/systemd-journald.service cpu,cpuacct:/system.slice/atd.service cpu,cpuacct:/system.slice/systemd-udev-trigger.service cpu,cpuacct:/system.slice/lxd.socket cpu,cpuacct:/system.slice/ssh.service cpu,cpuacct:/system.slice/dev-mqueue.mount cpu,cpuacct:/system.slice/ufw.service cpu,cpuacct:/system.slice/systemd-random-seed.service cpu,cpuacct:/system.slice/snapd.seeded.service cpu,cpuacct:/system.slice/rsyslog.service cpu,cpuacct:/system.slice/systemd-modules-load.service cpu,cpuacct:/system.slice/blk-availability.service cpu,cpuacct:/system.slice/systemd-tmpfiles-setup-dev.service cpu,cpuacct:/system.slice/rpcbind.service cpu,cpuacct:/system.slice/lxcfs.service cpu,cpuacct:/system.slice/grub-common.service cpu,cpuacct:/system.slice/ebtables.service cpu,cpuacct:/system.slice/snapd.socket cpu,cpuacct:/system.slice/kmod-static-nodes.service cpu,cpuacct:/system.slice/run-rpc_pipefs.mount cpu,cpuacct:/system.slice/lvm2-lvmetad.service cpu,cpuacct:/system.slice/docker.socket cpu,cpuacct:/system.slice/apport.service cpu,cpuacct:/system.slice/apparmor.service cpu,cpuacct:/system.slice/systemd-resolved.service cpu,cpuacct:/system.slice/system-lvm2\\x2dpvscan.slice cpu,cpuacct:/system.slice/dev-hugepages.mount cpu,cpuacct:/system.slice/dbus.service cpu,cpuacct:/system.slice/system-getty.slice cpu,cpuacct:/system.slice/keyboard-setup.service cpu,cpuacct:/system.slice/systemd-user-sessions.service cpu,cpuacct:/system.slice/systemd-logind.service cpu,cpuacct:/system.slice/setvtrgb.service 從上述的檔案名稱應該可以看到滿滿的系統服務。 修改完畢 kubelet 後重啟會發現 kubelet 啟動失敗，觀察 log 會得到一個告知 /system.slice 路徑不存在的錯誤 kubelet.go:1391] &quot;Failed to start ContainerManager&quot; err=&quot;Failed to enforce System Reserved Cgroup Limits on \\&quot;/system.slice\\&quot;: [\\&quot;system.slice\\&quot;] cgroup does not exist&quot; 實際上這個問題是 kubelet 會嘗試從眾多 cgroup 子系統去找，只要有一個沒有存在就直接當錯誤，根據下列的原始碼 func (m *cgroupManagerImpl) Exists(name CgroupName) bool { if libcontainercgroups.IsCgroup2UnifiedMode() { cgroupPath := m.buildCgroupUnifiedPath(name) neededControllers := getSupportedUnifiedControllers() enabledControllers, err := readUnifiedControllers(cgroupPath) if err != nil { return false } difference := neededControllers.Difference(enabledControllers) if difference.Len() &gt; 0 { klog.V(4).InfoS(&quot;The cgroup has some missing controllers&quot;, &quot;cgroupName&quot;, name, &quot;controllers&quot;, difference) return false } return true } // Get map of all cgroup paths on the system for the particular cgroup cgroupPaths := m.buildCgroupPaths(name) // the presence of alternative control groups not known to runc confuses // the kubelet existence checks. // ideally, we would have a mechanism in runc to support Exists() logic // scoped to the set control groups it understands. this is being discussed // in https://github.com/opencontainers/runc/issues/1440 // once resolved, we can remove this code. allowlistControllers := sets.NewString(&quot;cpu&quot;, &quot;cpuacct&quot;, &quot;cpuset&quot;, &quot;memory&quot;, &quot;systemd&quot;, &quot;pids&quot;) if _, ok := m.subsystems.MountPoints[&quot;hugetlb&quot;]; ok { allowlistControllers.Insert(&quot;hugetlb&quot;) } var missingPaths []string // If even one cgroup path doesn't exist, then the cgroup doesn't exist. for controller, path := range cgroupPaths { // ignore mounts we don't care about if !allowlistControllers.Has(controller) { continue } if !libcontainercgroups.PathExists(path) { missingPaths = append(missingPaths, path) } } if len(missingPaths) &gt; 0 { klog.V(4).InfoS(&quot;The cgroup has some missing paths&quot;, &quot;cgroupName&quot;, name, &quot;paths&quot;, missingPaths) return false } return true } kubelet 會於我的系統中去找 cpu, cpuacct, cpuset, memory, systemd, pids, hugetlb 這些子系統，很不幸我的系統中 cpuset, hugetlb, systemd 並沒有包含 /system.slice 這個路徑，這邊可以透過 mkdir -p 的方式創建 cgroup 的關係 這意味如果要使用這個參數來控制時，要好好設定目標的 cgroup 路徑才可以正常啟動 kubelet ○ → mkdir -p /sys/fs/cgroup/hugetlb/system.slice ○ → mkdir -p /sys/fs/cgroup/cpuset/system.slice ○ → mkdir -p /sys/fs/cgroup/systemd/system.slice ○ → systemctl restart kubelet 一切都正常執行完畢後就可以透過 cgroup 的指令來檢查前述設定的 system-reserved 資源是否都有被設定到對應的 cgroup 上 先複習一下先前的表格 資源類型\tSystemReserved\tKubeReserved\tEvictionHard\t總共CPU\t250m\t500m\t0\t750m Memory\t500Mi\t1Gi\t200Mi\t1724Mi Ephemeral-storage\t0\t0\t20Gi\t20Gi SystemReserved 的 cpu 是 250m, 而 Memory 是 500Mi ○ → cat /sys/fs/cgroup/cpu/system.slice/cpu.shares 256 ○ → cat /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes 524288000 這邊可以看到 CPU 是 256，以 1024 為單位去計算就是 25% 也就是 250m 的單位 而 Memory 的單位是 bytes，524288000/1024/1024 = 500Mi 可以看到 CPU 的設定完全與前述的設定一致，為了二次求證打開 kubelet 設定修改成不同的數值再次觀察 systemReserved: memory: 1Gi cpu: &quot;1&quot; ○ → cat /sys/fs/cgroup/cpu/system.slice/cpu.shares 1024 ○ → cat /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes 1073741824 這時候可以看到 cpu.shares 符合設定的 &quot;1&quot;，而 memory 則是 1024^3，也就是 1Gi. 所以這種情況下該 cgroup /system.slice 就會受到 cgroup 的控管確保其使用資源量不會超過設定。 講了這麼多回到前述三個問題 系統應用程式或是 Kubernetes 相關應用程式如果用超過設定(system-reserved, kube-reserved) 的系統資源，會發生什麼事情？什麼樣的應用程式歸類於 system-reserved? 什麼樣的應用程式歸類於 kube-reserved?自行開發的應用程式可以加入到其中一個類別嗎？ 充分理解後答案就是 看你有沒有透過 enforce-node-allocatable 讓 kubelet 請 kernel cgroup 幫忙控管透過 system-reserved-cgroup 與 kube-reseved-cgroup 兩個參數來指定 cgroup 路徑將你的應用程式加入到對應的 cgroup 群組，請記得不同的資源是不同的路徑 最後，如果你對這些設定有興趣也認為似乎可以更佳控管系統資源用量，請務必先行測試並且確認自己了解 cgroup 的一切概念，以免未來除錯時完全不知道該從何下手 此外，針對 system-reserved, kube-reserved 等應用程式的系統用量，請先用監控系統長期觀察獲得一個概念後再來設定 同時要抱持者一旦開啟這兩個設定，這些應用程式是有機會被 OOM 移除的心理準備與認知，以免到時候發生問題時一問三不知，不知道發生什麼事情","keywords":"Kubernetes resource management CPU Memory","version":"Next"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2023/gitops-repo-structure","content":"","keywords":"Kubernetes GitOps DevOps","version":"Next"},{"title":"Vanilla YAML​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#vanilla-yaml","content":"直接將所有 Kubernetes 物件以 YAML 格式來描述 優點: 直觀，適合初學者無需任何工具，準備 YAML 檔案就可以搭配 kubectl 使用 缺點: 缺乏彈性，有任何客製化需求就需要維護多份檔案沒有版本的概念部署環境愈多，維護與使用起來愈複雜與麻煩 因為上述原因，很少團隊會直接使用這種方式來管理複雜且多環境的應用程式。 但是有些第三方解決方案會透過這種方式來定義其使用，原因有 使用到的 K8s 物件少，不複雜專案更專注於應用程式開發，部署部分則由開發者自行處理透過 Git Branch 的方式來維護不同版本的差異 此種方式也很適合初學者用來學習 K8s 的概念與練習，熟悉概念後再使用接下來的工具去管理才不會太陌生。 以下圖總結來說，每個環境都需要維護一整份近乎一樣的一群檔案，維護上非常困難 ","version":"Next","tagName":"h2"},{"title":"Helm​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#helm","content":"Helm 可以說是目前 Kubernetes 應用程式定義最知名的工具之一，透過 Helm Chart 與 Go Template 等機制讓你可以更輕鬆的根據環境客製化部署資源。 優點： 基於 Go Template 來動態產出最後的 k8s 資源可打包成一個容易散佈與安裝的格式，並且支援版本控制支援 Dependency 概念，可以組合出 Umbrella Chart生態系豐富，眾多第三方專案都有支援 Helm Chart 的安裝，因此學會 Helm 對於使用這些專案會較少阻力 缺點: 學習曲線偏高，Template 的語法可能過於複雜需要額外安裝 Helm 指令使用有可能還需要額外維護 Helm Chart Server Helm 的特性使得團隊只需要針對主體維護一份檔案，針對不同的部署環境準備相對應的 values.yaml 就可以動態的產生出符合各環境需求的 K8s 物件，整個流程如下  Helm Chart 本身除了本地直接載入使用外，也可以將其打包並且發佈到 Helm Chart 伺服器 去，其本質基本上就是一個 web 伺服器，目前稍微有些規模的開源專案也都會透過這種方式來發布自己專案的 Helm Chart，因此使用者就可以很輕易地選擇所需要的版本並且搭配 values.yaml 來客製化，整個流程類似下圖  此外，有些團隊所開發且交付的不單純只是一個應用程式，而是一個由眾多應用程式所組合而成的服務，這種需求下會採用 Umbrella Chart 的方式，其概念非常簡單 交付與部署的不是一個應用程式，而是一個完整的服務該 Umbrella Helm Chart 實務上會依賴許多更多 Helm Chart部署該 Helm Chart 會一併把所有被依賴的 Helm Chart 一併部署Umbrella Helm Chart 的 Values.yaml 會龐大且複雜，可以動態決定哪些依賴需要安裝，同時也可以將設定之參數給傳遞過去 範例如下圖，整個環境中有 5 個 Helm Chart，其中有一個主要的 Helm Chart 會透過 helm dependency 來連結 Helm Chart{A/B/C/D}。 使用者只需要準備一個 Values.yaml 並且部署該 Umbrella Chart 即可順利的把整套服，四個 Helm Chart 都一併部署到目標叢集內。  ","version":"Next","tagName":"h2"},{"title":"Kustomize​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#kustomize","content":"不同於 Helm 使用 Template 的方式，Kustomize 提供更為簡單與乾淨的方式來達到客製化 Kubernetes 資源物件，所有檔案都會維持 K8s 原生的格式，透過 Patch 等疊加的概念來動態修改資源內容。 優點： 基於原生 YAML 格式，沒有任何 Template 等語法要學習已經整合至 kubectl 中，可以不需要安裝額外指令透過疊加與覆蓋的方式來 缺點： 生態性不如 Helm 豐富沒有版本概念，不方便散佈給不同使用者與客戶雖然沒有 Template，但是整體架構與使用方式還是需要學習 以下例範例來說，我們會於 base 資料夾內準備我們應用程式的基本物件，接者每個環境資料夾內則會準備想要客製化的內容，最後這一切都會依照 kustomization.yaml 的內容全部串連起來。  過往需要使用 kustomize 這個指令，現在已經整合到 kubectl 中，所以可以直接使用 kubectl apply -k 的方式來操作基於 kustomize 的部署格式。  ","version":"Next","tagName":"h2"},{"title":"Jsonnet​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#jsonnet","content":"Jsonnet 則是一種基於 JSON 格式的程式語言，所有的操作都是基於 JSON 並且最後輸出也是 JSON 格式，而前面也有提到 Kubernetes 也接受基於 JSON 格式來描述所有資源狀況，因此也有部分人會採用這種方式來維護 K8s 應用程式。 簡單來說，就是利用程式語言撰寫的習性來產生 K8s 的物件描述檔案 優點 支援 if/else, 迴圈, function 等程式語言常見的邏輯支援 library 的概念減少重複撰寫程式碼 缺點: 要學習全新工具 jsonnet文件與生態性不夠豐富 Jsonnet 本身是針對 Json 物件操作，而 jsonnet k8s與 k8s-libsonnet 則是實作各種介面讓你可以快速產生適合各種版本 Kubernetes 的物件資源。 舉例來說，下列的語法最後可以產生出一個 k8s deployment + service 的物件，這些檔案都可以被重複利用，還可以被動態覆蓋，整體來說彈性非常高 local k = import &quot;vendor/1.27/main.libsonnet&quot;; local s= { name: &quot;demo&quot;, }; [ k.apps.v1.deployment.new(name=&quot;demo&quot;, containers=[ k.core.v1.container.new(name=&quot;demo&quot;, image=&quot;hwchiu/netutils&quot;) ]), k.core.v1.service.new(&quot;demo&quot;, s, 5000) ]  azureuser@course:~/jsonnet$ jsonnet --yaml-stream main.jsonnet --- { &quot;apiVersion&quot;: &quot;apps/v1&quot;, &quot;kind&quot;: &quot;Deployment&quot;, &quot;metadata&quot;: { &quot;name&quot;: &quot;demo&quot; }, &quot;spec&quot;: { &quot;replicas&quot;: 1, &quot;selector&quot;: { &quot;matchLabels&quot;: { &quot;name&quot;: &quot;demo&quot; } }, &quot;template&quot;: { &quot;metadata&quot;: { &quot;labels&quot;: { &quot;name&quot;: &quot;demo&quot; } }, &quot;spec&quot;: { &quot;containers&quot;: [ { &quot;image&quot;: &quot;hwchiu/netutils&quot;, &quot;name&quot;: &quot;demo&quot; } ] } } } } --- { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;kind&quot;: &quot;Service&quot;, &quot;metadata&quot;: { &quot;name&quot;: &quot;demo&quot; }, &quot;spec&quot;: { &quot;ports&quot;: [ 5000 ], &quot;selector&quot;: { &quot;name&quot;: &quot;demo&quot; } } }  整個流程大致上如下，撰寫各種 jsonnet 的檔案，最後透過 jsonnet 指令產生出符合 K8s 需求的檔案，接者使用 kubectl 指令將其 Apply 到目標叢集。 ","version":"Next","tagName":"h2"},{"title":"Tools​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#tools","content":"上述四種工具沒有絕對的好壞且 ArgoCD 都支援，所以只要其特性與優缺點符合團隊所需，事實上都可以嘗試使用。 實務上更多可能概念則是混用，畢竟現在太多開源專案都是基於 Helm 去發布，而團隊可能採用 Kustomize/Jsonnet 的方式來管理自己的應用程式，所以實際上這些不同方式可能會同時存在，反正 ArgoCD 都支援都可以部署，唯一的問題則是使用者要有相關的背景知識來使用，除錯與設定。 Git Repo 從 Source Code 與 K8s Manifest 的角度來看，常見有兩種選擇 每一個應用程式有一個專屬的 Git Repo，含有該應用程式的 Source Code , Dockerfile 以及 K8s 相關物件檔案每一個應用程式有一個專屬的 Git Repo，含有該應用程式的 Source Code 與 Dockerfile。另外會有一個專屬的 Git Repo 包含所有應用程式所需要的 K8s 物件檔案。 ","version":"Next","tagName":"h2"},{"title":"Code/YAML in the Same Repo​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#codeyaml-in-the-same-repo","content":"第一種概念如下 這種模式下的特性有 從應用程式開發到 K8s 的部署所有資源都放一起，容易查閱彼此關聯如果應用程式有任何需求修改，可以連部署 YAML 一起修改，譬如新增某個環境變數的讀取。CI/CD Pipeline 可處理所有事情，甚至可以搭配 KIND 等架構來測試 K8s 部署如果採用 Helm Chart 的話，可於 CI/CD pipeline 流程中一起打包並且發布如果要採用 jsonnet/kustomize 的方式來處理，則跨應用共享的檔案就不方便處理可透過 Git branch/tag 的方式來控制版本維運人員很難輕鬆的瞭解到系統上全面使用的部署方式與內容，必須要每個 Git repo 逐步查找 此外，採用這種模型也很常面對如何處理 image tag 的問題 舉例來說 開發者開啟一個 PR 加入新功能由於使用者並不知道最後產生的 container image tag，因此無法同時更新 K8s YAML 檔案 常見的解法包含 永遠都使用 latest tag 來部署應用程式開啟第二個 PR 來更新創建一個 GitHub App 來動態更新 PR 內容 因此流程實作上還是有很多細節需要考慮與探討，並沒有想像中的這麼完美簡單。 ","version":"Next","tagName":"h2"},{"title":"Code/YAML in the Different Repo​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#codeyaml-in-the-different-repo","content":"更多常見的作法則是第二種，權責分離，讓開發人員專心處理自己應用程式的開發與容器化，而準備一個專屬的 Git Repo 來處理所有 GitOps 的管理與部署 基本概念如下圖 這種架構上常見的特性有 所有部署物件都放一起，容易查閱快速理解所有部署的差異性與應用程式的開發分離，更方便讓開發人員與維運人員分別處理若應用程式新版本有任何功能增減需要 YAML 配合，需要到此 Repo 額外處理，不能一次搞定由於環境都放一起，kustomize/jsonnet 等概念就相對容易共享可透過 Branch/Folder 等方式來區分不同環境的部署資源可有專屬的 CI/CD Pipeline 來驗證整體服務的部署，而非單一應用程式 以 Kustomize 或 Jsonnet 為基底的，很常看到下列的資料結構，透過資料夾來區分不同環境的部署，若有特別的需求還可以搭配 Git tag 的方式來定版，以便未來需要追蹤某個版本部署的內容。 ├── base │ ├── foo │ │ ├── deployment.yaml │ │ ├── kustomization.yaml │ │ └── service.yaml │ └── foo2 │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── dev │ ├── foo │ │ ├── kustomization.yaml │ │ └── resource.yaml │ └── foo2 │ ├── kustomization.yaml │ └── resource.yaml └── prod ├── foo │ ├── kustomization.yaml │ └── resource.yaml └── foo2 ├── kustomization.yaml └── resource.yaml  ├── base │ ├── app │ │ ├── foo │ │ │ └── app.jsonnet │ │ └── foo2 │ │ └── app.jsonnet │ └── component │ ├── deployment.jsonnet │ └── service.jsonnet └── env ├── production │ ├── foo │ │ ├── env.jsonnet │ │ └── main.jsonnet │ └── foo2 │ ├── env.jsonnet │ └── main.jsonnet └── staging ├── foo │ ├── env.jsonnet │ └── main.jsonnet └── foo2 ├── env.jsonnet └── main.jsonnet  此外，由於 Kustomize 目前也支援使用 Helm Chart 來部署，所以也有可能會看到如下列的變化行，將 Kustomize 與 Helm 給組合一同使用。 ├── base │ ├── foo │ │ ├── deployment.yaml │ │ ├── kustomization.yaml │ │ └── service.yaml │ └── foo2 │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml ├── helm │ └── app │ ├── charts │ ├── Chart.yaml │ ├── templates │ │ ├── deployment.yaml │ │ ├── _helpers.tpl │ │ ├── hpa.yaml │ │ ├── ingress.yaml │ │ ├── NOTES.txt │ │ ├── serviceaccount.yaml │ │ ├── service.yaml │ │ └── tests │ │ └── test-connection.yaml │ └── values.yaml └── overlays ├── dev │ ├── foo │ │ ├── kustomization.yaml │ │ └── resource.yaml │ └── foo2 │ ├── kustomization.yaml │ └── resource.yaml └── prod ├── foo │ ├── kustomization.yaml │ └── resource.yaml └── foo2 ├── kustomization.yaml └── resource.yaml  ","version":"Next","tagName":"h2"},{"title":"Mixed​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/gitops-repo-structure#mixed","content":"由於 Helm Chart 本身可以打包並且推向 Helm Chart Server，因此採用 Helm Chart 的方式更可能看到兩者結合的結果，如下圖。  這種情況下的特性有 應用程式 Repo 只專心維護 Helm Chart 的發佈與設定應用程式 Repo 會透過 CI/CD 來打包 Helm Chart 並且推向到 Helm Chart ServerK8s YAML Repo 內則透過 Helm Chart 指向遠方的 Helm Chart Server 來抓取建置好的檔案，並且搭配不同的 values 來部署到不同環境保留分離 Repo 的特性，職權分離，各自有各自的 Git 流程來管理 由於該架構通常需要一個額外的 Helm Chart Server，這部分因為 Helm Chart v3 支援 OCI 格式，所以可以採用支援 OCI 的 Container Registry，如 ECR, Harbor 等，或是繼續使用傳統的 Chart Museum 來部署。 Image Updater 採用第二與第三兩種架構下，很多團隊都會探討提升工作流程，特別是當 Container Image 有新版本時，要如何自動更新該 Repo 下的描述檔案。 有的團隊會採用 Argo Image Updater 或是自行撰寫 CI/CD 流程來更新。 Summary 本篇文章總結了 GitOps 架構下常見的 Git 結構問題，探討了常見的工具與使用方式K8s 的應用程式可以採用原生 YAML, Helm, Jsonnet, Kustomize 等方式來管理，不同方式有各自不同的特性以及適合的場景沒有一個完美的解法，所有規劃還是要以團隊規模，流程，能力，以及工作等來討論適合的工具 ","version":"Next","tagName":"h2"},{"title":"讓你的 Container Image 逃脫 Kubelet Image GC 的魔掌","type":0,"sectionRef":"#","url":"/docs/techPost/2023/k8s-gc","content":"","keywords":"Kubernetes","version":"Next"},{"title":"實作​","type":1,"pageTitle":"讓你的 Container Image 逃脫 Kubelet Image GC 的魔掌","url":"/docs/techPost/2023/k8s-gc#實作","content":"如文件所述， Kubelet 內的 ImageManager 會負責處理相關 GC 流程，因此從 Kubelet 內的程式碼可以看到其每五分鐘會呼叫一次 ImageManager 內的 GarbageCollect 來處理。 const ( ... // ImageGCPeriod is the period for performing image garbage collection. ImageGCPeriod = 5 * time.Minute ... ) ... go wait.Until(func() { ctx := context.Background() if err := kl.imageManager.GarbageCollect(ctx); err != nil { if prevImageGCFailed { klog.ErrorS(err, &quot;Image garbage collection failed multiple times in a row&quot;) // Only create an event for repeated failures kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error()) } else { klog.ErrorS(err, &quot;Image garbage collection failed once. Stats initialization may not have completed yet&quot;) } prevImageGCFailed = true } else { var vLevel klog.Level = 4 if prevImageGCFailed { vLevel = 1 prevImageGCFailed = false } klog.V(vLevel).InfoS(&quot;Image garbage collection succeeded&quot;) } }, ImageGCPeriod, wait.NeverStop) ...  而 GarbaeCollect 內的程式碼最後則有一個 HighThresholdPercent 的判斷。 當前使用量超過該標準時，就會計算需要移除多少空間來低於 LowThreholdPercent 並且呼叫內部的 freeSpace 去移除空間 ... usagePercent := 100 - int(available*100/capacity) if usagePercent &gt;= im.policy.HighThresholdPercent { amountToFree := capacity*int64(100-im.policy.LowThresholdPercent)/100 - available klog.InfoS(&quot;Disk usage on image filesystem is over the high threshold, trying to free bytes down to the low threshold&quot;, &quot;usage&quot;, usagePercent, &quot;highThreshold&quot;, im.policy.HighThresholdPercent, &quot;amountToFree&quot;, amountToFree, &quot;lowThreshold&quot;, im.policy.LowThresholdPercent) freed, err := im.freeSpace(ctx, amountToFree, time.Now()) if err != nil { return err } if freed &lt; amountToFree { err := fmt.Errorf(&quot;Failed to garbage collect required amount of images. Attempted to free %d bytes, but only found %d bytes eligible to free.&quot;, amountToFree, freed) im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.FreeDiskSpaceFailed, err.Error()) return err } } ...  fresSpace 首先會先呼叫 detectImages 從系統抓取當前運行的 Images 資訊，接者運行一個迴圈從中過濾掉不需要的 Image，最後再根據 Image 的最後被使用來進行排序，排序後則會有一個額外的迴圈去移除 Image 直到空間足夠為止。 這邊的過濾條件也補充了文件中沒有說明的細節，說明了哪些 Image 會被 GC 給忽略 Image 正在被使用Image 本身有 pinned 的屬性 func (im *realImageGCManager) freeSpace(ctx context.Context, bytesToFree int64, freeTime time.Time) (int64, error) { imagesInUse, err := im.detectImages(ctx, freeTime) if err != nil { return 0, err } // Get all images in eviction order. images := make([]evictionInfo, 0, len(im.imageRecords)) for image, record := range im.imageRecords { if isImageUsed(image, imagesInUse) { klog.V(5).InfoS(&quot;Image ID is being used&quot;, &quot;imageID&quot;, image) continue } // Check if image is pinned, prevent garbage collection if record.pinned { klog.V(5).InfoS(&quot;Image is pinned, skipping garbage collection&quot;, &quot;imageID&quot;, image) continue } images = append(images, evictionInfo{ id: image, imageRecord: *record, }) } sort.Sort(byLastUsedAndDetected(images)) ...  觀察到 pinned 的相關概念後，翻了sig-node/2040-kubelet-cri 內的文件也可以看到這段說明 Introduce field in the Image message to indicate an image should not be garbage collected: 相關程式碼也於 2021 左右於這隻 PR 內實作。 此外也可以從 Containerd 的相關 Issue 看到相關實作，而該功能最後於 Containerd 1.7 後釋出，使用者可以透過下列兩種指令去 pin image。 sudo ctr -n k8s.io images label docker.io/library/jenkins:2.60.1 io.cri-containerd.pinned=pinned sudo ctr -n k8s.io images pull --label=io.cri-containerd.pinned=pinned docker.io/library/jenkins:2.60.1  實驗 有了上述概念後，接下來就要準備一個 Kubernetes 環境來驗證上述概念 ","version":"Next","tagName":"h2"},{"title":"環境​","type":1,"pageTitle":"讓你的 Container Image 逃脫 Kubelet Image GC 的魔掌","url":"/docs/techPost/2023/k8s-gc#環境","content":"$ kubectl version Client Version: v1.28.2 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.28.2 $ ctr --version ctr github.com/containerd/containerd v1.7.6 $ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 23.04 Release: 23.04 Codename: lunar  ","version":"Next","tagName":"h2"},{"title":"實驗​","type":1,"pageTitle":"讓你的 Container Image 逃脫 Kubelet Image GC 的魔掌","url":"/docs/techPost/2023/k8s-gc#實驗-1","content":"首先透過 ctr 指令與 containerd 互動並且觀察相關 image 狀況，其中 Kubernetes 預設會使用 &quot;k8s.io&quot; namespace，因此使用上都要加上 &quot;-n k8s.io&quot; 透過 imags ls 去檢視所有 image 的狀況，可以發現 pause 系列的 image 預設就會有 &quot;io.cri-containerd.pinned=pinned&quot; 這個選項，而其他 image 都沒有。 $ sudo ctr -n k8s.io image ls ... registry.k8s.io/pause:3.6 application/vnd.docker.distribution.manifest.list.v2+json sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db 294.7 KiB linux/amd64,linux/arm/v7,linux/arm64,linux/ppc64le,linux/s390x,windows/amd64 io.cri-containerd.image=managed,io.cri-containerd.pinned=pinned registry.k8s.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db application/vnd.docker.distribution.manifest.list.v2+json sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db 294.7 KiB linux/amd64,linux/arm/v7,linux/arm64,linux/ppc64le,linux/s390x,windows/amd64 io.cri-containerd.image=managed,io.cri-containerd.pinned=pinned docker.io/calico/cni:v3.26.1 application/vnd.docker.distribution.manifest.list.v2+json sha256:3be3c67ddba17004c292eafec98cc49368ac273b40b27c8a6621be4471d348d6 89.0 MiB linux/amd64,linux/arm/v7,linux/arm64,linux/ppc64le,linux/s390x io.cri-containerd.image=managed docker.io/calico/cni@sha256:3be3c67ddba17004c292eafec98cc49368ac273b40b27c8a6621be4471d348d6 application/vnd.docker.distribution.manifest.list.v2+json sha256:3be3c67ddba17004c292eafec98cc49368ac273b40b27c8a6621be4471d348d6 89.0 MiB linux/amd64,linux/arm/v7,linux/arm64,linux/ppc64le,linux/s390x io.cri-containerd.image=managed ...  接下來可以嘗試透過 ctr 去 Pin ctr -n k8s.io images label xxxxxxxx io.cri-containerd.pinned=pinned  由於系統的硬碟空間只有 30G，因此我使用下列的腳本下載不同的 Image 並嘗試使得硬碟使用量超出 HighThresholdPercent (85%) 並且觸發相關資訊。 kubectl run a --image=docker.io/library/node:bullseye [1851/4810] kubectl run a1 --image=docker.io/library/node:current-bookworm kubectl run a2 --image=docker.io/library/node:bookworm kubectl run a3 --image=docker.io/library/node:current kubectl run a4 --image=docker.io/library/node:20.8.0-bullseye kubectl run a5 --image=docker.io/library/openjdk:22-oraclekubectl run a6 --image=docker.io/library/jenkins:2.60.1 kubectl run a7 --image=docker.io/library/jenkins:2.60.2 kubectl run a8 --image=docker.io/library/jenkins:2.60.3 kubectl run a9 --image=docker.io/library/jenkins:2.46.2 kubectl run a10 --image=docker.io/library/jenkins:2.46.1 kubectl run a11 --image=docker.io/pytorch/pytorch:latest kubectl run a12 --image=docker.io/pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel  此外我環境部署中，有特別開啟 kubelet 的設定檔案，打開其 log 等級來觀看更多運作 log。 接下來透過下列指令觀察 kubelet 指令並且觀察當硬碟空間超過後的相關 log $ sudo journalctl -f -u kubelet | grep image_gc  以下是當系統空間超過 HighThresholdPercent 後的相關 Log (移除用不到的資訊方便閱讀) image_gc_manager.go:340] &quot;Attempting to delete unused images&quot; ... image_gc_manager.go:255] &quot;Adding image ID to currentImages&quot; imageID=&quot;sha256:112170efb091e6c02eac19703986e3c59ce11e86 b826c1d70a4a4a73a333339b&quot; image_gc_manager.go:272] &quot;Image ID has size&quot; imageID=&quot;sha256:112170efb091e6c02eac19703986e3c59ce11e86b826c1d70a4a4a7 3a333339b&quot; size=366064122 image_gc_manager.go:275] &quot;Image ID is pinned&quot; imageID=&quot;sha256:112170efb091e6c02eac19703986e3c59ce11e86b826c1d70a4a4a 73a333339b&quot; pinned=true ... image_gc_manager.go:364] &quot;Image ID is being used&quot; imageID=&quot;sha256:c62308471249574d567c4fff9a927451ac999f50fe9190ceb50e9949922762ef&quot; image_gc_manager.go:364] &quot;Image ID is being used&quot; imageID=&quot;sha256:677ad13d73108d775aec52e9bd38c33042ad14bb3a780b67613b8eb7be5de5b2&quot; image_gc_manager.go:369] &quot;Image is pinned, skipping garbage collection&quot; imageID=&quot;sha256:6270bb605e12e581514ada5fd5b3216f727db55dc87d5889c790e4c760683fee&quot; image_gc_manager.go:364] &quot;Image ID is being used&quot; imageID=&quot;sha256:8065b798a4d6729605e3706c202db657bfbcb8109127ece6af5bfb6da106adb7&quot;  從上述的 log 看起來似乎運作正常，但是仔細觀察後發現所有手動加入 pinned 的 image 都沒有順利地被偵測到有 &quot;pinned=true&quot;，只有預設的 puase image 有被偵測到。 透過 crictl 指令觀察，會發現對於 Kubernetes 來說，並不認為該 Image 有被標示為 pinned $ sudo crictl inspecti docker.io/library/jenkins:2.60.2 { &quot;status&quot;: { &quot;id&quot;: &quot;sha256:112170efb091e6c02eac19703986e3c59ce11e86b826c1d70a4a4a73a333339b&quot;, &quot;repoTags&quot;: [ &quot;docker.io/library/jenkins:2.60.2&quot; ], &quot;repoDigests&quot;: [ &quot;docker.io/library/jenkins@sha256:5d628badc50487581da2b4cb95a7589fe1d39922391e128f6a031273ad351b71&quot; ], &quot;size&quot;: &quot;366064122&quot;, &quot;uid&quot;: null, &quot;username&quot;: &quot;jenkins&quot;, &quot;spec&quot;: null, &quot;pinned&quot;: false },  經過反覆實驗後觀察到若採用 ctr image label 加上的 label 似乎不會被認可為 pinned，只有透過 ctr image pull 的才有辦法被正式被辨識。 另外 kubelet 沒有辦法辨識的問題實際上是一個實作的 bug，該 bug 已經於PR Pass Pinned field to kubecontainer.Image 給修復，該修復預計於 v1.29 一起釋出。 因此嘗試下載 v1.29.0-alpha.1 版本的 kubelet並且進行替換來驗證，最後整個功能運作如預期，能夠順利的跳過 pinned image。 Summary 最後以一張圖來概括上述的流程 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2023/k8s-network-debug","content":"","keywords":"Kubernetes Network","version":"Next"},{"title":"南北向​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-network-debug#南北向","content":"南北向 代表的是流量有進出叢集，封包的來源或是目的有一端是不屬於叢集的一部份。 大抵上可能會有幾種流量方式 外部服務 如和存取 叢集內服務 IngressAPI-GatewayLoad-Balancer...等 叢集內服務 如何存取 外部網路 NAT (Network Address Translation)Internet Gateway 下圖是一個用來描述南北向流量的簡易畫法  這種圖只能單純描述封包的流量以及讓大家對於整個叢集封包流向有一點基本的概念，對於除錯整體是不夠的，因此若要針對網路問題除錯必須要能夠更細部的去描述整個參與到的元件，譬如下圖  舉例來說，該 Kubernetes 叢集外部配置一個 Load-Balancer，而該 Load-Balancer 將封包打到節點上並且透過 Service(Node-Port) 的方式把封包打到目標 Pod. 而目標 Pod 則是依賴 Routing Table 將封包都轉發到 NAT Gateway 讓 NAT GW 來處理 SNAT 並將封包給轉發到外部網路 此外，下圖也是另外一種不同的底層實作  Load-Balancer 與 Kubernetes Pod 天生就擁有共通的能力(AWS CNI, Azure CNI) ，這種情框下 Load-Balancer 就能夠直通 Pod 而不需要經過任何 Service(LB/NodePort) 來處理。 每個節點都依賴各自的 NAT 服務來直接進行 SNAT 的處理並且直接將網路送到外部網路。 第三種範例如下  這種架構下可能的情況就是外部使用 L4 LoadBalancer 將流量全部導向 Kubernetes 內的 Ingress Controller，讓 Ingress 來處理 L7 層級的處理與轉發。 同時環境架構中包含了 Internal/Public 兩種網路，節點會根據封包目的地搭配 Routing Table 來決定封包的走向。 以上三種範例都可以達到最初簡易圖示的效果，但是其底層的實作卻是截然不同，因此若要針對網路除錯則第一步驟就是要有能力且系統化的去闡述網路封包中經過的元件，先理解流程與相關元件才有辦法進行後續的除錯 ","version":"Next","tagName":"h2"},{"title":"東西向​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-network-debug#東西向","content":"而 東西向 代表封包於節點中穿梭，封包的來源與目的兩端都是屬於叢集內的一部分，譬如屬於不同的 Pod 或是節點本身。 存取方向 Pod &lt;--&gt; ServicePod &lt;--&gt; PodPod &lt;--&gt; Node 存取範圍 兩者屬於同節點兩者跨節點 東西向來說，最簡單的就是 Pod to Pod 之間的存取  然而大部分的應用程式為了搭配 Deployment 對 Pod 生命週期的管理，通常會使用 Service 來處理 Pod 的 IP 與存取，如下圖  基於 K8s Service 的概念，所有送到 Service 的封包會依賴 Kube-proxy 的設定來處理負載平衡的抉擇(iptables, ipvs). 從以上的探討可以基本知道網路世界沒有一個萬用架構圖，不同的環境與情境都會有不同的網路流向，因此探討網路問題的基本原則就是 釐清誰是送端，誰是收端釐清送端與收端與 Kubernetes 的定位釐清封包流向中經過的所有元件為何 Kubernetes 的網路元件 K8s 網路架構基本上我認為可以分成四個面向去探討，這四個面向互相整合使得 K8s 提供完善的網路功能，但是只要其中有任何一個地方出錯就會使得整個網路不通不如預期，這個面向分別是 底層基礎建設Kubernetes 內建網路功能CNI第三方解決方案整合 ","version":"Next","tagName":"h2"},{"title":"底層基礎建設​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-network-debug#底層基礎建設","content":"對於雲端使用者來說，這部分的設定就是仰賴雲端業者去完成，使用者則是花錢建設，譬如 VPCSubnetFirewallRoutingNAT/Internet GW 但是對於地端人員來說，這些東西就不是用滑鼠或是 Terraform 寫寫就會產生的資源，而是需要實際上架機器佈線與機房管理，譬如 節點與節點之間的網路連線，透過 L2 Switch, VLAN... 等串接基本的節點 IP 發放，是靜態 IP 還是動態 IP 取得DNS Server 的建置與管理跨機櫃的 Switch/Router 等 可能架構如下  ","version":"Next","tagName":"h2"},{"title":"Kubernetes 內建網路功能​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-network-debug#kubernetes-內建網路功能","content":"Kubernetes 內建多種網路相關資源，包含 Kubernetes Service 這部分主要是取決於 kube-proxy 的實作，預設的 iptables 或是修改為 ipvs，除了基本規則匹配方式外還有負載平衡演算法的實作不同。 Kubernetes Ingress Kubernetes 只提供單純的介面，實作則是根據安裝哪套 Ingress Controller，不同套的實作細節則不同，譬如 Nginx, Kong, Tarefik...等 CoreDNS 用來處理基本的 DNS 請求，所有內部 k8s service 的 DNS 都會由 CoreDNS 來解析處理，特別是有些網路環境還想要與外部 External DNS 進行整合。 Network Policy 針對 Pod 進行些許的防火牆規則，這部分也是單純的介面，實作都是由底層的 CNI 去完成。 將上述的概念給整合到前述圖片後，可能的架構如下 ","version":"Next","tagName":"h2"},{"title":"CNI​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-network-debug#cni","content":"Contaienr Network Interface(CNI) 主要用來幫忙處理 Pod 的 IP 分配 (IPAM) 節點上分配私有 IP節點上分配一個 &quot;基礎底層架構&quot; 可以直接存取的 IP，譬如 EKS/AKS 上的 IP 就來自 VPC 內的可用 IP 跨節點之間 Pod 的封包處理 一個簡易的概念就是，每個節點上的私有IP (Pod) 要如何與其他節點上的私有 IP (Pod) 進行處理？ 不同 CNI 都採取不同的網路技術處理 - Calico (BGP/IPIP) - Flannel (VXLAN) - Cilium (eBPF) - OVS (OpenFlow) - Cloud-Provider specified (AWS/Azure)  一切堆疊起來後的架構圖大致上如下  ","version":"Next","tagName":"h2"},{"title":"第三方解決方案整合​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-network-debug#第三方解決方案整合","content":"剩下的額外功能我都歸類於第三方功能，譬如 Service MeshCluster Federation... 等 這些功能都要建立於一個 &quot;可正常運作&quot; 的 Kubernetes 上，同時疊加更多功能來提供更進階的網路處理，然而一體兩面，進階的網路功能也意味著整個架構更為複雜，如果沒有辦法掌握這些概念與原理，基本上就是一個按照 README.MD 來操作的 YAML 工程師。 YAML 工程師可用，環境可通，功能可行，困擾就在於如何客製化，如何除錯，如何根據需求調整架構 舉例來說，假設 Cluster Federation 建立後，有可能會變成如下 Kubernetes 的除錯思路 用上述的基本概念敘述可得知，網路用起來非常簡單但是實際上背後牽扯的元件非常多，特別是當環境安裝愈來愈多的網路功能時，愈來愈多的元件牽扯其中，因此遇到網路問題的思路我推薦是 釐清方向性，到底問題是南北向還是東西向?問題發生點，到底問題是屬於哪個層級？ 是基礎建設出問題? K8s 內建功能沒設定好？ CNI 出問題還是第三方整合的服務有 Bug? 特別特別重要的事情是，網路問題千萬千萬不要用嘴除錯，每個人對網路的概念與背景知識不同，單純靠嘴巴用談有時候很難有一個相同的理解與共識，最好的做法就是畫圖，將圖畫出來逐一釐清縮小問題發生點。 為了有效的實作上述思路，可以採用一種方式來處理 畫出整個系統架構圖標示出你的網路情境，誰是發送端，誰是收端？將自己想像成一個封包，於整個架構圖上逐一解釋這個封包會怎麼流動 如果有一個部分沒有辦法解釋，就代表你對這個網路架構還是不夠熟悉，繼續念書學習 以上述過程為基礎開始除錯，縮小問題的可能範圍，針對範圍內可能是問題的元件進行除錯，不停循環整個流程最後定位整個問題發生點 以下是一個 &quot;我的 Pod 透過 Service 沒有辦法存取目標 Pod&quot; 的範例 簡單架構圖畫起來就會是 但是這張圖只能基本描述封包流向，對於除錯還是有些許的地方不夠清楚，這時候如果可以將這張圖用更為技術的細節去展開，可以得到下列這張圖  Pod 欲透過 Service DNS 存取服務Pod 內檢察系統的 /etc/resolve 找到 DNS 的 IP該 DNS 實際上會是 CoreDNS 的 Cluster SerivceIPDNS 請求打到 CoreDNS 去解析到後面的 Service ClusterIPPod 將請求送到 ClusterIP 並讓 k8s 將其轉發到後續的 Pod 然而上述的圖片也不是 100% 精準，有更多些許的網路細節被遺漏，譬如 CoreDNS 本身是基於 Hostnetwork 的方式來部署，因此 Pod -&gt; CoreDNS 的部分會變成 Pod -&gt; Node 的存取方式Pod -&gt; Service ClusterIP 這中間牽扯到 iptables/ipvs 的轉發，所以真正的流量並不會有一條 Pod -&gt; Service 的走向，而是節點本身進行 DNAT 找到一個合適的 Pod IP 後就直接打到目標 Pod 光是一個簡簡單單的 Pod-&gt;Service 就有非常的多的細節牽扯其中，大部分情況下這些東西都運作得好好的，大家的網路都沒有問提，然而只要有一個小元件出錯整個網路就不通了。 當理解上述的技術細節後，這個 Pod-&gt;Service 的問題可以有這樣去看待 跟 DNS 解析有關？ 直接使用 ClusterIP 打看看?跟 Service 轉換是否有關? 直接打 PodIP 試試看?跟節點是否有關， 直接打看看同節點上的 Pod 看看？跟出發者是誰有關？ 嘗試從節點的看看？是否有 Network Policy 擋住？ 一切都嘗試後還是沒有辦法縮小問題，可以嘗試從不同發生點錄製封包來分析 Server 沒收到封包Server 有收到封包，但是沒有回Server 有收到封包，也有回覆，但是 Client 沒有收到 此外也要考慮到封包是不是可能被封包給 Kernel 給丟棄導致沒有錄製到封包? 如果封包都錄製不到有沒有可能是底層網路出問題？譬如網路線壞了？ 一個一個列出來來排除與確認每個元件的運作狀況。 錄製封包的麻煩 當現存工具都沒有辦法釐清為什麼網路不通時，就可以借助抓取封包的方式來判端 但是錄製封包的一個前提是問題有辦法重製，否則事情已經發生錯誤的封包已經消失，這時候錄製封包通常沒有辦法得到什麼有用的情報。 當決定要錄製封包時，有兩個問題要確認 用什麼工具擷取分析要如何從茫茫大海流量中定位到目標封包 常見的工具如 wireshark/tcpdump/tshark.. 等都可以用來錄製封包，但是有些環境不一定有 GUI 可以運行 wireshark，所以熟悉些 CLI 的工具是不可獲或缺的技能 當有了工具後就要決定要誰去運行這些工具？ Pod 本身 Pod 本身是否能夠運行 tcpdump 取決於容器當初的 image，很多時候不一定有 tcpdump 可以用，甚至一些 image 連基本的 sh/ash/busybox 都沒有，就算仰賴 ksniff 來動態安裝 tcpdump 也有可能遇到執行問題節點 因為所有容器都運行到節點上，所以從節點上去錄製封包可以看到 95% 的容器封包(少部分特殊如 SR-IOV, DPDK...etc等無法)，此外節點通常比較方便去安裝各式各樣的工具來進行除錯。 不過也要注意的是如果這些節點是動態安裝，譬如透過 auto-scaling group 的概念是否就會導致每次除錯都要一直安裝工具？ 當可以錄製封包的時候，這時候又必須要將 CNI 與基本架構給整合進來，以 Calico 為範例  節點之間透過 IP-IP 的 Tunneling 協定進行封包處理，因此這時候你如果錄製封包你看到的不會是單純的 IP 協定，而是 IP-IP 協定，因此若沒有這些網路知識與理解，你錄製封包也沒有辦法找到你要的資訊。 如果想要從節點上去錄製封包且 CNI 是透過 veth 的方式將封包給轉發到容器內，如果你有能力找到每條 veth 與 Pod 的對應關係，你可以直接針對 veth 去錄製封包找到最直接往返 Pod 的封包，除錯的效率也是最佳的。 其餘工具 除了上述提到跟 Kubernetes 有關的範疇外， Linux 本身的網路工具也甚為重要，譬如 ip/tcpdumpconntrackiptables/ipvsethtoolrouting,NAT,rp_filter...etc 這些工具都有可能會影響節點層級的封包轉發，弄得不好就會使得封包不通。 總結 網路部分牽扯元件眾多，單靠嘴巴想要除錯幾乎不可行，而身為一個資深的開發人員/維運人員，遇到網路問題時千萬不要單純只用一句 “我網路不通” 簡單描述問題，能的話則是要詳細描述實際上遇到什麼問題，進行過什麼測試，排除過什麼困難，目前認為的可能問題為何。 除了能夠讓彼此更加清楚當前問題，反覆多次的來回訓練其實也都是淺移默化的加強對底層網路能力的理解，久而久之只會愈來愈熟悉，未來面對各種問題的時候會有各種不同的想法與理解。 ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2023/k8s-node-fault-recovery","content":"","keywords":"Kubernetes Network Linux Ubuntu","version":"Next"},{"title":"NodeStatus​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-node-fault-recovery#nodestatus","content":"如同前述， Kubelet 本身會需要回傳節點上的各種運作狀態以及 heartbeat 資訊，而最初的 Kubernetes 則是將這兩個資訊一起更新，其更新的頻率預設是 10 秒，可以透過 KubeletConfiguration內的 nodeStatusUpdateFrequency 的欄位更新 此外對於每次的狀態更新， kubelet 都有實作重試機制，每次傳遞預設都會嘗試五次，而這個數字目前是不可更改的，寫死於程式碼, kubelet.go中 const ( // nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed. nodeStatusUpdateRetry = 5 .... )  func (kl *Kubelet) updateNodeStatus(ctx context.Context) error { klog.V(5).InfoS(&quot;Updating node status&quot;) for i := 0; i &lt; nodeStatusUpdateRetry; i++ { if err := kl.tryUpdateNodeStatus(ctx, i); err != nil { if i &gt; 0 &amp;&amp; kl.onRepeatedHeartbeatFailure != nil { kl.onRepeatedHeartbeatFailure() } klog.ErrorS(err, &quot;Error updating node status, will retry&quot;) } else { return nil } } return fmt.Errorf(&quot;update node status exceeds retry count&quot;) }  下圖簡易的描述更新方式 然而這種實作方式實務上卻帶來的效能上的瓶頸，每次 kubelet 資訊回報都伴隨大量的狀態資訊，每十秒一次且節點數量過多時，就會對整個 etcd 造成系統壓力使得整個叢集的效能降低，因此 1.13 版本後決定採用新的實作方式並且於 1.17 版本正式宣佈為 stable 版本。 ","version":"Next","tagName":"h2"},{"title":"Lease​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-node-fault-recovery#lease","content":"為了改善整個 NodeStatus 更新的效率與效能問題，Official Proposal被提出並且打算採用基於 Lease 架構來完成，其核心概念就是將前述的 NodeStatus 與 Heartbeat 兩個資訊給拆開，兩件事情獨立去處理。 Heartbeat 本身的流量負擔小，維持過往的頻率並不會造成多大的效能問題，然而 NodeStatus 的資訊相對龐大，因此其更新頻率就進行調整。 ","version":"Next","tagName":"h2"},{"title":"Heartbeat​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-node-fault-recovery#heartbeat","content":"以 Heartbeat 來說，採用 Kubernetes 內建API Lease 的架構，當此架構運作時，可以觀察到系統會自動創建一個名為 kube-node-lease 的 namespace，並且所有個 K8s 節點都會與之對應到一個同名稱的 Lease 物件 azureuser@course:~$ kubectl -n kube-node-lease get lease NAME HOLDER AGE k8slab-control-plane k8slab-control-plane 3d13h k8slab-worker k8slab-worker 3d13h k8slab-worker2 k8slab-worker2 3d13h  azureuser@course:~$ kubectl -n kube-node-lease get lease k8slab-worker -o yaml apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: &quot;2023-08-23T17:08:00Z&quot; name: k8slab-worker namespace: kube-node-lease ownerReferences: - apiVersion: v1 kind: Node name: k8slab-worker uid: 70c3c25d-dc3d-4ad3-ba3a-36637d3b5b60 resourceVersion: &quot;623846&quot; uid: 08013bd8-2dd9-45fe-ad6d-38b77253b437 spec: holderIdentity: k8slab-worker leaseDurationSeconds: 60 renewTime: &quot;2023-08-27T03:21:06.584188Z&quot;  這些 Lease 物件則透過 renewTime 與 holderIdentity 來代表每個節點最後一次更新的時候，這些時間之後會被 Controller 用來判定節點本身是否 Ready/NotReady. 根據 kubelet 原始碼 const ( // nodeLeaseRenewIntervalFraction is the fraction of lease duration to renew the lease nodeLeaseRenewIntervalFraction = 0.25 ) ... leaseDuration := time.Duration(kubeCfg.NodeLeaseDurationSeconds) * time.Second renewInterval := time.Duration(float64(leaseDuration) * nodeLeaseRenewIntervalFraction) klet.nodeLeaseController = lease.NewController( klet.clock, klet.heartbeatClient, string(klet.nodeName), kubeCfg.NodeLeaseDurationSeconds, klet.onRepeatedHeartbeatFailure, renewInterval, string(klet.nodeName), v1.NamespaceNodeLease, util.SetNodeOwnerFunc(klet.heartbeatClient, string(klet.nodeName)))  可以觀察到 renewInternval 的計算方式是 nodeLeaseRenewIntervalFraction * NodeLeaseDurationSeconds，前者是一個固定的常數 0.25，而後者根據 kubelet 中關於 nodeLeaseDurationSeconds 的介紹，預設值是 40。 根據這個計算可以得到 0.25 * 40 = 10，因此 kubelet 每 10 秒會更新一次。 根據這個理論，嘗試透過指令觀察 Lease 物件的變化kubectl -n kube-node-lease get lease k8slab-worker2 -o yaml -w --- apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: &quot;2023-08-23T17:08:00Z&quot; name: k8slab-worker2 namespace: kube-node-lease ownerReferences: - apiVersion: v1 kind: Node name: k8slab-worker2 uid: 5ad224c5-11ad-4939-8cfa-0066eb86d6b9 resourceVersion: &quot;655385&quot; uid: bf558925-25b8-4483-9f9d-4a78521afa4c spec: holderIdentity: k8slab-worker2 leaseDurationSeconds: 40 renewTime: &quot;2023-08-27T07:39:35.899240Z&quot; --- apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: &quot;2023-08-23T17:08:00Z&quot; name: k8slab-worker2 namespace: kube-node-lease ownerReferences: - apiVersion: v1 kind: Node name: k8slab-worker2 uid: 5ad224c5-11ad-4939-8cfa-0066eb86d6b9 resourceVersion: &quot;655405&quot; uid: bf558925-25b8-4483-9f9d-4a78521afa4c spec: holderIdentity: k8slab-worker2 leaseDurationSeconds: 40 renewTime: &quot;2023-08-27T07:39:45.982209Z&quot;  從上述物件的更新狀態，觀察 renewTime 的差異，分別是 07:39:45 與 07:39:35，其差值為 10 秒，與理論一致。 嘗試將該 leaseDurationSeconds 改成 60 秒，觀察 renewTime 的變化。 apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: &quot;2023-08-23T17:08:00Z&quot; name: k8slab-worker namespace: kube-node-lease ownerReferences: - apiVersion: v1 kind: Node name: k8slab-worker uid: 70c3c25d-dc3d-4ad3-ba3a-36637d3b5b60 resourceVersion: &quot;654971&quot; uid: 08013bd8-2dd9-45fe-ad6d-38b77253b437 spec: holderIdentity: k8slab-worker leaseDurationSeconds: 60 renewTime: &quot;2023-08-27T07:36:14.454825Z&quot; --- apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: &quot;2023-08-23T17:08:00Z&quot; name: k8slab-worker namespace: kube-node-lease ownerReferences: - apiVersion: v1 kind: Node name: k8slab-worker uid: 70c3c25d-dc3d-4ad3-ba3a-36637d3b5b60 resourceVersion: &quot;655003&quot; uid: 08013bd8-2dd9-45fe-ad6d-38b77253b437 spec: holderIdentity: k8slab-worker leaseDurationSeconds: 60 renewTime: &quot;2023-08-27T07:36:29.654757Z&quot;  可以觀察到兩個時間分別為 07:36:29 與 07:36:14，間隔為 15 秒，與計算理論相符。 Lease 架構下透過此方式來更新節點的最新 heartbeat 狀態，至於 Controller 是如何利用這些資訊判斷節點是否為 Ready/NotReady 等等就會介紹。 以下列圖來總結一下 Lease 架構下 Heartbeat 的更新方式 ","version":"Next","tagName":"h3"},{"title":"Status​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-node-fault-recovery#status","content":"為了改善 Status 傳送資料頻繁造成 etcd 壓力的問題，新架構中將 Status 的傳送機制進行調整，首先將 Status 分成兩個階段 計算回報 計算階段就是去統計收集當前節點上的資訊，而回報則是將這些資訊給回報給 API Server，而這兩個階段是獨立進行，因此彼此的運作週期是不一致的。 以計算來說，目前預設情況下是每 10s 去計算一次，而回報部分則有兩個步驟 根據計算結果，若有任何有意義的更新則馬上回報給 API Server否則，等待 5m 的時間才更新到 API Server。 根據 KubeletConfiguration 中的介紹，可以透過 nodeStatusUpdateFrequency 與 nodeStatusReportFrequency 兩個變數來分別調整相關頻率。 根據說明 nodeStatusUpdateFrequency is the frequency that kubelet computes node status. If node lease feature is not enabled, it is also the frequency that kubelet posts node status to master. Note: When node lease feature is not enabled, be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller. Default: &quot;10s&quot; nodeStatusReportFrequency is the frequency that kubelet posts node status to master if node status does not change. Kubelet will ignore this frequency and post node status immediately if any change is detected. It is only used when node lease feature is enabled. nodeStatusReportFrequency's default value is 5m. But if nodeStatusUpdateFrequency is set explicitly, nodeStatusReportFrequency's default value will be set to nodeStatusUpdateFrequency for backward compatibility. Default: &quot;5m&quot;  根據 kubelet 相關程式碼 originalNode, err := kl.heartbeatClient.CoreV1().Nodes().Get(ctx, string(kl.nodeName), opts) if err != nil { return fmt.Errorf(&quot;error getting node %q: %v&quot;, kl.nodeName, err) } if originalNode == nil { return fmt.Errorf(&quot;nil %q node object&quot;, kl.nodeName) } node, changed := kl.updateNode(ctx, originalNode) shouldPatchNodeStatus := changed || kl.clock.Since(kl.lastStatusReportTime) &gt;= kl.nodeStatusReportFrequency  每次進行 NodeStatus 更新時都會嘗試跟當前物件進行比對，只有當有物件發生改變或是時間超過 nodeStatusReportFrequency 時才會真正的發送資訊到 API Server。 藉由這種機制降低整個更新頻率，並降低頻繁更新造成的效能影響。 將兩者結合起來的話，其示意圖如下 以預設設定下，運作邏輯圖如下， Kubelet 如今產生兩條不同的路，一條負責 Status，一條負責 Heartbeat  ","version":"Next","tagName":"h3"},{"title":"Controller Manager​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/k8s-node-fault-recovery#controller-manager","content":"前述探討了 Kubelet 是如何回報節點到控制平面，而真正判別節點為 Reday/NotReady 則是由 Controller 內的 node lifecycle controller 來判別的。 其概念很簡單，就是定期去檢查每個節點對應的 Lease 物件狀態，只要該 Lease 物件超過一定時間沒有 Renew，就認定該節點太久沒有回報資訊，因此會將其狀態設定為 NotReady。 上述概念內有兩個參數可以設定 Controller 多久去檢查 Lease 物件Renew 時間超過多久沒有更新視為 NotReady 根據 kube-controller-manager 中所述，有兩個參數可以調整，分別是--node-monitor-period duration 與 --node-monitor-grace-period duration node-monitor-period 的說明如下 (預設 5s) The period for syncing NodeStatus in cloud-node-lifecycle-controller.  而由 Controller 原始碼 可以觀察到下列的設定 ... // Incorporate the results of node health signal pushed from kubelet to master. go wait.UntilWithContext(ctx, func(ctx context.Context) { if err := nc.monitorNodeHealth(ctx); err != nil { logger.Error(err, &quot;Error monitoring node health&quot;) } }, nc.nodeMonitorPeriod) ...  期透過 go routine 每 nodeMonitorPeriod 的時間就去執行一次 monitorNodeHealth 來檢查節點的狀況。 而 node-monitor-grace-period 的說明如下(預設 40s) Amount of time which we allow running Node to be unresponsive before marking it unhealthy. Must be N times more than kubelet's nodeStatusUpdateFrequency, where N means number of retries allowed for kubelet to post node status.  根據 Controller 原始碼 func (nc *Controller) tryUpdateNodeHealth(ctx context.Context, node *v1.Node) (time.Duration, v1.NodeCondition, *v1.NodeCondition, error) { ... if currentReadyCondition == nil { ... } else { // If ready condition is not nil, make a copy of it, since we may modify it in place later. observedReadyCondition = *currentReadyCondition gracePeriod = nc.nodeMonitorGracePeriod } ... if nc.now().After(nodeHealth.probeTimestamp.Add(gracePeriod)) { // NodeReady condition or lease was last set longer ago than gracePeriod, so // update it to Unknown (regardless of its current value) in the master. .... } .... }  由上述的程式碼可以觀察到，其會先將設定好的 nodeMonitorGracePeriod 賦予到本地變數 gracePeriod，接者就會去檢查當前時間是否超過 lease + gracePeriod， 若是則將節點設定為 NotReady，並且理由設定為 &quot;Unknown&quot;。 範例如下 (kubectl get node k8slab-worker -o yaml) status: conditions: - lastHeartbeatTime: &quot;2023-08-27T14:03:45Z&quot; lastTransitionTime: &quot;2023-08-27T14:04:30Z&quot; message: Kubelet stopped posting node status. reason: NodeStatusUnknown status: Unknown type: Ready  所以將這些概念整合起來，可以得到下列的概念圖，Kubelet 本身與 Controller 是非同步工作，一個負責更新狀態，一個負責確認狀態並且更新  而整個邏輯工作流程則可以用下圖來表達 Evict Pod 前述探討的是 Kubernetes 如何將一個節點視為故障(NotReady)，那當節點為 NotReady 後，節點上運行的 Pod 會多久才被重生? 這部分 Kubernetes 是利用了 Taint/Toleration 的機制來達到自動重生的，詳細說明可以參閱 Kubernetes Taint Based Evicition。 簡單來說，當節點被判定為故障時，會自動被打上一個 node.kubernetes.io/not-ready 的 Taint，而每個 Pod 可以透過 Toleration 搭配 tolerationSeconds 來決定能夠忍受該節點多久，時間一到無法忍受則自動會被重新部署。 根據 API-Server 文件可以發現兩個相關設定 --default-not-ready-toleration-seconds int Default: 300 Indicates the tolerationSeconds of the toleration for notReady:NoExecute that is added by default to every pod that does not already have such a toleration. --default-unreachable-toleration-seconds int Default: 300 Indicates the tolerationSeconds of the toleration for unreachable:NoExecute that is added by default to every pod that does not already have such a toleration.  這兩個設定預設都是 300 秒，這意味者當節點被標示為損壞後，運行的 Pod 會存活至少 300s 才會被移除重新部署。 而該數值除了透過 API-Server 設定預設值外，每個 Pod 也可以獨立設定，範例如下 tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationSeconds: 20  因此如果希望 Pod 可以更快的因應節點故障而被重新部署，則部署的時候可以調整 tolerationSeconds 讓其更快被反應，另外也可以從 kubelet/controller 的參數去調整讓節點更快的被識別為損壞來觸發節點的重新調度行為。 Summary Kubelet 於 1.17 後都採用 Lease 的方式來回報 heartbeatKubelet 與 Controller 是非同步工作，一個負責回報，一個負責監控，彼此間的 timeout 設定上要仔細與小心kubelet 上的 nodeLeaseDurationSeconds 決定多久更新一次 Lease 物件，目前設定的數值*0.25 則是最後的秒數Contoller 上的 node-monitor-period 與 node-monitor-grace-period 則決定 Controller 多久檢查一次，以及超時多久要判定為 NotReady.預設情況下，最快需要 40 秒去偵測節點故障預設情況下，每個 Pod 可以於故障節點上存活 300 秒預設情況下，一個 Pod 最快需要 340 秒才可以從故障節點中被重新部署Pod 可以透過 Taint-Based Evicition 的方式來調整反應時間 Reference https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.mdhttps://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictionshttps://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589-efficient-node-heartbeats ","version":"Next","tagName":"h2"},{"title":"Preface","type":0,"sectionRef":"#","url":"/docs/techPost/2023/k8s-pod-affinity-1","content":"","keywords":"Kubernetes DevOps PodAffinity","version":"Next"},{"title":"requiredDuringSchedulingIgnoredDuringExecution​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2023/k8s-pod-affinity-1#requiredduringschedulingignoredduringexecution","content":"所有不熟悉的欄位都建議透過 kubectl explain 來閱讀一下其概念及用法，以下述範例可以觀察到 requiredDuringSchedulingIgnoredDuringExecution 底下要使用 nodeSelectorTerms 以 list 的形式去描述符合的節點資訊，若描述多個資訊則彼此的運算關係為 &quot;OR&quot;，這意味節點只要有符合其中一個關係，就可以被 Scheduler 給納入考慮。 2 $ kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution KIND: Pod VERSION: v1 RESOURCE: requiredDuringSchedulingIgnoredDuringExecution &lt;Object&gt; DESCRIPTION: If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node. A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms. FIELDS: nodeSelectorTerms &lt;[]Object&gt; -required- Required. A list of node selector terms. The terms are ORed.  繼續往下看可以看到 $ kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms KIND: Pod VERSION: v1 RESOURCE: nodeSelectorTerms &lt;[]Object&gt; DESCRIPTION: Required. A list of node selector terms. The terms are ORed. A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm. FIELDS: matchExpressions &lt;[]Object&gt; A list of node selector requirements by node's labels. matchFields &lt;[]Object&gt; A list of node selector requirements by node's fields.  nodeSelectorTerms 本身支援兩種格式，分別是 Node Labels 以及 Node Fields，所以 API 層面上不單純只有 labels 可以使用。 這邊繼續往下看一下 matchExpressions 的介紹與用法 $ kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchExpressions KIND: Pod VERSION: v1 RESOURCE: matchExpressions &lt;[]Object&gt; DESCRIPTION: A list of node selector requirements by node's labels. A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values. FIELDS: key &lt;string&gt; -required- The label key that the selector applies to. operator &lt;string&gt; -required- Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. Possible enum values: - `&quot;DoesNotExist&quot;` - `&quot;Exists&quot;` - `&quot;Gt&quot;` - `&quot;In&quot;` - `&quot;Lt&quot;` - `&quot;NotIn&quot;` values &lt;[]string&gt; An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.  這邊可以看到 matchExpressions 底下會使用 Key (Label Key)OperatorValue (Label Value) 三個欄位來幫你判斷節點是否符合條件，相較於 NodeSelector 來說，這邊的條件更多元，譬如 Exists, NotIn, Gt 等，請參閱 Operator 來學習更多 Operator 彼此的定義 以下範例是必須要將 Pod 給部署到所有含有 kind.zone Label 的節點 apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity-1 spec: replicas: 10 selector: matchLabels: app: node-affinity-1 template: metadata: labels: app: node-affinity-1 spec: containers: - name: www-server image: hwchiu/netutils affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kind.zone operator: Exists values: []  部署完畢的結果如下，四個節點都有 kind.zone 此 key，所以都符合需求。  由於 nodeSelectorTerms 底下的結果是採取 OR 運算，因此下列 YAML 則透過 OR 的概念去比對多個結果，可以得到跟前述類似的部署結果。 apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity-2 spec: replicas: 10 selector: matchLabels: app: node-affinity-2 template: metadata: labels: app: node-affinity-2 spec: containers: - name: www-server image: hwchiu/netutils affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kind.zone operator: In values: - zone1 - matchExpressions: - key: kind.zone operator: In values: - zone2  其他注意事項 如果今天想要達到的是反親和力的操作，想要讓 Pod 遠離某些節點，那就要仰賴 NotIn, DoesNotExist 等運算元來反向操作，同時若可與 NodeSelector 同時使用，但是使用時就會兩邊條件都要滿足MatchExpressions 底下若有多組 (key/operator/value) 的條件，這些條件彼此則是 AND 的結果 舉例來說，下列寫法就要求節點同時要擁有 kind.zone=Zone1 以及 kind.zone=Zone2，而測試環境內沒有任何節點可以滿足，因此所有 Pod 都會處於 Pending 狀態 apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity-3 spec: replicas: 10 selector: matchLabels: app: node-affinity-3 template: metadata: labels: app: node-affinity-3 spec: containers: - name: www-server image: hwchiu/netutils affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kind.zone operator: In values: - zone1 - key: kind.zone operator: In values: - zone2  $ kubectl get pods NAME READY STATUS RESTARTS AGE node-affinity-3-7c5574dd67-7spjw 0/1 Pending 0 71s node-affinity-3-7c5574dd67-98pbj 0/1 Pending 0 71s node-affinity-3-7c5574dd67-9bgfr 0/1 Pending 0 71s node-affinity-3-7c5574dd67-b9bs4 0/1 Pending 0 71s node-affinity-3-7c5574dd67-bqm8m 0/1 Pending 0 71s node-affinity-3-7c5574dd67-glmkx 0/1 Pending 0 71s node-affinity-3-7c5574dd67-hcmgp 0/1 Pending 0 71s node-affinity-3-7c5574dd67-l45bv 0/1 Pending 0 71s node-affinity-3-7c5574dd67-mn7js 0/1 Pending 0 71s node-affinity-3-7c5574dd67-th2pd 0/1 Pending 0 71s  ","version":"Next","tagName":"h2"},{"title":"preferredDuringSchedulingIgnoredDuringExecution​","type":1,"pageTitle":"Preface","url":"/docs/techPost/2023/k8s-pod-affinity-1#preferredduringschedulingignoredduringexecution","content":"相對於 requiredDuringSchedulingIgnoreDuringExecution， preferred 的版本則是一個偏好的設定，如果有符合就以符合為主，沒有的話也沒關係，因此除非遇到 Taint 或是節點資源不足，不然大部分情況下就不會遇到 Pending 卡住的情況。 $ kubectl explain pod.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution KIND: Pod VERSION: v1 RESOURCE: preferredDuringSchedulingIgnoredDuringExecution &lt;[]Object&gt; DESCRIPTION: The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding &quot;weight&quot; to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred. An empty preferred scheduling term matches all objects with implicit weight 0 (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op). FIELDS: preference &lt;Object&gt; -required- A node selector term, associated with the corresponding weight. weight &lt;integer&gt; -required- Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.  這邊可以觀察到 preferred 本身透過的是 prefernce 這個欄位來描述 node selector term，同時因為 preference 本身是偏好而非硬性部署，所以還多了 weight 權重的概念，可以讓你透過權重調整 Pod 的分配情況 下述範例則使用兩個不同的規則並且給予不同權重，能的話盡量將 Pod 部署到含有 kind.zone:zone2 的節點上 apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity-4 spec: replicas: 10 selector: matchLabels: app: node-affinity-4 template: metadata: labels: app: node-affinity-4 spec: containers: - name: www-server image: hwchiu/netutils affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: kind.zone operator: In values: - zone1 - weight: 4 preference: matchExpressions: - key: kind.zone operator: In values: - zone2  部署結果如下圖，大部分都座落於 worker3/worker4，與預期符合 Required 跟 Preferred 兩者是可以互相疊加的，譬如可以透過下列 YAML 達成 服務只能部署到含有 kind.zone: zone1 的節點若節點名稱為 k8slab-worker2，則給予更大權重 apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity-5 spec: replicas: 10 selector: matchLabels: app: node-affinity-5 template: metadata: labels: app: node-affinity-5 spec: containers: - name: www-server image: hwchiu/netutils affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kind.zone operator: In values: - zone1 preferredDuringSchedulingIgnoredDuringExecution: - weight: 4 preference: matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8slab-worker2   Summary 本篇文章探討了三個以 Node 為出發點的設定方式 NodeNameNodeSelectorNodeAffinity 除了 NodeName 是以 Node 名稱來設定外，後續都是基於 Node Label 為基礎來調整來達到更為彈性的設定方式，而 NodeAffinity 可視為 NodeSelector 的加強版，提供了基於 Required 與 Preferred 兩種模式來達到更細部得操作 而下篇文章會繼續從 Inter-Pod Affinity 以及 Pod TopologySpreadConstraints 來接續探討 ","version":"Next","tagName":"h2"},{"title":"Kubernetes 1.28 Sidecar Container 初體驗","type":0,"sectionRef":"#","url":"/docs/techPost/2023/k8s-sidecar","content":"","keywords":"","version":"Next"},{"title":"System​","type":1,"pageTitle":"Kubernetes 1.28 Sidecar Container 初體驗","url":"/docs/techPost/2023/k8s-sidecar#system","content":"由於環境會使用 kubeadm 來安裝，並且使用 containerd 作為 container runtime，因此準備下列腳本安裝所有相關軟體並且設定相關環境參數 # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Add the repository to Apt sources: echo \\ &quot;deb [arch=&quot;$(dpkg --print-architecture)&quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ &quot;$(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;)&quot; stable&quot; | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install containerd.io=1.6.24-1 https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz sudo mkdir -p /opt/cni/bin sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.3.0.tgz sudo apt-get install -y kubelet=1.28.2-1.1 kubeadm=1.28.2-1.1 kubectl=1.28.2-1.1 sudo modprobe br_netfilter sudo modprobe overlay sudo sysctl -w net.ipv4.ip_forward=1 cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo sysctl --system containerd config default | sudo tee /etc/containerd/config.toml sudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml sudo systemctl restart containerd  ","version":"Next","tagName":"h2"},{"title":"Kubeadm​","type":1,"pageTitle":"Kubernetes 1.28 Sidecar Container 初體驗","url":"/docs/techPost/2023/k8s-sidecar#kubeadm","content":"準備下列 Kubeadm 檔案來打開 Sidecar Container 的設定檔案 kubeadm.config apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration networking: podSubnet: 192.168.0.0/16 apiServer: extraArgs: feature-gates: &quot;SidecarContainers=true&quot; controllerManager: extraArgs: feature-gates: &quot;SidecarContainers=true&quot; scheduler: extraArgs: feature-gates: &quot;SidecarContainers=true&quot; --- apiVersion: kubelet.config.k8s.io/v1beta1 featureGates: SidecarContainers: true kind: KubeletConfiguration  sudo kubeadm init --config=kubeadm.config  創建完畢後安裝 Calico CNI kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml  當節點變成 Ready 後，執行下列指令將 taint 給移除 kubectl taint nodes master node-role.kubernetes.io/control-plane:NoSchedule-  Experiment 以下來示範兩種 sidecar container 使用上的可能性，並且探討如何使用 1.28 的新功能來解決 ","version":"Next","tagName":"h2"},{"title":"Case 1​","type":1,"pageTitle":"Kubernetes 1.28 Sidecar Container 初體驗","url":"/docs/techPost/2023/k8s-sidecar#case-1","content":"第一個情境探討於 Job 的情況下使用 sidecar container 造成的判定問題 舉例來說，以下列的 YAML 部署一個有 sidecar container 的 Job 服務，該範例中的 sidecar 單純是個示範，其功能不重要。 apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl:5.34.0 command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;] - name: sidecar image: hwchiu/netutils restartPolicy: Never backoffLimit: 4  部署該 YAML 可以觀察到下列情況 $ kubectl get pods,job NAME READY STATUS RESTARTS AGE pod/pi-6q4lh 1/2 NotReady 0 17m NAME COMPLETIONS DURATION AGE job.batch/pi 0/1 17m 17m  主要容器運行完畢結束，但是 sidecar container 繼續運行，因此使得當前的 Pod 沒有辦法達到 &quot;Completed&quot; 的狀態，因此 Job 無法正常判定。 而現在來嘗試看看 v1.28 的新功能 該功能的邏輯是建立在一個 &quot;會不停運行的 initContainer&quot; 之上，因此設定上是從 &quot;initContainer&quot; 出發，並且透過 &quot;restartPolicy&quot; 來開啟 sidecar container 的功能。 一旦 sidecare container 設定 &quot;restartPolicy: Always&quot;，其背後的運作邏輯就會有些許改變 本身不需要結束就可以繼續往下執行其他 Init Container本身若發生問題離開，會自動重啟本身的運行狀態不會影響 Pod 本身的狀態判定 接下來嘗試下列 YAML 檔案，我們將 sidecar container 搬移到 init container 的階段，來試試看這樣的情況下 Pod 是否可以順利結束 apiVersion: batch/v1 kind: Job metadata: name: pi-sidecar spec: template: spec: initContainers: - name: network-proxy image: hwchiu/python-example restartPolicy: Always containers: - name: pi image: perl:5.34.0 command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;] restartPolicy: Never backoffLimit: 4  $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-sidecar-bszf2 0/2 Completed 0 42s  從運作結果來看， Pod 本身所認為的 Conainer 數量依然是 2，但是這時候就可以順利結束完成 Completed 狀態 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 79s default-scheduler Successfully assigned default/pi-sidecar-bszf2 to master Normal Pulling 78s kubelet Pulling image &quot;hwchiu/python-example&quot; Normal Pulled 77s kubelet Successfully pulled image &quot;hwchiu/python-example&quot; in 1.511s (1.511s including waiting) Normal Created 77s kubelet Created container sidecar Normal Started 77s kubelet Started container sidecar Normal Pulled 76s kubelet Container image &quot;perl:5.34.0&quot; already present on machine Normal Created 76s kubelet Created container pi Normal Started 76s kubelet Started container pi Normal Killing 67s kubelet Stopping container network-proxy  另外從 kubectl describe pods 中去觀察，可以看到最後一項 Stopping container network-proxy，這意味當主要容器結束之後，sidecar container(network-proxy) 會自己被系統結束，並不會影響到主要容器的生命週期。 ","version":"Next","tagName":"h2"},{"title":"Case 2​","type":1,"pageTitle":"Kubernetes 1.28 Sidecar Container 初體驗","url":"/docs/techPost/2023/k8s-sidecar#case-2","content":"第二個範例中模擬的情境是透過 sidecar container 達成類似 Proxy 的連線，因此 sidecar container 必須要比主要容器更早啟動。 以下列 YAML 為範例 apiVersion: apps/v1 kind: Deployment metadata: name: proxy spec: replicas: 3 selector: matchLabels: run: proxy template: metadata: labels: run: proxy spec: containers: - name: app image: hwchiu/netutils command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;nc -zv localhost 5000 &amp;&amp; sleep 1d&quot;] - name: proxy image: hwchiu/python-example ports: - containerPort: 5000 startupProbe: httpGet: path: / port: 5000  該範例中部署兩個 Container，其中 sidecar container 是一個聽 5000 port 的服務器，而主要容器起來時若 sidecar 還沒準備好就會直接離開等待下次重啟 $ kubectl get pods NAME READY STATUS RESTARTS AGE proxy-74dc7b8d88-77cft 2/2 Running 1 (49s ago) 52s proxy-74dc7b8d88-rlz8m 2/2 Running 1 (47s ago) 52s proxy-74dc7b8d88-zjkdh 2/2 Running 1 (46s ago) 52s $ kubectl logs -p proxy-74dbbdccd5-cf9pg Defaulted container &quot;app&quot; out of: app, proxy localhost [127.0.0.1] 5000 (?) : Connection refused  重上述的部署結果可以觀察到所有的 Pod 都會因為順序問題使得主要容器會重啟一次，從前述失敗的 log 也可以觀察到因為 sidecar container 還沒準備好因此導致運行失敗。 透過 kubectl describe pod 觀察相關事件  Normal Scheduled 6m20s default-scheduler Successfully assigned default/proxy-74dbbdccd5-dzdmz to master Normal Pulled 6m18s kubelet Successfully pulled image &quot;hwchiu/netutils&quot; in 1.447s (1.447s including waiting) Normal Pulling 6m18s kubelet Pulling image &quot;hwchiu/python-example&quot; Normal Pulled 6m15s kubelet Successfully pulled image &quot;hwchiu/python-example&quot; in 2.459s (2.459s including waiting) Normal Created 6m15s kubelet Created container proxy Normal Started 6m15s kubelet Started container proxy Normal Pulling 6m14s (x2 over 6m19s) kubelet Pulling image &quot;hwchiu/netutils&quot; Normal Created 6m13s (x2 over 6m18s) kubelet Created container app Normal Pulled 6m13s kubelet Successfully pulled image &quot;hwchiu/netutils&quot; in 1.47s (1.47s including waiting) Normal Started 6m12s (x2 over 6m18s) kubelet Started container app  可以觀察到 Proxy 容器啟動後就馬上去抓取 App 的容器，中間幾乎沒有任何間隔。 接下來導入 sidecar container 的機制再次嘗試看看 apiVersion: apps/v1 kind: Deployment metadata: name: proxy-sidecar spec: replicas: 3 selector: matchLabels: run: proxy-sidecar template: metadata: labels: run: proxy-sidecar spec: initContainers: - name: proxy image: hwchiu/python-example ports: - containerPort: 5000 restartPolicy: Always startupProbe: httpGet: path: / port: 5000 containers: - name: app image: hwchiu/netutils command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;nc -zv localhost 5000 &amp;&amp; sleep 1d&quot;]  轉換為 $ kubectl get pods proxy-sidecar-5dd9ff76f8-47mll 2/2 Running 0 2m34s proxy-sidecar-5dd9ff76f8-cmjjs 2/2 Running 0 2m34s proxy-sidecar-5dd9ff76f8-qctk8 2/2 Running 0 2m34s $ kubectl describe pods proxy-sidecar-5dd9ff76f8-qctk8 ... Normal Scheduled 2m16s default-scheduler Successfully assigned default/proxy-sidecar-5dd9ff76f8-qctk8 to master Normal Pulling 2m14s kubelet Pulling image &quot;hwchiu/python-example&quot; Normal Pulled 2m11s kubelet Successfully pulled image &quot;hwchiu/python-example&quot; in 1.507s (2.923s including waiting) Normal Created 2m11s kubelet Created container proxy Normal Started 2m11s kubelet Started container proxy Normal Pulling 2m5s kubelet Pulling image &quot;hwchiu/netutils&quot; Normal Pulled 2m1s kubelet Successfully pulled image &quot;hwchiu/netutils&quot; in 1.475s (4.405s including waiting) Normal Created 2m kubelet Created container app Normal Started 2m kubelet Started container app  當導入 sidecar container 的設定後，可以觀察到 Proxy 容器起後動過了一段時間才去抓取新的 Image，這是因為 sidecar container 會等到其 StartupProbe 結束後才開始往下去執行主要容器，透過這種機制就可以確保 sidecar container 會比主要容器更早運行。 最後以這兩張圖來呈現一下案例二的流程，過往將所有容器都放到 containers 中來處理 sidecar container 的邏輯。 而新版架構則將其設定搬移到 initContainer 中，並且是從 Kubernetes 內部來處理專屬的生命週期  Summary 以目前初次體驗來說， sidecar container 帶來的好處非常的明顯，能夠減少很多過往的 workaround，讓 sidecar container 的模式更加自然，以 istio 來說，其新版本也支援使用 k8s 1.28 sidecar 的功能，不過因為還沒測試過因此不確定從 istio 的角度來說 實際上會有什麼樣的差異。 另外此項功能於 1.28 還只是 alpha 版本，接下來還有 Beta 以及 GA，最快也要兩個版本大概六個月，這樣可能就是 1.30，而各大公有雲平台 (GKE/EKS/AKS) 想要追到 1.30 想必也不短期內會發生的事情，因此除非自已去調整 feature gate 來啟動，不然短期內應該還很難大量落地。 Reference https://kubernetes.io/blog/2023/08/25/native-sidecar-containers/ ","version":"Next","tagName":"h2"},{"title":"前言","type":0,"sectionRef":"#","url":"/docs/techPost/2023/kind-fun-facts","content":"","keywords":"Kubernetes Network Linux","version":"Next"},{"title":"實作​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/kind-fun-facts#實作","content":"Docker DNS 的設計非常精巧，其利用 Linux namespace 的概念來完美處理這個問題，讓所有的 Docker DNS 伺服器運行於 Host 本身上(Pid namespace)，同時網路的部分則是聽到所有 container 裡面(network namespace)。 這種架構使得你有辦法透過 127.0.0.11 去存取 DNS Server 但是你又沒有辦法於 Container 內找到這個 DNS Server 的行程。 同時為了避免 DNS Server 會與使用者自己的 DNS 服務衝突，所以 Docker DNS 就不會使用 Port 53，而採取一個隨機的數字。 整體架構如下圖 但是對於 Container 服務來說，由於 /etc/hosts 已經被改為使用 127.0.0.11 當作預設的 DNS 搜尋，而且預設都會基於 Port 53 來使用，所以 Dockerd 這邊又仰賴 iptables 的幫助來動態調整規則，把所有送往 127.0.0.11:53 的封包都動態修改其目標 port 來處理連結問題。 因此你若透過 nsenter 等指令進入到容器內觀察，使用 ss 與 iptalbse 的指令可以觀察到如下圖的結果 其中 ss 顯示了環境中 127.0.0.11 有監聽兩個 Port，分別對應 TCP 與 UDP 的 DNS 請求，而 iptables 則顯示的相關 DNAT 的規則 透過這些處理就能夠讓 container 所有的 DNS 請求都由 Docker DNS 進行處理，同時又不會預先霸佔 Port 53。 這邊可以看到相關原始碼，該程式碼顯示了 Docker 會動態修改四條規則來處理 DNAT + SNAT 的需求。 ... resolverIP, ipPort, _ := net.SplitHostPort(os.Args[2]) _, tcpPort, _ := net.SplitHostPort(os.Args[3]) rules := [][]string{ {&quot;-t&quot;, &quot;nat&quot;, &quot;-I&quot;, outputChain, &quot;-d&quot;, resolverIP, &quot;-p&quot;, &quot;udp&quot;, &quot;--dport&quot;, dnsPort, &quot;-j&quot;, &quot;DNAT&quot;, &quot;--to-destination&quot;, os.Args[2]}, {&quot;-t&quot;, &quot;nat&quot;, &quot;-I&quot;, postroutingchain, &quot;-s&quot;, resolverIP, &quot;-p&quot;, &quot;udp&quot;, &quot;--sport&quot;, ipPort, &quot;-j&quot;, &quot;SNAT&quot;, &quot;--to-source&quot;, &quot;:&quot; + dnsPort}, {&quot;-t&quot;, &quot;nat&quot;, &quot;-I&quot;, outputChain, &quot;-d&quot;, resolverIP, &quot;-p&quot;, &quot;tcp&quot;, &quot;--dport&quot;, dnsPort, &quot;-j&quot;, &quot;DNAT&quot;, &quot;--to-destination&quot;, os.Args[3]}, {&quot;-t&quot;, &quot;nat&quot;, &quot;-I&quot;, postroutingchain, &quot;-s&quot;, resolverIP, &quot;-p&quot;, &quot;tcp&quot;, &quot;--sport&quot;, tcpPort, &quot;-j&quot;, &quot;SNAT&quot;, &quot;--to-source&quot;, &quot;:&quot; + dnsPort}, } ...  到這邊為止已經對 Docker DNS 有基本的瞭解，接下來看一下 Kubernetes 的情況 Kubernetes Kubernetes 叢集中也有一個 DNS Server，從早期的 Kube-DNS 到現在的 CoreDNS，其功用與 Docker DNS 非常類似 若 DNS 請求是 Kubernetes 內部服務，則回應內部資訊否則將該請求給轉發給上游 DNS 伺服器處理 與 Docker DNS 一樣，都是專注處理自己服務的請求，不行就往外轉發。 然而對於 CoreDNS 來說，所謂的上游 DNS 伺服器預設情況下就是節點所使用的 DNS 伺服器，也就是所謂的 127.0.0.11。 因此整個流程如下 假設 worker2 上的 Pod 想要詢問一個 DNS 請求，該請求輾轉送到了 CoreDNS 去處理，而 CoreDNS 無法解析的情況下，想要轉發給上游 DNS Server，因此就轉送給了 127.0.0.11，而這邊就是問題的發生所在。 ","version":"Next","tagName":"h2"},{"title":"Issue​","type":1,"pageTitle":"前言","url":"/docs/techPost/2023/kind-fun-facts#issue","content":"由於 127.0.0.11 是 Dockerd 特製的，因此由 Docker Continaer 上的 Containerd 所管理 CoreDNS 自然而然就沒有這個功能，既沒有相關 iptables 也沒有相關的 Docker DNS 伺服器 因此對於 CoreDNS 來說，所有送往 127.0.0.11 的 DNS 封包就如同送往黑洞般，沒人處理。 為了解決這個問題， KIND 的想法就是讓 CoreDNS 不要把封包送給 127.0.0.11，而是送往節點的 IP，並且透轉發最終送到節點上的 127.0.0.11 服務 整個流程如下 前述提到 CoreDNS 本身會使用節點上的 /etc/hosts 當作上游伺服器，因此這邊的做法就是動態修正節點上的 /etc/hosts，將預設 DNS 伺服器從 127.0.0.11 改成節點本身的 IP，以範例圖來說就是 172.18.0.2。 一旦修改了預設請求位置，所有 Dockerd 所設立的 iptables 規則就沒有辦法使用，因此時候又要重新修改 iptables 來滿足。 以下是一開始 Dockerd 所設定的 iptables 規則 而 KIND 則會將其修改成如下(來自不同節點的範例，該節點 IP 是 172.18.0.1) 可以看到這時候所有送往 172.18.0.1 的封包都會被轉向 127.0.0.11:33501/41285，也就是 Docker DNS 的位置，同時也針對 SNAT 的地方進行修改。 從 KIND 的原始碼 這邊可以看到  # well-known docker embedded DNS is at 127.0.0.11:53 local docker_embedded_dns_ip='127.0.0.11' # first we need to detect an IP to use for reaching the docker host local docker_host_ip docker_host_ip=&quot;$( (head -n1 &lt;(timeout 5 getent ahostsv4 'host.docker.internal') | cut -d' ' -f1) || true)&quot; # if the ip doesn't exist or is a loopback address use the default gateway if [[ -z &quot;${docker_host_ip}&quot; ]] || [[ $docker_host_ip =~ ^127\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then docker_host_ip=$(ip -4 route show default | cut -d' ' -f3) fi # patch docker's iptables rules to switch out the DNS IP iptables-save \\ | sed \\ `# switch docker DNS DNAT rules to our chosen IP` \\ -e &quot;s/-d ${docker_embedded_dns_ip}/-d ${docker_host_ip}/g&quot; \\ `# we need to also apply these rules to non-local traffic (from pods)` \\ -e 's/-A OUTPUT \\(.*\\) -j DOCKER_OUTPUT/\\0\\n-A PREROUTING \\1 -j DOCKER_OUTPUT/' \\ `# switch docker DNS SNAT rules rules to our chosen IP` \\ -e &quot;s/--to-source :53/--to-source ${docker_host_ip}:53/g&quot;\\ `# nftables incompatibility between 1.8.8 and 1.8.7 omit the --dport flag on DNAT rules` \\ `# ensure --dport on DNS rules, due to https://github.com/kubernetes-sigs/kind/issues/3054` \\ -e &quot;s/p -j DNAT --to-destination ${docker_embedded_dns_ip}/p --dport 53 -j DNAT --to-destination ${docker_embedded_dns_ip}/g&quot; \\ | iptables-restore # now we can ensure that DNS is configured to use our IP cp /etc/resolv.conf /etc/resolv.conf.original replaced=&quot;$(sed -e &quot;s/${docker_embedded_dns_ip}/${docker_host_ip}/g&quot; /etc/resolv.conf.original)&quot; if [[ &quot;${KIND_DNS_SEARCH+x}&quot; == &quot;&quot; ]]; then # No DNS search set, just pass through as is echo &quot;$replaced&quot; &gt;/etc/resolv.conf elif [[ -z &quot;$KIND_DNS_SEARCH&quot; ]]; then # Empty search - remove all current search clauses echo &quot;$replaced&quot; | grep -v &quot;^search&quot; &gt;/etc/resolv.conf else # Search set - remove all current search clauses, and add the configured search { echo &quot;search $KIND_DNS_SEARCH&quot;; echo &quot;$replaced&quot; | grep -v &quot;^search&quot;; } &gt;/etc/resolv.conf fi  KIND 所搭建的 K8s 節點每次運行起來時都會去修改 iptalbes 的規則，同時也更新 /etc/resolve.conf 的預設地址 透過這些修改， CoreDNS 就會將 DNS 請求給送往節點本，而這些 DNS 請求又會透過 iptables 轉發給 Docker DNS 進行處理，當 Docker DNS 不能處理時又會再度往上轉發到最初 Host 上的設定，一層又一層的轉出去。 Summary Docker 有內建的 DNS 伺服器用來處理 Docker Container 間的 DNS 請求Docker 透過 iptables 與 namespace 來簡化整個 DNS 的部署與運作Kubernetes 內的 CoreDNS 預設情況下會受到 Docker DNS 的影響，因為 /etc/resolve 被修改KIND 則是二次修改這些規則，改動 /etc/resolve 與 iptables 規則來修正所有路線，確保 DNS 都可以往外轉發。 ","version":"Next","tagName":"h2"},{"title":"ruamel.yaml 小筆記","type":0,"sectionRef":"#","url":"/docs/techPost/2023/python-ruamel","content":"ruamel.yaml 小筆記 使用 Python 管理最簡易的方式就是透過 pyyaml 這個套件來處理，其安裝也非常簡單 pip3 install pyyaml 安裝完畢後就可以透過下列一個簡單的範例來讀取檔案並且重新修改回去 import yaml with open('data1.yaml', 'r') as file: data = yaml.safe_load(file) with open('output_file.yaml', 'w') as file: yaml.dump(data, file) 假設今天 data1.yaml 的內容如下 # resources resources: requests: cpu: &quot;50m&quot; memory: &quot;256Mi&quot; limits: cpu: &quot;2000m&quot; memory: &quot;4096Mi&quot; # config config: enabled: false internal: - name: test data: port: 8080 size: 123 hosts: - a.b.com - c.b.com 執行上述的範例就會讀取該檔案並且重新輸出到一個名為 output_file.yaml 的檔案，這時後去檢視其內容會得到下列範例 config: enabled: false internal: - data: port: 8080 size: 123 hosts: - a.b.com - c.b.com name: test resources: limits: cpu: 2000m memory: 4096Mi requests: cpu: 50m memory: 256Mi 仔細觀察這個輸出，可以觀察到其與最原始的檔案有諸多差異 註解不見了內容順序調動list 底下的 indent 不一致，兩邊的 config.internal.data 順序不同字串的 quota 都被移除，如 &quot;2000m&quot; 如果今天的需求是動態產生全新 YAML 檔案，那上述這些問題就不復存在，但是當今天的需求是修改已經存在的 YAML 同時又希望盡量可能保持原樣，那原生的 Pyyaml 並沒有非常好的解法去處理這些問題。 譬如註解的問題已經存在很久目前也沒有正式解法 https://github.com/yaml/pyyaml/issues/90 因此如果有上述需求的寫法，會更推薦改用 ruamel.yaml 來進行處理 安裝部分也非常簡單，可以透過 pip3 安裝 pip3 install ruamel.yaml 以下是一個完全非常簡易的讀取並且輸出範例 from ruamel.yaml import YAML yaml = YAML() yaml.indent(mapping=2, sequence=4, offset=2) yaml.preserve_quotes = True with open('data1.yaml', 'r') as file: data = yaml.load(file) with open('output_file2.yaml', 'w') as file: yaml.dump(data, file) 該範例中會先初始化 YAML 的物件，並且設定幾個屬性 保留字串中的 quotes設定 indent 相關參數註解預設會被保留 因此執行上述範例得到的 output_file2.yaml 內容如下 # resources resources: requests: cpu: &quot;50m&quot; memory: &quot;256Mi&quot; limits: cpu: &quot;2000m&quot; memory: &quot;4096Mi&quot; # config config: enabled: false internal: - name: test data: port: 8080 size: 123 hosts: - a.b.com - c.b.com 可以觀察到此範例與原始內容完全一致，沒有任何字串或是任何欄位被自動處理，因此如果對於修改 YAML 又同時不希望改動既有檔案內容格式的都推薦改用 ruamel.yaml 來處理","keywords":"python yaml","version":"Next"},{"title":"解密 Assigning Pod To Nodes(下)","type":0,"sectionRef":"#","url":"/docs/techPost/2023/k8s-pod-affinity-2","content":"","keywords":"Kubernetes DevOps PodAffinity","version":"Next"},{"title":"Anti-Affinity​","type":1,"pageTitle":"解密 Assigning Pod To Nodes(下)","url":"/docs/techPost/2023/k8s-pod-affinity-2#anti-affinity","content":"第一個範例嘗試透過 required 來限制部署情況，而參照對象為自己 apiVersion: apps/v1 kind: Deployment metadata: name: pod-affinity-1 spec: replicas: 3 selector: matchLabels: app: pod-affinity-1 template: metadata: labels: app: pod-affinity-1 spec: containers: - name: www-server image: hwchiu/netutils affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - pod-affinity-1 topologyKey: &quot;kind.zone&quot;  基於 kind.zone 作為 TopologyKey 使用 AntiAffinity 配上 Required，要求同 ToplogyKey 中彼此不能出現第二個 Pod部署三個副本 根據前述對稱性所描述的規則，若當下環境沒有任何符合規則的 Pod，則可以隨意部署，所以第一個 Pod 可以順利部署。  第二個 Pod 部署的時候就會觀察到 Pod1 已經把左邊給佔據了，因此只剩下右邊該群可用  第三個 Pod 部署的時候因為兩個群上面都已經有 Pod 正在運行，而環境中沒有任何其他符合條件的節點可以用，因此最終就會卡到 Pending.  由結果可以觀察根據 TopologyKey=kind.zone 來分類，叢集中只能分到兩群，而第三個 Pod 則會因為 AntiAffinity + Required 的效果因此沒有辦法被部署，所以這種使用方法就要特別注意副本數量與分群數量，特別是當透過 HPA 來動態調整副本時更容易出錯。 若將 Rqeuired 改成 Prefer 的概念的話，則可以達到一樣將 Pod 給分散同時又不會出現 Pending 的狀況，因為此時的條件不是硬性規定，而是參考而已，所以前面兩個 Pod 都會盡量分散，而第三個 Pod 依然有能力被調度。 這時候得到的結果可能會是如下圖  第二個範例準備兩個檔案，模擬服務 A 與 B 彼此之間的 Anti-Affinity 設定 apiVersion: apps/v1 kind: Deployment metadata: name: pod-affinity-3 spec: replicas: 3 selector: matchLabels: app: pod-affinity-3 template: metadata: labels: app: pod-affinity-3 spec: containers: - name: www-server image: hwchiu/netutils affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - pod-affinity-4 topologyKey: &quot;kind.zone&quot;  該範例繼續使用 Anti-Affinity + required 的方式來限制，不過這次的參照對象是別的服務 pod-affinity-4，而環境中目前沒有這個服務出現 部署上不會出現問題，所有的 Pod 都可以正常運行，接者我們嘗試部署一個沒有任何 Affinity 的 pod-affinity-4 服務 apiVersion: apps/v1 kind: Deployment metadata: name: pod-affinity-4 spec: replicas: 1 selector: matchLabels: app: pod-affinity-4 template: metadata: labels: app: pod-affinity-4 spec: containers: - name: www-server image: hwchiu/netutils  可以觀察到這時候 pod-affinity-4 這個服務卻卡 Pending 了 透過 kubectl describe 可以看到原因為是因為不滿足當下運行 Pod 的 Anti-Affinity。 這也是設計文件中所描述的對稱性，因此即使後續服務沒有特別撰寫 Anti-Affinity，其調度的過程中也會根據當下其他 Pod 的資訊來判別是否可以調度 ","version":"Next","tagName":"h2"},{"title":"Affinity​","type":1,"pageTitle":"解密 Assigning Pod To Nodes(下)","url":"/docs/techPost/2023/k8s-pod-affinity-2#affinity","content":"接下來基於相同概念來測試 Affinity，服務 A 依賴於 服務 B(pod-affinity-6) apiVersion: apps/v1 kind: Deployment metadata: name: pod-affinity-5 spec: replicas: 5 selector: matchLabels: app: pod-affinity-5 template: metadata: labels: app: pod-affinity-5 spec: containers: - name: www-server image: hwchiu/netutils affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - pod-affinity-6 topologyKey: &quot;kind.zone&quot;  與 Anti-Affinity 不同，若當下找不到目標服務， Affinity 則會卡在 Pending 的狀況，沒有辦法部署。 註: 若參考對象是自己則為特殊情況，不會有 Pending 的情況發生，否則會有 deadlock 的情形出現。 接下來部署服務 B apiVersion: apps/v1 kind: Deployment metadata: name: pod-affinity-6 spec: replicas: 1 selector: matchLabels: app: pod-affinity-6 template: metadata: labels: app: pod-affinity-6 spec: containers: - name: www-server image: hwchiu/netutils  部署下去後可以觀察到服務 A 與 B 幾乎同時順利完成調度的決策，一起被分配到相同的 kind.zone 內 以過程中來說，最初的服務 A 因為找不到服務 B 可以匹配，所以全部卡 Pending 的狀況 而服務 B 本身沒有描述任何 Affinity 的規則，因此本身順利被調度 當服務 B 被調度到 kind.zone=zone1 後，所有卡住的服務 A 就有參照對象可以比較，所已全部 Pod 就直接部署上去了。 PodTopologySpread 前述所探討的 NodeAffinity 以及 Inter-Pod (Anti)Affinity 可以滿足許多人控制 Pod 調度的需求，然而實際使用上會遇到一些問題 透過 NodeAffinity 並沒有保證 Pod 可以均勻的分散到各節點上，有可能會遇到分佈不均勻的情況 (66981)Inter-Pod Anti Affinity 碰到 Deployment rolling upgrade 時會出問題，新的 Pod 要先被創立但是因為 Anti-Affinity 的限制導致沒有節點可用，所以新版本的 Pod 就會處於 Pending 而整個 Deployment 更新就會卡死 40358 因為上述問題所以就有了 Pod Topology Spread 的發展，而整個 Pod Topology Spread 中最重要的一個因素就稱為 Skew，該數值是用來處理 Pod 分配不均勻的問題，其定義為。 skew = Pods number matched in current topology - min Pods matches in a topology PodTopologySpread 每次分配 Pod 的時候都會針對每個節點計算當下的 Skew 數值並且以數值來影響調度的決策。 以下圖為範例  topologyKey 將節點分成三群每個群上面目前運行的 Pod 數量分別為 3,2,1群中最少的運行 Pod 數量為 1分別計算可以得到每群對應的 Skew 數值為 2,1,0 下列是透過 kubectl explain 得到的欄位介紹，由於篇幅過程所以只保留重點欄位，實際上還有很多選項不過部分選項即使到 v1.26 都還是 Beta 測試 $ kubectl explain pod.spec.topologySpreadConstraints KIND: Pod VERSION: v1 RESOURCE: topologySpreadConstraints &lt;[]Object&gt; DESCRIPTION: TopologySpreadConstraints describes how a group of pods ought to spread across topology domains. Scheduler will schedule pods in a way which abides by the constraints. All topologySpreadConstraints are ANDed. TopologySpreadConstraint specifies how to spread matching pods among the given topology. FIELDS: labelSelector &lt;Object&gt; LabelSelector is used to find matching pods. Pods that match this label selector are counted to determine the number of pods in their corresponding topology domain. maxSkew &lt;integer&gt; -required- MaxSkew describes the degree to which pods may be unevenly distributed. required field. Default value is 1 and 0 is not allowed. topologyKey &lt;string&gt; -required- TopologyKey is the key of node labels. Nodes that have a label with this key and identical values are considered to be in the same topology. We consider each &lt;key, value&gt; as a &quot;bucket&quot;, and try to put balanced number of pods into each bucket. whenUnsatisfiable &lt;string&gt; -required- WhenUnsatisfiable indicates how to deal with a pod if it doesn't satisfy Possible enum values: - `&quot;DoNotSchedule&quot;` instructs the scheduler not to schedule the pod when constraints are not satisfied. - `&quot;ScheduleAnyway&quot;` instructs the scheduler to schedule the pod even if constraints are not satisfied.  maxSkew 則是用來控制 skew 的上限值，若 Pod 部署到該節點後會使得 Skew 超過此限制，則該節點就會被跳過topologyKey 如同 Inter-Pod 的設定，用來判定如何分類節點whenUnsatisfiable 則是用來設定當 maxSkew 找不到任何符合規則節點時該怎處理，可以卡在 Pending 狀態或是就忽略當前設定按照其他設定部署labelSelector 則是用來選擇要把哪些 Pod 的數量納入考慮 因此使用上很常透過下列方式讓 Pod 可以均勻地散落到不同的 Zone 內 spec: containers: - name: www-server image: hwchiu/netutils topologySpreadConstraints: - maxSkew: 1 topologyKey: kind.zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: pod-ts-1  如果要細部調整現在還有下列參數可以使用，使用前請參閱官方文件 nodeAffinityPolicy: [Honor|Ignore]nodeTaintsPolicy: [Honor|Ignore]matchLabelKeys: listminDomains: integer Summary 本系列文探討如何透過 Kubernetes 內建的方式來影響 Scheduler 調度的決策，這類型的設定能夠於 Zone/Region 等架構下達到更好的高可用性設定，此外若 Pod 之間有強烈依賴且希望網路延遲盡可能低的，也可以考慮用這類型的設定來處理。 使用上請務必確保理解每個欄位的意思，特別是當參數本身為 List 型態的時候，要確認一下其結果是基於 OR 或是 AND 的運算，以免發生結果與預期不符導致花費太多時間除錯。 ","version":"Next","tagName":"h2"},{"title":"Spark SQL, ThriftServer, GCS in Kubernetes.","type":0,"sectionRef":"#","url":"/docs/techPost/2023/spark-gcs-k8s","content":"","keywords":"Kubernetes Network Linux Ubuntu","version":"Next"},{"title":"GCS​","type":1,"pageTitle":"Spark SQL, ThriftServer, GCS in Kubernetes.","url":"/docs/techPost/2023/spark-gcs-k8s#gcs","content":"為了 Spark 可以有權限存取 GCS 上的資料，我們必須要創建一組 Service Account 並且給予對應的權限 此外由於 Spark 目前並不支援 Workload Identity 的方式，因此該 Service Account 必須要創建一組基於 Json 格式的 credential.json，然後將該檔案掛載到 Kubernetes 內讓 Spark Executor 可以透過其來與 GCS 互動。 流程為 創建一個 Service Account賦予該 Service Account 一個 IAM Role 來讀寫 GCS產生一個對應的 Credential Key將該 Credential 給寫入到 Kubernetes 內 流程對應指令如下 $ gcloud iam service-accounts --project my-project create spark-example --description=&quot;For Spark To Access GCS&quot; --display-name=spark-example $ gcloud projects add-iam-policy-binding my-project --member=&quot;serviceAccount:spark-example@my-project.iam.gserviceaccount.com&quot; --role=&quot;roles/storage.admin&quot; $ gcloud iam service-accounts --project my-project keys create spark_test.json --iam-account=spark-example@my-project.iam.gserviceaccount.com $ kubectl create secret generic gcs-sa --from-file=gcp-credentials.json=spark_test.json  執行完畢後，環境中就會有一跟名為 gcs-sa 的 Kubernetes secret，之後部署 Spark 的時候必須要將該掛載到環境中並且告知使用 gcp-credentials.json。 ","version":"Next","tagName":"h2"},{"title":"Spark​","type":1,"pageTitle":"Spark SQL, ThriftServer, GCS in Kubernetes.","url":"/docs/techPost/2023/spark-gcs-k8s#spark","content":"由於 Spark Server 需要動態創建 Spark Executor(Pod) ，因此本身需要 Kubernetes 的 Service Account 來獲得權限 以下 YAML 基於 RBAC 的規則準備好相關權限，並且賦予到名為 &quot;spark&quot; 的 Service Account 上 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: spark-server rules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;, &quot;persistentvolumeclaims&quot;, &quot;configmaps&quot;, &quot;services&quot;] verbs: [&quot;get&quot;, &quot;deletecollection&quot;, &quot;create&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;delete&quot;] --- apiVersion: v1 kind: ServiceAccount metadata: name: spark --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: spark-rolebinding subjects: - kind: ServiceAccount name: spark namespace: dev roleRef: kind: Role name: spark-server apiGroup: rbac.authorization.k8s.io  一切準備就緒後，接下來就是準備一個 statefulset 來部署 Spark Thrift Server 首先， Spark 的官方 Image 中目前沒有包含可以跟 GCS 溝通用的 Connector，因此我們必需要額外安裝 Google Cloud Storage Connector for Spark 到環境中 這邊有兩個做法 重新建置 Spark Image，將相關檔案直接包含到 Image 內採用 Init Container 的方式下載該檔案，並且透過 Volume 的方式共享給主要的 Spark Container，並且放到 /opt/spark/jars 的資料夾內讓 Thrift Server 啟動時可以一併初始化 這邊採取 (2) 的方式來示範抓取 hadoop3-2.2.16-shared.jar initContainers: - name: download-file image: busybox command: [&quot;sh&quot;, &quot;-c&quot;, &quot;if [ ! -e /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar ]; then wget -O /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.16/gcs-connector-hadoop3-2.2.16-shaded.jar; fi&quot;] volumeMounts: - name: data-volume mountPath: /tmp volumes: - name: data-volume emptyDir: {} containers: - name: thrift-server image: apache/spark:3.4.0 volumeMounts: - name: data-volume mountPath: /app/data command: - 'bash' - '-c' - &gt;- cp /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar /opt/spark/jars/ &amp;&amp; /opt/spark/sbin/start-thriftserver.sh --jars /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar --packages com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.16,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0  此做法的缺點就是效率比較低，但是彈性高，如果有版本需求更動時只需要改動 YAML 即可，不需要每次重新建置 Container Image。 接下來還要把前述提到的 Service Account 以及 GCS 的相關權限也帶入到環境中 serviceAccountName: spark volumes: - secret: secretName: gcs-sa name: gcs-sa containers: - name: thrift-server image: apache/spark:3.4.0 volumeMounts: - name: gcs-sa mountPath: /etc/secrets readOnly: true command: --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.google.cloud.auth.service.account.enable=true --conf spark.hadoop.fs.gs.project.id=my-project --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/gcp-credentials.json  再來處理 hive server 相關的設定，本範例部署採用的是本地 hive metastore 的部署，因此會使用 Kubernetes PVC 來存放 hive 的資料。 volumeClaimTemplates: - metadata: name: spark-data spec: accessModes: [ &quot;ReadWriteOnce&quot; ] resources: requests: storage: 100Gi containers: - name: thrift-server image: apache/spark:3.4.0 volumeMounts: - name: gcs-sa mountPath: /etc/secrets readOnly: true - name: spark-data mountPath: /opt/spark/work-dir command: --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.server2.thrift.bind.port=0.0.0.0 --conf spark.kubernetes.driver.ownPersistentVolumeClaim=true --conf spark.kubernetes.driver.reusePersistentVolumeClaim=true  最後就是微調一些跟 Kubernetes 有關的參數，詳細設定都可以參閱 Running Spark on Kubernetes apiVersion: v1 kind: Service metadata: name: spark-thrift-service spec: clusterIP: None selector: app: spark-thrift-server ports: - name: thrift-server-port protocol: TCP port: 10000 targetPort: 10000 - protocol: TCP name: spark-driver-port port: 7078 targetPort: 7078 - protocol: TCP name: spark-ui-port port: 4040 targetPort: 4040 --- containers: - name: thrift-server image: apache/spark:3.4.0 command: --master k8s://https://kubernetes.default.svc.cluster.local:443 --conf spark.dynamicAllocation.enabled=true --conf spark.kubernetes.container.image=apache/spark:v3.4.0 --conf spark.kubernetes.driver.pod.name=spark-thrift-server-0 --conf spark.kubernetes.executor.request.cores=&quot;500m&quot; --conf spark.kubernetes.executor.request.memory=&quot;1g&quot; --conf spark.kubernetes.executor.secrets.gcs-sa=/etc/secrets --conf spark.kubernetes.namespace=dev --conf spark.driver.host=spark-thrift-service --conf spark.driver.bindAddress=spark-thrift-server-0 --conf spark.driver.port=7078 &amp;&amp; tail -f /opt/spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-spark-thrift-server-0.out  上述的設定包含 告知 Kubernetes API Server 的位置Executor 會將 gcs-sa secret 給掛載到環境中的 /etc/secrets 內準備一個 Service 來提供網路服務供未來其他應用程式存取使用 dev namespacethrift-server 預設會把 log 寫出來到檔案，因此透過 tail 的方式轉出來 最後全部整理起來可以得到下列的 YAML 檔案 apiVersion: v1 kind: Service metadata: name: spark-thrift-service spec: clusterIP: None selector: app: spark-thrift-server ports: - name: thrift-server-port protocol: TCP port: 10000 targetPort: 10000 - protocol: TCP name: spark-driver-port port: 7078 targetPort: 7078 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: spark-thrift-server spec: serviceName: spark-thrift-service replicas: 1 selector: matchLabels: app: spark-thrift-server volumeClaimTemplates: - metadata: name: spark-data spec: accessModes: [ &quot;ReadWriteOnce&quot; ] resources: requests: storage: 100Gi template: metadata: labels: app: spark-thrift-server spec: serviceAccountName: spark initContainers: - name: download-file image: busybox command: [&quot;sh&quot;, &quot;-c&quot;, &quot;if [ ! -e /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar ]; then wget -O /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.16/gcs-connector-hadoop3-2.2.16-shaded.jar; fi&quot;] volumeMounts: - name: data-volume mountPath: /tmp volumes: - secret: secretName: gcs-sa name: gcs-sa - name: data-volume emptyDir: {} containers: - name: thrift-server image: apache/spark:3.4.0 volumeMounts: - name: gcs-sa mountPath: /etc/secrets readOnly: true - name: data-volume mountPath: /app/data - name: spark-data mountPath: /opt/spark/work-dir command: - 'bash' - '-c' - &gt;- cp /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar /opt/spark/jars/ &amp;&amp; /opt/spark/sbin/start-thriftserver.sh --master k8s://https://kubernetes.default.svc.cluster.local:443 --jars /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar --packages com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.16,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0 --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.server2.thrift.bind.port=0.0.0.0 --conf spark.dynamicAllocation.enabled=true --conf spark.kubernetes.container.image=apache/spark:v3.4.0 --conf spark.kubernetes.driver.pod.name=spark-thrift-server-0 --conf spark.kubernetes.driver.ownPersistentVolumeClaim=true --conf spark.kubernetes.driver.reusePersistentVolumeClaim=true --conf spark.kubernetes.executor.request.cores=&quot;500m&quot; --conf spark.kubernetes.executor.request.memory=&quot;1g&quot; --conf spark.kubernetes.executor.secrets.gcs-sa=/etc/secrets --conf spark.kubernetes.namespace=dev --conf spark.driver.host=spark-thrift-service --conf spark.driver.bindAddress=spark-thrift-server-0 --conf spark.driver.port=7078 --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.google.cloud.auth.service.account.enable=true --conf spark.hadoop.fs.gs.project.id=my-project --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/gcp-credentials.json &amp;&amp; tail -f /opt/spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-spark-thrift-server-0.out  將上述檔案部署到環境內後，可以觀察部署的情況 $ kubectl -n dev logs -f spark-thrift-server-0 ... 23/09/23 01:58:13 INFO AbstractService: Service:ThriftBinaryCLIService is started. 23/09/23 01:58:13 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads 23/09/23 01:58:13 INFO AbstractService: Service:HiveServer2 is started. 23/09/23 01:58:13 INFO HiveThriftServer2: HiveThriftServer2 started ...  出現 HiveThriftServe2 就代表一切部署完畢，接下來就來進行環境驗證 Verify 為了驗證存取，我們先創建一個 GCS 來存放資料 gcloud storage buckets create gs://hungwei_spark_test  由於接下來要透過 Spark SQL 的語法操作，所以資料本身必須包含 database/tablescheme data kc port-forward -n dev --address 0.0.0.0 svc/spark-thrift-service 10000:10000 因此 GCS 上的資料存取可以分成兩種類型 創建全新的 table 與 data存取已經存在的資料 以下使用 beeline CLI 來示範兩種情境 先透過 kubectl port-forward 創立 Tunnel 並且來存取 $ kubectl port-forward -n dev --address 0.0.0.0 svc/spark-thrift-service 10000:10000  接者透過 beeline 來連線(也可以使用 dbeaver) $ ./beeline -u jdbc:hive2://localhost:10000  此外由於前述安裝時有特別設定 spark.dynamicAllocation.enabled=true，因此每次執行都會動態產生 Pod 來運行，所以第一次執行指令都會比較花費時間 ","version":"Next","tagName":"h2"},{"title":"Case 1​","type":1,"pageTitle":"Spark SQL, ThriftServer, GCS in Kubernetes.","url":"/docs/techPost/2023/spark-gcs-k8s#case-1","content":"接下來的示範流程為 使用 gcs 內的資料夾來創建一個 database，並且設定相關 sceheme寫入資料讀取資料 其中創建 table 的時候要特別加上 OPTIONS (path 'gs://$bucket_name/$folder') 來使用 0: jdbc:hive2://localhost:10000&gt; CREATE TABLE test (id int, name string) OPTIONS (path 'gs://hungwei_spark_test/case1'); 0: jdbc:hive2://localhost:10000&gt; INSERT INTO TABLE test VALUES (1234, 'test'); 0: jdbc:hive2://localhost:10000&gt; INSERT INTO TABLE test VALUES (2345, 'test2'); 0: jdbc:hive2://localhost:10000&gt; INSERT INTO TABLE test VALUES (1234, 'test3'); 0: jdbc:hive2://localhost:10000&gt; INSERT INTO TABLE test VALUES (1234, 'test3'); 0: jdbc:hive2://localhost:10000&gt; INSERT INTO TABLE test VALUES (5678, 'test3'); 0: jdbc:hive2://localhost:10000&gt; select * from test where name=&quot;test3&quot;; +-------+--------+ | id | name | +-------+--------+ | 5678 | test3 | | 1234 | test3 | | 1234 | test3 | +-------+--------+ 3 rows selected (3.415 seconds)  上述指令的執行過程可以觀察到相關的 Pod 被創立 thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7 1/1 Running 0 1s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6 0/1 Completed 0 5m28s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6 0/1 Terminating 0 5m28s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6 0/1 Terminating 0 5m30s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6 0/1 Terminating 0 5m30s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6 0/1 Terminating 0 5m30s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7 0/1 Completed 0 65s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7 0/1 Terminating 0 65s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7 0/1 Terminating 0 67s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7 0/1 Terminating 0 67s thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7 0/1 Terminating 0 67s  觀察 GCS 內的狀態，可以看到有些許檔案被創建來描述資料 $ gsutil list gs://hungwei_spark_test/case1 gs://hungwei_spark_test/case1/ gs://hungwei_spark_test/case1/part-00000-26846a09-e18e-4d38-8b2a-c7c005e2c7e8-c000 gs://hungwei_spark_test/case1/part-00000-29411642-fd0b-4c1c-bcad-fe6f77adef53-c000 gs://hungwei_spark_test/case1/part-00000-34bc8daf-3944-4156-8ff9-f8d2839f6a9f-c000 gs://hungwei_spark_test/case1/part-00000-7ca852d0-97e1-4c06-a3e1-e329c1bfebde-c000 gs://hungwei_spark_test/case1/part-00000-8a105d8b-f314-4763-ba5e-d82ff00506bf-c000  透過相同的 port-forward 方式去存取 4040 port，就可觀察到 Spark UI  ","version":"Next","tagName":"h2"},{"title":"Case 2​","type":1,"pageTitle":"Spark SQL, ThriftServer, GCS in Kubernetes.","url":"/docs/techPost/2023/spark-gcs-k8s#case-2","content":"第二個範例我們要存取一個事先存在的資料，因此會先透過 gsutil 的工具將資料上傳到 bucket 內的 case2 資料夾，總共有三個檔案 每個檔案的內容都是一樣的格式，為 int, string 的格式，範例如下 ... 287, 341feba68a7c515288ba 288, 7db6394632c5ee7c2bc3 289, 5c2fb14de85ac40013bc 290, 831596bd2c3051aa128e 291, 673c62da4b7b0eee8efd 292, 46107b341ed115c03ac2 293, 140fde027a05d316fa95 294, ba0760ff44610f797ad0 ...  先透過 gsutil 上傳到 case2 資料夾內 $ gsutil cp data* gs://hungwei_spark_test/case2/ Copying file://data1 [Content-Type=application/octet-stream]... Copying file://data2 [Content-Type=application/octet-stream]... Copying file://data3 [Content-Type=application/octet-stream]... | [3 files][ 2.9 MiB/ 2.9 MiB] Operation completed over 3 objects/2.9 MiB. $ gsutil ls gs://hungwei_spark_test/case2/ gs://hungwei_spark_test/case2/data1 gs://hungwei_spark_test/case2/data2 gs://hungwei_spark_test/case2/data3  檔案準備就緒後就回到 beeline CLI 的介面，這時候要透過 EXTERNAL TABLE 搭配其他變數來描述該檔案格式，範例如下 0: jdbc:hive2://localhost:10000&gt; CREATE EXTERNAL TABLE case2 (id int, name string) row format delimited fields terminated by ',' stored as textfile OPTIONS (path 'gs://hungwei_spark_test/case2'); 0: jdbc:hive2://localhost:10000&gt; select * from case2; .... | 111291 | 3b8b2b1eca0561d4ab62 | | 111292 | 01a20fc8e8f91984e447 | | 111293 | ecf8d25c0ed6f8576f96 | | 111294 | 558f78477c1b2151f6e9 | | 111295 | b5ae29bda237add37650 | | 111296 | ea2caeabbf3559a6cdea | | 111297 | 0d56273274b4012f690f | | 111298 | 20a25f019018272013fd | +-------+------------------------+ | id | name | +-------+------------------------+ | 111299 | dead09f62d571453339e | | 111300 | 3ab89d041368f1717543 | +-------+------------------------+ 106,902 rows selected (51.873 seconds) 0: jdbc:hive2://localhost:10000&gt; select count(*) from case2; +-----------+ | count(1) | +-----------+ | 106902 | +-----------+ 1 row selected (42.869 seconds)  Summary Spark 原生不支持 GCS，需要安裝相關 ConnectorSpark 不支援 Workload Identity，需要創建 Service Account，設定好權限並且創建相關的 KeySpark 於 Kubernetes 上有眾多參數可以微調 ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}