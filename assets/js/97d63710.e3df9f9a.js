"use strict";(self.webpackChunkhwchiu=self.webpackChunkhwchiu||[]).push([[94322],{3905:(e,t,r)=>{r.d(t,{Zo:()=>i,kt:()=>m});var a=r(67294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function s(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?s(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):s(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function c(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},s=Object.keys(e);for(a=0;a<s.length;a++)r=s[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)r=s[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var p=a.createContext({}),l=function(e){var t=a.useContext(p),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},i=function(e){var t=l(e.components);return a.createElement(p.Provider,{value:t},e.children)},d="mdxType",k={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,s=e.originalType,p=e.parentName,i=c(e,["components","mdxType","originalType","parentName"]),d=l(r),u=n,m=d["".concat(p,".").concat(u)]||d[u]||k[u]||s;return r?a.createElement(m,o(o({ref:t},i),{},{components:r})):a.createElement(m,o({ref:t},i))}));function m(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var s=r.length,o=new Array(s);o[0]=u;var c={};for(var p in t)hasOwnProperty.call(t,p)&&(c[p]=t[p]);c.originalType=e,c[d]="string"==typeof e?e:n,o[1]=c;for(var l=2;l<s;l++)o[l]=r[l];return a.createElement.apply(null,o)}return a.createElement.apply(null,r)}u.displayName="MDXCreateElement"},84021:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>k,frontMatter:()=>s,metadata:()=>c,toc:()=>l});var a=r(87462),n=(r(67294),r(3905));const s={title:"Spark SQL, ThriftServer, GCS in Kubernetes.",keywords:["Kubernetes","Network","Linux","Ubuntu"],date:new Date("2023-09-23T22:04:29.000Z"),tags:["Kubernetes","Spark","GCP"],description:"\u7d00\u9304\u5982\u4f55\u65bc K8s \u4e0a\u5b89\u88dd Spark SQL/ThriftServer \u4e26\u4e14\u64cd\u4f5c GCS \u4e0a\u7684\u8cc7\u6599",image:"https://hackmd.io/_uploads/Bk5TEXjJ6.png"},o=void 0,c={unversionedId:"2023/spark-kubernetes",id:"2023/spark-kubernetes",title:"Spark SQL, ThriftServer, GCS in Kubernetes.",description:"\u7d00\u9304\u5982\u4f55\u65bc K8s \u4e0a\u5b89\u88dd Spark SQL/ThriftServer \u4e26\u4e14\u64cd\u4f5c GCS \u4e0a\u7684\u8cc7\u6599",source:"@site/docs/2023/spark-kubernetes.md",sourceDirName:"2023",slug:"/2023/spark-kubernetes",permalink:"/docs/2023/spark-kubernetes",draft:!1,tags:[{label:"Kubernetes",permalink:"/docs/tags/kubernetes"},{label:"Spark",permalink:"/docs/tags/spark"},{label:"GCP",permalink:"/docs/tags/gcp"}],version:"current",lastUpdatedBy:"HungWei Chiu",frontMatter:{title:"Spark SQL, ThriftServer, GCS in Kubernetes.",keywords:["Kubernetes","Network","Linux","Ubuntu"],date:"2023-09-23T22:04:29.000Z",tags:["Kubernetes","Spark","GCP"],description:"\u7d00\u9304\u5982\u4f55\u65bc K8s \u4e0a\u5b89\u88dd Spark SQL/ThriftServer \u4e26\u4e14\u64cd\u4f5c GCS \u4e0a\u7684\u8cc7\u6599",image:"https://hackmd.io/_uploads/Bk5TEXjJ6.png"},sidebar:"techPost",previous:{title:"ruamel.yaml \u5c0f\u7b46\u8a18",permalink:"/docs/2023/python-yaml"},next:{title:"2022",permalink:"/docs/category/2022"}},p={},l=[{value:"GCS",id:"gcs",level:2},{value:"Spark",id:"spark",level:2},{value:"Case 1",id:"case-1",level:2},{value:"Case 2",id:"case-2",level:2}],i={toc:l},d="wrapper";function k(e){let{components:t,...s}=e;return(0,n.kt)(d,(0,a.Z)({},i,s,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"\u672c\u7bc7\u6587\u7ae0\u8a18\u9304\u5982\u4f55\u65bc Kubernetes \u5b89\u88dd Spark SQL \u8207 Spark Thrift Server\uff0c\u4e26\u4e14\u4f7f\u7528 GCS \u4f5c\u70ba Spark SQL \u4f86\u6e90\uff0c\u6700\u5f8c\u900f\u904e JDBC \u76f8\u5bb9\u7684\u61c9\u7528\u7a0b\u5f0f\u8207\u4e4b\u6e9d\u901a\u4e4b\u4f86\u904b\u884c Spark SQL."),(0,n.kt)("p",null,"\u6587\u7ae0\u4e26\u4e0d\u6703\u91dd\u5c0d Spark SQL, Thrift Server \u7b49\u6709\u904e\u591a\u7422\u78e8\uff0c\u55ae\u7d14\u7d00\u9304\u4f7f\u7528\u4e4b\u6307\u4ee4\u8207\u6a94\u6848\u3002"),(0,n.kt)("h1",{id:"\u67b6\u69cb"},"\u67b6\u69cb"),(0,n.kt)("p",null,"\u672c\u6587\u7684\u76ee\u7684\u67b6\u69cb\u5982\u4e0b\uff0c"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\u4ee5 GKE \u70ba\u57fa\u790e\u7576\u4f5c Kubernetes \u5e73\u53f0"),(0,n.kt)("li",{parentName:"ol"},"\u65bc GKE \u4e2d\u90e8\u7f72 Spark Thrift Server"),(0,n.kt)("li",{parentName:"ol"},"\u6b32\u67e5\u8a62\u4e4b\u8cc7\u6599\u5b58\u653e\u65bc GCS \u4e2d"),(0,n.kt)("li",{parentName:"ol"},"\u900f\u904e Beeline CLI \u6216\u5176\u4ed6 JDBC \u76f8\u5bb9\u4e4b\u5de5\u5177\u8207 Spark Thrift Server \u6e9d\u901a\u4e26\u4e14\u57f7\u884c SQL \u76f8\u95dc\u8a9e\u6cd5\u3002"),(0,n.kt)("li",{parentName:"ol"},"Spark Thrift \u6536\u5230\u8acb\u6c42\u7522\u751f\u5c0d\u61c9\u7684 Spark Executor\uff0c\u4e26\u5f9e GCS \u4e2d\u8b80\u53d6\u76f8\u95dc\u8cc7\u6599\u4e26\u56de\u61c9\u3002")),(0,n.kt)("p",null,"\u6574\u500b\u67b6\u69cb\u5982\u4e0b\u5716\u6240\u8ff0"),(0,n.kt)("p",null,(0,n.kt)("img",{src:r(69722).Z,width:"2403",height:"1365"})),(0,n.kt)("p",null,"\u63a5\u4e0b\u4f86\u5c31\u91dd\u5c0d\u5716\u4e2d\u6bcf\u500b\u5143\u4ef6\u8a18\u9304\u4e00\u4e0b\u5be6\u969b\u4e0a\u7684\u8a2d\u5b9a"),(0,n.kt)("h2",{id:"gcs"},"GCS"),(0,n.kt)("p",null,"\u70ba\u4e86 Spark \u53ef\u4ee5\u6709\u6b0a\u9650\u5b58\u53d6 GCS \u4e0a\u7684\u8cc7\u6599\uff0c\u6211\u5011\u5fc5\u9808\u8981\u5275\u5efa\u4e00\u7d44 Service Account \u4e26\u4e14\u7d66\u4e88\u5c0d\u61c9\u7684\u6b0a\u9650"),(0,n.kt)("p",null,"\u6b64\u5916\u7531\u65bc Spark \u76ee\u524d\u4e26\u4e0d\u652f\u63f4 Workload Identity \u7684\u65b9\u5f0f\uff0c\u56e0\u6b64\u8a72 Service Account \u5fc5\u9808\u8981\u5275\u5efa\u4e00\u7d44\u57fa\u65bc Json \u683c\u5f0f\u7684 credential.json\uff0c\u7136\u5f8c\u5c07\u8a72\u6a94\u6848\u639b\u8f09\u5230 Kubernetes \u5167\u8b93 Spark Executor \u53ef\u4ee5\u900f\u904e\u5176\u4f86\u8207 GCS \u4e92\u52d5\u3002"),(0,n.kt)("p",null,"\u6d41\u7a0b\u70ba"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\u5275\u5efa\u4e00\u500b Service Account"),(0,n.kt)("li",{parentName:"ol"},"\u8ce6\u4e88\u8a72 Service Account \u4e00\u500b IAM Role \u4f86\u8b80\u5beb GCS"),(0,n.kt)("li",{parentName:"ol"},"\u7522\u751f\u4e00\u500b\u5c0d\u61c9\u7684 Credential Key"),(0,n.kt)("li",{parentName:"ol"},"\u5c07\u8a72 Credential \u7d66\u5beb\u5165\u5230 Kubernetes \u5167")),(0,n.kt)("p",null,"\u6d41\u7a0b\u5c0d\u61c9\u6307\u4ee4\u5982\u4e0b"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash="},'$ gcloud iam service-accounts --project my-project create spark-example --description="For Spark To Access GCS" --display-name=spark-example\n$ gcloud projects add-iam-policy-binding my-project --member="serviceAccount:spark-example@my-project.iam.gserviceaccount.com" --role="roles/storage.admin"\n$ gcloud iam service-accounts --project my-project keys create spark_test.json --iam-account=spark-example@my-project.iam.gserviceaccount.com\n$ kubectl create secret generic gcs-sa --from-file=gcp-credentials.json=spark_test.json\n')),(0,n.kt)("p",null,"\u57f7\u884c\u5b8c\u7562\u5f8c\uff0c\u74b0\u5883\u4e2d\u5c31\u6703\u6709\u4e00\u8ddf\u540d\u70ba ",(0,n.kt)("strong",{parentName:"p"},"gcs-sa")," \u7684 Kubernetes secret\uff0c\u4e4b\u5f8c\u90e8\u7f72 Spark \u7684\u6642\u5019\u5fc5\u9808\u8981\u5c07\u8a72\u639b\u8f09\u5230\u74b0\u5883\u4e2d\u4e26\u4e14\u544a\u77e5\u4f7f\u7528 ",(0,n.kt)("strong",{parentName:"p"},"gcp-credentials.json"),"\u3002"),(0,n.kt)("h2",{id:"spark"},"Spark"),(0,n.kt)("p",null,"\u7531\u65bc Spark Server \u9700\u8981\u52d5\u614b\u5275\u5efa Spark Executor(Pod) \uff0c\u56e0\u6b64\u672c\u8eab\u9700\u8981 Kubernetes \u7684 Service Account \u4f86\u7372\u5f97\u6b0a\u9650"),(0,n.kt)("p",null,'\u4ee5\u4e0b YAML \u57fa\u65bc RBAC \u7684\u898f\u5247\u6e96\u5099\u597d\u76f8\u95dc\u6b0a\u9650\uff0c\u4e26\u4e14\u8ce6\u4e88\u5230\u540d\u70ba "spark" \u7684 Service Account \u4e0a'),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: spark-server\nrules:\n- apiGroups: [""]\n  resources: ["pods", "persistentvolumeclaims", "configmaps", "services"]\n  verbs: ["get", "deletecollection", "create", "list", "watch", "delete"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: spark-rolebinding\nsubjects:\n- kind: ServiceAccount\n  name: spark\n  namespace: dev\nroleRef:\n  kind: Role\n  name: spark-server\n  apiGroup: rbac.authorization.k8s.io\n')),(0,n.kt)("p",null,"\u4e00\u5207\u6e96\u5099\u5c31\u7dd2\u5f8c\uff0c\u63a5\u4e0b\u4f86\u5c31\u662f\u6e96\u5099\u4e00\u500b statefulset \u4f86\u90e8\u7f72 Spark Thrift Server\n\u9996\u5148\uff0c Spark \u7684\u5b98\u65b9 Image \u4e2d\u76ee\u524d\u6c92\u6709\u5305\u542b\u53ef\u4ee5\u8ddf GCS \u6e9d\u901a\u7528\u7684 Connector\uff0c\u56e0\u6b64\u6211\u5011\u5fc5\u9700\u8981\u984d\u5916\u5b89\u88dd ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/master/gcs"},"Google Cloud Storage Connector for Spark ")," \u5230\u74b0\u5883\u4e2d"),(0,n.kt)("p",null,"\u9019\u908a\u6709\u5169\u500b\u505a\u6cd5"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\u91cd\u65b0\u5efa\u7f6e Spark Image\uff0c\u5c07\u76f8\u95dc\u6a94\u6848\u76f4\u63a5\u5305\u542b\u5230 Image \u5167"),(0,n.kt)("li",{parentName:"ol"},"\u63a1\u7528 Init Container \u7684\u65b9\u5f0f\u4e0b\u8f09\u8a72\u6a94\u6848\uff0c\u4e26\u4e14\u900f\u904e Volume \u7684\u65b9\u5f0f\u5171\u4eab\u7d66\u4e3b\u8981\u7684 Spark Container\uff0c\u4e26\u4e14\u653e\u5230 /opt/spark/jars \u7684\u8cc7\u6599\u593e\u5167\u8b93 Thrift Server \u555f\u52d5\u6642\u53ef\u4ee5\u4e00\u4f75\u521d\u59cb\u5316")),(0,n.kt)("p",null,"\u9019\u908a\u63a1\u53d6 (2) \u7684\u65b9\u5f0f\u4f86\u793a\u7bc4\u6293\u53d6 ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudDataproc/hadoop-connectors/releases"},"hadoop3-2.2.16-shared.jar")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml="},'initContainers:\n- name: download-file\n  image: busybox\n  command: ["sh", "-c", "if [ ! -e /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar ]; then wget -O /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.16/gcs-connector-hadoop3-2.2.16-shaded.jar; fi"] \n  volumeMounts:\n    - name: data-volume\n      mountPath: /tmp\nvolumes:\n- name: data-volume\n  emptyDir: {}       \ncontainers:\n- name: thrift-server\n  image: apache/spark:3.4.0\n  volumeMounts:\n    - name: data-volume\n      mountPath: /app/data\n  command:\n    - \'bash\'\n    - \'-c\'\n    - >-\n      cp /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar /opt/spark/jars/ &&\n      /opt/spark/sbin/start-thriftserver.sh  \n      --jars /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar\n      --packages com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.16,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0      \n')),(0,n.kt)("p",null,"\u6b64\u505a\u6cd5\u7684\u7f3a\u9ede\u5c31\u662f\u6548\u7387\u6bd4\u8f03\u4f4e\uff0c\u4f46\u662f\u5f48\u6027\u9ad8\uff0c\u5982\u679c\u6709\u7248\u672c\u9700\u6c42\u66f4\u52d5\u6642\u53ea\u9700\u8981\u6539\u52d5 YAML \u5373\u53ef\uff0c\u4e0d\u9700\u8981\u6bcf\u6b21\u91cd\u65b0\u5efa\u7f6e Container Image\u3002"),(0,n.kt)("p",null,"\u63a5\u4e0b\u4f86\u9084\u8981\u628a\u524d\u8ff0\u63d0\u5230\u7684 Service Account \u4ee5\u53ca GCS \u7684\u76f8\u95dc\u6b0a\u9650\u4e5f\u5e36\u5165\u5230\u74b0\u5883\u4e2d"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml="},"serviceAccountName: spark\nvolumes:\n- secret:\n    secretName: gcs-sa  \n  name: gcs-sa\ncontainers:\n- name: thrift-server\n  image: apache/spark:3.4.0\n  volumeMounts:\n    - name: gcs-sa\n      mountPath: /etc/secrets\n      readOnly: true      \n  command:\n      --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n      --conf spark.hadoop.google.cloud.auth.service.account.enable=true\n      --conf spark.hadoop.fs.gs.project.id=my-project\n      --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/gcp-credentials.json      \n")),(0,n.kt)("p",null,"\u518d\u4f86\u8655\u7406 hive server \u76f8\u95dc\u7684\u8a2d\u5b9a\uff0c\u672c\u7bc4\u4f8b\u90e8\u7f72\u63a1\u7528\u7684\u662f\u672c\u5730 hive metastore \u7684\u90e8\u7f72\uff0c\u56e0\u6b64\u6703\u4f7f\u7528 Kubernetes PVC \u4f86\u5b58\u653e hive \u7684\u8cc7\u6599\u3002"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml="},'volumeClaimTemplates:\n- metadata:\n  name: spark-data\n  spec:\n    accessModes: [ "ReadWriteOnce" ]\n    resources:\n      requests:\n        storage: 100Gi\ncontainers:\n- name: thrift-server\n  image: apache/spark:3.4.0\n  volumeMounts:\n    - name: gcs-sa\n      mountPath: /etc/secrets\n      readOnly: true\n    - name: spark-data\n      mountPath: /opt/spark/work-dir      \n  command:\n      --hiveconf hive.server2.thrift.port=10000\n      --hiveconf hive.server2.thrift.bind.port=0.0.0.0\n      --conf spark.kubernetes.driver.ownPersistentVolumeClaim=true\n      --conf spark.kubernetes.driver.reusePersistentVolumeClaim=true      \n\n')),(0,n.kt)("p",null,"\u6700\u5f8c\u5c31\u662f\u5fae\u8abf\u4e00\u4e9b\u8ddf Kubernetes \u6709\u95dc\u7684\u53c3\u6578\uff0c\u8a73\u7d30\u8a2d\u5b9a\u90fd\u53ef\u4ee5\u53c3\u95b1 ",(0,n.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/running-on-kubernetes.html"},"Running Spark on Kubernetes")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml="},'apiVersion: v1\nkind: Service\nmetadata:\n  name: spark-thrift-service\nspec:\n  clusterIP: None\n  selector:\n    app: spark-thrift-server\n  ports:\n    - name: thrift-server-port\n      protocol: TCP\n      port: 10000\n      targetPort: 10000\n    - protocol: TCP\n      name: spark-driver-port\n      port: 7078\n      targetPort: 7078\n    - protocol: TCP\n      name: spark-ui-port\n      port: 4040\n      targetPort: 4040      \n---     \ncontainers:\n- name: thrift-server\n  image: apache/spark:3.4.0\n  command:\n      --master k8s://https://kubernetes.default.svc.cluster.local:443\n      --conf spark.dynamicAllocation.enabled=true\n      --conf spark.kubernetes.container.image=apache/spark:v3.4.0\n      --conf spark.kubernetes.driver.pod.name=spark-thrift-server-0\n      --conf spark.kubernetes.executor.request.cores="500m"\n      --conf spark.kubernetes.executor.request.memory="1g"\n      --conf spark.kubernetes.executor.secrets.gcs-sa=/etc/secrets\n      --conf spark.kubernetes.namespace=dev\n      --conf spark.driver.host=spark-thrift-service\n      --conf spark.driver.bindAddress=spark-thrift-server-0\n      --conf spark.driver.port=7078\n      && tail -f /opt/spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-spark-thrift-server-0.out\n')),(0,n.kt)("p",null,"\u4e0a\u8ff0\u7684\u8a2d\u5b9a\u5305\u542b"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\u544a\u77e5 Kubernetes API Server \u7684\u4f4d\u7f6e"),(0,n.kt)("li",{parentName:"ol"},"Executor \u6703\u5c07 ",(0,n.kt)("strong",{parentName:"li"},"gcs-sa")," secret \u7d66\u639b\u8f09\u5230\u74b0\u5883\u4e2d\u7684 ",(0,n.kt)("strong",{parentName:"li"},"/etc/secrets")," \u5167"),(0,n.kt)("li",{parentName:"ol"},"\u6e96\u5099\u4e00\u500b Service \u4f86\u63d0\u4f9b\u7db2\u8def\u670d\u52d9\u4f9b\u672a\u4f86\u5176\u4ed6\u61c9\u7528\u7a0b\u5f0f\u5b58\u53d6"),(0,n.kt)("li",{parentName:"ol"},"\u4f7f\u7528 dev namespace"),(0,n.kt)("li",{parentName:"ol"},"thrift-server \u9810\u8a2d\u6703\u628a log \u5beb\u51fa\u4f86\u5230\u6a94\u6848\uff0c\u56e0\u6b64\u900f\u904e tail \u7684\u65b9\u5f0f\u8f49\u51fa\u4f86")),(0,n.kt)("p",null,"\u6700\u5f8c\u5168\u90e8\u6574\u7406\u8d77\u4f86\u53ef\u4ee5\u5f97\u5230\u4e0b\u5217\u7684 YAML \u6a94\u6848"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml="},'apiVersion: v1\nkind: Service\nmetadata:\n  name: spark-thrift-service\nspec:\n  clusterIP: None\n  selector:\n    app: spark-thrift-server\n  ports:\n    - name: thrift-server-port\n      protocol: TCP\n      port: 10000\n      targetPort: 10000\n    - protocol: TCP\n      name: spark-driver-port\n      port: 7078\n      targetPort: 7078\n---     \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: spark-thrift-server\nspec:\n  serviceName: spark-thrift-service\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spark-thrift-server\n  volumeClaimTemplates:\n  - metadata:\n      name: spark-data\n    spec:\n      accessModes: [ "ReadWriteOnce" ]\n      resources:\n        requests:\n          storage: 100Gi\n  template:\n    metadata:\n      labels:\n        app: spark-thrift-server\n    spec:\n      serviceAccountName: spark\n      initContainers:\n        - name: download-file\n          image: busybox\n          command: ["sh", "-c", "if [ ! -e /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar ]; then wget -O /tmp/gcs-connector-hadoop3-2.2.16-shaded.jar https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.16/gcs-connector-hadoop3-2.2.16-shaded.jar; fi"]\n          volumeMounts:\n            - name: data-volume\n              mountPath: /tmp\n      volumes:\n        - secret:\n            secretName: gcs-sa\n          name: gcs-sa\n        - name: data-volume\n          emptyDir: {}\n      containers:\n        - name: thrift-server\n          image: apache/spark:3.4.0\n          volumeMounts:\n            - name: gcs-sa\n              mountPath: /etc/secrets\n              readOnly: true\n            - name: data-volume\n              mountPath: /app/data\n            - name: spark-data\n              mountPath: /opt/spark/work-dir\n          command:\n            - \'bash\'\n            - \'-c\'\n            - >-\n              cp /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar /opt/spark/jars/ &&\n              /opt/spark/sbin/start-thriftserver.sh\n              --master k8s://https://kubernetes.default.svc.cluster.local:443\n              --jars /app/data/gcs-connector-hadoop3-2.2.16-shaded.jar\n              --packages com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.16,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0\n              --hiveconf hive.server2.thrift.port=10000\n              --hiveconf hive.server2.thrift.bind.port=0.0.0.0\n              --conf spark.dynamicAllocation.enabled=true\n              --conf spark.kubernetes.container.image=apache/spark:v3.4.0\n              --conf spark.kubernetes.driver.pod.name=spark-thrift-server-0\n              --conf spark.kubernetes.driver.ownPersistentVolumeClaim=true\n              --conf spark.kubernetes.driver.reusePersistentVolumeClaim=true\n              --conf spark.kubernetes.executor.request.cores="500m"\n              --conf spark.kubernetes.executor.request.memory="1g"\n              --conf spark.kubernetes.executor.secrets.gcs-sa=/etc/secrets\n              --conf spark.kubernetes.namespace=dev\n              --conf spark.driver.host=spark-thrift-service\n              --conf spark.driver.bindAddress=spark-thrift-server-0\n              --conf spark.driver.port=7078\n              --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n              --conf spark.hadoop.google.cloud.auth.service.account.enable=true\n              --conf spark.hadoop.fs.gs.project.id=my-project\n              --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/gcp-credentials.json\n              && tail -f /opt/spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-spark-thrift-server-0.out\n')),(0,n.kt)("p",null,"\u5c07\u4e0a\u8ff0\u6a94\u6848\u90e8\u7f72\u5230\u74b0\u5883\u5167\u5f8c\uff0c\u53ef\u4ee5\u89c0\u5bdf\u90e8\u7f72\u7684\u60c5\u6cc1"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash="},"$ kubectl -n dev logs -f spark-thrift-server-0\n...\n23/09/23 01:58:13 INFO AbstractService: Service:ThriftBinaryCLIService is started.\n23/09/23 01:58:13 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads\n23/09/23 01:58:13 INFO AbstractService: Service:HiveServer2 is started.\n23/09/23 01:58:13 INFO HiveThriftServer2: HiveThriftServer2 started\n...\n")),(0,n.kt)("p",null,"\u51fa\u73fe ",(0,n.kt)("strong",{parentName:"p"},"HiveThriftServe2")," \u5c31\u4ee3\u8868\u4e00\u5207\u90e8\u7f72\u5b8c\u7562\uff0c\u63a5\u4e0b\u4f86\u5c31\u4f86\u9032\u884c\u74b0\u5883\u9a57\u8b49"),(0,n.kt)("h1",{id:"verify"},"Verify"),(0,n.kt)("p",null,"\u70ba\u4e86\u9a57\u8b49\u5b58\u53d6\uff0c\u6211\u5011\u5148\u5275\u5efa\u4e00\u500b GCS \u4f86\u5b58\u653e\u8cc7\u6599"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash="},"gcloud storage buckets create gs://hungwei_spark_test\n")),(0,n.kt)("p",null,"\u7531\u65bc\u63a5\u4e0b\u4f86\u8981\u900f\u904e Spark SQL \u7684\u8a9e\u6cd5\u64cd\u4f5c\uff0c\u6240\u4ee5\u8cc7\u6599\u672c\u8eab\u5fc5\u9808\u5305\u542b"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"database/table"),(0,n.kt)("li",{parentName:"ol"},"scheme "),(0,n.kt)("li",{parentName:"ol"},"data kc port-forward -n dev --address 0.0.0.0 svc/spark-thrift-service 10000:10000")),(0,n.kt)("p",null,"\u56e0\u6b64 GCS \u4e0a\u7684\u8cc7\u6599\u5b58\u53d6\u53ef\u4ee5\u5206\u6210\u5169\u7a2e\u985e\u578b"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\u5275\u5efa\u5168\u65b0\u7684 table \u8207 data"),(0,n.kt)("li",{parentName:"ol"},"\u5b58\u53d6\u5df2\u7d93\u5b58\u5728\u7684\u8cc7\u6599")),(0,n.kt)("p",null,"\u4ee5\u4e0b\u4f7f\u7528 beeline CLI \u4f86\u793a\u7bc4\u5169\u7a2e\u60c5\u5883\n\u5148\u900f\u904e kubectl port-forward \u5275\u7acb Tunnel \u4e26\u4e14\u4f86\u5b58\u53d6"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"$ kubectl port-forward -n dev --address 0.0.0.0 svc/spark-thrift-service 10000:10000\n")),(0,n.kt)("p",null,"\u63a5\u8005\u900f\u904e beeline \u4f86\u9023\u7dda(\u4e5f\u53ef\u4ee5\u4f7f\u7528 dbeaver)"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"$ ./beeline -u jdbc:hive2://localhost:10000\n")),(0,n.kt)("p",null,"\u6b64\u5916\u7531\u65bc\u524d\u8ff0\u5b89\u88dd\u6642\u6709\u7279\u5225\u8a2d\u5b9a ",(0,n.kt)("inlineCode",{parentName:"p"},"spark.dynamicAllocation.enabled=true"),"\uff0c\u56e0\u6b64\u6bcf\u6b21\u57f7\u884c\u90fd\u6703\u52d5\u614b\u7522\u751f Pod \u4f86\u904b\u884c\uff0c\u6240\u4ee5\u7b2c\u4e00\u6b21\u57f7\u884c\u6307\u4ee4\u90fd\u6703\u6bd4\u8f03\u82b1\u8cbb\u6642\u9593"),(0,n.kt)("h2",{id:"case-1"},"Case 1"),(0,n.kt)("p",null,"\u63a5\u4e0b\u4f86\u7684\u793a\u7bc4\u6d41\u7a0b\u70ba"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\u4f7f\u7528 gcs \u5167\u7684\u8cc7\u6599\u593e\u4f86\u5275\u5efa\u4e00\u500b database\uff0c\u4e26\u4e14\u8a2d\u5b9a\u76f8\u95dc sceheme"),(0,n.kt)("li",{parentName:"ol"},"\u5beb\u5165\u8cc7\u6599"),(0,n.kt)("li",{parentName:"ol"},"\u8b80\u53d6\u8cc7\u6599")),(0,n.kt)("p",null,"\u5176\u4e2d\u5275\u5efa table \u7684\u6642\u5019\u8981\u7279\u5225\u52a0\u4e0a ",(0,n.kt)("inlineCode",{parentName:"p"},"OPTIONS (path 'gs://$bucket_name/$folder')")," \u4f86\u4f7f\u7528"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash="},"0: jdbc:hive2://localhost:10000> CREATE TABLE test (id int, name string) OPTIONS (path 'gs://hungwei_spark_test/case1');    \n0: jdbc:hive2://localhost:10000> INSERT INTO TABLE test VALUES (1234, 'test');\n0: jdbc:hive2://localhost:10000> INSERT INTO TABLE test VALUES (2345, 'test2');\n0: jdbc:hive2://localhost:10000> INSERT INTO TABLE test VALUES (1234, 'test3');\n0: jdbc:hive2://localhost:10000> INSERT INTO TABLE test VALUES (1234, 'test3');\n0: jdbc:hive2://localhost:10000> INSERT INTO TABLE test VALUES (5678, 'test3');\n0: jdbc:hive2://localhost:10000> select * from test where name=\"test3\";\n+-------+--------+\n|  id   |  name  |\n+-------+--------+\n| 5678  | test3  |\n| 1234  | test3  |\n| 1234  | test3  |\n+-------+--------+\n3 rows selected (3.415 seconds)\n")),(0,n.kt)("p",null,"\u4e0a\u8ff0\u6307\u4ee4\u7684\u57f7\u884c\u904e\u7a0b\u53ef\u4ee5\u89c0\u5bdf\u5230\u76f8\u95dc\u7684 Pod \u88ab\u5275\u7acb"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash="},"thrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7   1/1     Running             0             1s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6   0/1     Completed           0             5m28s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6   0/1     Terminating         0             5m28s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6   0/1     Terminating         0             5m30s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6   0/1     Terminating         0             5m30s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-6   0/1     Terminating         0             5m30s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7   0/1     Completed           0             65s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7   0/1     Terminating         0             65s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7   0/1     Terminating         0             67s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7   0/1     Terminating         0             67s\nthrift-jdbc-odbc-server-7e310a8abfc209d1-exec-7   0/1     Terminating         0             67s\n")),(0,n.kt)("p",null,"\u89c0\u5bdf GCS \u5167\u7684\u72c0\u614b\uff0c\u53ef\u4ee5\u770b\u5230\u6709\u4e9b\u8a31\u6a94\u6848\u88ab\u5275\u5efa\u4f86\u63cf\u8ff0\u8cc7\u6599"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"$ gsutil list gs://hungwei_spark_test/case1\ngs://hungwei_spark_test/case1/\ngs://hungwei_spark_test/case1/part-00000-26846a09-e18e-4d38-8b2a-c7c005e2c7e8-c000\ngs://hungwei_spark_test/case1/part-00000-29411642-fd0b-4c1c-bcad-fe6f77adef53-c000\ngs://hungwei_spark_test/case1/part-00000-34bc8daf-3944-4156-8ff9-f8d2839f6a9f-c000\ngs://hungwei_spark_test/case1/part-00000-7ca852d0-97e1-4c06-a3e1-e329c1bfebde-c000\ngs://hungwei_spark_test/case1/part-00000-8a105d8b-f314-4763-ba5e-d82ff00506bf-c000\n")),(0,n.kt)("p",null,"\u900f\u904e\u76f8\u540c\u7684 port-forward \u65b9\u5f0f\u53bb\u5b58\u53d6 4040 port\uff0c\u5c31\u53ef\u89c0\u5bdf\u5230 Spark UI"),(0,n.kt)("p",null,(0,n.kt)("img",{src:r(85949).Z,width:"1913",height:"1038"})),(0,n.kt)("h2",{id:"case-2"},"Case 2"),(0,n.kt)("p",null,"\u7b2c\u4e8c\u500b\u7bc4\u4f8b\u6211\u5011\u8981\u5b58\u53d6\u4e00\u500b\u4e8b\u5148\u5b58\u5728\u7684\u8cc7\u6599\uff0c\u56e0\u6b64\u6703\u5148\u900f\u904e gsutil \u7684\u5de5\u5177\u5c07\u8cc7\u6599\u4e0a\u50b3\u5230 bucket \u5167\u7684 case2 \u8cc7\u6599\u593e\uff0c\u7e3d\u5171\u6709\u4e09\u500b\u6a94\u6848\n\u6bcf\u500b\u6a94\u6848\u7684\u5167\u5bb9\u90fd\u662f\u4e00\u6a23\u7684\u683c\u5f0f\uff0c\u70ba int, string \u7684\u683c\u5f0f\uff0c\u7bc4\u4f8b\u5982\u4e0b"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"...\n287, 341feba68a7c515288ba\n288, 7db6394632c5ee7c2bc3\n289, 5c2fb14de85ac40013bc\n290, 831596bd2c3051aa128e\n291, 673c62da4b7b0eee8efd\n292, 46107b341ed115c03ac2\n293, 140fde027a05d316fa95\n294, ba0760ff44610f797ad0\n...\n")),(0,n.kt)("p",null,"\u5148\u900f\u904e gsutil \u4e0a\u50b3\u5230 case2 \u8cc7\u6599\u593e\u5167"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"$ gsutil cp data*   gs://hungwei_spark_test/case2/\nCopying file://data1 [Content-Type=application/octet-stream]...\nCopying file://data2 [Content-Type=application/octet-stream]...\nCopying file://data3 [Content-Type=application/octet-stream]...\n| [3 files][  2.9 MiB/  2.9 MiB]\nOperation completed over 3 objects/2.9 MiB.\n\n$ gsutil ls   gs://hungwei_spark_test/case2/\ngs://hungwei_spark_test/case2/data1\ngs://hungwei_spark_test/case2/data2\ngs://hungwei_spark_test/case2/data3\n")),(0,n.kt)("p",null,"\u6a94\u6848\u6e96\u5099\u5c31\u7dd2\u5f8c\u5c31\u56de\u5230 beeline CLI \u7684\u4ecb\u9762\uff0c\u9019\u6642\u5019\u8981\u900f\u904e ",(0,n.kt)("inlineCode",{parentName:"p"},"EXTERNAL TABLE")," \u642d\u914d\u5176\u4ed6\u8b8a\u6578\u4f86\u63cf\u8ff0\u8a72\u6a94\u6848\u683c\u5f0f\uff0c\u7bc4\u4f8b\u5982\u4e0b"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash="},"0: jdbc:hive2://localhost:10000> CREATE EXTERNAL TABLE case2 (id int, name string) row format delimited fields terminated by ',' stored as textfile OPTIONS (path 'gs://hungwei_spark_test/case2');  \n0: jdbc:hive2://localhost:10000> select * from case2;\n....\n| 111291  |  3b8b2b1eca0561d4ab62  |\n| 111292  |  01a20fc8e8f91984e447  |\n| 111293  |  ecf8d25c0ed6f8576f96  |\n| 111294  |  558f78477c1b2151f6e9  |\n| 111295  |  b5ae29bda237add37650  |\n| 111296  |  ea2caeabbf3559a6cdea  |\n| 111297  |  0d56273274b4012f690f  |\n| 111298  |  20a25f019018272013fd  |\n+-------+------------------------+\n|  id   |          name          |\n+-------+------------------------+\n| 111299  |  dead09f62d571453339e  |\n| 111300  |  3ab89d041368f1717543  |\n+-------+------------------------+\n106,902 rows selected (51.873 seconds)\n0: jdbc:hive2://localhost:10000> select count(*) from case2;\n+-----------+\n| count(1)  |\n+-----------+\n| 106902    |\n+-----------+\n1 row selected (42.869 seconds)\n")),(0,n.kt)("h1",{id:"summary"},"Summary"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Spark \u539f\u751f\u4e0d\u652f\u6301 GCS\uff0c\u9700\u8981\u5b89\u88dd\u76f8\u95dc Connector"),(0,n.kt)("li",{parentName:"ol"},"Spark \u4e0d\u652f\u63f4 Workload Identity\uff0c\u9700\u8981\u5275\u5efa Service Account\uff0c\u8a2d\u5b9a\u597d\u6b0a\u9650\u4e26\u4e14\u5275\u5efa\u76f8\u95dc\u7684 Key"),(0,n.kt)("li",{parentName:"ol"},"Spark \u65bc Kubernetes \u4e0a\u6709\u773e\u591a\u53c3\u6578\u53ef\u4ee5\u5fae\u8abf")))}k.isMDXComponent=!0},69722:(e,t,r)=>{r.d(t,{Z:()=>a});const a=r.p+"assets/images/Bk5TEXjJ6-7bbb6fe658b858bdc5553e6f38ffd599.png"},85949:(e,t,r)=>{r.d(t,{Z:()=>a});const a=r.p+"assets/images/rJUnkVnkp-f73c885287605b445dc4a83772eb328e.png"}}]);